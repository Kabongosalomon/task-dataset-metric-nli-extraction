<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning document embeddings along with their uncertainties</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Kesiraju</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oldřich</forename><surname>Plchot</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suryakanth</forename><forename type="middle">V</forename><surname>Gangashetty</surname></persName>
						</author>
						<title level="a" type="main">Learning document embeddings along with their uncertainties</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Bayesian methods</term>
					<term>embeddings</term>
					<term>topic identifica- tion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Majority of the text modelling techniques yield only point-estimates of document embeddings and lack in capturing the uncertainty of the estimates. These uncertainties give a notion of how well the embeddings represent a document. We present Bayesian subspace multinomial model (Bayesian SMM), a generative log-linear model that learns to represent documents in the form of Gaussian distributions, thereby encoding the uncertainty in its covariance. Additionally, in the proposed Bayesian SMM, we address a commonly encountered problem of intractability that appears during variational inference in mixed-logit models. We also present a generative Gaussian linear classifier for topic identification that exploits the uncertainty in document embeddings. Our intrinsic evaluation using perplexity measure shows that the proposed Bayesian SMM fits the data better as compared to the state-of-the-art neural variational document model on (Fisher) speech and (20Newsgroups) text corpora. Our topic identification experiments show that the proposed systems are robust to over-fitting on unseen test data. The topic ID results show that the proposed model is outperforms state-of-the-art unsupervised topic models and achieve comparable results to the state-of-the-art fully supervised discriminative models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning document embeddings along with their uncertainties Santosh Kesiraju, Oldřich Plchot, Lukáš Burget, and Suryakanth V Gangashetty</p><p>Abstract-Majority of the text modelling techniques yield only point-estimates of document embeddings and lack in capturing the uncertainty of the estimates. These uncertainties give a notion of how well the embeddings represent a document. We present Bayesian subspace multinomial model (Bayesian SMM), a generative log-linear model that learns to represent documents in the form of Gaussian distributions, thereby encoding the uncertainty in its covariance. Additionally, in the proposed Bayesian SMM, we address a commonly encountered problem of intractability that appears during variational inference in mixed-logit models. We also present a generative Gaussian linear classifier for topic identification that exploits the uncertainty in document embeddings. Our intrinsic evaluation using perplexity measure shows that the proposed Bayesian SMM fits the data better as compared to the state-of-the-art neural variational document model on (Fisher) speech and (20Newsgroups) text corpora. Our topic identification experiments show that the proposed systems are robust to over-fitting on unseen test data. The topic ID results show that the proposed model is outperforms state-of-the-art unsupervised topic models and achieve comparable results to the state-of-the-art fully supervised discriminative models.</p><p>Index Terms-Bayesian methods, embeddings, topic identification</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>L EARNING word and document embeddings have proven to be useful in wide range of information retrieval, speech and natural language processing applications <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>. These embeddings elicit the latent semantic relations present among the co-occurring words in a sentence or bag-of-words from a document. Majority of the techniques for learning these embeddings are based on two complementary ideologies, (i) topic modelling, and (ii) word prediction. The former methods are primarily built on top of bag-of-words model and tend to capture higher level semantics such as topics. The latter techniques capture lower level semantics by exploiting the contextual information of words in a sequence <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref>.</p><p>On the other hand, there is a growing interest towards developing pre-trained language models <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, that are then finetuned for specific tasks such as document classification, question answering, named entity recognition, etc. Although these models achieve state-of-the-art results in several NLP tasks; they require enormous computational resources to train <ref type="bibr" target="#b10">[11]</ref>.</p><p>Latent variable models <ref type="bibr" target="#b11">[12]</ref> are a popular choice in unsupervised learning; where the observed data is assumed to be S. <ref type="bibr">Kesiraju</ref>  generated through the latent variables according to a stochastic process. The goal is then to estimate the model parameters, and also the latent variables. In probabilistic topic models (PTMs) <ref type="bibr" target="#b12">[13]</ref> the latent variables are attributed to topics, and the generative process assumes that every topic is a sample from a distribution over words in the vocabulary and documents are generated from the distribution of (latent) topics. Recent works showed that auto-encoders can also be seen as generative models for images and text <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Generative models allows us to incorporate prior information about the latent variables, and with the help of variational Bayes (VB) techniques <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, one can infer posterior distribution over the latent variables instead of just point-estimates. The posterior distribution captures uncertainty of the latent variable estimates while trying to explain (fit) the observed data and our prior belief. In the context of text modelling, these latent variables are seen as embeddings.</p><p>In this paper, we present Bayesian subspace multinomial model (Bayesian SMM) as a generative model for bag-ofwords representation of documents. We show that our model can learn to represent each document in the form of a Gaussian distribution, there by encoding the uncertainty in its covariance. Further, we propose a generative Gaussian classifier that exploits this uncertainty for topic identification (ID). The proposed VB framework can be extended in a straightforward way for subspace n-gram model <ref type="bibr" target="#b17">[18]</ref>, that can model n-gram distribution of words in sentences.</p><p>Earlier, (non-Bayesian) SMM was used for learning document embeddings in an unsupervised fashion. They were then used for training linear classifiers for topic ID from spoken and textual documents <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. However, one of the limitations was that the learned document embeddings (also termed as document i-vectors) were only point-estimates and were prone to over-fitting, especially for shorter documents. Our proposed model can overcome this problem by capturing the uncertainty of the embeddings in the form of posterior distributions.</p><p>Given the significant prior research in PTMs and related algorithms for learning representations, it is important to draw precise relations between the presented model and former works. We do this from the following viewpoints: (a) Graphical models illustrating the dependency of random and observed variables, (b) assumptions of distributions over random variables and their limitations, and (c) approximations made during the inference and their consequences.</p><p>The contributions of this paper are as follows: (a) we present Bayesian subspace multinomial model and analyse its relation to popular models such as latent Dirichlet allocation (LDA) <ref type="bibr" target="#b20">[21]</ref>, correlated topic model (CTM) <ref type="bibr" target="#b21">[22]</ref>, paragraph vector (PV-DBOW) <ref type="bibr" target="#b7">[8]</ref> and neural variational document model (NVDM) <ref type="bibr" target="#b14">[15]</ref>, (b) we adapt tricks from <ref type="bibr" target="#b13">[14]</ref> for faster and efficient variational inference of the proposed model, (c) we combine optimization techniques from <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> and use them to train the proposed model, (d) we propose a generative Gaussian classifier that exploits uncertainty in the posterior distribution of document embeddings, (e) we provide experimental results on both text and speech data showing that the proposed document representations achieve state-of-theart perplexity scores, and (f) with our proposed classification systems, we illustrate robustness of the model to over-fitting and at the same time obtain superior classification results when compared systems based on state-of-the-art unsupervised models.</p><p>We begin with the description of Bayesian SMM in Section II, followed by VB for the model in Section III. The complete VB training procedure and algorithm is presented in Section III-A. The procedure for inferring the document embedding posterior distributions for (unseen) documents is described in Section III-B. Section IV presents a generative Gaussian classifier that exploits the uncertainty encoded in document embedding posterior distributions. Relationship between Bayesian SMM and existing popular topic models is described in Section V. Experimental details are given in Section VI, followed by results and analysis in Section VII. Finally, we conclude and discuss directions for future research in Section VIII</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BAYESIAN SUBSPACE MULTINOMIAL MODEL</head><p>Our generative probabilistic model assumes that the training data (bag-of-words) were generated as follows:</p><p>For each document, a K-dimensional latent vector w is generated from isotropic Gaussian prior with mean 0 and precision λ:</p><formula xml:id="formula_0">w ∼ N (w | 0 , diag((λI) −1 ))<label>(1)</label></formula><p>The latent vector w is a low dimensional embedding (K V ) of document-specific distribution of words, where V is the size of the vocabulary. More precisely, for each document, the Vdimensional vector of word probabilities is calculated as:</p><formula xml:id="formula_1">θ = softmax(m + T w),<label>(2)</label></formula><p>where {m, T } are parameters of the model. The vector m known as universal background model represents log unigram probabilities of words. T known as total variability matrix <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> is a low-rank matrix defining subspace of document-specific distributions. Finally, for each document, a vector of word counts x (bagof-words) is sampled from Multinomial distribution:</p><formula xml:id="formula_2">x ∼ Multi(θ; N ),<label>(3)</label></formula><p>where N is the number of words in the document. The above described generative process fully defines our Bayesian model, which we will now use to address the following problems: given training data X, we estimate model parameters {m, T } and, for any given document x, we infer posterior distribution over corresponding document embedding Note that such distribution also encodes the inferred uncertainty about such representation.</p><p>Using Bayes' rule, the posterior distribution of document embedding w is written as 1 :</p><formula xml:id="formula_3">p(w|x) = p(x|w)p(w) p(x|w)p(w) dw .<label>(4)</label></formula><p>In numerator of (4), p(w) represents prior distribution of document embeddings (1) and p(x|w) represents the likelihood of observed data. According to our generative process, we assume that every document x is a sample from Multinomial distribution (3), and the log-likelihood is given as follows:</p><formula xml:id="formula_4">log p(x|w) = V i=1 x i log θ i ,<label>(5)</label></formula><formula xml:id="formula_5">= V i=1 x i log exp{m i + t i w} j exp{m j + t j w} ,<label>(6)</label></formula><formula xml:id="formula_6">= V i=1 x i (m i + t i w)− log   V j=1 exp{m j + t j w}   ,<label>(7)</label></formula><p>where t i represents a row in matrix T . The problem arises while computing the denominator in <ref type="bibr" target="#b3">(4)</ref>. It involves solving the integral over a product of likelihood term containing the softmax function and Gaussian distribution (prior). There exists no analytical form for this integral. This is a generic problem that arises while performing Bayesian inference for mixed-logit models <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b26">[27]</ref>, multi-class logistic regression or any other model where likelihood function and prior are not conjugate to each other <ref type="bibr" target="#b15">[16]</ref>. In such cases, one can resort to variational inference and find an approximation to the posterior distribution p(w|x). This approximation to the true posterior is referred as variational distribution q(w), and is obtained by minimizing the Kullback-Leibler (KL) divergence, D KL (q || p) between the approximate and true posterior. We can express log marginal (evidence) of the data as:</p><formula xml:id="formula_7">log p(x) = E q [log p(x, w)] + H[q] + D KL (q || p),<label>(8)</label></formula><formula xml:id="formula_8">= L(q) + D KL (q || p).<label>(9)</label></formula><p>Here H[q] represents the entropy of q(w). Given the data x, log p(x) is a constant with respect to w, and D KL (q || p) can be minimized by maximizing L(q), which is known as Evidence Lower BOund (ELBO) for a document. This is the standard formulation of variational Bayes <ref type="bibr" target="#b15">[16]</ref>, where the problem of finding an approximate posterior is transformed into optimization of the functional L(q).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. VARIATIONAL BAYES</head><p>In this section, using the VB framework, we derive and explain the procedure for estimating model parameters {m, T } and inferring the variational distribution, q(w). Before proceeding, we note that our model assumes that all documents and the corresponding document embeddings (latent variables) are independent. This can be seen from the graphical model in <ref type="figure" target="#fig_0">Fig. 1</ref>. Hence, we derive the inference only for one document embedding w, given an observed vector of word counts x.</p><p>We chose the variational distribution q(w) to be Gaussian, with mean ν and precision Γ, i.e., q(w) = N (w | ν, Γ −1 ). The functional L(q) now becomes:</p><formula xml:id="formula_9">L(q) = E q [log p(x, w)] + H[q],<label>(10)</label></formula><formula xml:id="formula_10">= − D KL (q || p) A + E q [log p(x | w)] B .<label>(11)</label></formula><p>The term A from (11) is the negative KL divergence between the variational distribution q(w) and the documentindependent prior from (1). This can be computed analytically <ref type="bibr" target="#b27">[28]</ref> as:</p><formula xml:id="formula_11">D KL (q || p) = 1 2 λ tr Γ −1 + log|Γ| − K log λ + λ ν T ν − K ,<label>(12)</label></formula><p>where K denotes the document embedding dimensionality. The term B from (11) is the expectation over log-likelihood of a document <ref type="formula" target="#formula_6">(7)</ref>:</p><formula xml:id="formula_12">E q [log p(x | w)] = V i=1 x i (m i + t i ν) − E q log   V j=1 exp{m j + t j w}   F .<label>(13)</label></formula><p>(13) involves solving the expectation over log-sum-exp operation (denoted by F), which is intractable. It appears when dealing with variational inference in mixed-logit models <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b26">[27]</ref>. We can approximate F with empirical expectation using samples from q(w), but F is a function of q(w), whose parameters we are seeking by optimizing L(q). The corresponding gradients of L(q) with respect to q(w) will exhibit high variance if we directly take samples from q(w) for the empirical expectation. To overcome this, we will reparametrize the random variable w <ref type="bibr" target="#b13">[14]</ref>. This is done by introducing a differentiable function g over another random variable . If p( ) = N (0, I), then,</p><formula xml:id="formula_13">w = g( ) = ν + L ,<label>(14)</label></formula><p>where L is the Cholesky factor of Γ −1</p><p>. Using this reparametrization of w, we obtain the following approximation:</p><formula xml:id="formula_14">F ≈ 1 R R r=1 log   V j=1 exp{m j + t j g(˜ r )}   ,<label>(15)</label></formula><p>where R denotes the total number of samples˜ r from p( ).</p><p>Combining <ref type="formula" target="#formula_0">(12)</ref>, <ref type="bibr" target="#b12">(13)</ref> and <ref type="formula" target="#formula_0">(15)</ref>, we get the approximation to L(q). We will introduce the document suffix d, to make the notation explicit:</p><formula xml:id="formula_15">L(q d ) ≈ −D KL (q d || p) + V i=1 x di (m i + t i ν d ) − 1 R R r=1 log   V j=1 exp{m j + t j g(˜ dr )}   . (16)</formula><p>For the entire training data X, the complete ELBO will be simply the summation over all the documents, i.e., d L(q d ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training</head><p>The variational Bayes (VB) training procedure for Bayesian SMM is stochastic because of the sampling involved in the re-parametrization trick <ref type="bibr" target="#b13">(14)</ref>. Like the standard VB approach <ref type="bibr" target="#b15">[16]</ref>, we optimize ELBO alternately with respect to q(w) and {m, T }. Since we do not have closed form update equations, we perform gradient-based updates. Additionally, we regularize rows in matrix T while optimizing. Thus, the final objective function becomes,</p><formula xml:id="formula_16">L = D d=1 L(q d ) − ω V i=1 ||t i || 1 ,<label>(17)</label></formula><p>where we have added the term for 1 regularization of rows in matrix T , with corresponding weight ω. The same regularization was previously used for non Bayesian SMM in <ref type="bibr" target="#b19">[20]</ref>. This can also be seen as obtaining a maximum a posteriori estimate of T with Laplace priors. 1) Parameter initialization: The vector m is initialized to log uni-gram probabilities estimated from training data. The values in matrix T are randomly initialized from N (0, 0.001). The prior over latent variables p(w) is set to isotropic Gaussian distribution with mean 0 and λ = {1, 10}. The variational distribution q(w) is initialized to N (0, diag(0.1)). Later in Section VII-A, we will show that initializing the posterior to a sharper Gaussian distribution helps to speed up the convergence.</p><p>2) Optimization: The gradient-based updates are done by ADAM optimization scheme <ref type="bibr" target="#b22">[23]</ref>; in addition to the following tricks:</p><p>We simplified the variational distribution q(w) by making its precision matrix Γ diagonal 2 . Further, while updating it, we used log standard deviation parametrization, i.e.,</p><formula xml:id="formula_17">Γ −1 = diag(exp{2 ς}).<label>(18)</label></formula><p>The gradients of the objective (16) w.r.t. the mean ν is given as follows:</p><formula xml:id="formula_18">∇ν = V i=1 t T i (x i − 1 R R r=1 θ ir V k=1 x k ) − λν<label>(19)</label></formula><p>where,</p><formula xml:id="formula_19">θ ir = exp{m i + t j g( r )} j exp{m j + t j g( r )}<label>(20)</label></formula><p>The gradient w.r.t log standard deviation ς is given as:</p><formula xml:id="formula_20">∇ς = 1 − λ exp{2ς} − V k=1 x k 1 R R r=1 V i=1 θ ir t T i exp{ς} r ,<label>(21)</label></formula><p>where 1 represents a column vector of ones, denotes element-wise product, and exp is element-wise exponential operation.</p><p>The 1 regularization term makes the objective function (17) discontinuous (non-differentiable) at points where it crosses the orthant. Hence, we used sub-gradients and employed orthant-wise learning <ref type="bibr" target="#b23">[24]</ref>. The gradient of the objective <ref type="formula" target="#formula_0">(17)</ref> w.r.t. a row t i in matrix T is computed as follows:</p><formula xml:id="formula_21">∇t i = −ω sign(t i ) + D d=1 x di ν T d − V k=1 x ki 1 R R r=1 θ dir (ν T d + T dr exp{ς T }) . (22)</formula><p>Here, sign and exp operate element-wise. The sub-gradient ∇t i is defined as:</p><formula xml:id="formula_22">∇t ik        ∇t ik + ω, t ik = 0, ∇t ik &lt; −ω ∇t ik − ω, t ik = 0, ∇t ik &gt; ω 0, t ik = 0, |∇t ik | ≤ ω ∇t ik , |t ik | &gt; 0 .<label>(23)</label></formula><p>Finally, the rows in matrix T are updated according to,</p><formula xml:id="formula_23">t i ← P O (t i + d i )<label>(24)</label></formula><p>where, d i is the step in ascent direction,</p><formula xml:id="formula_24">d i = η diag( √ s i + ) −1 f i .<label>(25)</label></formula><p>Here, η is the learning rate, f i and s i represents bias corrected first and second moments (as required by ADAM) of sub-gradient∇t i respectively. P O represents orthant projection, which ensures that the update step does not cross the point of non-differentiability. It is defined as,</p><formula xml:id="formula_25">P O (t i + d i ) 0 if t ik (t ik + d ik ) &lt; 0, t ik + d ik otherwise .<label>(26)</label></formula><p>The orthant projection introduces explicit zeros in the estimated T matrix and, results in sparse solution. Unlike in <ref type="bibr" target="#b19">[20]</ref>, we do not require to apply the sign projection, because both the gradient∇t i and step d point to the same orthant (due to properties of ADAM). The stochastic VB training is outlined in Algorithm 1. compute sub-gradients∇t i using <ref type="formula" target="#formula_1">(22)</ref> and <ref type="formula" target="#formula_1">(23)</ref> 12 update rows in T using (24) 13 until convergence or max iterations</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Inferring embeddings for new documents</head><p>After obtaining the model parameters from VB training, we can infer (extract) the posterior distribution of document embedding q(w) for any given document x. This is done by iteratively updating the parameters of q(w) that maximize L(q) from <ref type="bibr" target="#b15">(16)</ref>. These updates are performed by following the same ADAM optimization scheme as in training.</p><p>Note that the embeddings are extracted by maximizing the ELBO, that does not involve any supervision (topic labels). These embeddings which are in the form of posterior distributions will be used as input features for training topic ID classifiers. Alternatively, one can use only the mean of the posterior distributions as point estimates of document embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. GAUSSIAN CLASSIFIER WITH UNCERTAINTY</head><p>In this section, we will present a generative Gaussian classifier that exploits the uncertainty in posterior distributions of document embedding. Moreover, it also exploits the same uncertainty while computing the posterior probability of class labels. The proposed classifier is called Gaussian linear classifier with uncertainty (GLCU) and is inspired by <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. It can be seen as an extension to the simple Gaussian linear classifier (GLC) <ref type="bibr" target="#b15">[16]</ref>. Let = 1 . . . L denote class labels, d = 1 . . . D represent document indices, and h d represent the class label of document d in one-hot encoding.</p><p>GLC assumes that every class is Gaussian distributed with a specific mean µ , and a shared precision matrix D. Let M denote a matrix of class means, with µ representing a column. GLC is described by the following model:</p><formula xml:id="formula_26">w d = µ d + ε d ,<label>(27)</label></formula><p>where </p><formula xml:id="formula_27">µ d = M h d , p(ε) = N (ε | 0, D<label>−</label></formula><formula xml:id="formula_28">D d=1 p(w d | h d , Θ) = D d=1 N (w d | µ d , D −1 ).<label>(28)</label></formula><p>In our case, however, the training examples come in the form of posterior distributions,</p><formula xml:id="formula_29">q(w d ) = N (w d | ν d , Γ −1 d</formula><p>) as extracted using our Bayesian SMM. In such case, the proper ML training procedure should maximize the expected classconditional likelihood, with the expectation over w d calculated for each training example with respect to its posterior distribution</p><formula xml:id="formula_30">q(w d ) i.e., E q [N (w d | µ d , D −1 )].</formula><p>However, it is more convenient to introduce an equivalent model, where the observations are the means ν d of the posteriors q(w d ) and the uncertainty encoded in Γ −1 d is introduced into the model through the latent variable y d as,</p><formula xml:id="formula_31">ν d = µ d + y d + ε d ,<label>(29)</label></formula><formula xml:id="formula_32">where, p(y d ) = N (y d | 0, Γ −1 d ).</formula><p>The resulting model is called GLCU. Since the random variables y d and d are Gaussiandistributed, the resulting class conditional likelihood is obtained using convolution of two Gaussians <ref type="bibr" target="#b15">[16]</ref>, i.e,</p><formula xml:id="formula_33">p(ν d | h d , Θ) = N (ν d | µ d , Γ −1 d + D −1 ).<label>(30)</label></formula><p>GLCU can be trained by estimating its parameters Θ, that maximize the class conditional likelihood of training data <ref type="bibr" target="#b29">(30)</ref>. This can be done efficiently by using the following EM algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. EM algorithm</head><p>In the E-step, we calculate the posterior distribution of latent variables:</p><formula xml:id="formula_34">p(y d | ν d , h d , Θ) ∝ p(ν d | y d , h d Θ) p(y d ) ∝ N (y d | u d , V −1 d ),<label>(31)</label></formula><p>where,</p><formula xml:id="formula_35">V d = D + Γ d ,<label>(32)</label></formula><formula xml:id="formula_36">u d = [I + D −1 Γ d ] −1 (ν d − µ d ).<label>(33)</label></formula><p>In the M-step, we maximize the auxiliary function Q with respect to model parameters Θ. It is the expectation of log joint-probability with respect to p(y d | ν d ), i.e.,</p><formula xml:id="formula_37">Q = E p [ D d=1 log p(ν d , y d | Θ)] (34) = −D 2 log|D|− 1 2 D d=1 tr(DV −1 d ) + (u d − (ν d − µ d )) T D(u d − (ν d − µ d )) + const.<label>(35)</label></formula><p>Maximizing the auxiliary function Q w.r.t. Θ, we have:</p><formula xml:id="formula_38">µ := 1 |I | d∈I (ν d − u d ) ∀ = 1 . . . L<label>(36)</label></formula><formula xml:id="formula_39">D −1 := 1 D D d=1 (a d a T d ) + V −1 d ,<label>(37)</label></formula><p>where, a d = u d − (ν d − µ d ), and, I is the set of documents from class . To train the GLCU model, we alternate between E-step and M-step until convergence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Classification</head><p>Given a test document embedding posterior distribution q(w) = N (w | ν, Γ −1 ), we compute the class conditional likelihood according to <ref type="bibr" target="#b29">(30)</ref>, and the posterior probability of a class C k is obtained by applying the Bayes' rule:</p><formula xml:id="formula_40">p(C k | ν, Γ, Θ) = p(ν | µ k , D, Γ) p(C k ) p(ν | µ , D, Γ) p(C ) .<label>(38)</label></formula><p>V. RELATED MODELS In this section, we review and relate some of the popular PTMs and neural network based document models. We begin with a brief review of LDA <ref type="bibr" target="#b20">[21]</ref>, a probabilistic generative model for bag-of-words representation of documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Latent Dirichlet allocation</head><p>Let φ 1:K represent K topics. LDA assumes that every topic φ k is a distribution over a fixed vocabulary of size V . Every document d is generated by a two step process: First, a document-specific vector (embedding) representing a distribution over K topics is sampled, i.e., θ d ∼ Dir(α). Then, for each word in the document d, a topic indicator variable z i is sampled: z i ∼ Multi(θ d ; 1) and the word x i is in turn sampled from the topic-specific distribution: x i ∼ Multi(φ zi ; 1).</p><p>The topic (φ) and document (θ) vectors live in (V − 1) and (K −1) simplexes respectively. For every word x i in document d, there is a discrete latent variable z i that tells which topic was responsible for generating the word. This can be seen from the respective graphical model in <ref type="figure" target="#fig_2">Fig. 2</ref>.</p><p>During inference, the generative process is inverted to obtain posterior distribution over latent variables, p(θ, z | x, α, φ 1:K ), given the observed data and prior belief. Since the true posterior is intractable, Blei <ref type="bibr" target="#b20">[21]</ref> resorted to variational inference which finds an approximation to the true posterior as a variational distribution q(θ, z). Further, meanfield approximation was made to make the inference tractable, i.e., q(θ, z) = q(θ) i q(z i ).</p><p>In the original model proposed by Blei <ref type="bibr" target="#b20">[21]</ref>, the parameters φ were obtained using maximum likelihood approach. The choice of Dirichlet distribution for q(θ) simplifies the inference process because of the Dirichlet-Multinomial conjugacy. However, the assumption of Dirichlet distribution causes limitations to the model, and q(θ) cannot capture correlations between topics in each document. This was the motivation for Blei <ref type="bibr" target="#b21">[22]</ref> to model documents with Gaussian distributions, and the resulting model is called correlated topic model (CTM). The generative process for a document in CTM <ref type="bibr" target="#b21">[22]</ref> is same as in LDA, except for document vectors are now drawn from Gaussian, i.e.,</p><formula xml:id="formula_41">p(η) = N (η | µ, diag(λ) −1 ),<label>(39)</label></formula><formula xml:id="formula_42">θ = softmax(η).<label>(40)</label></formula><p>In this formulation, the document embeddings η are no longer in the (K − 1) simplex, rather they are dependent through the logistic normal. This is the same as in our proposed Bayesian SMM (1). The advantage is that the document vectors can model the correlations in topics. The topic distributions over vocabulary φ, however, still remained Discrete. In Bayesian SMM, the topic-word distributions (T ) are not Discrete , hence it can model the correlations between words and (latent) topics <ref type="bibr" target="#b21">[22]</ref>. The variational inference in CTM is similar to that of LDA including the mean-field approximation, because of the discrete latent variable z <ref type="figure" target="#fig_3">(Fig. 3</ref>). An additional problem is dealing with the non-conjugacy. More specifically, it is the intractability while solving the expectation over log-sumexp function (see F from <ref type="formula" target="#formula_0">(13)</ref>). Blei <ref type="bibr" target="#b21">[22]</ref> used Jensen's inequality to form an upper bound on F, and this in-turn acted as lower bound on ELBO. In our proposed Bayesian SMM, we also encountered the same problem, and we approximated F using the re-parametrization trick (Section III). There exist similar approximation techniques based on Quasi Monte Carlo sampling <ref type="bibr" target="#b26">[27]</ref>.</p><p>Unlike in LDA or CTM, Bayesian SMM does not require to make mean-field approximation, because the topic-word mixture is not Discrete thus eliminating the need for discrete latent variable z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Subspace multinomial model</head><p>SMM is a log-linear model; originally proposed for modelling discrete prosodic features for the task of speaker verification <ref type="bibr" target="#b24">[25]</ref>. Later, it was used for phonotatic language recognition <ref type="bibr" target="#b30">[31]</ref> and eventually for topic identification and document clustering <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Similar model was proposed by Maas <ref type="bibr" target="#b31">[32]</ref> for unsupervised learning of word representations. One of the major differences among these works is the type of regularization used for matrix T .</p><p>Another major difference is in obtaining embeddings w d for a given test document. Maas <ref type="bibr" target="#b31">[32]</ref> obtained them by projecting the vector of word counts x d onto the matrix T , i.e., w d = T x d , whereas <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> extracted the embeddings by maximizing regularized log-likelihood function. The embeddings extracted using SMM are prone to over-fitting. Our Bayesian SMM overcomes this problem by capturing the uncertainty of document embeddings in the posterior distribution. Our experimental analysis in section VII-C illustrates the robustness of Bayesian SMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Paragraph vector</head><p>Paragraph vector bag-of-words (PV-DBOW) <ref type="bibr" target="#b7">[8]</ref> is also a log-linear model, which is trained stochastically to maximize the likelihood of a set of words from a given document. SMM can be seen as a special case of PV-DBOW, since it maximizes the likelihood of all the words in a document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Neural network based models</head><p>Neural variational document model (NVDM) is an adaptation of variational auto-encoders for document modelling <ref type="bibr" target="#b14">[15]</ref>. The encoder models the posterior distribution of latent variables given the input, i.e., p θ (z | x), and the decoder models distribution of input data given the latent variable, i.e., p θ (x | z). In NVDM, the authors used bag-of-words as input, while their encoder and decoders are two-layer feed-forward neural networks. The decoder part of NVDM is similar to Bayesian SMM, as both the models maximize expected loglikelihood of data, assuming Multinomial distribution. In simple terms, Bayesian SMM is a decoder with a single feed forward layer. For a given test document, in NVDM, the approximate posterior distribution of latent variables is obtained directly by forward propagating through the encoder; whereas in Bayesian SMM, it is obtained by iteratively optimizing ELBO. The experiments in Section VII show that the posterior distributions obtained from Bayesian SMM represent the data better as compared to the ones obtained directly from the encoder of NVDM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Sparsity in topic models</head><p>Sparsity is often one of the desired properties in topic models <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. Sparse coding inspired topic model was proposed by <ref type="bibr" target="#b34">[35]</ref>, where the authors have obtained sparse representations for both documents and words. 1 regularization over T for SMM ( 1 SMM) was observed to yield better results when compared to LDA, STC and 2 regularized SMM ( 2 SMM) <ref type="bibr" target="#b19">[20]</ref>. Relation between SMM and sparse additive generative model (SAGE) <ref type="bibr" target="#b32">[33]</ref> was explained in <ref type="bibr" target="#b18">[19]</ref>. In <ref type="bibr" target="#b35">[36]</ref>, the authors proposed an algorithm to obtain sparse document embeddings (called sparse composite document vector (SCDV)) from pre-trained word embeddings. In our proposed Bayesian SMM, we introduce sparsity into the model parameters T by applying 1 regularization and using orthantwise learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS A. Datasets</head><p>We have conducted experiments on both speech and text corpora. The speech data used is Fisher phase 1 corpus 3 , which is a collection of 5850 conversational telephone speech recordings with a closed set of 40 topics. Each conversation  <ref type="table" target="#tab_2">Topic ID training  2748  244  Topic ID test  2744  226</ref> is approximately 10 minutes long with two sides of the call and is supposedly about one topic. We considered each side of the call (recording) as an independent document, which resulted in a total of 11700 documents. <ref type="table" target="#tab_2">Table I</ref> presents the details of data splits; they are the same as used in earlier research <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>. Our preprocessing involved removing punctuation and special characters, but we did not remove any stop words. Using Kaldi open-source toolkit <ref type="bibr" target="#b38">[39]</ref>, we trained a sequence discriminative DNN-HMM automatic speech recognizer (ASR) system <ref type="bibr" target="#b39">[40]</ref> to obtain automatic transcriptions. The ASR system resulted in 18% word-errorrate on a held-out test set. We report experimental results on both manual and automatic transcriptions. The vocabulary size while using manual transcriptions was 24854, for automatic, it was 18292, and the average document length is 830, and 856 words respectively. The text corpus used is 20Newsgroups 4 , which contains 11314 training and 7532 test documents over 20 topics. Our preprocessing involved removing punctuation and words that do not occur in at least two documents, which resulted in a vocabulary of 56433 words. The average document length is 290 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hyper-parameters of Bayesian SMM</head><p>In our topic ID experiments, we observed that the embedding dimension (K) and regularization weight (ω) for rows in matrix T are the two important hyper-parameters. The embedding dimension was chosen from K = {100, . . . , 800}, and regularization weight from ω = {0.0001, . . . , 10.0}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proposed topic ID systems</head><p>Our Bayesian SMM is an unsupervised model trained iteratively by optimizing the ELBO; it does not necessarily correlate with the performance of topic ID. It is valid for SMM, NVDM or any other generative model trained without supervision. A typical way to overcome this problem is to have an early stopping mechanism (ESM), which requires to evaluate the topic ID accuracy on a held-out (or crossvalidation) set at regular intervals during the training. It can then be used to stop the training earlier if needed.</p><p>Using the above described scheme, we trained three different classifiers: (i) Gaussian linear classifier (GLC), (ii) multi-class logistic regression (LR), and, (iii) Gaussian linear classifier with uncertainty (GLCU). Note that GLC and LR cannot exploit the uncertainty in the document embeddings; and are trained using only the mean parameter ν of the posterior distributions; whereas GLCU is trained using the full posterior distribution q(w), i.e., along with the uncertainties of document embeddings as described in Section IV. GLC and GLCU does not have any hyper-parameters to tune, while the 2 regularization weight of LR was tuned using crossvalidation experiments.</p><p>D. Baseline topic ID systems 1) NVDM: Since NVDM and our proposed Bayesian SMM share similarities, we chose to extract the embeddings from NVDM and use them for training linear classifiers. Given a trained NVDM model, embeddings for any test document can be extracted just by forward propagating through the encoder. Although this is computationally cheaper, one needs to decide when to stop training, as a fully converged NVDM may not yield optimal embeddings for discriminative tasks such as topic ID. Hence, we used the same early stopping mechanism as described in earlier section. We used the same three classifier pipelines (LR, GLC, GLCU) as we used for Bayesian SMM. Our architecture and training scheme are similar to ones proposed in <ref type="bibr" target="#b14">[15]</ref>, i.e., two feed forward layers with either 500 or 1000 hidden units and {sigmoid, ReLU, tanh} activation functions. The latent dimension was chosen from K = {100, . . . , 800}. The hyper-parameters were tuned based on cross-validation experiments.</p><p>2) SMM: Our second baseline system is non-Bayesian SMM with 1 regularization over the rows in T matrix, i.e., 1 SMM. It was trained with hyper-parameters such as embedding dimension K = {100, . . . , 800}, and regularization weight ω = {0.0001, . . . , 10.0}. The embeddings obtained from SMM were then used to train GLC and LR classifiers. Note that we cannot use GLCU here, because SMM yields only point-estimates of embeddings. We used the same early stopping mechanism to train the classifiers. The experimental analysis in Section VII-C shows that Bayesian SMM is more robust to over-fitting when compared to SMM and NVDM, and does not require an early stopping mechanism.</p><p>3) ULMFiT: The third baseline system is the universal language model fine-tuned for classification (ULMFiT) <ref type="bibr" target="#b8">[9]</ref>. The pre-trained 5 model consists of 3 BiLSTM layers. Finetuning the model involves two steps: (a) fine-tuning LM on the target dataset and (b) training classifier (MLP layer) on the target dataset. We trained several models with various drop-out rates. More specifically, the LM was fine-tuned for 15 epochs 6 , with drop-out rates from: {0.2, . . . , 0.6}. The classifier was fine-tuned for 50 epochs with drop-out rates from: {0.2, . . . , 0.6}. A held-out development set was used to tune the hyper-parameters (drop-out rates, and fine-tuning epochs). 4) TF-IDF: The fourth baseline system is a standard term frequency-inverse document frequency (TF-IDF) based document representation, followed by multi-class logistic regression (LR). Although TF-IDF is not a topic model, the classification performance of TF-IDF based systems are often close to state-of-the-art systems <ref type="bibr" target="#b18">[19]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RESULTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Convergence rate of Bayesian SMM</head><p>We observed that the posterior distributions extracted using Bayesian SMM are always much sharper than standard Normal distribution. Hence we initialized the variational distribution to N (0, diag(0.1)) to speed up the convergence. <ref type="figure" target="#fig_4">Fig. 4</ref> shows objective (ELBO) plotted for two different initializations of variational distribution. Here, the model was trained on 20Newsgroups corpus, with the embedding dimension K = 100, regularization weight ω = 1.0 and prior set to standard Normal. We can observe that the model initialized to N (0, diag(0.1)) converges faster as compared to the one initialized to standard Normal. In all the further experiments, we initialized 7 both the prior and variational distributions to N (0, diag(0.1)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Perplexity</head><p>Perplexity is an intrinsic measure for topic models <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b40">[41]</ref>. It is computed as an average of every test document according to:</p><formula xml:id="formula_43">PPL DOC = exp −1 D D d=1 log p(x d ) N d ,<label>(41)</label></formula><p>or for an entire test corpus according to:</p><formula xml:id="formula_44">PPL CORPUS = exp − D d=1 log p(x d ) D d=1 N d ,<label>(42)</label></formula><p>where N d is the number of word tokens in document d.</p><p>In our case, log p(x) from (9) cannot be evaluated, because the KL divergence from variational distribution q to the true posterior p cannot be computed; as the true posterior is intractable <ref type="bibr" target="#b3">(4)</ref>. We can only compute L(q), which is a lower bound on log p(x); thus the resulting perplexity values  <ref type="bibr" target="#b41">(42)</ref> act as upper bounds. This is true for NVDM <ref type="bibr" target="#b14">[15]</ref> or any other model in the VB framework where the true posterior is intractable <ref type="bibr" target="#b15">[16]</ref>. We estimated L(q) from (16) using 32 samples, i.e., R = 32, in order to compute perplexity. In <ref type="bibr" target="#b14">[15]</ref>, the authors used 20 samples. We present the comparison of 20Newsgroups test data perplexities obtained using Bayesian SMM and NVDM in <ref type="table" target="#tab_2">Table II</ref>. It shows the perplexities of 20Newsgroups corpus under full and a limited vocabulary of 2000 words <ref type="bibr" target="#b14">[15]</ref>. We also show the perplexity computed using the maximum likelihood probabilities estimated on the test data. It acts as the lower bound on the test perplexities. NVDM was shown <ref type="bibr" target="#b14">[15]</ref> to achieve superior perplexity scores when compared to LDA, docNADE <ref type="bibr" target="#b41">[42]</ref>, Deep Auto Regressive Neural Network models <ref type="bibr" target="#b42">[43]</ref>. To the best of our knowledge, our model achieves state-of-the-art perplexity scores on 20Newsgroups corpus under limited and full vocabulary conditions.</p><p>In further investigation, we trained both Bayesian SMM and NVDM until convergence. At regular checkpoints during the training, we froze the model, extracted the embeddings for both training and test data, and computed the perplexities; shown in <ref type="figure">Figures 5a and 5b</ref>. We can observe that both the Bayesian SMM and NVDM fit the training data equally well (low perplexities). However, in the case of NVDM, the perplexity of test data increases after certain number of iterations; suggesting that NVDM fails to generalize and over-fits on the training data. In the case of Bayesian SMM, the perplexity of the test data decreases and remains stable, illustrating the robustness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Early stopping mechanism for topic ID systems</head><p>The embeddings extracted from a model trained purely in an unsupervised fashion does not necessarily yield optimum results when used in a supervised scenario. As discussed earlier in Sections VI-C, and VI-D, an early stopping mechanism (ESM) during the training of an unsupervised model (eg: NVDM, SMM, and Bayesian SMM) is required to get optimal performance from the subsequent topic ID system. The following experiment illustrates the idea of ESM:</p><p>We trained SMM, Bayesian SMM and NVDM on Fisher data until convergence. At regular checkpoints during the training, we froze the model, extracted the embeddings for both training and test data. We chose GLC for SMM, GLCU for NVDM, and Bayesian SMM as topic ID classifiers. We then evaluated the topic ID accuracy on the cross-validation 8  represents the best cross-validation score and the corresponding test score obtained using the early stopping mechanism (ESM). The embedding dimension was set to 100 for all the models. and test sets. <ref type="figure">Fig. 6</ref> shows the topic ID accuracy on crossvalidation and test sets obtained at regular checkpoints for all the three models. The circular dot (•) represents the best cross-validation score and the corresponding test score that is obtained by employing ESM. In case of (non-Bayesian) SMM, the test accuracy drops significantly after certain number of iterations; suggesting the strong need of ESM. The crossvalidation accuracies of NVDM and Bayesian SMM are similar and remain consistent over the iterations. However, the test accuracy of NVDM is much lower than that of Bayesian SMM and also decreases over the iterations. On the other hand, the test accuracy of Bayesian SMM increases and stays consistent. It shows the robustness of our proposed model, which in addition, does not require any ESM. In all the further topic ID experiments, we report classification results for Bayesian SMM without ESM; while the results for SMM, and NVDM are with ESM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Topic ID results</head><p>This section presents the topic ID results in terms of classification accuracy (in %) and cross-entropy (CE) on the test sets. Cross-entropy gives a notion of how confident the classifier is about its prediction. A well calibrated classifier tends to have lower cross-entropy. <ref type="table" target="#tab_2">Table III</ref> presents the classification results on Fisher speech corpora with manual and automatic transcriptions, where the first two rows are the results from earlier published works. Hazen <ref type="bibr" target="#b36">[37]</ref>, used discriminative vocabulary selection followed by a naïve Bayes (NB) classifier. Having a limited (small) vocabulary is the major drawback of this approach. Although we have used the same training and test splits, May <ref type="bibr" target="#b18">[19]</ref> had slightly larger vocabulary than ours, and their best system is similar to our baseline TF-IDF based system. The remaining rows in <ref type="table" target="#tab_2">Table III</ref> show our baselines and proposed systems. We can see that our proposed systems achieve consistently better accuracies; notably, GLCU which exploits the uncertainty in document embeddings has much lower cross-entropy than its counter part, GLC. To the best of our knowledge, the proposed systems achieve the best classification results on Fisher corpora with the current set-up, i.e., treating each side of the conversation as an independent document. It can be observed ULMFiT has the lowest cross-entropy among all the systems.</p><p>Table IV presents classification results on 20Newsgroups dataset. The first three rows give the results as reported in earlier works. Pappagari et al. <ref type="bibr" target="#b43">[44]</ref>, proposed a CNN-based discriminative model trained to jointly optimize categorical cross-entropy loss for classification task along with binary cross-entropy for verification task. Sparse composite document vector (SCDV) <ref type="bibr" target="#b35">[36]</ref> exploits pre-trained word embeddings to obtain sparse document embeddings, whereas neural tensor skip-gram model (NTSG) <ref type="bibr" target="#b44">[45]</ref> extends the idea of a skipgram model for obtaining document embeddings. The authors in (SCDV) <ref type="bibr" target="#b35">[36]</ref> have shown superior classification results as compared to paragraph vector, LDA, NTSG, and other systems. The next rows in <ref type="table" target="#tab_2">Table IV</ref> present our baselines and proposed systems. We see that the topic ID systems based on Bayesian SMM and logistic regression is better than all the other models, except for the purely discriminative CNN model. We can also see that all the topic ID systems based on Bayesian SMM are consistently better than variational auto encoder inspired NVDM, and (non-Bayesian) SMM.</p><p>The advantages of the proposed Bayesian SMM are summarized as follows: (a) the document embeddings are Gaussian distributed which enables to train simple generative classifiers like GLC, or GLCU; that can extended to newer classes easily, (b) although the Bayesian is trained in an unsupervised fashion, it does not require any early stopping mechanism to yield optimal topic ID results; document embeddings extracted from a fully converged or model can be directly used for classification tasks without any fine-tuning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Uncertainty in document embeddings</head><p>The uncertainty captured in the posterior distribution of document embeddings correlates strongly with size of the document. The trace of the covariance matrix of the inferred posterior distributions gives us the notion of such a correlation. <ref type="figure" target="#fig_7">Fig. 7</ref> shows an example of uncertainty captured in the embeddings. Here, the Bayesian SMM was trained on 20Newsgroups with an embedding dimension of 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSIONS AND FUTURE WORK</head><p>We have presented a generative model for learning document representations (embeddings) and their uncertainties. Our proposed model achieved state-of-the-art perplexity results on the standard 20Newsgroups and Fisher datasets. Next, we have shown that the proposed model is robust to overfitting and unlike in SMM and NVDM, it does not require any early stopping mechanism for topic ID. We proposed an extension to simple Gaussian linear classifier that exploits the uncertainty in document embeddings and achieves better crossentropy scores on the test data as compared to the simple  GLC. Using simple linear classifiers on the obtained document embeddings, we achieved superior classification results on Fisher speech 20Newsgroups text corpora. We also addressed a commonly encountered problem of intractability while performing variational inference in mixed-logit models by using the re-parametrization trick. This idea can be translated in a straightforwardly for subspace n-gram model for learning sentence embeddings and also for learning word embeddings along with their uncertainties. The proposed Bayesian SMM can be extended to have topic-specific priors for document embeddings, which enables to encode topic label uncertainty explicitly in the document embeddings. There exists other scoring mechanisms that exploit the uncertainty in embeddings <ref type="bibr" target="#b45">[46]</ref>, which we plan to explore in our future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A GRADIENTS OF LOWER BOUND</head><p>The variational distribution is diagonal with the following parametrization: q(w) = N (w | ν, diag(exp{2ς})).</p><p>The lower bound for a single document is:</p><formula xml:id="formula_46">L d ≈ − 1 2 λ tr(diag(exp{2ς})) − log|diag(exp{2ς})| − K log λ + λν T ν − K + V i=1 x i (m i + t i ν) − 1 R R r=1 log   V j=1 exp{m j + t j g( r )}   ,<label>(44)</label></formula><p>where g( ) = ν + diag(exp{ς})˜ .</p><p>It is convenient to have the following derivatives:</p><formula xml:id="formula_48">∂g( ) ∂ν = I.<label>(46)</label></formula><formula xml:id="formula_49">∂(t i g( )) ∂ς = diag(t T i ) diag(exp{ς}) diag(˜ ) = t T i exp{ς} ˜ .<label>(47)</label></formula><p>Derivatives of the parameters of variational distribution:</p><p>Taking derivative of the objective function ( <ref type="formula" target="#formula_3">(44)</ref>) with respect to mean parameter ν and using <ref type="formula" target="#formula_3">(46)</ref>:</p><formula xml:id="formula_50">∂L d ∂ν = −λν + V i=1 x i t T i − 1 R R r=1 V k=1 t T k I exp{m k + t k g( r )} j exp{m j + t j g( r )} θ kr (48) = V i=1 x i t T i − V i=1 t T i 1 R R r=1 θ ir V k=1 x k − λν (49) ∇ν = V i=1 t T i (x i − 1 R R r=1 θ ir V k=1 x k ) − λν.<label>(50)</label></formula><p>Taking the derivative of objective function ((44)) with respect to ς and using (47):</p><formula xml:id="formula_51">∂L d ∂ς = − 1 2 2λ exp{2ς} − 2I + V i=1 x i − 1 R R r=1 V k=1 t T k T r exp{m k + t k g( r )} j exp{m j + t j g( r )} θ kr = 1 − λ exp{2ς} − V i=1 x i 1 R R r=1 V k=1 t T k exp{ς} ˜ T r θ kr (51) ∇ς = 1 − λ exp{2ς} − V i=1 x i 1 R R r=1 V k=1 θ kr t T k exp{ς} ˜ r .<label>(52)</label></formula><p>Derivatives of the model parameters:</p><p>Taking the derivative of complete objective <ref type="bibr" target="#b16">(17)</ref> with respect to a row t k from matrix T :</p><formula xml:id="formula_52">∂L ∂t k = ∂ ∂t k D d=1 V i=1 x di (m i + t i ν d ) − 1 R R r=1 log   V j=1 exp{m j + t j g( r )}   − ω V i=1 ||t i || 1 (53) = D d=1 x dk ν T d − V i=1 x di 1 R R r=1 g( dr ) T exp{m i + t k g( dr )} j exp{m j + t j g( dr )} θ dkr − ω sign(t k ) (54) = D d=1 x dk ν T d − V i=1 x di 1 R R r=1 g( dr ) T θ dkr − ω sign(t k ) (55) ∇t k = D d=1 x dk ν T d − V i=1 x di 1 R R r=1 θ dkr g( dr ) T − ω sign(t k ).<label>(56)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B EM ALGORITHM FOR GLCU E-STEP:</head><p>Obtaining the posterior distribution of latent variable p(y d | ν d , Θ). Using the results from <ref type="bibr" target="#b27">[28]</ref>  </p><formula xml:id="formula_53">u d = (D + Γ d ) −1 (D(ν d − µ d ) + Γ d 0) = [D −1 (D + Γ d )] −1 (ν d − µ d )</formula><p>resulting in: Maximizing the auxiliary function Q with respect to model parameters Θ = {M , D} Taking derivative with respect to each column µ in M and equating it to zero:</p><formula xml:id="formula_54">u d = [I + D −1 Γ d ] −1 (ν d − µ d ) V d = D + Γ d<label>(</label></formula><formula xml:id="formula_55">∂Q ∂µ = − 1 2 ∂ ∂µ d∈I (u d − (ν d − µ )) T D(u d − (ν d − µ l )) (61) = − 1 2 d∈I 2D (µ − (ν d − u d )) = −D n∈I µ − n∈I (ν d − u d ) µ = 1 |I | n∈I (ν d − u d )<label>(62)</label></formula><p>Taking derivative with respect to shared precision matrix D and equating it to zero:</p><formula xml:id="formula_56">∂Q ∂D = D 2 D −1 − 1 2 D d=1 V −1 d T − 1 2 D d=1 (u d − (ν d − µ d ))(u d − (ν d − µ nl )) T T (63) D −1 = 1 D D d=1 V −1 d + D d=1 (u d − (ν d − µ nl ))(u d − (ν d − µ d )) T (64)</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Graphical model for Bayesian SMM p(w | x). Parameters of such posterior distribution can be then used as a low dimensional representation of the document.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 : 3 for d = 1 . . . D do 4 sample˜ 6 compute gradient ∇ν d using ( 19 ) 7 compute gradient ∇ς d using ( 21 ) 8 update ν d and ς d using ADAM 9 end 10 compute</head><label>13146197218910</label><figDesc>Stochastic VB training 1 initialize the model and the variational parameters 2 repeat dr ∼ N (0, I) r = 1 . . . R 5 compute L(q d ) using (16) L using (17) 11</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Graphical model for LDA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Graphical model for CTM B. Correlated topic model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>The hyper-parameter ( Convergence of Bayesian SMM for various initializations of variational distribution. The model was trained on 20Newsgroups corpus with K = 100, and ω = 1. regularization weight) of LR was selected based on 5-fold cross-validation experiments on training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>Comparison of training and test data perplexities obtained using Bayesian SMM and NVDM for both Fisher and 20Newsgroups datasets. The horizontal solid green line shows the test data perplexity computed using the maximum likelihood (ML) probabilities estimated on the test data. The latent (embedding) dimension was set to 200 for both the models. Performance of topic ID systems on Fisher data at various checkpoints during model training. The circular dot (•)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Uncertainty (trace of covariance of posterior distribution) captured in the document embeddings of 20Newsgroups dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>d</head><label></label><figDesc>(p. 41, (358)):log p(y d | ν d , h d , Θ) = log p(ν d | y d , h d ) + log p(y d ) − log p(ν d ) = log N (ν d | µ d + y d , D −1 ) + log N (y d | 0, Γ − (µ d + y d )) T D(ν d − (µ d + y d )) − (ν d − µ d )) T D(y d − (ν d − µ d )) Γ d y d + const = N (y d | u d , V −1 d )where u d is simplified as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>1 )E</head><label>1</label><figDesc>) = p(y | w, Θ old ).(60)Using the results from<ref type="bibr" target="#b27">[28]</ref>[p. 43, (378)], the auxiliary function Q(Θ, Θ old ) is computed as:Q(Θ, Θ old ) = E q [ D d=1 log p(ν d , y d )] = D d=1 E q [log p(ν d | y d )] + E q [log p(y d )] = D d=1 E q [log N (ν d | µ d + y d , D −q [(ν d − (µ d + y d )) T D (ν d − (µ d + y d ))d − (ν d − µ d )) T D(u d − (ν d − µ d ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>is with Brno University of Technology and International Institute of Information Technology, Hyderabad. e-mail: kesiraju@fit.vutbr.cz .</figDesc><table /><note>L. Burget and O. Plchot are with Brno University of Technology. SV Gangashetty is with International Institute of Information Technology, Hyderabad.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Data splits from Fisher phase 1 corpus, where each document represents one side of the conversation.</figDesc><table><row><cell>Set</cell><cell cols="2"># docs. Duration (hrs.)</cell></row><row><cell>ASR training</cell><cell>6208</cell><cell>553</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Comparison of perplexity (PPL) results on 20Newsgroups. The values in the brackets indicate results with a limited vocabulary of 2000 words.</figDesc><table><row><cell>Model</cell><cell>K</cell><cell>PPL CORPUS</cell><cell>PPL DOC</cell></row><row><cell>NVDM</cell><cell>50</cell><cell>1287 (769)</cell><cell>1421 (820)</cell></row><row><cell>NVDM</cell><cell>200</cell><cell>1387 (852)</cell><cell>1519 (870)</cell></row><row><cell>Bayesian SMM</cell><cell>50</cell><cell cols="2">1043 (629) 1064 (639)</cell></row><row><cell cols="2">Bayesian SMM 200</cell><cell>882 (519)</cell><cell>851 (515)</cell></row><row><cell>ML estimate</cell><cell>-</cell><cell>153 (90)</cell><cell>93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Comparison of results on Fisher test sets, from earlier published works, our baselines and proposed systems. indicates a pure discriminative model.</figDesc><table><row><cell>Systems</cell><cell>Model</cell><cell cols="2">Classifier Accuracy (%)</cell><cell cols="2">CE Accuracy (%)</cell><cell>CE</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Manual transcriptions Automatic transcriptions</cell></row><row><cell>Prior works</cell><cell>BoW [37] TF-IDF [19]</cell><cell>NB LR</cell><cell>87.61 86.41</cell><cell>--</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell>TF-IDF</cell><cell>LR</cell><cell>86.59</cell><cell>0.93</cell><cell>86.77</cell><cell>0.94</cell></row><row><cell></cell><cell>ULMFiT</cell><cell>MLP</cell><cell>86.41</cell><cell>0.50</cell><cell>86.08</cell><cell>0.50</cell></row><row><cell>Our Baseline</cell><cell>1 SMM 1 SMM</cell><cell>LR GLC</cell><cell>86.81 85.17</cell><cell>0.91 1.64</cell><cell>87.02 85.53</cell><cell>1.09 1.54</cell></row><row><cell></cell><cell>NVDM</cell><cell>LR</cell><cell>81.16</cell><cell>0.94</cell><cell>83.67</cell><cell>1.15</cell></row><row><cell></cell><cell>NVDM</cell><cell>GLC</cell><cell>84.47</cell><cell>1.25</cell><cell>84.15</cell><cell>1.22</cell></row><row><cell></cell><cell>NVDM</cell><cell>GLCU</cell><cell>83.96</cell><cell>0.93</cell><cell>83.01</cell><cell>0.97</cell></row><row><cell></cell><cell>Bayesian SMM</cell><cell>LR</cell><cell>89.91</cell><cell>0.89</cell><cell>88.23</cell><cell>0.95</cell></row><row><cell>Proposed</cell><cell>Bayesian SMM</cell><cell>GLC</cell><cell>89.47</cell><cell>1.05</cell><cell>87.23</cell><cell>1.46</cell></row><row><cell></cell><cell>Bayesian SMM</cell><cell>GLCU</cell><cell>89.54</cell><cell>0.68</cell><cell>87.54</cell><cell>0.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>Comparison of results on 20Newsgroups from earlier published works, our baselines and proposed systems.indicates a pure discriminative model.</figDesc><table><row><cell>Systems</cell><cell>Model</cell><cell cols="2">Classifier Accuracy (%)</cell><cell>CE</cell></row><row><cell></cell><cell>CNN [44]</cell><cell>-</cell><cell>86.12</cell><cell>-</cell></row><row><cell>Prior works</cell><cell>SCDV [36]</cell><cell>SVM</cell><cell>84.60</cell><cell>-</cell></row><row><cell></cell><cell>NTSG-1 [45]</cell><cell>SVM</cell><cell>82.60</cell><cell>-</cell></row><row><cell></cell><cell>TF-IDF</cell><cell>LR</cell><cell cols="2">84.47 0.73</cell></row><row><cell></cell><cell>ULMFiT</cell><cell>MLP</cell><cell cols="2">83.06 0.89</cell></row><row><cell>Our Baselines</cell><cell>1 SMM 1 SMM</cell><cell>LR GLC</cell><cell cols="2">82.01 82.02 1.33 0.75</cell></row><row><cell></cell><cell>NVDM</cell><cell>LR</cell><cell cols="2">79.57 0.86</cell></row><row><cell></cell><cell>NVDM</cell><cell>GLC</cell><cell cols="2">77.60 1.65</cell></row><row><cell></cell><cell>NVDM</cell><cell>GLCU</cell><cell cols="2">76.86 0.88</cell></row><row><cell></cell><cell>Bayesian SMM</cell><cell>LR</cell><cell cols="2">84.65 0.53</cell></row><row><cell>Proposed</cell><cell>Bayesian SMM</cell><cell>GLC</cell><cell cols="2">83.22 1.28</cell></row><row><cell></cell><cell>Bayesian SMM</cell><cell>GLCU</cell><cell cols="2">82.81 0.79</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For clarity, explicit conditioning on T and m is omitted in the subsequent equations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This is not a limitation of the model, but only a simplification.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://catalog.ldc.upenn.edu/LDC2004S13</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://qwone.com/˜jason/20Newsgroups/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/fastai/fastai 6 Fine-tuning LM for higher number of epochs degraded the classification performance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">One can introduce hyper-priors and learn the parameters of prior distribution.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">5-fold cross-validation on training set.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">LDA-based document models for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 29th Annual International ACM SIGIR</title>
		<meeting>of the 29th Annual International ACM SIGIR</meeting>
		<imprint>
			<date type="published" when="2006-08" />
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Context dependent recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE SLT Workshop</title>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="234" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Limited resource term detection for effective topic identification of speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wintrode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICASSP</title>
		<imprint>
			<date type="published" when="2014-05" />
			<biblScope unit="page" from="7118" to="7122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recurrent neural network language model adaptation for multi-genre broadcast speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lanchantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech. ISCA</title>
		<meeting>Interspeech. ISCA</meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="3511" to="3515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">i-Vectors in Language Modeling: An Efficient Way of Domain Adaptation for Feed-Forward Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Beneš</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kesiraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech. ISCA</title>
		<meeting>Interspeech. ISCA</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3383" to="3387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2014 Conference on EMNLP, ACL</title>
		<meeting>of the 2014 Conference on EMNLP, ACL</meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ICML</title>
		<meeting>of the ICML</meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Universal Language Model Fine-tuning for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 56th Annual Meeting of the ACL</title>
		<meeting>of the 56th Annual Meeting of the ACL<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the NAACL: HLT. ACL</title>
		<meeting>of the NAACL: HLT. ACL</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805v1</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Latent variable models,&quot; in Learning in Graphical Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999-01" />
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="371" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Probabilistic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="77" to="84" />
			<date type="published" when="2012-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd ICLR</title>
		<meeting>of the 2nd ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural variational inference for text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd ICML, ser. ICML&apos;16. JMLR.org</title>
		<meeting>the 33rd ICML, ser. ICML&apos;16. JMLR.org</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1727" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning (Information Science and Statistics)</title>
		<meeting><address><addrLine>Secaucus, NJ, USA; New York, Inc.</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 31st ICML, ser. Proc. of Machine Learning</title>
		<editor>Research, E. P. Xing and T. Jebara</editor>
		<meeting>of the 31st ICML, ser. . of Machine Learning<address><addrLine>Bejing, China</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Regularized subspace n-gram model for phonotactic ivector extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soufifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cumani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocký</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH. ISCA</title>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="74" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Topic identification and discovery on text and speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wintrode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2015 Conference on EMNLP</title>
		<meeting>of the 2015 Conference on EMNLP</meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="2377" to="2387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning Document Representations Using Subspace Multinomial Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kesiraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Szöke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Černocký</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IN-TERSPEECH. ISCA</title>
		<meeting>of IN-TERSPEECH. ISCA</meeting>
		<imprint>
			<date type="published" when="2016-09" />
			<biblScope unit="page" from="700" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Correlated topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems NIPS</title>
		<imprint>
			<date type="published" when="2005-12" />
			<biblScope unit="page" from="147" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
	<note>in 3rd ICLR</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scalable Training of L1-Regularized Log-Linear Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 24th ICML</title>
		<meeting>of the 24th ICML<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Prosodic speaker verification using subspace multinomial models with intersession compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kockmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Černocký</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTERSPEECH. ISCA, September 2010</title>
		<meeting>of INTERSPEECH. ISCA, September 2010</meeting>
		<imprint>
			<biblScope unit="page" from="1061" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Frontend factor analysis for speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech &amp; Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="788" to="798" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A comparison of variational approximations for fast inference in mixed logit models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Depraetere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vandebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="125" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The Matrix Cookbook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Pedersen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PLDA for speaker verification with utterances of arbitrary duration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013-05" />
			<biblScope unit="page" from="7649" to="7653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploiting i-vector posterior covariances for short-duration language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cumani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fér</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTER-SPEECH</title>
		<meeting>of INTER-SPEECH</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1002" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">iVector Approach to Phonotactic Language Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soufifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kockmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTERSPEECH. ISCA</title>
		<meeting>of INTERSPEECH. ISCA</meeting>
		<imprint>
			<date type="published" when="2011-08" />
			<biblScope unit="page" from="2913" to="2916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 49th Annual Meeting of the ACL: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sparse Additive Generative Models of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 28th ICML</title>
		<meeting>of the 28th ICML<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1041" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sparse Overcomplete Latent Variable Decomposition of Counts Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V S</forename><surname>Shashanka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007-12" />
			<biblScope unit="page" from="1313" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sparse Topical Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 27th Conference on UAI</title>
		<meeting>of the 27th Conference on UAI</meeting>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="831" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scdv : Sparse composite document vectors using soft clustering over distributional representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mekala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Karnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2017 Conference on EMNLP</title>
		<meeting>of the 2017 Conference on EMNLP<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="659" to="669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Topic Identification from Audio Recordings using Word and Phone Recognition Lattices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Hazen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Margolis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on ASRU</title>
		<imprint>
			<date type="published" when="2007-12" />
			<biblScope unit="page" from="659" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">MCE Training Techniques for Topic Identification of Spoken Audio Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Hazen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2451" to="2460" />
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The Kaldi Speech Recognition Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Silovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vesely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on ASRU. IEEE Signal Processing Society</title>
		<imprint>
			<date type="published" when="2011-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sequencediscriminative training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Veselý</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTER-SPEECH. ISCA</title>
		<meeting>of INTER-SPEECH. ISCA</meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="2345" to="2349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeling documents with a deep boltzmann machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Twenty-Ninth Conference on UAI, ser. UAI&apos;13</title>
		<meeting>of the Twenty-Ninth Conference on UAI, ser. UAI&apos;13<address><addrLine>Arlington, Virginia, United States</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="616" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A neural autoregressive topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="2717" to="2725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural variational inference and learning in belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 31th ICML</title>
		<meeting>of the 31th ICML</meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1791" to="1799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Joint verification-identification in end-to-end multi-scale cnn framework for topic identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pappagari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICASSP</title>
		<imprint>
			<date type="published" when="2018-04" />
			<biblScope unit="page" from="6199" to="6203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning context-sensitive word embeddings with neural tensor skip-gram model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 24th International Conference on Artificial Intelligence, ser. IJCAI&apos;15</title>
		<meeting>of the 24th International Conference on Artificial Intelligence, ser. IJCAI&apos;15</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1284" to="1290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Gaussian metaembeddings for efficient scoring of a heavy-tailed PLDA model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brümmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Silnova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Odyssey 2018 The Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey 2018 The Speaker and Language Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
