<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Encoder-Decoder Based Convolutional Neural Networks with Multi-Scale-Aware Modules for Crowd Counting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pongpisit</forename><surname>Thanasutives</surname></persName>
							<email>pongpisit.tha@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Science and Technology</orgName>
								<orgName type="institution">Osaka University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Fukui</surname></persName>
							<email>fukui@ai.sanken.osaka-u.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Scientific and Industrial Research</orgName>
								<orgName type="institution">Osaka University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Numao</surname></persName>
							<email>numao@ai.sanken.osaka-u.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Scientific and Industrial Research</orgName>
								<orgName type="institution">Osaka University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boonserm</forename><surname>Kijsirikul</surname></persName>
							<email>boonserm.k@chula.ac.th</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Engineering Faculty of Engineering Chulalongkorn University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Encoder-Decoder Based Convolutional Neural Networks with Multi-Scale-Aware Modules for Crowd Counting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose two modified neural networks based on dual path multi-scale fusion networks (SFANet)   and SegNet for accurate and efficient crowd counting. Inspired by SFANet, the first model, which is named M-SFANet, is attached with atrous spatial pyramid pooling (ASPP) and contextaware module (CAN). The encoder of M-SFANet is enhanced with ASPP containing parallel atrous convolutional layers with different sampling rates and hence able to extract multi-scale features of the target object and incorporate larger context. To further deal with scale variation throughout an input image, we leverage the CAN module which adaptively encodes the scales of the contextual information. The combination yields an effective model for counting in both dense and sparse crowd scenes. Based on the SFANet decoder structure, M-SFANet's decoder has dual paths, for density map and attention map generation. The second model is called M-SegNet, which is produced by replacing the bilinear upsampling in SFANet with max unpooling that is used in SegNet. This change provides a faster model while providing competitive counting performance. Designed for high-speed surveillance applications, M-SegNet has no additional multi-scale-aware module in order to not increase the complexity. Both models are encoder-decoder based architectures and are end-to-end trainable. We conduct extensive experiments on five crowd counting datasets and one vehicle counting dataset to show that these modifications yield algorithms that could improve state-of-the-art crowd counting methods. Codes are available at https://github.com/Pongpisit-Thanasutives/ Variations-of-SFANet-for-Crowd-Counting. arXiv:2003.05586v5 [cs.CV]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Crowd counting is an important task due to its wide range of applications such as public safety, surveillance monitoring, traffic control and intelligent transportation. However, it is a challenging computer vision task and not so trivial to efficiently solve the problem at first glance due to heavy occlusion, perspective distortion, scale variation and diverse crowd distribution throughout real-world images. These problems are emphasized especially when the target objects are in a crowded space. Some of the early methods <ref type="bibr" target="#b0">[1]</ref> treat crowd counting as a detection problem. The handcrafted features from multiple sources are also investigated in <ref type="bibr" target="#b1">[2]</ref>. These approaches are not suitable when the targeted objects are overlapping each other and the handcrafted features can not handle the diversity of crowd distribution in input images properly. In order to take characteristics of the crowd distribution into account, one should not consider developing models predicting only the number of people in the target image because the characteristics are neglected. Thus, more recent methods rely more on the density map automatically generated from the head annotation ground truth instead. On the other hand, the authors of <ref type="bibr" target="#b2">[3]</ref> consider the density map as a likelihood describing "how the spatial pixels would be" given the annotation ground truth and propose the novel Bayesian loss. However, in our experiments, we consider the density map ground truth as the learning target (except our experiment on UCF-QNRF <ref type="bibr" target="#b3">[4]</ref>) in order to investigate the improved accuracy caused by the proposed architecture modifications comparing with most of the stateof-the-art methods.</p><p>Convolutional Neural Networks (CNNs) have been utilized to estimate accurate density maps. By considering convolutional filters as sliding windows, CNNs are capable of feature extraction throughout various regions of an input image. Consequently, the diversity of crowd distribution in the image is handled more properly. In order to cope with head scale variation problems caused by camera diverse perspectives, previous works mostly make use of multi-column/multi-resolution based architectures <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. However, the study in <ref type="bibr" target="#b8">[9]</ref> shows that the features learned at each column structure of MCNN <ref type="bibr" target="#b4">[5]</ref> are nearly identical and it is not efficient to train such architecture when networks go deeper. As opposed to the multi-column network architecture, a deep single column network based on a truncated VGG16 <ref type="bibr" target="#b9">[10]</ref> feature extractor and a decoder with dilated convolutional layers is proposed in <ref type="bibr" target="#b8">[9]</ref> and carries out the breakthrough counting performance on ShanghaiTech <ref type="bibr" target="#b4">[5]</ref> dataset. The proposed architecture demonstrates the strength of VGG16 encoder, which is pretrained on ImageNet <ref type="bibr" target="#b10">[11]</ref> for higher semantics information extraction and the ability to transfer knowledge across vision tasks. Moreover, the study demonstrates how to attach atrous convolutional layers to the network instead of adding more pooling layers which could cause loss of spatial information. Nevertheless, <ref type="bibr" target="#b11">[12]</ref> has raised the issue of using the same filters and pooling operations over the whole image. The authors of <ref type="bibr" target="#b11">[12]</ref> have pointed out that the receptive field size should be changed across the image due to perspective distortion. To deal with this problem, the scale-aware contextual module, named CAN, capable of feature extraction over multiple receptive field sizes, has been proposed in <ref type="bibr" target="#b11">[12]</ref>. By the module design, the importance of each such extracted feature at every image location is learnable. However, CAN contains no mechanism to reduce the background noise, which possibly causes false predictions, especially when facing sparse and complex scenes.</p><p>Aside from crowd counting, objects overlapping is also a crucial problem for image segmentation. As a result, scaleaware modules such as spatial pyramid pooling (SPP) <ref type="bibr" target="#b12">[13]</ref> and atrous spatial pyramid pooling (ASPP) <ref type="bibr" target="#b13">[14]</ref> are devised to capture the contextual information at multiple scales. By employing values of the atrous rate from small to large, the model's field-of-view is enlarged and hence enables object encoding at multiple scales <ref type="bibr" target="#b13">[14]</ref>. Encoder-decoder based CNNs <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> have prior success in image segmentation ascribed to the ability to reconstruct precise object boundaries. Encoder-decoder based networks have also been proposed for crowd counting such as <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Bridging the gap between image segmentation and crowd counting, <ref type="bibr" target="#b20">[21]</ref> introduces dual path multi-scale fusion networks (SFANet) which integrate UNet <ref type="bibr" target="#b14">[15]</ref>-like decoder with the dual path structure to predict the head regions among complicated background then regress for the headcounts. Unfortunately, unlike CAN <ref type="bibr" target="#b11">[12]</ref>, SFANet has no explicit module to cope with the scale variation problem.</p><p>To redress the weaknesses of the state-of-the-art methods, we propose two modified networks for crowd counting. The first proposed model is called "M-SFANet" (Modified-SFANet) in which the multi-scale-aware modules, CAN and ASPP, are additionally connected to the VGG16-bn <ref type="bibr" target="#b9">[10]</ref> encoder of SFANet at different encoding stages to resolve the mentioned SFANet's problem and handle occlusion by capturing the contexts around the targeted objects at multiple scales. As a result of SFANet's dual path decoder for suppressing noisy background, the CAN's problem is seamlessly cured as well. M-SFANet learns the contextual scales adaptively (following the perspective) and statically; therefore, the model becomes effective for both dense and sparse scenes. Second, we integrate the dual path structure into ModSegNet <ref type="bibr" target="#b21">[22]</ref>-like encoder-decoder network instead of Unet and call this model "M-SegNet" (Modified-SegNet). Designed for medical image segmentation, ModSegNet is similar to UNet but leverage max-unpooling <ref type="bibr" target="#b15">[16]</ref> instead of transpose convolution which is not parameter-free. To the best of our knowledge, the max unpooling is not yet employed in the literature about crowd counting. M-SegNet is designed to be computationally faster (See <ref type="table" target="#tab_0">Table IX</ref>.) than SFANet, while providing the similar performance. Furthermore, we also test the performance of the ensemble model between M-SegNet and M-SFANet by average prediction. For some surveillance applications which speed are not the constraint, the ensemble model should be considered because of its lower variance prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS A. Traditional Approaches</head><p>Some early proposals rely on sliding window-based detection algorithms. This requires the extracted feature from human heads or human bodies such as histogram oriented gradients (HOG) <ref type="bibr" target="#b22">[23]</ref>. Unfortunately, these methods fail to detect people when encountering images with high occlusion.</p><p>Regression-based methods attempt to learn the mapping function from low-level information <ref type="bibr" target="#b23">[24]</ref> generated by features such as foreground and texture to the number of targeted objects. Mapping functions have also been studied in <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CNN-based Approaches</head><p>Lately, CNN-based methods have shown significant improvements from traditional methods on the task. Zhang et al. <ref type="bibr" target="#b25">[26]</ref> proposed a deep trained CNN to estimate crowd count while predicting crowd density level. In <ref type="bibr" target="#b4">[5]</ref>, Zhang et al. proposed a multi-column CNN (MCNN) in which each column was designed to respond to different scales. Li et al. <ref type="bibr" target="#b8">[9]</ref> proposed a deep single column CNN based on a truncated VGG16 encoder and dilated convolutional layers as a decoder to aggregate the multi-scale contextual information. Cao et al. <ref type="bibr" target="#b6">[7]</ref> presented an encoder-decoder scale aggregation network (SANet). Jiang et al. <ref type="bibr" target="#b26">[27]</ref> proposed a trellis encoderdecoder network (TEDnet) that incorporates multiple decoding paths. Shi et al. <ref type="bibr" target="#b27">[28]</ref> exploited the perspective information of pedestrian height, which is used to combine the multi-scale density maps. Liu et al. <ref type="bibr" target="#b18">[19]</ref> applied an attention mechanism to crowd counting by integrating an attention-aware network into a multi-scale deformable network to detect crowd regions. Wang et al. <ref type="bibr" target="#b17">[18]</ref> escalated the performance of crowd counting by pretraining a crowd counter on the synthetic data and proposed a crowd counting method via domain adaptation to deal with the lack of labeled data. Liu et al. <ref type="bibr" target="#b11">[12]</ref> proposed a VGG16-based model with the scale-aware contextual structure (CAN) that combines features extracted from multiple receptive field sizes and learns the importance of each such feature over image location. Zhu et al. <ref type="bibr" target="#b20">[21]</ref> proposed the dual path multi-scale fusion networks (SFANet) with an additional path to supervisedly learn generated attention maps. SFANet's decoder reuses coarse features and high-level features from encoding stages similar to Unet <ref type="bibr" target="#b14">[15]</ref>. Similarly, Sindagi et al. <ref type="bibr" target="#b28">[29]</ref> also introduced the attention mechanism, but embedded as the spatial attention module (SAM) in the network design. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED APPROACH</head><p>Since there are two base neural network architectures we modify and experiment with, SFANet <ref type="bibr" target="#b20">[21]</ref> and SegNet <ref type="bibr" target="#b15">[16]</ref>, we coin our models "M-SFANet" (Modified SFANet) and "M-SegNet" (Modified SegNet) respectively. Both of them are encoder-decoder based deep convolutional neural networks. They commonly have the convolutional layers of VGG16bn as the encoder which gradually reduces feature maps size and captures high-level semantics information. In the case of M-SFANet, the features are passed through CAN <ref type="bibr" target="#b11">[12]</ref> module and ASPP <ref type="bibr" target="#b13">[14]</ref> to extract the scale-aware contextual features. Finally, the decoders recover the spatial information to generate the final high-resolution density map. As a result of the combination, M-SFANet is more heavyweight and usually predicts more accurate crowd counts compared to the proposed M-SegNet. Based on SegNet, M-SegNet is lightweight and has no additional multi-scale-aware module. However, M-SegNet can achieve competitive results on some crowd counting benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Modified SFANet (M-SFANet)</head><p>The model architecture consists of 3 novel components, VGG16-bn feature map encoder, the multi-scale-aware modules, and the dual path multi-scale fusing decoder <ref type="bibr" target="#b20">[21]</ref>. First, the input images are fed into the encoder to learn useful highlevel semantics meaning. Then, the feature maps are fed into the multi-scale-aware modules in order to highlight the multiscale features of the target objects and the context. There are two multi-scale-aware modules in M-SFANet architecture, one connected with the 13 th layer of VGG16-bn is ASPP, and the other module connected with the 10 th layer of VGG16-bn is CAN. Lastly, the decoder paths use concatenate and bilinear upsampling to fuse the multi-scale features into the density maps and attention maps. Before producing the final density maps, the crowd regions are segmented from background by the generated attention maps. This mechanism spaces out noise background and lets the model focus more on the regions of interest. We leverage the multi-task loss function in <ref type="bibr" target="#b20">[21]</ref> to gain the advantage of the attention map branch. The overview of M-SFANet is shown in <ref type="figure" target="#fig_1">Fig. 1</ref>. Every convolutional layer is followed by batch normalization <ref type="bibr" target="#b31">[32]</ref> and ReLU except the last convolutional layer for predicting the final density map.</p><p>Feature map encoder (13 layers of VGG16-bn): We leverage the first pretrained 13 layers of VGG16 with batch normalization because the stack of 3x3 Convolutional layers is able to extract multi-scale features and multi-level semantic information. This is a more efficient way to deal with scale variation problems than multi-column architecture with different kernel sizes <ref type="bibr" target="#b8">[9]</ref>. The high-level feature maps (1/8 in size of the original input) from the 10 th layer are fed into CAN to adaptively encode the scales of the contextual information <ref type="bibr" target="#b11">[12]</ref>. Moreover, the top feature maps (1/16 in size of the original input) from the 13 th layer are fed into ASPP to statically learn (not specialized for each image location) image-level features (e.g. human head in our experiments) and the contextual information at multiple rates. Only encoding multi-scale information at the top layer is not efficient due to the most shrunken features, which are not appropriate for generating high-quality density maps <ref type="bibr" target="#b8">[9]</ref>.</p><p>Context-aware module: CAN <ref type="bibr" target="#b11">[12]</ref> module is capable of producing scale-aware contextual features using multiple receptive fields of average pooling operation. Following <ref type="bibr" target="#b11">[12]</ref>, The pooling output scales are 1, 2, 3, 6. The module extracts those features and learns the importance of each such feature at every image location, thus accounting for potentially rapid scale changes within an image <ref type="bibr" target="#b11">[12]</ref>. The importance of the extracted features varies according to the difference from their neighbors. Due to discriminative information fused from different scales, CAN module performs very well when encountering perspective distortion in the crowded scene.</p><p>Atrous spatial pyramid pooling: ASPP <ref type="bibr" target="#b13">[14]</ref> module applies several effective fields-of-view of atrous convolution and image pooling to the incoming features, thus capturing multi-scale information. The atrous rates are 1, 6, 12, 18. Thanks to atrous convolution, loss of information related to object boundaries (between human heads and background) throughout convolutional layers in the encoder is alleviated. The field of view of filters is enlarged to incorporate larger context without losing image resolution. Unlike CAN, ASPP treats the importance of the extracted scale features equally across spatial locations; hence, the module is proper for sparse scenes where the contexts are less informative.</p><p>Dual path multi-scale fusion decoder: The decoder architecture consists of the density map path and the attention map path as described in <ref type="bibr" target="#b20">[21]</ref>. The following strategy is applied to both the density map path and the attention map path. First, the output feature maps from ASPP are upsampled by a factor of 2 using bilinear interpolation and then concatenated with the output feature maps from CAN. Next, the concatenated feature maps are passed through 1x1x256 and 3x3x256 convolutional layers. Again, The fused features are upsampled by a factor of 2 and concatenated with the conv3-3 and the upsampled (by a factor of 4) feature maps from ASPP (depicted as the red connector in <ref type="figure" target="#fig_1">Fig. 1</ref>) before passing through 1x1x128 and 3x3x128 convolutional layers. This skip connection helps the network remind itself of the learned multi-scale features from high-level image representation. Finally, the 128 fused features are upsampled by a factor of 2 and concatenated with the conv2-2 before passing through 1x1x64, 3x3x64 and 3x3x32 convolutional layers respectively. Because of the use of three upsampling layers, the model can retrieve high-resolution feature maps with 1/2 size of the original input. Element-wise multiplication is applied to the attention map and the density feature maps to generate the refined final density map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Modified SegNet (M-SegNet)</head><p>M-SegNet shares almost the same components as presented in M-SFANet except for the fact that there are no CAN module and ASPP to additionally emphasize multi-scale information and the bilinear upsampling is replaced with max unpooling operation using the memorized max-pooling indices <ref type="bibr" target="#b15">[16]</ref> from the corresponding encoder layer. The first 10 layers of VGG16bn are employed as the feature map encoder. Hence, M-SegNet requires less computational resources than M-SFANet (See <ref type="table" target="#tab_0">Table VIII and Table IX</ref>.) and more suitable for speedconstrained applications. The overview of M-SegNet is shown in <ref type="figure" target="#fig_2">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. TRAINING METHOD</head><p>In this section, we explain how the density map ground truth and the attention map ground truth are automatically generated in our experiments. Training settings for each dataset are shown in <ref type="table" target="#tab_0">Table I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Density map ground truth</head><p>To generate the density map ground truth D(x), we follow the Gaussian method with fixed standard deviation kernel described in <ref type="bibr" target="#b4">[5]</ref>. Assuming that there is a head annotation at pixel x i represented as δ(x − x i ), the density map can be constructed by convolution with Gaussian kernel <ref type="bibr" target="#b24">[25]</ref>. This processes are formulated as:</p><formula xml:id="formula_0">D(x) = C i=1 δ (x − x i ) * G σ (x)<label>(1)</label></formula><p>In the ground truth annotation, we convolve each δ(x−x i ) with a Gaussian kernel (blurring each head annotation) with parameter σ, where C is a number of total headcounts. In our experiment we set σ = 5, 4, 4, 10 for ShanghaiTech <ref type="bibr" target="#b4">[5]</ref>, UCF_CC_50 <ref type="bibr" target="#b32">[33]</ref>, WorldExpo'10 <ref type="bibr" target="#b25">[26]</ref>, and TRANCOS <ref type="bibr" target="#b33">[34]</ref> dataset. For Beijing BRT <ref type="bibr" target="#b34">[35]</ref> dataset, we use the code provided in https: //github.com/XMU-smartdsp/Beijing-BRT-dataset for density map ground truth generation. For UCF_CC_50 <ref type="bibr" target="#b32">[33]</ref> and WE <ref type="bibr" target="#b25">[26]</ref>, we borrow the code from https://github.com/gjy3035/ C-3-Framework <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attention map ground truth</head><p>Following <ref type="bibr" target="#b20">[21]</ref>, Attention map ground truth is generated based on the threshold applied to the corresponding density map ground truth. The formulated equation is as follows:</p><formula xml:id="formula_1">A(i) = 0 0.001 &gt; D(i) 1 0.001 ≤ D(i)<label>(2)</label></formula><p>where i is a coordinate in the density map ground truth. The threshold is set to 0.001 according to <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training details</head><p>We leverage the image augmentation strategy as described in <ref type="bibr" target="#b20">[21]</ref> but the size of the cropped image differs across datasets. Therefore, when training each dataset, we use different learning rates and batch sizes. The main strategy can be summarized as random resizing by small portion, image cropping, horizontal flip, and gamma adjustment. The main difference to <ref type="bibr" target="#b20">[21]</ref> is that we use Adam <ref type="bibr" target="#b36">[37]</ref> with lookahead optimizer <ref type="bibr" target="#b37">[38]</ref> to train our models since it shows faster convergence than standard Adam optimizer and experimentally proved to improve the model's performance in <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL EVALUATION</head><p>In this section, we show the results of our M-SFANet and M-SegNet on 5 challenging crowd counting datasets and 1 congested vehicle counting dataset, TRANCOS <ref type="bibr" target="#b33">[34]</ref>. We mostly evaluate the performance using mean absolute error (MAE) and root mean square error (RMSE). The metrics are defined as follows:</p><formula xml:id="formula_2">M AE = 1 N N i=1 |C P rediction i − C Ground i | (3) RM SE = 1 N N i=1 (C P rediction i − C Ground i ) 2 (4)</formula><p>where N is the number of test images. C P rediction i and C Ground i refer to the prediction headcounts and the ground truth headcounts for the i th test image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>learning rate batch size crop size ShanghaiTech <ref type="bibr" target="#b4">[5]</ref> 5e-4 8 | 8 400x400 UCF_CC_50 <ref type="bibr" target="#b32">[33]</ref> 8e-4 5 | 8 512x512 WE <ref type="bibr" target="#b25">[26]</ref> 8e-4 | 5e-4 42 | 45 224x224 BRT <ref type="bibr" target="#b34">[35]</ref> 6e-4 42 | 45 224x224 UCF-QRNF <ref type="bibr" target="#b3">[4]</ref> 5e-4 5 | 5 512x512 TRANCOS <ref type="bibr" target="#b33">[34]</ref> 5e-4 5 | 8 full image** Note: "|" separates batch size for M-SFANet (left) and M-SegNet (right). **Full image training means no random resize and no image cropping.   <ref type="bibr" target="#b19">[20]</ref>) and SHB (SANet+SPANet <ref type="bibr" target="#b38">[39]</ref>). According to the ablation study in <ref type="table" target="#tab_0">Table II</ref>  Note: M-SFANet* denotes fine-tuning the pretrained model from section V-E using the Bayesian loss <ref type="bibr" target="#b2">[3]</ref>. "M-SFANet+M-SegNet" determines average prediction between the two models. The best performance is on boldface. Note: "M-SFANet+M-SegNet" determines average prediction between the two models. The best performance is on boldface. the useful features extracted from ASPP, the skip connection also enhances the counting performance. In the ablation study, we have found that fine-tuning our M-SFANet* pretrained on UCF-QNRF (described in section V-E) using Bayesian loss <ref type="bibr" target="#b2">[3]</ref> is a viable transfer learning approach to achieve accurate counting on SHB. The visualization of estimated density maps by our models on SHA is shown in <ref type="figure" target="#fig_3">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. UCF_CC_50 dataset</head><p>Proposed by <ref type="bibr" target="#b32">[33]</ref>, the dataset contains extremely crowded scenes with limited training samples. It includes only 50 highresolution images with numbers of head annotations ranging from 94 to 4543. Because of the limited numbers of training samples, we pretrain our models on ShanghaiTech Part A. To evaluate model performance, 5-fold cross-validation is performed following the standard setting in <ref type="bibr" target="#b32">[33]</ref>. The results compared to the state-of-the-art methods are listed in <ref type="table" target="#tab_0">Table III</ref>. M-SFANet obtains the competitive performance with 20.5% MAE improvement compared with the second-best approach, S-DCNet <ref type="bibr" target="#b19">[20]</ref>. The results indicate that the ensemble model does not always produce superior performance since the two proposed networks are similar in encoder and decoder structure, the trained models could be minor in diversity. The visualization of the predicted density maps on a dense scene of this dataset is depicted in the left column of <ref type="figure" target="#fig_4">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. WorldExpo'10 dataset</head><p>It includes 1,132 annotated video sequences collected from 103 different scenes. There are 3,980 annotated frames and 3,380 of them are used for model training. Each scene has a Region Of Interest (ROI). Having no access to the original dataset, we use the images and the density map ground truth generated by <ref type="bibr" target="#b35">[36]</ref> to train our models. In <ref type="table" target="#tab_0">Table IV</ref>, the performance on each test scene is reported in MAE. M-SegNet and M-SFANet achieve the best performance in scene 1 (sparse crowd) and scene 4 (dense crowd) respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Beijing BRT dataset</head><p>Beijing BRT dataset <ref type="bibr" target="#b34">[35]</ref> is a crowd counting dataset for intelligent transportation. The number of heads varies from 1 to 64. The images are all 640×360 pixels and taken from the Note: The result of "M-SFANet+M-SegNet" is not included because of no improvement from the state-of-the-art-methods. The best performance is on boldface. Note: "M-SFANet+M-SegNet" determines average prediction between the two models. The best performance is on boldface.</p><p>Bus Rapid Transit (BRT) in Beijing. The images are taken from morning until night, therefore they contain shadows, glare, and sunshine interference. <ref type="table" target="#tab_6">Table V</ref> reports our models' performance on this dataset. M-SFANet+M-SegNet obtains the new best performance with 17.27%/9.50% MAE/RMSE relative improvement. The visualization of the estimated density maps on a sample of this dataset is shown in the middle column of <ref type="figure" target="#fig_4">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. UCF-QNRF dataset</head><p>UCF-QNRF <ref type="bibr" target="#b3">[4]</ref> is a new and challenging dataset due to the extremely congested scenes. The dataset contains 1535 high-resolution (2013×2902 on average) images (train:1201, test:334) with 1.25M head annotations. We leverage the training scheme described in BL <ref type="bibr" target="#b2">[3]</ref> except the crowd counter architecture and the optimizer. Following the successful use of VGG19 <ref type="bibr" target="#b9">[10]</ref> backbone in BL, we replace the encoder of M-SFANet and M-SegNet with 16 layers of VGG19, input the BL decoder's output feature maps (128 channels) to the D3 block and remove the attention map branch to reduce the complexity. The results are reported in <ref type="table" target="#tab_0">Table VI</ref>. Upgraded from BL, M-SFANet* is able to push the current state-of-theart performance. <ref type="figure" target="#fig_6">Fig 5.</ref> depicts the density map visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. TRANCOS dataset</head><p>Apart from crowd counting, we also evaluate our models on TRANCOS <ref type="bibr" target="#b33">[34]</ref>, a vehicle counting dataset, to demonstrate the robustness and generalization of our approaches. The dataset contains 1244 images of different congested traffic scenes taken by surveillance cameras. Each image has a region of interest (ROI) used for evaluation. Following the Method MAE RMSE TEDNet <ref type="bibr" target="#b26">[27]</ref> 113.0 188.0 CAN <ref type="bibr" target="#b11">[12]</ref> 107.0 183.0 HA-CCN <ref type="bibr" target="#b28">[29]</ref> 118.1 180.4 SFCN <ref type="bibr" target="#b17">[18]</ref> 102.0 171.4 SFANet <ref type="bibr" target="#b20">[21]</ref> 100.8 174.5 S-DCNet <ref type="bibr" target="#b19">[20]</ref> 104. Note: * refers to the modifications described in section V-E. "M-SFANet*+M-SegNet*" determines average prediction between the two models. The best performance is on boldface. Note: "M-SFANet+M-SegNet" determines average prediction between the two models. MAE is equivalent to GAME(0). The best performance is on boldface.</p><p>work in <ref type="bibr" target="#b33">[34]</ref>, we use Grid Average Mean Absolute Error (GAME) for model performance evaluation. The metric is defined in equation 5. Our approaches, especially M-SFANet, surpass the best previous method as shown in <ref type="table" target="#tab_0">Table VII</ref>. The results show that the average density map estimation improves counting accuracy (reduce MAE) but does not provide better localization of target objects. The generated density maps from our models are shown in the right column of <ref type="figure" target="#fig_4">Fig. 4</ref>.</p><formula xml:id="formula_3">GAM E(L) = 1 N N i=1 4 L l=1 |C P rediction l,i − C Ground l,i |<label>(5)</label></formula><p>where N corresponds to the number of test images. C P rediction l,i and C Ground l,i are the predicted and ground truth counts of the l th sub-region of i th test image.    Note: * refers to the modifications described in section V-E. Each value is the average result from 100 runs on a single Google Cloud's NVIDIA Tesla V4 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we propose two modified end-to-end trainable neural networks, named M-SFANet and M-SegNet by combining novel architectures, designed for crowd counting and image segmentation. For M-SFANet, we add the multiscale-aware modules to SFANet architecture for better tackling drastic scale changes of target objects. The model alleviates the drawbacks present in the state-of-the-art methods and therefore shows superior performance on both crowd and vehicle counting. Furthermore, the decoder structure of M-SFANet is adjusted to have more residual connections in order to ensure that the learned multi-scale features of high-level semantic information will impact how the model regress for the final density map. However, the sampling rates of the scaleaware modules are not learnable and the number of these rates is fixed before training. This could lead to limited performance in certain unseen scenes. Hence, an adaptive implementation of the modules in which the sampling rates or dilated rates are adjustable is considered as possible future work. For M-SegNet, we change the up-sampling algorithm from bilinear to max unpooling using the memorized indices employed in SegNet. This yields the cheaper computation model while providing competitive counting performance applicable to realworld applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Sam et al. [30] delivered a separate top-down feedback CNN to correct the initial prediction. Likewise in [31], Sindagi et al. designed bottom-top and top-bottom feature fusion method to utilize information present at each of the network layers. Recently, instead of employing density maps as learning targets, Ma et al. [3] constructed a density contribution model and trained a VGG19[10]-based network using Bayesian loss rather than the vanilla mean square error (MSE) loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>The architecture of M-SFANet. The convolutional layers' parameters are denoted as Conv (kernel size)-(kernel size)-(number of filters). Max pooling is conducted over a 2 × 2 pixel window with stride 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The architecture of the proposed M-SegNet. The convolutional layers' parameters are denoted as Conv (kernel size)-(kernel size)-(number of filters). Max pooling is conducted over a 2 × 2 pixel window with stride 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Visualization of estimated density maps. The first row is sample images from ShanghaiTech Part A. The second row is the ground truth. The 3 rd to 5 th rows correspond to the estimated density maps from M-SegNet, M-SFANet and M-SegNet+M-SFANet respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Visualization of estimated density maps. The first row are sample images from UCF_CC_50<ref type="bibr" target="#b32">[33]</ref>, Beijing BRT<ref type="bibr" target="#b34">[35]</ref> and TRANCOS<ref type="bibr" target="#b33">[34]</ref> dataset (from left to right). The second row is the ground truth. The 3 rd to 5 th rows correspond to the estimated density maps from M-SegNet, M-SFANet and M-SegNet+M-SFANet respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Note: * refers to the modifications described in section V-E. The parameters and MACs are computed with the input size of 224×224. The training time is the average time to train one batch (8 samples) of ShanghaiTech part B<ref type="bibr" target="#b4">[5]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Visualization of estimated density maps from M-SFANet* (1 st row) and M-SegNet* (2 nd row) on test images of UCF-QNRF [4] dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I TRAINING</head><label>I</label><figDesc>SETTINGS FOR EACH DATASET</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>SHA) and Part B (SHB). SHA contains 482 (train:300, test:182) highly congested images downloaded from the internet. SHB includes 716 (train:400, test:316) relatively sparse crowd scenes taken from streets in Shanghai. Table II shows the results of our models compared to state-of-the-art methods and the ablation study. Compared to the base model, SFANet, M-SFANet can reduce MAE by 2.03% on SHB. Indicating MAE/RMSE improvement by 1.45%/4.50%, M-SegNet is able to outperform SFANet on SHB as well. By average prediction between M-SFANet and M-SegNet, we can gain 3.76% MAE and 8.41% MAE relative improvement on SHA and SHB respectively. Moreover, M-SFANet and M-SegNet both show competitive results compared to the best methods on SHA (S-DCNet</figDesc><table><row><cell>A. ShanghaiTech dataset</cell></row><row><cell>ShanghaiTech [5] dataset consists of 1198 labeled images</cell></row><row><cell>with 330,165 annotated people. The dataset is divided into</cell></row><row><cell>Part A (</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>ASPP on SHA while the result is converse on SHB. This empirically shows that CAN and ASPP are effective for crowded scenes and sparse scenes respectively. Subsequently, integrating both modules successfully decreases MAE/RMSE on both SHA and SHB. Retaining</figDesc><table><row><cell>, M-SFANet w/o CAN attain a</cell></row><row><cell>higher MAE than M-SFANet w/o</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II COMPARISON</head><label>II</label><figDesc>WITH STATE-OF-THE-ART METHODS ON SHANGHAITECH [5] DATASET AND ABLATION STUDY OF M-SFANET</figDesc><table><row><cell>Method</cell><cell cols="2">Part A</cell><cell cols="2">Part B</cell></row><row><cell></cell><cell cols="4">MAE RMSE MAE RMSE</cell></row><row><cell>MCNN [5]</cell><cell cols="3">110.2 173.2 26.4</cell><cell>41.3</cell></row><row><cell>CSRNet [9]</cell><cell>68.2</cell><cell cols="2">115.0 10.6</cell><cell>16.0</cell></row><row><cell>TEDNet [27]</cell><cell>64.2</cell><cell>109.1</cell><cell>8.2</cell><cell>12.8</cell></row><row><cell>CAN [12]</cell><cell>62.3</cell><cell>100.0</cell><cell>7.8</cell><cell>12.2</cell></row><row><cell>HA-CCN [29]</cell><cell>62.9</cell><cell>94.9</cell><cell>8.1</cell><cell>13.4</cell></row><row><cell>SFANet [21]</cell><cell>59.8</cell><cell>99.3</cell><cell>6.9</cell><cell>10.9</cell></row><row><cell>BL [3]</cell><cell>62.8</cell><cell>101.8</cell><cell>7.7</cell><cell>12.7</cell></row><row><cell>MBTTBF-SCFB [31]</cell><cell>60.2</cell><cell>94.1</cell><cell>8.0</cell><cell>15.5</cell></row><row><cell>S-DCNet [20]</cell><cell>58.3</cell><cell>95.0</cell><cell>6.7</cell><cell>10.7</cell></row><row><cell>SANet+SPANet [13]</cell><cell>59.4</cell><cell>92.5</cell><cell>6.5</cell><cell>9.9</cell></row><row><cell cols="5">M-SFANet w/o CAN [12] 62.41 101.13 7.40 12.14</cell></row><row><cell cols="5">M-SFANet w/o ASPP [14] 61.25 102.37 7.67 13.28</cell></row><row><cell cols="5">M-SFANet w/o skip conn. 60.07 99.47 7.34 12.10</cell></row><row><cell>M-SFANet*</cell><cell cols="4">62.49 106.11 6.38 10.22</cell></row><row><cell>M-SegNet</cell><cell cols="4">60.55 100.80 6.80 10.41</cell></row><row><cell>M-SFANet</cell><cell cols="4">59.69 95.66 6.76 11.89</cell></row><row><cell>M-SFANet+M-SegNet</cell><cell cols="4">57.55 94.48 6.32 10.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell cols="3">COMPARISON WITH STATE-OF-THE-ART METHODS ON UCF_CC_50 [33]</cell></row><row><cell>Method</cell><cell cols="2">MAE RMSE</cell></row><row><cell>CSRNet [9]</cell><cell>266.1</cell><cell>397.5</cell></row><row><cell>TEDNet [27]</cell><cell>249.4</cell><cell>354.5</cell></row><row><cell>CAN [12]</cell><cell>212.2</cell><cell>243.7</cell></row><row><cell>HA-CCN [29]</cell><cell>256.2</cell><cell>348.4</cell></row><row><cell>SFANet [21]</cell><cell>219.6</cell><cell>316.2</cell></row><row><cell>BL [3]</cell><cell>229.3</cell><cell>308.2</cell></row><row><cell cols="2">MBTTBF-SCFB [31] 233.1</cell><cell>300.9</cell></row><row><cell>SANet+SPANet [13]</cell><cell>232.6</cell><cell>311.7</cell></row><row><cell>S-DCNet [20]</cell><cell>204.2</cell><cell>301.3</cell></row><row><cell>M-SegNet</cell><cell cols="2">188.40 262.21</cell></row><row><cell>M-SFANet</cell><cell cols="2">162.33 276.76</cell></row><row><cell cols="3">M-SFANet+M-SegNet 167.51 256.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>WITH STATE-OF-THE-ART METHODS ON WE [26] DATASET Method Sce.1 Sce.2 Sce.3 Sce.4 Sce.5 Ave. DSSINet [41] 1.57 9.51 9.46 10.35 2.49 6.67 M-SegNet 1.45 11.72 10.29 21.15 5.47 10.03</figDesc><table><row><cell>MCNN [5]</cell><cell>3.4</cell><cell cols="2">20.6 12.9</cell><cell>13.0</cell><cell>8.1</cell><cell>11.6</cell></row><row><cell>CSRNet [9]</cell><cell>2.9</cell><cell>11.5</cell><cell>8.6</cell><cell>16.6</cell><cell>3.4</cell><cell>8.6</cell></row><row><cell>CAN [12]</cell><cell>2.9</cell><cell cols="2">12.0 10.0</cell><cell>7.9</cell><cell>4.3</cell><cell>7.4</cell></row><row><cell>PGCNet [40]</cell><cell>2.5</cell><cell>12.7</cell><cell>8.4</cell><cell>13.7</cell><cell>3.2</cell><cell>8.1</cell></row><row><cell>M-SFANet</cell><cell cols="3">1.88 13.24 10.07</cell><cell>7.5</cell><cell cols="2">3.87 7.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V COMPARISON</head><label>V</label><figDesc>WITH STATE-OF-THE-ART METHODS ON BEIJING BRT<ref type="bibr" target="#b34">[35]</ref> </figDesc><table><row><cell>DATASET</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">MAE RMSE</cell></row><row><cell>MCNN [5]</cell><cell>2.24</cell><cell>3.35</cell></row><row><cell>ResNet-14 [35]</cell><cell>1.48</cell><cell>2.22</cell></row><row><cell>DR-ResNet [35]</cell><cell>1.39</cell><cell>2.00</cell></row><row><cell>M-SegNet</cell><cell>1.26</cell><cell>1.98</cell></row><row><cell>M-SFANet</cell><cell>1.16</cell><cell>1.90</cell></row><row><cell cols="2">M-SFANet+M-SegNet 1.15</cell><cell>1.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc>WITH STATE-OF-THE-ART METHODS ON UCF-QNRF<ref type="bibr" target="#b3">[4]</ref> </figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VII COMPARISON</head><label>VII</label><figDesc></figDesc><table><row><cell cols="5">WITH STATE-OF-THE-ART METHODS ON TRANCOS [34]</cell></row><row><cell>Method</cell><cell cols="4">MAE GAME(1) GAME(2) GAME(3)</cell></row><row><cell>CSRNet [9]</cell><cell>3.56</cell><cell>5.49</cell><cell>8.57</cell><cell>15.04</cell></row><row><cell cols="2">ADCrowdNet [19] 2.44</cell><cell>4.14</cell><cell>6.78</cell><cell>13.58</cell></row><row><cell>S-DCNet [20]</cell><cell>2.92</cell><cell>4.29</cell><cell>5.54</cell><cell>7.05</cell></row><row><cell>M-SegNet</cell><cell>2.51</cell><cell>5.43</cell><cell>7.59</cell><cell>9.49</cell></row><row><cell>M-SFANet</cell><cell>2.23</cell><cell>3.46</cell><cell>4.86</cell><cell>6.91</cell></row><row><cell cols="2">M-SFANet+M-SegNet 2.22</cell><cell>3.87</cell><cell>5.51</cell><cell>7.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VIII THE</head><label>VIII</label><figDesc>PARAMETERS AND MACS OF OUR MODELS</figDesc><table><row><cell>Method</cell><cell cols="3">#Parameters(M) MACs(G) Training time (sec/batch)</cell></row><row><cell>CAN [12]</cell><cell>18.10</cell><cell>21.99</cell><cell>0.28</cell></row><row><cell>SFANet [21]</cell><cell>17.00</cell><cell>19.94</cell><cell>0.53</cell></row><row><cell>M-SegNet</cell><cell>9.75</cell><cell>18.14</cell><cell>0.46</cell></row><row><cell>M-SFANet</cell><cell>22.90</cell><cell>22.05</cell><cell>1.06</cell></row><row><cell>M-SegNet*</cell><cell>22.58</cell><cell>22.79</cell><cell>0.27</cell></row><row><cell>M-SFANet*</cell><cell>28.62</cell><cell>25.08</cell><cell>0.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE IX</head><label>IX</label><figDesc></figDesc><table><row><cell cols="5">INFERENCE TIME FOR DIFFERENT RESOLUTIONS IN MSEC</cell></row><row><cell>Method</cell><cell></cell><cell cols="2">Inference speed (ms)</cell><cell></cell></row><row><cell></cell><cell cols="4">320×240 640×480 1280×960 1600×1200</cell></row><row><cell>CAN [12]</cell><cell>15.9</cell><cell>54.9</cell><cell>226.0</cell><cell>370.7</cell></row><row><cell>SFANet [21]</cell><cell>16.6</cell><cell>60.2</cell><cell>245.3</cell><cell>365.5</cell></row><row><cell>M-SegNet</cell><cell>13.6</cell><cell>51.8</cell><cell>208.7</cell><cell>349.4</cell></row><row><cell>M-SFANet</cell><cell>20.0</cell><cell>70.9</cell><cell>263.6</cell><cell>392.9</cell></row><row><cell>M-SegNet*</cell><cell>15.1</cell><cell>51.99</cell><cell>211.5</cell><cell>332.6</cell></row><row><cell>M-SFANet*</cell><cell>20.0</cell><cell>68.0</cell><cell>270.3</cell><cell>418.6</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="743" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-source multi-scale counting in extremely dense crowd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2547" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bayesian loss for crowd count estimation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Composition loss for counting, density map estimation and localization in dense crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tayyab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Athrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Al-Maadeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Single-image crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards perspective-free object counting with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Onoro-Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>López-Sastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="615" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scale aggregation network for accurate and efficient crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive scenario discovery for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2382" to="2386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Context-aware crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning from synthetic data for crowd counting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adcrowdnet: An attention-injective deformable convolutional network for crowd understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">From open set to closed set: Counting objects by spatial divide-and-conquer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Dual path multiscale fusion networks with attention for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01115</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for segmentation under semantic constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Ganaye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sdika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Benoit-Cattin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="595" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE computer society conference on computer vision and pattern recognition (CVPR&apos;05)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bayesian poisson regression for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 12th international conference on computer vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="545" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Crowd counting and density estimation by trellis encoderdecoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Revisiting perspective information for efficient crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7279" to="7288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ha-ccn: Hierarchical attention-based crowd counting network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Top-down feedback for crowd counting convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-level bottom-top and topbottom feature fusion for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">People counting in high density crowds from still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Venkatesh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.08445</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Extremely overlapping vehicle counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guerrero-Gómez-Olmedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Torre-Jiménez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>López-Sastre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maldonado-Bascón</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Onoro-Rubio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Iberian Conference on Pattern Recognition and Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A deeply-recursive convolutional network for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">C 3 framework: An open-source pytorch code for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02724</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Lookahead optimizer: k steps forward, 1 step back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9593" to="9604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning spatial awareness to improve crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6152" to="6161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Perspective-guided convolution networks for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="952" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Crowd counting with deep structured scale integration network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
