<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Training Object Detectors from Few Weakly-Labeled and Many Unlabeled Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-11-02">2 Nov 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Yang</surname></persName>
							<email>zhaohuiyang@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Machine Intelligence</orgName>
								<orgName type="laboratory">Zhaohui Yang and Chao Xu are with Key Lab of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaojing</forename><surname>Shi</surname></persName>
							<email>miaojing.shi@kcl.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
							<email>xuchao@cis.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Machine Intelligence</orgName>
								<orgName type="laboratory">Zhaohui Yang and Chao Xu are with Key Lab of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
							<email>vittoferrari@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Vittorio Ferrari is with Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
							<email>yannis@avrithis.net</email>
							<affiliation key="aff2">
								<orgName type="department">Yannis Avrithis is with Inria</orgName>
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<region>IRISA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Training Object Detectors from Few Weakly-Labeled and Many Unlabeled Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-11-02">2 Nov 2020</date>
						</imprint>
					</monogr>
					<note type="submission">Preprint submitted to Pattern Recognition November 3, 2020</note>
					<note>2 Miaojing Shi is with King&apos;s College London.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Object detection</term>
					<term>weakly-supervised learning</term>
					<term>semi-supervised learning</term>
					<term>unlabelled set * Corresponding author</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly-supervised object detection attempts to limit the amount of supervision by dispensing the need for bounding boxes, but still assumes image-level labels on the entire training set. In this work, we study the problem of training an object detector from one or few images with image-level labels and a larger set of completely unlabeled images. This is an extreme case of semi-supervised learning where the labeled data are not enough to bootstrap the learning of a detector. Our solution is to train a weakly-supervised student detector model from image-level pseudo-labels generated on the unlabeled set by a teacher classifier model, bootstrapped by region-level similarities to labeled images. Building upon the recent representative weakly-supervised pipeline PCL [1], our method can use more unlabeled images to achieve performance competitive or superior to many recent weakly-supervised detection solutions. Code will be made available at https://github.com/zhaohui-yang/NSOD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The objective of visual object detection is to place a tight bounding box on every instance of an object class. With the advent of deep learning, recent methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> have significantly boosted the detection performance. Most are fully supervised, using a large amount of data with carefully annotated bounding boxes. However, annotating bounding boxes is expensive.</p><p>To reduce the amount of supervision, the most common setting is weaklysupervised object detection (WSOD) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. In this setting, we are given a set of images known to contain instances of certain classes as specified by labels, but we do not know the object locations in the form of bounding boxes or otherwise. Many works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11]</ref> formulate weakly supervised object detection as multiple instance learning (MIL) <ref type="bibr" target="#b11">[12]</ref>, which has been extended to be learnable end-to-end <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>There are mixed approaches where a small number of images are annotated with bounding boxes and labels, and a large amount of images have only imagelevel labels <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. This is often referred as a semi-supervised setting <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref>, but there is no consensus.</p><p>Semi-supervised learning <ref type="bibr" target="#b15">[16]</ref> refers to using a small amount of labeled data and a large amount of unlabeled data. It is traditionally studied for classification <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>, with one class label per image and no bounding boxes. In object detection, this would normally translate to a small number of images having labels and bounding boxes, and a large number of images having no annotation at all. This problem has been studied for the case where the fully annotated data (with bounding boxes) are enough to train a detector in the first place <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, resulting in two-stage learning. But what if these data are very scarce?</p><p>In this work, we study object detection in the challenging setting where only one or few images per class are given with only image-level class label per image, and a large amount of images with no annotation at all. We use no bounding boxes or other information. This setting is illustrated in <ref type="figure" target="#fig_1">Fig. 1</ref>  exploration can be found in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> before deep learning. The few weaklylabeled images can be obtained via either labeling images from an unlabeled collection <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> or using the top-ranking images from web image search with the class name as the query <ref type="bibr" target="#b23">[24]</ref>. Both paradigms are studied in our work. The latter is preferable as it requires no human effort.</p><p>Our deep learning solution is called nano-supervised object detection (NSOD).</p><p>It begins by computing region-level class scores based on the similarity between the unlabeled images and the few weakly-labeled images, which we then pool into image-level class probabilities. This yields image-level pseudo-labels on the entire unlabeled set, which we use to train a teacher model on a classification task. Then, by predicting new image-level multi-class pseudo-labels on the unlabeled set, we train a student model on a detection task, using a weaklysupervised object detection pipeline.</p><p>Contributions. We study the very challenging problem of training an object detector from few images with only image-level labels and many images with no annotation at all. We introduce a new method for this problem that is simple, efficient (cost comparable to standard WSOD), and modular (can build on any WSOD pipeline). By using the recent pipeline of PCL <ref type="bibr" target="#b0">[1]</ref> and more unlabeled images, we achieve performance competitive or superior to many recent WSOD solutions. On PASCAL VOC 2007 test set for instance, using 20 web images per class, we get a detection mAP of 42, compared to 43.5 of PCL, which is using image-level labels on the entire training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Weakly supervised object detection (WSOD).</head><p>In this setting, all training images have image-level class labels. A classic approach is multiple instance learning (MIL) <ref type="bibr" target="#b11">[12]</ref>, considering each training image as a "bag" and iteratively selecting high-scoring object proposals from each bag, treating them as ground truth to learn an object detector.</p><p>Bilen and Vedaldi <ref type="bibr" target="#b4">[5]</ref> introduce weakly-supervised deep detection network (WSDDN), which pools region-level scores into image-level class probabilities and enables end-to-end learning from image-level labels. Tang et al . <ref type="bibr" target="#b6">[7]</ref> extend WSDDN to multiple instance detection network including online instance classifier refinement (OICR) and introduce a weakly-supervised region proposal network as a plugin <ref type="bibr" target="#b7">[8]</ref>. In proposal cluster learning (PCL) <ref type="bibr" target="#b0">[1]</ref>, pre-clustering of object proposals followed by OICR accelerates learning and boosts performance.</p><p>Zeng et al . <ref type="bibr" target="#b24">[25]</ref> propose a novel WSOD framework with objectness distillation by jointly considering bottom-up and top-down objectness from low-level measurement and CNN confidences with an adaptive linear combination. Wan et al . <ref type="bibr" target="#b25">[26]</ref> introduce a min-entropy model to learn object locations and a metric to measure the randomness of object localization during learning. Ren et al . <ref type="bibr" target="#b9">[10]</ref> employ an instance-aware self-training strategy for WSOD with Concrete Drop-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Block.</head><p>Besides improvements in the network architecture, there are also attempts to incorporate additional cues into WSOD that are still weaker than bounding boxes, e.g. object size <ref type="bibr" target="#b5">[6]</ref> and count <ref type="bibr" target="#b26">[27]</ref>. It is also common to use extra data to transfer knowledge from a source domain and help localize objects in the target domain <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. Large-scale weakly-labelled web images <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> and videos <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> with noisy labels are also common as extra data.</p><p>Our problem is different from WSOD in that the few labeled images have no bounding boxes and the bulk of the training set is completely unlabeled. We build our work on PCL <ref type="bibr" target="#b0">[1]</ref> but train it with image-level pseudo-labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Semi-supervised learning.</head><p>There are several works that assume a few images are annotated with object bounding boxes and the rest still have image-level labels as in WSOD <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>These are often called semi-supervised <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref>. However, semi-supervised may also refer to the situation where some images are labeled (at image-level or with bounding boxes) and the rest have no annotation at all <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. This situation is consistent with the standard definition of semi-supervised learning <ref type="bibr" target="#b15">[16]</ref>.</p><p>Despite advances in deep semi-supervised learning <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>, most work focuses on classification tasks. In pseudo-label <ref type="bibr" target="#b17">[18]</ref> for instance, classifier predictions on unlabeled data are used as labels along with true labels on labeled data. Few exceptions focusing on detection <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> still assume enough labeled images to learn a detector in the first place, which is not the case in our work.</p><p>Dong et al . <ref type="bibr" target="#b36">[37]</ref> use few images with object bounding boxes and class labels along with many unlabeled images. However, this method relies on several models and iterative training, which is computationally expensive. By contrast, we develop an efficient solution that allows us to use more unlabeled images.</p><p>Shi et al . <ref type="bibr" target="#b21">[22]</ref> use a mixture of weakly-labeled images and unlabeled images for object detection. This method involves hand-crafted features and iterative message passing, which would not be straightforward or efficient to extend to a deep learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Curated data.</head><p>Investigation of unsupervised settings relies on removing the labels from labeled datasets by default. This is the case e.g. for object discovery <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> and semi-supervised classification until today <ref type="bibr" target="#b39">[40]</ref>. Such datasets are curated, i.e., still depict the same classes and are more or less balanced. Working with unknown classes is a different problem of open-set recognition <ref type="bibr" target="#b40">[41]</ref>. At very large  <ref type="figure" target="#fig_5">Figure 2</ref>: Overview of our nano-supervised object detection (NSOD) framework. We are given a support set G and a large unlabeled set X. G contains one or few weaklylabeled images per class, obtained from the web or randomly labeled from X. Using the images in G and a feature extractor pre-trained on classification, we infer image-level class probabilities of images in X (stage 1). We then extract pseudo-labels on X and train a teacher network T on a C-way classification task (stage 2). T is used to classify each proposal of images in X, resulting in new image-level class probabilities (stage 3).</p><p>We average these with the ones obtained in stage 1, based on G. Finally, we extract multi-class pseudo-labels on X and train a student network U on weakly-supervised detection by PCL <ref type="bibr" target="#b0">[1]</ref> (stage 4).</p><p>scale, keeping the top-ranking examples according to predicted class scores may be enough to address this problem <ref type="bibr" target="#b41">[42]</ref>. We experiment on both curated and unlabeled data in the wild to show the robustness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Problem. We are given a support set G containing k images per class, each associated with an image-level label over C classes. We are also given an unlabeled set of images X, where each image depicts one or more instances of the C classes, along with background clutter. In a harder setting, images in X may depict zero or more instances of the C classes, along with instances of unknown classes or background clutter. There is no bounding box or any other information in either G or X. Using these data and a feature extractor φ pre-trained on classification, the problem is to learn a detector to recognize instances of the C classes and localize them with bounding boxes in new images.</p><p>Motivation. This problem relates to both weakly-supervised detection and semi-supervised classification. Similar to the former, we study multiple instance learning but without image-level labels in the unlabeled set. Unlike the latter, at least in its common setting where thousands of examples are used <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b34">35]</ref>, G is too small to bootstrap the learning of a good classifier or detector: k can be as few as one example per class. For this reason, we propagate labels from G to X to initiate training.</p><p>Method overview. As shown in <ref type="figure" target="#fig_5">Fig. 2</ref>, we begin by collecting the support set G (Sec. 4.1). We extract object proposals <ref type="bibr" target="#b42">[43]</ref> from images in X and compare region-level features obtained by a feature extractor φ against global features on G. We estimate class probabilities on X by propagating these similarities to image level (stage 1, Sec. 3.2). We infer pseudo-labels on X and train a teacher network T inherited from φ on a C-way classification task (stage 2, Sec. 3.3).</p><p>We use T to classify regions in images of X, resulting in new image-level class probabilities (stage 3), which we average with the ones of stage 1. Finally, we infer multi-class pseudo-labels on X and train a student network U on a WSOD task by PCL <ref type="bibr" target="#b0">[1]</ref> (stage 4).</p><p>Collecting the support set G. The support set can be obtained either by random selection from some existing dataset or by web image search. The latter is preferable as we would like images to be clean, e.g. depicting only one class per image. We experiment with both options.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Inferring class probabilities on X</head><p>Given the support set G and corresponding labels, we begin by propagating the label information from G to the unlabeled set X. For each image x in X,</p><p>we use edge boxes <ref type="bibr" target="#b42">[43]</ref> to extract a collection of R object proposals (regions).</p><p>Ideally, we would like to have one label per region so we can train an object detector. Since the supervision in our case is very limited, it is not realistic to assign an accurate label per region based only on G. Instead, it is more reliable to estimate image-level class probabilities on X. Inspired by the two-stream CNN architecture of WSDDN <ref type="bibr" target="#b4">[5]</ref>, we introduce a new way to infer image-level probabilities on X, by aggregating region-level class probabilities.</p><p>Similarity. We extract a feature vector φ(r) for each region r of image x. We do the same for each image g in G, extracting a feature vector φ(g). This is a global feature vector. Let G j be the support images labeled as class j, with</p><formula xml:id="formula_0">|G j | = k. Let also r i be the i-th region of x. We define the R × C similarity matrix S = {s ij } with elements s ij := 1 k g∈Gj c(φ(r i ), φ(g)),<label>(1)</label></formula><p>where c denotes cosine similarity.</p><p>Voting. Inspired by <ref type="bibr" target="#b4">[5]</ref>, we form R × C classification matrix σ cls (S) with each row being the softmax of the same row of S, implying competition over classes per region; similarly, we form R × C detection matrix σ det (S) with each column being the softmax of the same column of S, implying competition over regions per class:</p><formula xml:id="formula_1">σ cls (S) ij := e sij C j=1 e sij , σ det (S) ij := e sij R i=1 e sij .<label>(2)</label></formula><p>The i-th row of σ cls (S) expresses a vector of class probabilities for region r i , while the j-th column of σ det (S) a vector of region probabilities (spatial distribution) for class j.</p><p>The final image-level class scores σ(S) are obtained by element-wise product of σ cls (S) and σ det (S) followed by sum pooling over regions</p><formula xml:id="formula_2">σ(S) j := R i=1 σ cls (S) ij σ det (S) ij .<label>(3)</label></formula><p>Each score σ(S) j is in [0, 1] and can be interpreted as the probability of object class j occurring in image x.</p><p>Discussion. The above is a robust voting strategy which propagates proposallevel information to the image level, while suppressing noise. Formula (1) suggests that region r i will respond for class j if it is similar to any of the support images in G j . While this response is noisy since it is only based on a few examples, it is only maintained if it is among the strongest over all classes and all regions in an image. Note that in <ref type="bibr" target="#b4">[5]</ref>, softmax is applied to two separate data streams during learning, whereas it is applied to the same matrix in our work.</p><p>Alternative ways to transfer label information from G to X would be to directly learn a parametric classifier on G or define a nearest-neighbor classifier on G and infer image-level labels on X. We consider such baselines in our experiments. Their performance is not satisfactory, which highlights the importance of robustly propagating labels from region to image level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Teacher and student training</head><p>Having class probability vectors (3) per image in X, a next step would be to convert them to multi-class pseudo-labels and train the student directly on a weakly-supervised detection task. Nevertheless, probabilities generated this way rely on the few support images in G for classification, while the object information in the unlabeled set X is not exploited. To further enhance performance, we use distillation <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b20">21]</ref> to transfer knowledge between data (labeled to unlabeled) and models (classification to detection). In particular, we distill knowledge from the support set G to the unlabeled set X using a teacher classifier T , and then distill this knowledge from the teacher to a student detector U .</p><p>Data distillation. We form the teacher T as the feature extractor network φ followed by a randomly initialized C-output fully-connected layer and softmax.</p><p>We then fine-tune T on a C-way classification task on X. The probabilities <ref type="formula" target="#formula_2">(3)</ref> are meant for multi-label classification (C independent binary classifiers), while here we are learning a single C-way classifier, i.e. for mutually exclusive labels.</p><p>Given the class probability vector σ(S) for each image x in X, we take the most likely class arg max j σ(S) j as a C-way pseudo-label. We fine-tune T on these pseudo-labels with a standard cross-entropy loss.</p><p>We have also tried several multi-label variants <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>, which are inferior to the simple C-way cross-entropy loss in our experiments. This may be attributed to the class sample distribution in X being unbalanced.</p><p>Knowledge distillation. The fine-tuned teacher T encodes object information of X into its network parameters. Directly using its image-level predictions on X would not be appropriate to train the student U for detection, because the latter would need multi-class labels. On the other hand, using it as feature extractor to repeat the process of Sec. 3.2 would not make much difference either, as it still produces class probabilities based on G. Instead, we use T to directly classify object proposals in X. Each proposal ideally contains one object, so it is particularly suitable to use T as it was designed: a C-way classifier.</p><p>Given an input image x in X, we collect output class probabilities of T on </p><formula xml:id="formula_3">each region r i of x into a R × C matrix A with element a ij</formula><p>corresponding to image x.</p><p>An image-level multi-class pseudo-labelŷ ∈ {0, 1} C is then obtained fromq by element-wise thresholding. An elementŷ j = 1 specifies that an object of class j occurs in image x. In the absence of prior knowledge or validation data, we choose 1 2 as threshold. Importantly, an all-negative pseudo-labelq = (0, . . . , 0) is possible, e.g. when an image does not depict any known class. This simple mechanism allows our method to work in the harder setting where images in X may depict only unknown classes.</p><p>Those image-level pseudo-labels are all that is needed to obtain an object detector if we use any WSOD pipeline. In particular, we train the student model U on weakly-supervised detection on X using proposal cluster learning (PCL) <ref type="bibr" target="#b0">[1]</ref>. Weakly-labeled images in G are also included into the training with loss weight 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference.</head><p>At inference, the teacher classifier is not needed. The trained student detector is used directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>Unlabeled set X. We choose the standard object detection datasets PASCAL  <ref type="figure" target="#fig_3">Fig. 3 (top)</ref>). Notwithstanding, they are not perfect, lacking diverse appearance and poses of the object class. Collecting images from the web is easy and does not need any human effort. We choose this option by default.</p><p>Another common option is to randomly sample k images per class from an existing collection <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> (e.g. VOC 2007). This is a harder setting, as these images may depict small objects, multiple instances, object classes in non-canonical pose, clutter and occlusion, e.g. bottle, chair, and person in <ref type="figure" target="#fig_3">Fig. 3</ref> (bottom). We experiment with both options.</p><p>Networks. We choose VGG16 <ref type="bibr" target="#b47">[48]</ref> as our student U by default, which is consistent with most WSOD methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b0">1]</ref>. Since the teacher network T (including the feature extractor φ) is not used at inference time, we choose the more powerful ResNet-152 <ref type="bibr" target="#b48">[49]</ref>. Both networks are pre-trained on the ILSVRC classification task <ref type="bibr" target="#b49">[50]</ref>.</p><p>Implementation details. We use k = 20 images per class by default for G.</p><p>Following representative WSOD methods [5, 7, 1, 51], we adopt edge boxes <ref type="bibr" target="#b42">[43]</ref> to extract 2000 proposals on average per image in X. For the default teacher model T, we first resize the input image to 256 pixels on the short side and then crop it to 224 × 224. We set the batch size to 128 and the learning rate to 10   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Support set G by web search</head><p>We first collect the support set by web search and evaluate our NSOD on both VOC 2007 and 2012. We also combine the two sets as well as images from</p><p>ImageNet as distractors to evaluate our method in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Results on VOC 2007</head><p>Comparison to weakly-supervised methods. We compare to several representative WSOD methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b8">9]</ref> in <ref type="table">Table 1</ref>. For fair comparison, all these methods use the same VGG16 backbone as we do, without bells and whistles.</p><p>NSOD requires no annotation on the unlabeled set X, while weakly-supervised methods assume image-level labels for all images in X.</p><p>One directly competing method is PCL trained on ground truth image-level labels in X. Despite using no annotation on X, NSOD achieves an mAP that is only 5.5% below that of PCL (38.0 vs. 43.5). The result is is also competitive to other methods, e.g. OICR <ref type="bibr" target="#b6">[7]</ref>, WSRPN <ref type="bibr" target="#b0">[1]</ref>. There are also WSOD methods employing large-scale web images/videos as extra data. For instance, <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b31">[32]</ref> build on the WSDDN pipeline <ref type="bibr" target="#b4">[5]</ref> and produce mAP 36.8 and 39.4 on VOC 2007, respectively. Unlike these works, our NSOD uses few web images, an unlabeled set, and an advanced WSOD pipeline. Importantly, NSOD also delivers competitive mAP. <ref type="figure" target="#fig_6">Fig. 4</ref> gives some examples of detection results of NSOD on PASCAL VOC 2007.</p><p>Comparison to semi-supervised methods. We first compare NSOD to two semi-supervised baselines: (1) fine-tune the teacher T on G as a C-way classifier and use it to make predictions on X, referred to as NS-FT; (2) use φ from T as a global feature extractor to find the nearest neighbor in G for each image in X, referred to as NS-NN. In both cases, we use the same support set with NSOD, infer image-level C-way pseudo-labels on X and use them to train the student U by PCL. As shown in <ref type="table">Table 1</ref>, NS-FT and NS-NN deliver a mAP of 27.9 and 28.3, respectively. Comparing to the mAP 38.0 of NSOD on the same setting (k = 20), these baselines are not satisfactory. This is due to the limited the supervision from the support set and justifies our choice of propagating labels from region to image level.</p><p>We then adapt the mean teacher <ref type="bibr" target="#b35">[36]</ref> semi-supervised classification method to our setting. Using image-level class probabilitiesq (4), we select the topz scored images as positive for each class and the rest we treat as negative.</p><p>With those pseudo-labels, we train PCL on VGG16, applying the consistency loss of <ref type="bibr" target="#b35">[36]</ref> to image-level predictions on X. We call this nano-supervised mean reacher (NS-MT). We choose z = 300 as it works the best in practice. NS-MT   then yields an mAP of 27.0 as shown in <ref type="table">Table 1</ref>. This result is lower than our NSOD by 11.0% (27.0 vs. 38.0). This suggests it is not straightforward to transfer a successful semi-supervised approach from the classification to the detection task.</p><p>We have also tried to directly infer object bounding boxes on the test set of X using naive approaches like k-NN classification on regions directly. Those fail, producing mAP lower than 10. We should emphasize the importance of propagating similarity scores from region-level to image-level as we do in NSOD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Results on VOC 2012</head><p>Using the same support set G, we train an object detector with our NSOD on VOC 2012. The mAP is reported on the test set of VOC 2012 and compared to representative WSOD methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b50">51]</ref> in <ref type="table" target="#tab_5">Table 2</ref>. Despite not using any VOC 2012 labels, NSOD is only 4.0% below PCL (36.6 vs. 40.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Results on VOC 2007 + 2012</head><p>Because X is unlabeled and our method is computationally efficient, we can easily improve performance by simply using more unlabeled data. As shown in <ref type="table">Table 1</ref>  Since neither set is labeled, this improvement comes at almost no cost. This result is only 1.5% below PCL (42.0 vs. 43.5), and even outperforms WSDDN <ref type="bibr" target="#b4">[5]</ref> and OICR <ref type="bibr" target="#b6">[7]</ref> when trained on VOC 2007 with image-level labels. This is a strong result that confirms the value of our core contribution; similarly, on VOC 2012, NSOD (07+12) increases the mAP to 38.6, now outperforming OICR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">Results on PASCAL VOC + Distractors</head><p>Despite being used without labels, VOC 2007 and 2012 are still curated, i.e. images depict at least one of the target classes. To further validate the effectiveness of our methd, we experiment with unlabeled data in the wild for X, i.e., using images depicting unknown rather than target classes. In particular, we randomly select 5k (10k) images from ImageNet <ref type="bibr" target="#b49">[50]</ref> and use the union of this set and VOC 2007, denoted by 07+Dis5k (07+Dis10k), as X. Although there may be overlap between the 1000 ImageNet classes and the 20 PASCAL VOC classes, these images mostly contain unknown classes and play the role of distractors. The evaluation is on the test set of VOC 2007. As shown in <ref type="table" target="#tab_6">Table 3</ref>    are mostly assigned no pseudo-labels due to thresholding ofq (4) in NSOD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Support set by sampling VOC 2007</head><p>As discussed in Sec. 4.1, the support set G can be collected by randomly selecting k images per class from the unlabeled set X. This is more challenging than web search, as one image may depict more than one object, as shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>We conduct the ablation study on our labeling strategy, support set size, and localization on the trainval set of PASCAL VOC 2007. The support set G is collected by web search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Labeling strategy (classification)</head><p>Referring to subsection 3.3 in the paper, we ablate combining σ(S) and σ(A)</p><p>to generate image-level pseudo-labels. σ(S) is computed based on G alone, while σ(A) is computed based on the teacher model trained on X. We apply a hard  threshold of 1 2 on the predicted class probabilities of σ(S) and σ(A) to generate two sets of image-level pseudo-labels. We train two different models separately on the two sets of pseudo-labels, which we denote by NSOD G and NSOD X , respectively.</p><p>The classification accuracy of the two sets of pseudo-labels is first evaluated on the trainval set of VOC 2007 and shown in <ref type="table" target="#tab_9">Table 4</ref>. It can be seen that NSOD G and NSOD X produce a similar classification mAP of 76.3 vs. 76.7, while the AP on individual classes differs. However, in terms of top-1 class accuracy, NSOD X is better than NSOD G . This is reasonable, as NSOD X is fine-tuned as a C-way classifier, which takes the top-1 class predictions of σ(S) as pseudo-labels. The two sets of pseudo-labels are complementary by averaging σ(S) and σ(A) according to Eq.(4), denoted by NSOD. This improves both multi-class and top-1 class predictions, reaching the highest scores of 79.2 and 85.9, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">Labeling strategy (detection)</head><p>To further investigate the complementary effect of NSOD G and NSOD X , we evaluate their detection result on the test set of VOC 2007 ( <ref type="table" target="#tab_11">Table 5</ref>). The mAP of NSOD X (34.5) is slightly greater than that of NSOD G (33.9). Their combination (our full method NSOD) further increases mAP by +3.5% to 38.0.</p><p>The detection result on the test set is consistent with the classification result on the trainval set, which validates our idea of distilling knowledge from the support set to the unlabeled set and from the teacher to the student model.   <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b8">9]</ref> use the image-level labels in X; our NSOD does not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">Support set size</head><p>We evaluate performance for different number k of web images per class of the support set G in <ref type="table" target="#tab_11">Table 5</ref>: mAP is 30.0 for k = 1, 33.2 for k = 10 and 38.0 for k = 20. Further increasing k presumably brings more noisy examples. How to deal with large-scale noisy web images/videos is an open problem <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>We keep G small to avoid bringing too many noisy images, while at the same time using the unlabeled unlabeled set X for more diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4.">Localization on the trainval set</head><p>Apart from the mAP on the test set, in <ref type="table" target="#tab_13">Table 6</ref> we report CorLoc on the trainval set of VOC 2007, as is common for weakly-supervised detection methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b8">9]</ref>. Our NSOD delivers CorLoc 56.6, which is very close to other WSOD methods despite using no annotations on X. Like in subsubsec-  <ref type="table" target="#tab_13">Table 6</ref>) is increased to 60.0, which is only 2.7% below PCL (62.7) and generally among the best-performing WSOD methods (e.g. OICR has 60.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Our nano-supervised object detection framework (NSOD) basically begins with a combination of few-shot and semi-supervised classification. The former is using the few images as class prototypes <ref type="bibr" target="#b51">[52]</ref> to estimate class probabilities per region, which are propagated at image level using the voting process of WSDDN <ref type="bibr" target="#b4">[5]</ref>. The latter is generating pseudo-labels on the unlabeled set from these probabilities to train a classifier <ref type="bibr" target="#b17">[18]</ref>.</p><p>By using the PCL pipeline <ref type="bibr" target="#b0">[1]</ref> and extending the unlabeled set to both VOC 2007 and VOC 2012, our NSOD achieves detection mAP very close to PCL itself trained on VOC 2007 with image-level labels. Moreover, our result is already competitive or superior to many recent WSOD solutions.</p><p>It is reasonable to expect further improvement by applying our method to very large unlabeled collections. This is facilitated by the fact that NSOD is robust to unknown classes and can discover relevant data even among non-curated collections. Moreover, since NSOD produces image-level pseudo-labels that can be used to train any weakly-supervised detection pipeline, further improvement could be expected by using these pseudo-labels with more advanced WSOD methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>We learn an object detector from a set of completely unlabeled images and one or few images per class with image-level label per image and no other information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>being the probability of class j. From this matrix, it is possible to estimate new image-level class probabilities by σ(A), similar to (3). Because it is based on T being trained on X as classifier, while σ(S) (3) is based on G alone, we combine their strength by averaging both into a probability vector q := 1 2 (σ(S) + σ(A))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>(Top) examples of top-ranking web images, using class names as queries. (Bottom) random selection of images from PASCAL VOC 2007.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 :</head><label>1</label><figDesc>Detection mAP on the test set of PASCAL VOC 2007. NSOD: our nano-supervised object detection framework; NS-FT: nano-supervised fine-tuning; NS-NN: nano-supervised nearest neighbor; NS-MT: Nano-supervised mean teacher. Unless otherwise stated, NSOD, NS-FT, NS-NN use k = 20 support images per class by default. All compared methods [5, 7, 8, 1, 9] use the image-level labels in the unlabeled set X; NSOD, NS-FT, NS-NN and NS-MT do not. the test set by mAP. At test time, the detector can localize multiple instances of the same class per image and mAP is identical to what is used to evaluate fully supervised object detectors. Evaluation scenarios. Below, we first present the object detection results on the test set of X under two scenarios: support set G by web search (subsection 4.2) and by sampling VOC 2007 (subsection 4.3). Then in the ablation study (subsection 4.4), we provide detection and classification results on the trainval set of X using the web search scenario.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>and 2 ,</head><label>2</label><figDesc>if we train NSOD on the union of VOC 2007 and VOC 2012 (07+12) on a large-scale, the mAP can be further improved on the test set of both VOC 2007 and 2012. For instance, on VOC 2007, NSOD (07+12) yields a mAP of 42%, which is an improvement by +4% over using VOC 2007 alone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Detection results of NSOD on PASCAL VOC 2007, using default settings (k = 20). Top 2 rows: positive results (red boxes). Bottom row: failure cases (white boxes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 3 .Figure 5 :</head><label>35</label><figDesc>We randomly sample k ∈ {1, 5, 10, 20, 40, 60, 80, 100} images per class from VOC 2007 with image-level labels as G and evaluate on its test set. We compare NSOD with two baselines: (1) only using G to train the student U , Detection mAP of NSOD, NS-Base, NS-FT and PCL on PASCAL VOC 2007, using different number k of images per class as support set. denoted by NS-Base; (2) using NS-FT as described in Sec. 4.2.1. As shown in Figure 5, NSOD yields significantly higher mAP at every k compared to the baselines. In particularly, with small k, our improvement is substantial; with k = 80 (around 30% of VOC 2007 training data), NSOD achieves accuracy already very close (on par) to PCL [1] (dotted horizontal line) that uses image-level labels of 100% data in VOC 2007.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>tion 4 . 2 . 3 ,</head><label>423</label><figDesc>if we train NSOD on the union of VOC 2007 and VOC 2012 (07+12) on a large-scale, the CorLoc of NSOD (07+12) on VOC 2007 (see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Person, Bicycle Teacher T Supervision classification scores Image scores Proposal scores Student U Extractor ROI align ROI align Supervision proposal s Pseudo labels G (small, labelled) X (unlabeled) sum Pretrained on ILSVRC Pretrained on ILSVRC Aeroplane Bicycle Bottle Bus Cat Chair Person … Web Stage 1, Inference Stage 2, Training Stage 3, Inference Stage 4, Training B (unlabeled) (proposals)</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Each image in the support set G should depict one of the known C classes (i.e. 20 VOC classes). A preferable way to collect G is from the web<ref type="bibr" target="#b23">[24]</ref>: we use the class names as text queries and collect the top-k results per</figDesc><table /><note>VOC 2007 and 2012 [47] for the unlabeled set, having 20 classes. Each dataset contains a trainval set and a test set. For VOC 2007, the trainval set has 5011 images and the test set 4952 images. For VOC 2012, the size of trainval and test sets are 11540 and 10991, respectively. We use the trainval sets as X to train the object detector by default. We evaluate the detector on the test set. Importantly, except for the support set, we do not use any labels, not even image-level labels in the training set. Support set G.class from web image search (e.g. Google). The motivation is that these images are clean, i.e. they mostly contain objects against a simple background and in a canonical pose and viewpoint, without clutter or occlusion (see examples in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>We evaluate the performance of our NSOD framework on both image classification and object detection. For image classification, we measure the average precision (AP) and mean AP (mAP) for multi-class predictions<ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>, as well as the accuracy of the top-1 class prediction per image on the trainval set of X. For object detection, we quantify localization performance on the trainval set by CorLoc [5, 6, 7, 51] and detection performance on Method aero bike bird boat bott bus car cat char cow tabl dog hors mbik prsn plat shep sofa tran tv mAP NSOD 57.9 59.7 43.2 10.5 13.1 62.7 58.6 43.9 10.6 51.1 25.7 49.8 39.3 60.6 14.9 10.9 33.5 45.2 42.5 27.8 38.0 NSOD (07+12) 51.5 65.2 48.9 13.2 19.7 64.8 59.3 55.5 12.4 59.3 24.3 54.1 47.4 62.8 20.7 15.0 39.5 51.3 53.8 21.4 42.0</figDesc><table><row><cell>NS-FT</cell><cell>56.7 37.2 31.8 10.7 4.6 44.7 42.7 51.4 3.5 17.7 4.2 37.6 22.5 51.6 13.1 10.0 28.9 36.3 39.2 14.3 27.9</cell></row><row><cell>NS-NN</cell><cell>59.2 33.3 28.3 22.5 5.4 43.7 39.3 32.3 2.3 40.1 7.5 42.2 34.2 33.2 12.6 7.7 30.5 31.1 47.6 13.7 28.3</cell></row><row><cell cols="2">NS-MT (z = 300) 49.6 33.9 29.6 15.5 9.5 47.9 32.9 49.1 0.2 13.2 21.1 34.4 19.7 31.5 9.6 9.9 35.6 43.1 38.9 15.0 27.0</cell></row><row><cell>WSDDN [5]</cell><cell>39.4 50.1 31.5 16.3 12.6 64.5 42.8 42.6 10.1 35.7 24.9 38.2 34.4 55.6 9.4 14.7 30.2 40.7 54.7 46.9 34.8</cell></row><row><cell>OICR [7]</cell><cell>58.0 62.4 31.1 19.4 13.0 65.1 62.2 28.4 24.8 44.7 30.6 25.3 37.8 65.5 15.7 24.1 41.7 46.9 64.3 62.6 41.2</cell></row><row><cell>WSRPN [8]</cell><cell>57.9 70.5 37.8 5.7 21.0 66.1 69.2 59.4 3.4 57.1 57.3 35.2 64.2 68.6 32.8 28.6 50.8 49.5 41.1 30.0 45.3</cell></row><row><cell>PCL [1]</cell><cell>54.4 69.0 39.3 19.2 15.7 62.9 64.4 30.0 25.1 52.5 44.4 19.6 39.3 67.7 17.8 22.9 46.6 57.5 58.6 63.0 43.5</cell></row><row><cell>WS-JDS [9]</cell><cell>52.0 64.5 45.5 26.7 27.9 60.5 47.8 59.7 13.0 50.4 46.4 56.3 49.6 60.7 25.4 28.2 50.0 51.4 66.5 29.7 45.6</cell></row></table><note>−3 initially with cosine decay. For the default student model U, we feed the network with one image per batch. The training lasts for 50, 000 iterations in total; the learning rate starts at 10 −5 and decays by an order of magnitude at 35, 000 iterations. Evaluation protocol.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Method aero bike bird boat bott bus car cat char cow tabl dog hors mbik prsn plat shep sofa tran tv mAP OICR [7] 67.7 61.2 41.5 25.6 22.2 54.6 49.7 25.4 19.9 47.0 18.1 26.0 38.9 67.7 2.0 22.6 41.1 34.3 37.9 55.3 37.9 ZLDN [51] 54.3 63.7 43.1 16.9 21.5 57.8 60.4 50.9 1.2 51.5 44.4 36.6 63.6 59.3 12.8 25.6 47.8 47.2 48.9 50.6 42.9 PCL [1] 58.2 66.0 41.8 24.8 27.2 55.7 55.2 28.5 16.6 51.0 17.5 28.6 49.7 70.5 7.1 25.7 47.5 36.6 44.1 59.2 40.6 NSOD 56.3 27.6 42.2 10.9 23.8 55.1 46.2 36.6 5.6 51.8 15.5 55.9 54.0 63.6 23.5 10.8 43.1 39.2 49.0 21.5 36.6 NSOD (07+12) 57.3 50.7 49.2 11.3 21.2 56.8 46.4 55.0 6.6 52.7 12.8 61.8 45.8 64.7 18.9 10.5 34.9 41.0 48.1 19.9 38.6</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Detection mAP on test set of PASCAL VOC 2012. Our NSOD uses k = 20 support images per class. All compared methods<ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b50">51]</ref> use the image-level labels in the unlabeled set X; our NSOD does not.</figDesc><table><row><cell>Method</cell><cell>aero bike bird boat bott bus car cat char cow tabl dog hors mbik prsn plat shep sofa tran tv mAP</cell></row><row><cell>NSOD (07+Dis5k)</cell><cell>59.3 35.4 37.6 16.6 7.5 59.1 59.0 42.2 9.0 47.4 33.2 50.8 46.3 52.4 15.1 18.7 44.2 50.3 51.6 35.3 37.6</cell></row><row><cell cols="2">NSOD (07+Dis10k) 56.5 36.0 34.6 12.7 5.7 56.6 56.2 40.1 8.5 44.9 31.1 46.0 41.6 55.1 15.7 15.1 39.9 46.8 47.6 31.2 36.5</cell></row><row><cell cols="2">NSOD (07+12+Dis5k) 59.8 65.8 50.1 12.5 16.5 58.6 52.1 57.0 15.8 51.1 31.5 53.9 36.4 58.8 18.1 15.4 43.3 50.4 48.1 38.8 41.7</cell></row><row><cell cols="2">NSOD (07+12+Dis10k) 51.4 68.1 36.1 11.8 17.7 59.6 63.1 61.8 10.2 46.5 32.1 57.0 37.1 61.3 17.7 17.1 44.0 47.7 44.9 33.0 40.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Detection mAP on the test set of PASCAL VOC 2007 in the presence of distractors.</figDesc><table /><note>NSOD: our object detection framework.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>). aero bike bird boat bott bus car cat char cow tabl dog hors mbik prsn plat shep sofa tran tv mAP NSODG 88.8 85.8 98.0 67.8 79.4 68.4 96.8 95.1 80.6 72.1 38.9 93.4 82.3 65.2 98.0 56.7 70.1 55.6 72.0 60.2 76.3 NSODX 86.4 96.9 97.1 71.4 98.5 67.1 89.9 95.1 80.0 66.8 36.5 92.9 74.2 62.9 96.9 53.1 59.9 58.8 70.1 78.9 76.7 NSOD 91.2 90.7 98.0 71.1 94.3 73.8 95.8 95.5 80.5 74.7 39.1 95.3 81.2 66.9 98.4 58.7 73.8 59.7 75.6 70.4 79.2 Method aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mAcc</figDesc><table><row><cell>Method NSODG 92.2 97.7 99.1 78.7 100.0 73.0 93.2 98.5 89.2 82.0 41.8 97.7 77.7 72.0 99.7 63.7 68.7 63.0 77.5 87.5 82.7</cell></row><row><cell>NSODX 93.1 93.4 98.2 79.2 100.0 78.9 96.3 96.7 84.0 83.5 45.1 95.7 84.2 72.9 98.5 73.3 77.2 66.1 83.3 87.7 84.3</cell></row><row><cell>NSOD 93.8 92.4 99.3 80.4 100.0 81.1 97.8 97.1 78.6 86.7 49.7 97.1 88.2 77.2 99.6 79.7 79.1 67.9 87.5 85.6 85.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Classification mAP for multi-class prediction (top) and classification mAcc for top-1 class prediction (bottom) on the trainval set of PASCAL VOC 2007. NSOD: our Nanosupervised object detection framework.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Method aero bike bird boat bott bus car cat char cow tabl dog hors mbik prsn plat shep sofa tran tv mAP NSODG 57.2 52.7 36.0 14.1 11.0 50.6 46.9 35.8 5.7 47.1 16.1 52.8 34.3 54.4 14.8 11.4 29.0 48.8 43.4 13.9 33.9 NSODX 58.5 51.5 37.5 11.6 10.6 55.3 48.2 40.4 5.8 49.9 16.0 51.3 31.6 56.3 14.6 9.0 34.3 45.5 42.2 20.3 34.5 NSOD 57.9 59.7 43.2 10.5 13.1 62.7 58.6 43.9 10.6 51.1 25.7 49.8 39.3 60.6 14.9 10.9 33.5 45.2 42.5 27.8 38.0 NSOD (k = 1) 53.0 58.0 24.4 13.3 11.3 41.3 43.8 43.6 2.3 50.3 6.1 32.4 19.0 50.5 15.0 8.7 35.7 41.7 42.8 6.2 30.0 NSOD (k = 10) 57.2 27.8 40.4 9.7 11.2 61.2 57.0 25.9 13.4 47.2 6.2 45.5 35.7 53.0 21.2 14.1 34.8 43.7 39.8 19.8 33.2 NSOD (k = 20) 57.9 59.7 43.2 10.5 13.1 62.7 58.6 43.9 10.6 51.1 25.7 49.8 39.3 60.6 14.9 10.9 33.5 45.2 42.5 27.8 38.0</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Ablation study. Detection mAP on the test set of PASCAL VOC 2007. NSOD: our nano-supervised object detection framework.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Method aero bike bird boat bott bus car cat char cow tabl dog hors mbik prsn plat shep sofa tran tv mAP WSDDN [5] 65.1 58.8 58.5 33.1 39.8 68.3 60.2 59.6 34.8 64.5 30.5 43.0 56.8 82.4 25.5 41.6 61.5 55.9 65.9 63.7 53.5 OICR [7] 81.7 80.4 48.7 49.5 32.8 81.7 85.4 40.1 40.6 79.5 35.7 33.7 60.5 88.8 21.8 57.9 76.3 59.9 75.3 81.4 60.6 WSRPN [8] 77.5 81.2 55.3 19.7 44.3 80.2 86.6 69.5 10.1 87.7 68.4 52.1 84.4 91.6 57.4 63.4 77.3 58.1 57.0 53.8 63.8 PCL [1] 79.6 85.5 62.2 47.9 37.0 83.8 83.4 43.0 38.3 80.1 50.6 30.9 57.8 90.8 27.0 58.2 75.3 68.5 75.7 78.9 62.7 WS-JDS [9] 82.9 74.0 73.4 47.1 60.9 80.4 77.5 78.8 18.6 70.0 56.7 67.0 64.5 84.0 47.0 50.1 71.9 57.6 83.3 43.5 64.5 NSOD 80.0 73.3 66.1 34.0 29.0 72.6 76.5 56.4 17.7 74.7 47.5 61.4 60.5 86.4 31.9 36.6 60.8 59.1 57.4 49.1 56.6 NSOD (07+12) 78.3 78.4 70.3 34.0 34.0 75.1 76.6 66.9 24.8 76.0 45.6 69.8 67.7 88.8 34.4 41.4 67.0 62.1 67.3 40.9 60.0</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 6 :</head><label>6</label><figDesc>CorLoc on the trainval set of PASCAL VOC 2007. All compared methods</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">PCL: Proposal cluster learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multi-fold mil training for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Weakly supervised object localization using size estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multiple instance detection network with online instance classifier refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Weakly supervised region proposal network and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cyclic guidance for weakly supervised joint detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Instance-aware, context-focused, and memory-efficient weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00873</idno>
		<title level="m">Transfer learning by ranking for weakly supervised object annotation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Support vector machines for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Large scale semi-supervised object detection using visual and semantic knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dellandréa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Detector discovery in the wild: Joint multiple instance and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Weakly-and semi-supervised object detection with expectation-maximization algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08740</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<title level="m">Semi-Supervised Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ICML</publisher>
			<biblScope unit="page" from="1168" to="1175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICMLW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semisupervised learning with ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Co-occurrence matrix analysis-based semi-supervised training for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06964</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<title level="m">Data distillation: Towards omni-supervised learning</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bayesian joint modelling for object localisation in weakly labelled images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1959" to="1972" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Drawing an automatic sketch of deformable objects using only a few images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marvaniya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Manickavasagam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning semantic part-based models from google images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Modolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1502" to="1509" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Wsod2: Learning bottom-up and top-down objectness distillation for weakly-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Min-entropy latent model for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">C-wsl: Count-guided weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Weakly supervised object localization using things and stuff transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Revisiting knowledge transfer for training object class detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1101" to="1110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Curriculumnet: Weakly supervised learning from large-scale web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploiting web images for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">You reap what you sow: Using videos to generate high precision object proposals for weakly-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Towards computational baby learning: A weakly-supervised approach for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ailon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01449</idno>
		<title level="m">Semi-supervised deep learning by metric embedding</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weightaveraged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Few-example object detection with model communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1641" to="1654" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised object discovery: A comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="284" to="302" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Discovering objects and their location in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
		<title level="m">Towards open set deep networks</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1563" to="1572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Billion-scale semisupervised learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Z</forename><surname>Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00546</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hcp: A flexible cnn framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1901" to="1907" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning spatial regularization with image-level supervisions for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for largescale image recognition</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<title level="m">Imagenet large scale visual recognition challenge</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Zigzag learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
