<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MTI-Net: Multi-Scale Task Interaction Networks for Multi-Task Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Vandenhende</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leuven</forename><forename type="middle">/</forename><surname>Ku</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Esat-Psi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zurich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">/</forename><surname>Cvl</surname></persName>
						</author>
						<title level="a" type="main">MTI-Net: Multi-Scale Task Interaction Networks for Multi-Task Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Multi-Task Learning, Scene Understanding</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we argue about the importance of considering task interactions at multiple scales when distilling task information in a multi-task learning setup. In contrast to common belief, we show that tasks with high affinity at a certain scale are not guaranteed to retain this behaviour at other scales, and vice versa. We propose a novel architecture, namely MTI-Net, that builds upon this finding in three ways. First, it explicitly models task interactions at every scale via a multiscale multi-modal distillation unit. Second, it propagates distilled task information from lower to higher scales via a feature propagation module. Third, it aggregates the refined task features from all scales via a feature aggregation unit to produce the final per-task predictions. Extensive experiments on two multi-task dense labeling datasets show that, unlike prior work, our multi-task model delivers on the full potential of multi-task learning, that is, smaller memory footprint, reduced number of calculations, and better performance w.r.t. single-task learning. The code is made publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and prior work</head><p>The world around us is flooded with complex problems that require solving a multitude of tasks concurrently. An autonomous car should be able to detect all objects in the scene, localize them, understand what they are, estimate their distance and trajectory, etc., in order to safely navigate itself in its surroundings. In a similar vein, an intelligent advertisement system should be able to detect the presence of people in its viewpoint, understand their gender and age group, analyze their appearance, track where they are looking at, etc., in order to provide personalized content. The examples are countless. Understandably, this calls for efficient computational models in which multiple learning tasks can be solved simultaneously.</p><p>Multi-task learning (MTL) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b36">37]</ref> tackles this problem. Compared to the single-task case, where each individual task is solved separately by its own network, multi-task networks theoretically bring several advantages to the table. First, due to their layer sharing, the resulting memory footprint is substantially reduced. Second, as they explicitly avoid to repeatedly calculate the features in the shared layers, once for every task, they show increased inference speeds. Most importantly, they have the potential for improved performance if the associated tasks share complementary information, or act as a regularizer for one another. Evidence for the former has been provided in the literature for certain pairs of tasks, e.g. detection and classification <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34]</ref>, detection and segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref>, segmentation and depth estimation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b46">47]</ref>, while for the latter recent efforts point to that direction <ref type="bibr" target="#b41">[42]</ref>.</p><p>Motivated by these observations, researchers started designing architectures capable of learning shared representations from multi-task supervisory signals. Misra et al. <ref type="bibr" target="#b30">[31]</ref> proposed to use "cross-stitch" units to combine features from multiple networks to learn a better combination of shared and task-specific representations. Kokkinos <ref type="bibr" target="#b18">[19]</ref> introduced a multi-head architecture called UberNet that jointly handles as many as seven tasks in a unified framework, which can be trained end-to-end. Zamir et al. <ref type="bibr" target="#b48">[49]</ref> modeled the structure of the visual tasks' space by finding transfer learning dependencies across a dictionary of twenty six tasks. Despite the progress reported by these or similar works <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b43">44]</ref>, the joint learning of multiple tasks can lead to single-task performance degradation if information sharing happens between unrelated tasks. The latter is known as negative transfer <ref type="bibr" target="#b51">[52]</ref>, and has been well documented in <ref type="bibr" target="#b18">[19]</ref>, where an improvement in estimating normals leads to a decline in object detection, or in <ref type="bibr" target="#b11">[12]</ref> where the multi-task version underperforms the single-task ones.</p><p>To remedy this situation, a group of methods carefully balance the losses of the individual tasks, in an attempt to find an equilibrium where no task declines significantly. For example, Kendall et al. <ref type="bibr" target="#b15">[16]</ref> used the homoscedastic uncertainty of each individual task to re-weigh the losses. Gradient normalization <ref type="bibr" target="#b4">[5]</ref> was proposed to balance the losses by adaptively normalizing the magnitude of each task's gradients. Similarly, Sinha et al. <ref type="bibr" target="#b40">[41]</ref> tried to balance the losses by adapting the gradients magnitude, but differently, they employed adversarial training to this end. Dynamic task prioritization <ref type="bibr" target="#b9">[10]</ref> proposed to dynamically sort the order of task learning, and prioritized 'difficult' tasks over 'easy' ones. Zhao et al. <ref type="bibr" target="#b51">[52]</ref> introduced a modulation module to encourage feature sharing among 'relevant' tasks and disentangle the learning of 'irrelevant' tasks. Sener and Koltun <ref type="bibr" target="#b37">[38]</ref> proposed to cast multi-task learning into a multi-objective optimization scheme, where the weighting of the different losses is adaptively changed such that a Pareto optimal solution is achieved.</p><p>In a different vein, Maninis et al. <ref type="bibr" target="#b28">[29]</ref> followed a 'single-tasking' route. That is, in a multi-tasking framework they performed separate forward passes, one for each task, that activate shared responses among all tasks, plus some residual responses that are task-specific. Furthermore, to suppress the negative transfer issue they applied adversarial training on the gradients level that enforces them to be statistically indistinguishable across tasks during the update step.</p><p>Note that all aforementioned works so far follow a common pattern: they directly predict all task outputs from the same input in one processing cycle (i.e. all predictions are generated once, in parallel or sequentially, and are not refined afterwards). By doing so, they fail to capture commonalities and differences among tasks, that are likely fruitful for one another (e.g. depth discontinuities are usually aligned with semantic edges). Arguably, this might be the reason for the only moderate performance improvements achieved by this group of works (see <ref type="bibr" target="#b28">[29]</ref>). To alleviate this issue, a few recent works first employed a multi-task network to make initial task predictions, and then leveraged features from these initial predictions in order to further improve each task output -in a one-off or recursive manner. In particular, Xu et al. <ref type="bibr" target="#b46">[47]</ref> proposed to distil information from the initial predictions of other tasks, by means of spatial attention, before adding it as a residual to the task of interest. Zhang et al. <ref type="bibr" target="#b49">[50]</ref> opted for sequentially predicting each task, with the intention to utilize information from the past predictions of one task to refine the features of another task at each iteration. In <ref type="bibr" target="#b50">[51]</ref>, they extended upon this idea. They used a recursive procedure to propagate similar cross-task and task-specific patterns found in the initial task predictions. To do so, they operated on the affinity matrices of the initial predictions, and not on the features themselves, as was the case before <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>Although better performance improvements have been reported in these works, albeit for specific datasets (see <ref type="bibr" target="#b46">[47]</ref>), they are all based on the principle that the interactions between tasks, which are essential in the distillation or propagation procedures described above, only happen at a fixed, local or global, scale 2 . For all we know, however, this is not always the case. In fact, two tasks with high pattern affinity at a certain scale are not guaranteed to retain this behaviour at other scales, and vice versa. Take for example the tasks of semantic segmentation and depth estimation, and consider the case where two cars at different distances are in front of our camera's viewpoint, with one partially occluding the other.</p><p>Looking at the local scale (i.e. patch level), the discontinuity in depth labels in the region in-between cars suggests that a similar pattern should be present in the semantic labels, i.e. there should be a change of semantic labels in the exact same region, despite the fact that this is incorrect. However, looking at the global scale this ambiguity can be resolved. An analogous observation can be made if we swapped the order of tasks, and went from global to local scale.</p><p>We conclude that pattern affinities should not be considered at the task level only, as existing works do <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref>, but be conditioned on the scale level too (for a more detailed discussion visit Section 2.2).</p><p>In this paper, we go beyond these limitations and explicitly consider interactions at separate scales when propagating features across tasks. We propose a novel architecture, namely MTI-Net, that builds upon this idea. Starting from a multi-scale feature representation of the input image, generated from an offthe-shelf backbone network (e.g. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45]</ref>), we make an initial prediction for each task at each considered scale (four scales in our case). Next, for each task we distill information from other tasks by means of spatial attention to refine the features of the initial predictions. Note that this process happens at each scale separately in order to capture the unique task interactions that happen at each individual scale, as discussed above. To tackle the limited field-of-view at higher scales of the backbone network, which can hinder task predictions at these scales, we propose to propagate distilled task information from the lower scales. At the final stage, the distilled features of each task from all scales are aggregated to arrive at the final task predictions.</p><p>Our contributions are threefold: (1) we propose to explicitly consider multiscale interactions when distilling information across tasks in multi-task networks;</p><p>(2) we introduce an architecture that builds upon this idea with dedicated modules, i.e. multi-scale multi-modal distillation (Section 2.1), feature propagation across scales (Section 2.4), and feature aggregation (Section 2.5); (3) we overcome a common obstacle of performance degradation in multi-task networks, and observe that tasks can mutually benefit from each other, resulting in significant improvements w.r.t their single-task counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-task learning by multi-modal distillation</head><p>Visual tasks can be related. For example, they can share complementary information (surface normals and depth can directly be derived from each other), act as a regularizer for one another (using RGB-D images to predict scene semantics <ref type="bibr" target="#b10">[11]</ref> improves the quality of the prediction due to the available depth information), and so on. Motivated by this observation, recent MTL methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref> tried to explicitly distill information from other tasks, as a complementary signal to improve task performance. Typically, this is achieved by combining an existing backbone network, that makes initial task predictions, with a multi-step decoding process (see <ref type="figure" target="#fig_0">Figure 1</ref> (left)).</p><p>In more detail, the shared features of the backbone network are processed by a set of task-specific heads, that produce an initial prediction for every task. We further refer to the backbone and the task-specific heads as the front-end of the network. The task-specific heads produce a per-task feature representation of the scene that is more task-aware than the shared features of the backbone network. The information from these task-specific feature representations is then combined via a multi-modal distillation unit, before making the final task predictions. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, it is possible that some tasks are only predicted in the front-end of the network. The latter are known as auxiliary tasks, since they serve as proxies in order to improve the performance on the final tasks.</p><p>Prior works only differ in the way that the task-specific feature representations are combined. PAD-Net <ref type="bibr" target="#b46">[47]</ref> distills information from other tasks by applying spatial attention to these features, before adding them as a residual. PAP-Net <ref type="bibr" target="#b50">[51]</ref> recursively combines the pixel affinities from these features during the decoding step. Zhang et al. <ref type="bibr" target="#b49">[50]</ref> sequentially predict one task in order to refine its features based on the features of the other task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scale</head><p>1/32  (Left) The architecture used in PAD-Net <ref type="bibr" target="#b46">[47]</ref> and PAP-Net <ref type="bibr" target="#b50">[51]</ref>. Features extracted from a backbone network are used to make initial task predictions. The task features are combined through a distillation unit before making the final task predictions. (Right) The architecture of the proposed MTI-Net. Starting from a backbone that extracts multi-scale features, initial task predictions are made at each scale. The task features are distilled separately at every scale, allowing our model to capture task interactions at multiple scales, i.e. receptive fields. After distillation, the distilled task features from all scales are aggregated to make the final task predictions. To boost performance, we extend our model with a feature propagation mechanism that passes distilled information from lower resolution task features to higher ones.</p><p>For brevity, we adopt the following notations. Backbone features: the shared features (at the last layer) of the backbone network; Task features: the taskspecific feature representations (at the last layer) of each task-specific head; Distilled task features: the task features after multi-modal distillation; Initial task predictions: the per-task predictions at the front-end of the network; Final task predictions: the network outputs. Note that, backbone features, task features and distilled task features can be defined at a single scale or multiple scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task interactions at different scales</head><p>The approaches described in Section 2.1 follow a common pattern: they perform multi-modal distillation at a fixed scale, i.e. the backbone features. This rests on the assumption that all relevant task interactions can solely be modeled through a single filter operation with specific receptive field. For all we know, this is not always the case. In fact, tasks can influence each other differently for different receptive field sizes. Consider, for example, <ref type="figure" target="#fig_2">Figure 2a</ref>. The local patches in the depth map provide little information about the semantics of the scene. However, when we enlarge the receptive field, the depth map reveals a person's shape, hinting at the scene's semantics. Note that the local patches can still provide valuable information, e.g. to improve the local alignment of edges between tasks.</p><p>(a) Three local patches from a depth map. Depending on the patch size, i.e. receptive field, depth information can be utilized differently by other tasks, e.g. semantic segmentation and edges.  To quantify the degree to which tasks share common local structures w.r.t. the size of the receptive field, we conduct the following experiment. We measure the pixel affinity in local patches on the label space of each task, using kernels of fixed size. The size of the receptive field can be selected by choosing the dilation for the kernel. We consider the tasks of semantic segmentation, depth estimation and edge detection on the NYUD-v2 dataset. A pair of semantic pixels is considered similar when both pixels belong to the same category. For the depth estimation task, we threshold the relative difference between pairs of pixels; pixels below the threshold are similar. Once the pixel affinities are calculated for every task, we measure how well similar and dissimilar pairs are matched across tasks. We repeat this experiment using different dilations for the kernel, effectively changing the receptive field. <ref type="figure" target="#fig_2">Figure 2b</ref> illustrates the result.</p><p>A first observation is that affinity patterns are matched well across tasks, with up to 65% of pair correspondence in some cases. This indicates that different tasks can share common structures in parts of the image. This is in agreement with a similar observation made earlier by <ref type="bibr" target="#b50">[51]</ref>. A second observation is that the degree to which the affinity patterns are matched across tasks is dependent on the receptive field, which in turn, corresponds to the used dilation. This validates our initial assumption that the statistics of task interactions do not always remain constant, but rather depend on the scale, i.e. receptive field.</p><p>Based on these findings, in the next section we introduce a model that distills information from different tasks at multiple scales 3 . By doing so, we are able to capture the unique task interactions at each individual scale, overcoming the limitations of the models described in Section 2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-scale multi-modal distillation</head><p>We propose a multi-task architecture that explicitly takes into account task interactions at multiple scales. Our model is shown in <ref type="figure" target="#fig_0">Figure 1</ref> (right). First, an off-the-shelf backbone network extracts a multi-scale feature representation from the input image. Such multi-scale feature extractors have been used in semantic segmentation <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b16">17]</ref>, object detection <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45]</ref>, pose estimation <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b42">43]</ref>, etc. In Section 3 we verify our approach using two such backbones, i.e. HRNet <ref type="bibr" target="#b44">[45]</ref> and FPN <ref type="bibr" target="#b22">[23]</ref>, but any multi-scale feature extractor can be used instead.</p><p>From the multi-scale feature representation (i.e. backbone features) we make initial task predictions at each scale. These initial task predictions at a particular scale are found by applying a set of task-specific heads to the backbone features extracted at that scale. The result is a per-task representation of the scene (i.e. task features) at a multitude of scales. Not only does this add deep supervision to our network, but the task features can now be distilled at each scale separately. This allows us to have multiple task interactions, each modeled for a specific receptive field size, as proposed in Section 2.2.</p><p>Next, we refine the task features by distilling information from the other tasks using a spatial attention mechanism <ref type="bibr" target="#b46">[47]</ref>. Yet, our multi-modal distillation process is repeated at each scale, i.e. we apply multi-scale, multi-modal distillation. The distilled task features F o k,s for task k at scale s are found as:</p><formula xml:id="formula_0">F o k,s = F i k,s + l =k σ W k,l,s F i l,s W k,l,s F i l,s ,<label>(1)</label></formula><p>where σ W k,l,s F i l,s returns a per-scale spatial attention mask, that is applied to the task features F i l,s from task l at scale s. Note that our approach is not necessarily limited to the use of spatial attention, but any type of feature distillation (e.g. squeeze-and-excitation <ref type="bibr" target="#b13">[14]</ref>) can easily be plugged in. Through repetition, we calculate the distilled task features at every scale. As the bulk of filter operations is performed on low resolution feature maps, the computational overhead of our model is limited. We make a detailed resource analysis in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Feature propagation across scales</head><p>In Section 2.3 actions at each scale were performed in isolation. To sum up, we made initial task predictions at each scale, from which we refined the task  <ref type="figure">Fig. 3</ref>: Our Feature Propagation Module. First, task features from a lower scale are concatenated and mapped to a shared representation by the feature harmonization module. The task features are then refined by extracting information from the shared representation through squeeze-and-excitation (SE) <ref type="bibr" target="#b13">[14]</ref>, and are added as a residual to the original ones. Finally, these refined task features will be concatenated with the backbone features of the preceding higher scale.</p><p>features through multi-modal distillation at each individual scale separately. However, as the higher resolution scales have a limited receptive field, the frontend of the network could have a hard time to make good initial task predictions at these scales, which in turn, would lead to low quality task features there. To remedy this situation we introduce a feature propagation mechanism, where the backbone features of a higher resolution scale are concatenated with the task features from the preceding lower resolution scale, before feeding them to the task-specific heads of the higher resolution scale to get the task features there. A trivial implementation for our Feature Propagation Module (FPM) would be to just upsample the task features from the previous scale and pass them to the next scale. We opt for a different approach however, and design the FPM to behave similarly to the multi-modal distillation unit of Section 2.3, in order to model task interactions at this stage too. <ref type="figure">Figure 3</ref> gives an overview of our FPM. We first use a feature harmonization block to combine the task features from the previous scale to a shared representation. We then use this shared representation to refine the task features from the previous scale, before passing them to the next scale. The refinement happens by selecting relevant information from the shared representation through a squeeze-and-excitation block <ref type="bibr" target="#b13">[14]</ref>. Note that, since we refine the features from a single shared representation, instead of processing each task independently as done in the multi-modal distillation unit of Section 2.3, the computational cost is significantly smaller. Feature harmonization. The FPM receives as input the task features from N tasks of shape C × H × W . Our feature harmonization module combines the received task features into a shared representation. In particular, the set of N task features is first concatenated and processed by a learnable non-linear function f . The output is split into N chunks along the channel dimension, that match the original number of channels C. We then apply a softmax function along the task dimension to generate a task attention mask. The attended features are concatenated and further processed to reduce the number of channels from N · C to C. The output is a shared representation based on information from all tasks. Refinement through Squeeze-And-Excitation. The use of a shared representation can degrade performance when tasks are unrelated. We resolve this situation by applying a per-task channel gating function to the shared representation. This effectively allows each task to select the relevant features from the shared representation. The channel gating mechanism is implemented here as a squeeze-and-excitation (SE) block <ref type="bibr" target="#b13">[14]</ref>. This is due to the fact that SE has shown great potential in MTL (e.g. <ref type="bibr" target="#b28">[29]</ref>), yet other gating mechanisms could be used instead. After applying the SE module, the refined task features are added as a residual to the original task features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Feature aggregation</head><p>The multi-scale, multi-modal distillation described in Section 2.3 results in distilled task features at every scale. The latter are upsampled to the highest scale and concatenated, resulting in a final feature representation for every task. The final task predictions are found by decoding these final feature representations by task-specific heads again. All implementation details are discussed in Section 3. It is worth mentioning that our model has the possibility to add auxiliary tasks in the front-end of the network, similar to PAD-Net <ref type="bibr" target="#b46">[47]</ref>. In our case however, the auxiliary tasks are predicted at multiple scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental setup</head><p>Datasets. We perform our experimental evaluation on the PASCAL <ref type="bibr" target="#b7">[8]</ref> and NYUD-v2 <ref type="bibr" target="#b39">[40]</ref> datasets. <ref type="table" target="#tab_2">Table 1</ref> contains the tasks that we considered for each dataset. We use the original 795 train and 654 test images for the NYUD-v2 dataset. For PASCAL, we use the split from PASCAL-Context <ref type="bibr" target="#b3">[4]</ref> which has annotations for semantic segmentation, human part segmentation and edge detection. We obtain the surface normals and saliency labels from <ref type="bibr" target="#b28">[29]</ref>, that distilled them from pre-trained state-of-the-art models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>. Implementation details. We build our approach on top of two different backbone networks, i.e. FPN <ref type="bibr" target="#b22">[23]</ref> and HRNet <ref type="bibr" target="#b42">[43]</ref>. We use the different output scales of the selected backbone networks to perform multi-scale operations. This translates to four scales (1/4, 1/8, 1/16, 1/32). The task-specific heads are implemented as two basic residual blocks <ref type="bibr" target="#b12">[13]</ref>. All our experiments are conducted with pre-trained ImageNet weights.</p><p>We use the L1 loss for depth estimation and the cross-entropy loss for semantic segmentation on NYUD-v2. As in prior work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>, the edge detection task is trained with a positively weighted w pos = 0.95 binary cross-entropy loss. We do not adopt a particular loss weighing strategy on NYUD-v2, but simply sum the losses together. On PASCAL, we reuse the training setup from <ref type="bibr" target="#b28">[29]</ref> to facilitate a fair comparison. We reuse the loss weights from there. The initial task predictions in the front-end of the network use the same loss weighing as the final task predictions. In contrast to <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b49">50]</ref>, we do not use a two-step training procedure where the front-end is pre-trained separately. Instead, we simply train the complete architecture end-to-end. We refer to the supplementary material for further implementation details. Evaluation metrics. We evaluate the performance of the backbone networks on the single tasks first. The optimal dataset F-measure (odsF ) <ref type="bibr" target="#b29">[30]</ref> is used to evaluate the edge detection task. The semantic segmentation, saliency estimation and human part segmentation tasks are evaluated using mean intersection over union (mIoU ). We use the mean error (mErr ) in the predicted angles to evaluate the surface normals. The depth estimation task is evaluated using the root mean square error (rmse). We measure the multi-task learning performance ∆ m as in <ref type="bibr" target="#b28">[29]</ref>, i.e. the multi-task performance of model m is defined as the average per-task drop in performance w.r.t. the single-task baseline b:</p><formula xml:id="formula_1">∆ m = 1 T T i=1 (−1) li (M m,i − M b,i ) /M b,i ,<label>(2)</label></formula><p>where l i = 1 if a lower value means better for performance measure M i of task i, and 0 otherwise. The single-task performance is measured for a fully-converged model that uses the same backbone network only for that task.</p><p>Baselines. On NYUD-v2, we compare MTI-Net against the state-of-the-art PAD-Net <ref type="bibr" target="#b46">[47]</ref>. PAD-Net was originally designed for a single scale, but it is easy to plug-in a multi-scale backbone network and directly compare the two approaches. In contrast, a comparison with <ref type="bibr" target="#b49">[50]</ref> is not possible, as this work was strictly designed for a pair of tasks, without any straightforward extension to the MTL setting. Finally, PAP-Net <ref type="bibr" target="#b50">[51]</ref> adopts an architecture that is similar to PAD-Net, but the multi-modal distillation is performed recursively on the feature affinities. We chose to draw the comparison with the more generic PAD-Net, since it performs on par with PAP-Net (see Section 3.3). On PASCAL, we compare our method against the state-of-the-art ASTMT <ref type="bibr" target="#b28">[29]</ref>. Note that a direct comparison with ASTMT is also not straightforward, as this model is by design single-scale and heavily based on a DeepLab-v3+ (DLv3+) backbone network that contains dilated convolutions. Due to the latter, simply plugging the same DLv3+ backbone into MTI-Net would break the multi-scale features required to uniquely model the task interactions at a multitude of scales. Yet, we provide a fair comparison with ASTMT by combining it with a ResNet-50 FPN backbone, to show that it is not just using a multi-scale backbone that leads to improved results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation studies</head><p>Network components. In <ref type="table" target="#tab_3">Table 2</ref> we visualize the results of our ablation studies on NYUD-v2 and PASCAL with an HRNet18 backbone to verify how different components of our model contribute to the multi-task improvements. Additional results using different backbones are in the supplementary materials. We focus on the smaller NYUD-v2 dataset first (see <ref type="table" target="#tab_3">Table 2a</ref>), that contains arguably related tasks. These are semantic segmentation (Seg) and depth prediction (Dep) as main tasks, edge detection (E) and surface normals (N) as auxiliary tasks. The MTL baseline (i.e. a shared encoder with task-specific heads) has lower performance (−1.71%) than the single-task models. This is inline with prior work <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b28">29]</ref>. PAD-Net retains performance over the set of single-task models (−0.02%), and improves when adding the auxiliary tasks (+0.52%). Using our model without the FPM between scales further improves the results (w/o auxiliary tasks: +3.85%, w/ auxiliary tasks: +4.48%). When including the FPM another significant boost in performance is achieved (+6.40%). Further adding the auxiliary tasks can help to improve the quality of our predictions (+10.91%). <ref type="table" target="#tab_3">Table 2b</ref> shows the ablation on PASCAL. We discriminate between a small set (s) and a complete set (a) of tasks. The small set contains the high-level (semantic and human parts segmentation) and mid-level (saliency) vision tasks. The complete set also adds the low-level (edges and normals) vision tasks. The MTL baseline leads to decreased performance, −4.26% and −3.70% on the small and complete set respectively. Instead, our model improves over the set of single-task models (+3.35%) on the small task set (s), where we obtain solid improvements on all tasks. We also report the influence of adding additional auxiliary tasks to the front-end of the network. Adding edges improves the multi-task performance to 3.98%, adding normals slightly decreases it to +2.69%, while adding   both keeps it stable +3.36%. Finally, when learning all five tasks together, our model outperforms (+2.74%) the set of single-task models. In general, all tasks gain significantly, except for normals, where we observe a small decrease in performance. We argue that this is due to the inevitable negative transfer that characterizes all models with shared operations (also <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b50">51]</ref>). Yet, to the best of our knowledge, this is the first work to not only report overall improved multi-task performance, but also to maximize the gains over the single-task models, when jointly predicting an increasing and diverse set of tasks. We refer to <ref type="figure">Figure 4</ref> for qualitative results obtained with an HRNet-18 backbone. Influence of scales. So far, our experiments included all four scales of the backbone network (1/4, 1/8, 1/16, 1/32). Here, we study the influence of using a different number of scales for the backbone. <ref type="table" target="#tab_4">Table 3</ref> summarizes this ablation on NYUD-v2. Note that the use of a single scale (1/4) reduces our model to a PAD-Net like architecture. Using an increasing number of scales (1/4 vs + 1/8 vs + 1/16, ...) gradually improves performance. The results confirm our hypothesis from Section 2.2, i.e. task interactions should be modeled at multiple scales. Information flow. To quantify the flow of information, we measure the performance of the initial task predictions at different locations in the front-end of the network. <ref type="table" target="#tab_5">Table 4</ref> illustrates the results on NYUD-v2. We observe that the performance gradually increases at the higher scales, due to the information that is being propagated from the lower scales via the FPM. The final prediction after aggregating the information from all scales is further improved substantially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with the state-of-the-art</head><p>Comparison on PASCAL. <ref type="table" target="#tab_6">Table 5</ref> visualizes the comparison of our model against ASTMT and PAD-Net on PASCAL. We report the multi-tasking performance both w.r.t. the single-task models using the same backbone (ST) and the single-task models based on the R50-FPN backbone. As explained, in the only <ref type="table">Table 6</ref>: Comparison with the state-of-the-art on NYUD-v2.</p><p>(a) Results on depth estimation.</p><p>Method rmse rel δ 1 δ 2 δ 3 HCRF <ref type="bibr" target="#b19">[20]</ref> 0.821 0.232 0.621 0.886 0.968 DCNF <ref type="bibr" target="#b23">[24]</ref> 0.824 0.230 0.614 0.883 0.971 Wang <ref type="bibr" target="#b45">[46]</ref> 0.745 0.220 0.605 0.890 0.970 NR forest <ref type="bibr" target="#b35">[36]</ref> 0.774 0.187 ---Xu <ref type="bibr" target="#b47">[48]</ref> 0.593 0.125 0.806 0.952 0.986 PAD-Net <ref type="bibr" target="#b46">[47]</ref> 0.582 0.120 0.817 0.954 0.987 PAP-Net <ref type="bibr" target="#b50">[51]</ref> 0  possible fair comparison, i.e. when using the same R50-FPN backbone, our model achieves higher multi-tasking performance compared to ASTMT (+1.36% vs −0.87%). Yet, as ASTMT is by design single-scale and heavily based on DLv3+, we also report results using different backbones. Overall, MTI-Net achieves significantly higher gains over its single-task variants compared to ASTMT (see ∆ m ↑ (ST)). Surprisingly, we find that our model with R18-FPN backbone even outperforms the deeper ASTMT R50-DLv3+ model in terms of multi-tasking performance (+0.29% vs −0.08%), despite the fact that the ASTMT single-task models perform better than ours, due to the use of the stronger DLv3+ backbone. Note that we are the first to report consistent multi-task improvements when solving such a diverse task dictionary. Finally, our model also outperforms PAD-Net in terms of multi-tasking performance (+2.74% vs −3.08%).</p><p>Comparison on NYUD-v2. <ref type="table">Table 6</ref> shows a comparison with the state-ofthe-art approaches on NYUD-v2. We leave out methods that rely on extra input modalities, or additional training data. As these methods are built on top of stronger single-scale backbones, we also use the multi-scale HRNet48-v2 backbone here. Again, our model improves w.r.t the single-task models. Furthermore, we perform on par with the state-of-the-art on the depth estimation task, while performing slightly worse on the semantic segmentation task. We refer the reader to the supplementary materials for qualitative results. Resource analysis. We compare our model in terms of computational requirements against PAD-Net and ASTMT. The comparison with PAD-Net is performed on NYUD-v2 using the HRNet-18 backbone, while for the comparison with ASTMT on PASCAL we use a ResNet-50 FPN backbone.  <ref type="figure">Fig. 4</ref>: Qualitative results on PASCAL. We compare the predictions made by a set of single-task models (first row for every image) against the predictions made by our MTI-Net (second row for every image). Differences can be seen for semantic segmentation, edge detection and saliency estimation.</p><p>task models. The reason for the increased amount of parameters is the use of a shallow backbone, and the small number of tasks (i.e. 2). Furthermore, we significantly outperform PAD-Net in terms of FLOPS and multi-task performance. This is due to the fact that PAD-Net performs the multi-modal distillation at a single higher scale (1/4) with 4 · C channels, C being the number of backbone channels at a single scale. Instead, we perform most of the computations at smaller scales (1/32, 1/16, 1/8), while operating on only C channels at the higher scale (1/4). On PASCAL, we significantly improve on all three metrics compared to the single-task models. We also outperform ASTMT in terms of FLOPS and multi-task performance, as the latter has to perform a separate forward pass per task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have shown the importance of modeling task interactions at multiple scales, enabling tasks to maximally benefit each other. We achieved this by introducing dedicated modules on top of an off-the-shelf multi-scale feature extractor, i.e. multi-scale multi-modal distillation, feature propagation across scales, and feature aggregation. Our multi-task model delivers on the full potential of multitask learning, i.e. smaller memory footprint, reduced number of calculations and better performance. Our experiments show that our multi-task models consistently outperform their single-tasking counterparts by medium to large margins. Acknowledgment The authors acknowledge support by Toyota via the TRACE project and MACCHINA (KULeuven,C14/18/065).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary Materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Difference with Cross-Stitch Networks</head><p>Cross-stitch networks <ref type="bibr" target="#b30">[31]</ref> also share features at multiple scales. However, the intended purpose, and implementation differ significantly from MTI-Net. We analyze the most notable points here.</p><p>-Cross-stitch networks model task interactions at the encoding stage by softly sharing features between task-specific encoders, before branching out to taskspecific decoders without further interaction. Differently, MTI-Net operates at the decoding stage fusing task features close to the output that contain more disentangled task information. The latter is arguably better for distilling task information in structured output tasks with co-occurring patterns. -Cross-stitch Networks distil task information sequentially layer-by-layer, greedily modeling interactions at a local scale. In contrast, MTI-Net models task interactions in parallel, globally fusing information across all scales (cf. FA module). The benefits are two-fold, i.e. enabling the modeling of long-term relationships, and allowing for the use of sufficient context information which is crucial in dense prediction tasks <ref type="bibr" target="#b2">[3]</ref>. -The model size of cross-stitch networks increases linearly with the number of tasks, thus scaling poorly to multiple tasks. Instead, MTI-Net provides a more computationally efficient alternative (see resource analysis), that is much closer to the single-task model size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training setup</head><p>We include additional details of the training setup used for each experiment. We considered two different multi-scale backbone networks, i.e. HRNet and FPN. For HRNet, we use bilinear upsampling and concatenation followed by two convolutional layers to decode the multi-scale features in the feature aggregation unit. For FPN, the feature aggregation module decodes the multi-scale features as in panoptic feature pyramid networks <ref type="bibr" target="#b16">[17]</ref>. In both cases, the non-linear function that produces the task attention mask in the FPM is implemented as two basic residual blocks -that aggressively reduce the number of channels -followed by a 1 × 1 convolutional layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NYUD-v2</head><p>We applied the data augmentation strategy of PAD-Net <ref type="bibr" target="#b46">[47]</ref>. The RGB and depth images were randomly scaled with the selected ratio in {1, 1.2, 1.5}, and randomly horizontally flipped. The model was trained for 80 epochs with an Adam optimizer with initial learning rate 1e-4 and batches of size 6. We used a poly learning rate decay scheme. PASCAL We essentially plugged our model into the code base that was shared by <ref type="bibr" target="#b28">[29]</ref>. In particular, the single-task models were trained with stochastic gradient descent with momentum 0.9. We used batches of size 8 and a poly learning rate decay scheme. The initial learning rate was 0.01. We applied weight decay λ = 1e − 4. These hyperparameters are the same as the ones used in <ref type="bibr" target="#b28">[29]</ref>, ensuring fair comparison. The multi-task baseline models were trained using the same hyperparameters. The multi-task loss weighing was taken from <ref type="bibr" target="#b28">[29]</ref>. We also tested the use of an Adam optimizer, but this did not yield better results. Our MTI-Net was trained under the same settings as the single-task models, but we used an Adam optimizer with initial learning rate 1e-4. We re-used the loss weights from before to weight the losses from the initial task predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Extra experiments on PASCAL</head><p>We perform an additional ablation experiment using a ResNet-18 FPN backbone in <ref type="table" target="#tab_2">Table S1</ref>. The conclusions are similar to the ones reported for the HRNet-18 in <ref type="table" target="#tab_3">Table 2b</ref> of the main paper. This shows that our method can be used in combination with various backbone architectures. Again, our model improves over the single-tasking models, both for the small (+2.77%) and complete (+3.84%) set of tasks. We also consider the effect of adding additional auxiliary tasks when predicting the small task set. When adding edge detection as an auxiliary task, the results are further improved (+2.77% to +3.61%). However, this is not the case when we add surface normals prediction as an auxiliary task (+2.77% to 2.52%). We observe a similar effect when including both edge detection and surface normals prediction (3.61% to 3.06%). We believe that this is due to the approximate nature of the surface normals in this dataset, as the latter were obtained through distillation, and as such they are rather noisy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Extra experiments on NYUD-v2</head><p>This section contains additional results on the NYUD-v2 dataset. Section A.4 gives a more detailed view on the ablation studies that we performed on NYUD-v2 using an HRNet-18 backbone. Note that the main results of this experiment were already discussed in the experiments section of the paper. In Section A.4, we perform an additional experiment using an FPN backbone based on ResNet-18. <ref type="table" target="#tab_3">Table S2</ref>: Ablation studies on NYUD-v2 using an HRNet18-V2 backbone. Auxiliary tasks are indicated in brackets.  HRNet18-V2 <ref type="table" target="#tab_3">Table S2</ref> contains additional metrics for the depth estimation and semantic segmentation task on the NYUD-v2 dataset, when using an HRNet-18 backbone. This is an extension to the metrics shown in <ref type="table" target="#tab_3">Table 2a</ref> of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FPN -ResNet-18</head><p>We repeated a smaller version of our ablation studies on the NYUD-v2 dataset when using an FPN backbone based on ResNet-18. <ref type="table" target="#tab_4">Table S3</ref> contains the results. We end up at similar findings compared to the model based on HRNet-18. Again, we see a significant improvement over the set of single-task models. Additionally, we find that the use of auxiliary tasks can help to improve the quality of the predictions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Qualitative results on NYUD-v2</head><p>Figure S1 shows predictions made by our HRNet-48 model on images from the NYUD-v2 test set. The quantitative results were already reported in <ref type="table">Table 6</ref> of the paper. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>An overview of different MTL architectures as described in Section 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>To quantify task interactions w.r.t. scale, pixel affinities on the label space of each task, as defined in<ref type="bibr" target="#b50">[51]</ref>, are calculated in local patches. The correspondences in the affinity patterns between tasks are plotted as a function of the patch size, i.e. kernel dilation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Unlike the common belief, in this paper we question whether task interactions remain constant across all scales (see Section 2.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Results on the depth estimation task.Method rmse ↓ rel ↓ δ1 ↑ δ2 ↑ δ3 ↑0.181 0.747 0.937 0.982 Ours -w/o FPM (N) 0.642 0.175 0.753 0.940 0.983 Ours -w/o FPM (N+E) 0.637 0.174 0.757 0.939 0.984 Ours -w/ FPM 0.620 0.161 0.781 0.946 0.986 Ours -w/ FPM (N) 0.600 0.162 0.788 0.947 0.985 Ours -w/ FPM (N+E) 0.607 0.166 0.783 0.945 0.985 (b) Results on the semantic segmentation task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 Fig. S1 :</head><label>3S1</label><figDesc>Qualitative results on NYUD-v2: Semantic and depth predictions made by our HRNet-48 model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Our multi-task learning benchmarks. We predict five tasks on PASCAL. On NYUD-v2 we only consider semantic segmentation and depth, but include edges and normals as auxiliary tasks. Distilled labels are marked with *.</figDesc><table><row><cell cols="3">Dataset Edge Seg Parts Normals Saliency Depth</cell></row><row><cell>PASCAL</cell><cell>*</cell><cell>*</cell></row><row><cell>NYUD-v2</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies on (a) NYUD-v2 and (b) PASCAL using an HRNet-18 backbone network. Auxiliary tasks are indicated between brackets.</figDesc><table><row><cell cols="3">(a) Results on NYUD-v2.</cell><cell cols="3">(b) Results on PASCAL.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Seg ↑ Dep ↓ ∆m% ↑</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Single task</cell><cell cols="2">33.18 0.667 + 0.00</cell><cell>Single task</cell><cell cols="2">60.07 60.74 67.18 69.70</cell><cell>14.59</cell><cell>+ 0.00</cell></row><row><cell>MTL</cell><cell>32.09 0.668</cell><cell>-1.71</cell><cell>MTL (s)</cell><cell>54.53 59.54 65.60</cell><cell>-</cell><cell>-</cell><cell>-4.26</cell></row><row><cell>PAD-Net</cell><cell>32.80 0.660</cell><cell>-0.02</cell><cell>MTL (a)</cell><cell cols="2">53.60 58.45 65.13 70.60</cell><cell>15.08</cell><cell>-3.70</cell></row><row><cell>PAD-Net (N)</cell><cell cols="2">33.85 0.658 + 1.65</cell><cell>Ours (s)</cell><cell>64.06 62.39 68.09</cell><cell>-</cell><cell>-</cell><cell>+ 3.35</cell></row><row><cell>PAD-Net (N+E)</cell><cell cols="2">32.92 0.655 + 0.52</cell><cell>Ours (s)(E)</cell><cell>64.98 62.90 67.84</cell><cell>-</cell><cell>-</cell><cell>+ 3.98</cell></row><row><cell>Ours (w/o FPM)</cell><cell cols="2">34.38 0.640 + 3.85</cell><cell>Ours (s)(N)</cell><cell>63.74 61.75 67.90</cell><cell>-</cell><cell>-</cell><cell>+ 2.69</cell></row><row><cell>Ours (w/o FPM) (N)</cell><cell cols="2">34.49 0.642 + 3.84</cell><cell cols="2">Ours (s)(E+N) 64.33 62.33 68.00</cell><cell>-</cell><cell>-</cell><cell>+ 3.36</cell></row><row><cell cols="3">Ours (w/o FPM) (N+E) 34.68 0.637 + 4.48</cell><cell>Ours (a)</cell><cell cols="2">64.27 62.06 68.00 73.40</cell><cell>14.75</cell><cell>+ 2.74</cell></row><row><cell>Ours (w/ FPM)</cell><cell cols="2">35.12 0.620 + 6.40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours (w/ FPM) (N)</cell><cell cols="2">36.22 0.600 + 9.57</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Ours (w/ FPM) (N+E) 37.49 0.607 + 10.91</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Method Seg ↑ Parts ↑ Sal ↑ Edge ↑ Norm ↓ ∆m% ↑</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Influence of using a different number of scales for the backbone network on NYUD-v2.</figDesc><table><row><cell>Method</cell><cell>Seg ↑ Dep ↓ ∆m % ↑</cell></row><row><cell>ST</cell><cell>33.18 0.667 + 0.00</cell></row><row><cell>1/4 (Pad-Net)</cell><cell>32.80 0.660 -0.02</cell></row><row><cell>1/4, 1/8</cell><cell>34.88 0.650 + 3.80</cell></row><row><cell>1/4, 1/8, 1/16</cell><cell>35.01 0.630 + 5.53</cell></row><row><cell cols="2">1/4, 1/8, 1/16, 1/32 (Ours) 35.12 0.620 + 6.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablating the information flow within the proposed MTI-Net model on NYUD-v2.</figDesc><table><row><cell>Method</cell><cell>Seg ↑ Dep ↓ ∆m% ↑</cell></row><row><cell>ST</cell><cell>33.18 0.667 + 0.00</cell></row><row><cell cols="2">Front-end @ 1/32 scale 32.02 0.670 -1.87</cell></row><row><cell cols="2">Front-end @ 1/16 scale 33.02 0.660 + 0.02</cell></row><row><cell cols="2">Front-end @ 1/8 scale 33.67 0.640 + 2.72</cell></row><row><cell cols="2">Front-end @ 1/4 scale 34.05 0.633 + 3.78</cell></row><row><cell>Final output</cell><cell>35.12 0.620 + 6.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison with the state-of-the-art on PASCAL.</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell cols="3">Seg ↑ ST MT ST MT ST MT ST MT ST MT Parts ↑ Sal ↑ Edge ↑ Norm ↓ ∆m ↑ (ST) ∆m ↑ (R50-FPN)</cell></row><row><cell></cell><cell cols="2">R26-DLv3+ 64.9 64.6 57.1 57.3 64.2 64.7 71.3 71.0 14.9 15.0</cell><cell>-0.11</cell><cell>-3.42</cell></row><row><cell>ASTMT [29]</cell><cell cols="2">R50-DLv3+ 68.3 68.0 60.70 61.1 65.4 65.7 72.7 72.4 14.6 14.7</cell><cell>-0.04</cell><cell>-0.08</cell></row><row><cell></cell><cell>R50-FPN</cell><cell>67.7 66.8 61.8 61.1 67.2 66.1 71.1 70.9 14.8 14.7</cell><cell>-0.87</cell><cell>-0.87</cell></row><row><cell cols="2">PAD-Net[47] HRNet-18</cell><cell>60.1 53.6 60.7 59.6 67.2 65.8 69.7 72.5 14.6 15.3</cell><cell>-3.08</cell><cell>-5.58</cell></row><row><cell></cell><cell>R18-FPN</cell><cell>64.5 65.7 57.4 61.6 66.4 66.8 68.2 73.9 14.8 14.6</cell><cell>+ 3.84</cell><cell>+ 0.29</cell></row><row><cell>Ours</cell><cell>R50-FPN</cell><cell>67.7 66.6 61.8 63.3 67.2 66.6 71.1 74.9 14.8 14.6</cell><cell>+ 1.36</cell><cell>+ 1.36</cell></row><row><cell></cell><cell>HRNet-18</cell><cell>60.1 64.3 60.7 62.1 67.2 68.0 69.7 73.4 14.6 14.8</cell><cell>+ 2.74</cell><cell>-0.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Computational resource analysis (number of parameters and FLOPS).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(b) Results on PASCAL (Res-50 FPN).</cell></row><row><cell>Method</cell><cell cols="3">Params (M) FLOPS (G) ∆m%</cell><cell>Method</cell><cell cols="3">Params (M) FLOPS (G) ∆m%</cell></row><row><cell>Single Task</cell><cell>8.0</cell><cell>22.0</cell><cell>+0.00%</cell><cell>Single Task</cell><cell>140</cell><cell>219</cell><cell>+0.00%</cell></row><row><cell>Multi-Task</cell><cell>−50%</cell><cell>−45%</cell><cell>−1.71%</cell><cell>Multi-Task</cell><cell>−75.0%</cell><cell>−66%</cell><cell>−4.55%</cell></row><row><cell>PAD-Net</cell><cell>−15%</cell><cell>+204%</cell><cell>−0.02%</cell><cell>ASTMT</cell><cell>−51.0%</cell><cell>−1.0%</cell><cell>−0.87%</cell></row><row><cell>MTI-Net (Ours)</cell><cell>+57%</cell><cell>−13%</cell><cell>+6.40%</cell><cell>Ours</cell><cell>−35.0%</cell><cell>−19.9%</cell><cell>+1.36%</cell></row></table><note>(a) Results on NYUD-v2 (HRNet-18).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>reports</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table S1 :</head><label>S1</label><figDesc>Multi-task learning on PASCAL using a ResNet-18 FPN backbone.</figDesc><table><row><cell>Method</cell><cell cols="4">Seg ↑ Parts ↑ Sal ↑ Edge ↑ Norm ↓ ∆m ↑</cell></row><row><cell>Single task</cell><cell cols="4">64.49 57.43 66.38 68.20 14.77 + 0.00</cell></row><row><cell>MTL (s)</cell><cell>54.51 55.12 64.76</cell><cell>-</cell><cell>-</cell><cell>-7.32</cell></row><row><cell>MTL (a)</cell><cell cols="4">59.61 56.88 64.96 70.60 15.17 -1.80</cell></row><row><cell>Ours (s)</cell><cell>65.47 61.32 66.37</cell><cell>-</cell><cell>-</cell><cell>+ 2.77</cell></row><row><cell>Ours (s)(E)</cell><cell>65.93 62.21 66.80</cell><cell>-</cell><cell>-</cell><cell>+ 3.61</cell></row><row><cell>Ours (s)(N)</cell><cell>64.99 61.09 66.80</cell><cell>-</cell><cell>-</cell><cell>+ 2.52</cell></row><row><cell cols="2">Ours (s)(E+N) 65.46 61.71 66.62</cell><cell>-</cell><cell>-</cell><cell>+ 3.06</cell></row><row><cell>Ours (a)</cell><cell cols="4">65.69 61.59 66.76 73.90 14.55 + 3.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table S3 :</head><label>S3</label><figDesc>Additional results on NYUD-v2 when using an FPN backbone based on ResNet-18. Similarly toTable 2, auxiliary tasks are indicated in brackets.</figDesc><table><row><cell cols="5">(a) Multi-task learning performance.</cell></row><row><cell>Method</cell><cell cols="4">Seg (IoU) ↑ Dep (rmse) ↓ ∆m%</cell></row><row><cell>Single task</cell><cell></cell><cell>34.46</cell><cell>0.659</cell><cell>+0.00</cell></row><row><cell>MTL</cell><cell></cell><cell>33.52</cell><cell>0.665</cell><cell>-1.82</cell></row><row><cell>PAD-Net</cell><cell></cell><cell>34.15</cell><cell>0.662</cell><cell>-0.69</cell></row><row><cell>PAD-Net (N)</cell><cell></cell><cell>34.18</cell><cell>0.657</cell><cell>-0.23</cell></row><row><cell>PAD-Net (N+E)</cell><cell></cell><cell>34.60</cell><cell>0.668</cell><cell>-0.45</cell></row><row><cell>Ours</cell><cell></cell><cell>36.01</cell><cell>0.630</cell><cell>+4.43</cell></row><row><cell>Ours (N)</cell><cell></cell><cell>36.81</cell><cell>0.628</cell><cell>+5.74</cell></row><row><cell>Ours (N+E)</cell><cell></cell><cell>36.65</cell><cell>0.618</cell><cell>+6.27</cell></row><row><cell cols="5">(b) Results on the depth estimation task.</cell></row><row><cell>Method</cell><cell cols="4">rmse ↓ rel ↓ δ1 ↑ δ2 ↑ δ3 ↑</cell></row><row><cell>Single task</cell><cell></cell><cell cols="3">0.659 0.183 0.730 0.935 0.982</cell></row><row><cell>MTL</cell><cell></cell><cell cols="3">0.665 0.190 0.726 0.930 0.980</cell></row><row><cell>PAD-Net</cell><cell></cell><cell cols="3">0.662 0.188 0.731 0.931 0.979</cell></row><row><cell>PAD-Net (N)</cell><cell></cell><cell cols="3">0.657 0.185 0.735 0.934 0.980</cell></row><row><cell cols="5">PAD-Net (N+E) 0.668 0.185 0.729 0.933 0.980</cell></row><row><cell>Ours</cell><cell></cell><cell cols="3">0.630 0.173 0.767 0.939 0.981</cell></row><row><cell>Ours (N)</cell><cell></cell><cell cols="3">0.628 0.180 0.755 0.939 0.982</cell></row><row><cell>Ours (N+E)</cell><cell></cell><cell cols="3">0.618 0.169 0.768 0.944 0.984</cell></row><row><cell cols="5">(c) Results on the semantic segmentation task.</cell></row><row><cell>Method</cell><cell></cell><cell cols="3">pixel-acc ↑ mean-acc ↑ IoU ↑</cell></row><row><cell>Single task</cell><cell></cell><cell>65.51</cell><cell>46.50</cell><cell>34.46</cell></row><row><cell>MTL</cell><cell></cell><cell>64.85</cell><cell>45.33</cell><cell>33.52</cell></row><row><cell>PAD-Net</cell><cell></cell><cell>65.23</cell><cell>45.65</cell><cell>34.15</cell></row><row><cell>PAD-Net (N)</cell><cell></cell><cell>65.07</cell><cell>45.80</cell><cell>34.18</cell></row><row><cell cols="2">PAD-Net (N+E)</cell><cell>65.68</cell><cell>46.77</cell><cell>34.60</cell></row><row><cell>Ours</cell><cell></cell><cell>66.44</cell><cell>49.03</cell><cell>36.01</cell></row><row><cell>Ours (N)</cell><cell></cell><cell>66.89</cell><cell>50.50</cell><cell>36.81</cell></row><row><cell>Ours (N+E)</cell><cell></cell><cell>67.23</cell><cell>49.93</cell><cell>36.65</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/SimonVandenhende/Multi-Task-Learning-PyTorch arXiv:2001.06902v5 [cs.CV] 8 Jul 2020</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">With the exception of<ref type="bibr" target="#b49">[50]</ref>, where a first attempt for multi-scale processing happens at the decoding stage, in a strict sequential manner. Note that, their approach is only suitable for a pair of tasks, and can not be extended to multi-task learning.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Cross-stitch nets<ref type="bibr" target="#b30">[31]</ref> also exchange features at multiple scales, but in the encoder. A summary of differences with our approach is provided in the suppl. materials.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06506</idno>
		<title level="m">Pixelnet: Representation of the pixels, by the pixels, and for the pixels</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Detect what you can: Detecting and representing objects using holistic models and body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1971" to="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Blitznet: A real-time deep network for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4154" to="4162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Dynamic task prioritization for multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02680</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Pushing the boundaries of boundary detection using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07386</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1119" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">End-to-end multi-task learning with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fully-adaptive feature sharing in multi-task networks with applications in person attribute classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Javidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional oriented boundaries: From image segmentation to high-level tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="819" to="833" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attentive single-tasking of multiple tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1851" to="1860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="549" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Cross-stitch networks for multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast scene understanding for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IV Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Monocular depth estimation using neural regression forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="5506" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An overview of multi-task learning in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
	</analytic>
	<monogr>
		<title level="m">deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Multi-task learning as multi-objective optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08028</idno>
		<title level="m">Gradient adversarial training of neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Standley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07553</idno>
		<title level="m">Which tasks should be learned together in multi-task learning? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02920</idno>
		<title level="m">Branched multitask networks: Deciding what layers to share</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07919</idno>
		<title level="m">Deep high-resolution representation learning for visual recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2800" to="2809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pad-net: Multi-tasks guided predictionand-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Structured attention guided convolutional neural fields for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3917" to="3925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Joint task-recursive learning for semantic segmentation and depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="235" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pattern-affinitive propagation across depth, surface normal and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4106" to="4115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A modulation module for multi-task learning with applications in image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
