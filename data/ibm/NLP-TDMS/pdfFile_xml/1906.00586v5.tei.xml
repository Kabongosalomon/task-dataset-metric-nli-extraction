<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discovering Neural Wirings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">PRIOR</orgName>
								<orgName type="institution">Allen Institute for AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">PRIOR</orgName>
								<orgName type="institution">Allen Institute for AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">XNOR.AI</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
							<email>mohammad@xnor.ai</email>
							<affiliation key="aff0">
								<orgName type="laboratory">PRIOR</orgName>
								<orgName type="institution">Allen Institute for AI</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">XNOR.AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Discovering Neural Wirings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The success of neural networks has driven a shift in focus from feature engineering to architecture engineering. However, successful networks today are constructed using a small and manually defined set of building blocks. Even in methods of neural architecture search (NAS) the network connectivity patterns are largely constrained. In this work we propose a method for discovering neural wirings. We relax the typical notion of layers and instead enable channels to form connections independent of each other. This allows for a much larger space of possible networks. The wiring of our network is not fixed during training -as we learn the network parameters we also learn the structure itself. Our experiments demonstrate that our learned connectivity outperforms hand engineered and randomly wired networks. By learning the connectivity of MobileNetV1 <ref type="bibr" target="#b11">[12]</ref> we boost the ImageNet accuracy by 10% at ∼ 41M FLOPs. Moreover, we show that our method generalizes to recurrent and continuous time networks. Our work may also be regarded as unifying core aspects of the neural architecture search problem with sparse neural network learning. As NAS becomes more fine grained, finding a good architecture is akin to finding a sparse subnetwork of the complete graph. Accordingly, DNW provides an effective mechanism for discovering sparse subnetworks of predefined architectures in a single training run. Though we only ever use a small percentage of the weights during the forward pass, we still play the so-called initialization lottery [8] with a combinatorial number of subnetworks. Code and pretrained models are available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks have shifted the prevailing paradigm from feature engineering to feature learning. The architecture of deep neural networks, however, must still be hand designed in a process known as architecture engineering. A myriad of recent efforts attempt to automate the process of the architecture design by searching among a set of smaller well-known building blocks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20]</ref>. While methods of search range from reinforcement learning to gradient based approaches <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b19">20]</ref>, the space of possible connectivity patterns is still largely constrained. NAS methods explore wirings between predefined blocks, and <ref type="bibr" target="#b27">[28]</ref> learns the recurrent structure of CNNs. We believe that more efficient solutions may arrive from searching the space of wirings at a more fine grained level, i.e. single channels.</p><p>In this work, we consider an unconstrained set of possible wirings by allowing channels to form connections independent of each other. This enables us to discover a wide variety of operations (e.g. depthwise separable convs <ref type="bibr" target="#b11">[12]</ref>, channel shuffle and split <ref type="bibr" target="#b35">[36]</ref>, and more). Formally, we treat the network as a large neural graph where each each node processes a single channel.</p><p>One key challenge lies in searching the space of all possible wirings -the number of possible sub-graphs is combinatorial in nature. When considering thousands of nodes, traditional search methods are either prohibitive or offer approximate solutions. In this paper we introduce a simple and efficient algorithm for discovering neural wirings (DNW). Our method searches the space of all possible wirings with a simple modification of the backwards pass.</p><p>Recent work in randomly wired neural networks <ref type="bibr" target="#b34">[35]</ref> aims to explore the space of novel neural network wirings. Intriguingly, they show that constructing neural networks with random graph algorithms often outperforms a manually engineered architecture. However, these wirings are fixed at training.</p><p>Our method for discovering neural wirings is as follows: First, we consider the sole constraint that that the total number of edges in the neural graph is fixed to be k. Initially we randomly assign a weight to each edge. We then choose the weighted edges with the highest magnitude and refer to the remaining edges as hallucinated. As we train, we modify the weights of all edges according to a specified update rule. Accordingly, a hallucinated edge may strengthen to a point it replaces a real edge. We tailor the update rule so that when swapping does occur, it is beneficial.</p><p>We consider the application of DNW for static and dynamic neural graphs. In the static regime each node has a single output and the graphical structure is acyclic. In the case of a dymanic neural graph we allow the state of a node to vary with time. Dymanic neural graphs may contain cycles and express popular sequential models such as LSTMs <ref type="bibr" target="#b10">[11]</ref>. As dymanic neural graphs are strictly more expressive than static neural graphs, they can also express feed-forward networks (as in <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>Our work may also be regarded as a unification between the problem of neural architecture search and sparse neural network learning. As NAS becomes less restrictive and more fine grained, finding a good architecture is akin to finding a sparse sub-network of the complete graph. Accordingly, DNW provides an effective mechanism for discovering sparse networks in a single training run.</p><p>The Lottery Ticket Hypothesis <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> demonstrates that dense feed-forward neural networks contain so-called winning-tickets. These winning-tickets are sparse subnetworks which, when reset to their initialization and trained in isolation, reach an accuracy comparable to their dense counterparts. This hypothesis articulate an advantage of overparameterization during training -having more parameters increases the chance of winning the initialization lottery. We leverage this idea to train a sparse neural network without retraining or fine-tuning. Though we only ever use a small percentage of the weights during the forward pass, we still play the lottery with a combinatorial number of sub-networks.</p><p>We demonstrate the efficacy of DNW on small and large scale data-sets, and for feed-forward, recurrent, continuous, and sparse networks. Notably, we augment MobileNetV1 <ref type="bibr" target="#b11">[12]</ref> with DNW to achieve a 10% improvement on ImageNet <ref type="bibr" target="#b4">[5]</ref> from the hand engineered MobileNetV1 at ∼ 41M FLOPs 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Discovering Neural Wirings</head><p>In this section we describe our method for jointly discovering the structure and learning the parameters of a neural network. We first consider the algorithm in a familiar setting, a feed-forward neural network, which we abstract as a static neural graph. We then present a more expressive dynamic neural graph which extends to discrete and continuous time and generalizes feed-forward, recurrent, and continuous time neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[3x3-conv2D, stride=1]</head><p>[3x3-conv2D, stride=2] Output zero-padded <ref type="figure">Figure 2</ref>: An example of a dynamic (left) and static (right) neural graph. Details in Section 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Static Neural Graph</head><p>A static neural graph is a directed acyclic graph G = (V, E) consisting of nodes V and edges E ⊆ V × V. The state of a node v ∈ V is given by the random variable Z v . At each node v we apply a function f θv and with each edge (u, v) we associate a weight w uv . In the case of a multi-layer perceptron, f is simply a parameter-free non-linear activation like ReLU <ref type="bibr" target="#b16">[17]</ref>.</p><p>For any set A ⊆ V we let Z A denote (Z v ) v∈A and so Z V is the state of all nodes in the network.</p><p>V contains a subset of input nodes V 0 with no parents and output nodes V E with no children. The input data X ∼ p x flows into the network through V 0 as Z V0 = g φ (X ) for a function g which may have parameters φ. Similarly, the output of the networkŶ is given by h ψ (Z V E ).</p><formula xml:id="formula_0">Z v = f θv (u,v)∈E w uv Z u v ∈ V \ V 0 g (v) φ (X ) v ∈ V 0 .<label>(1)</label></formula><p>For brevity, we let I v denote the "input" to node v, where I v may be expressed</p><formula xml:id="formula_1">I v = (u,v)∈E w uv Z u .<label>(2)</label></formula><p>In this work we consider the case where the input and output of each node is a two-dimensional matrix, commonly referred to as a channel. Each node performs a non-linear activation followed by normalization and convolution (which may be strided to reduce the spatial resolution). As in <ref type="bibr" target="#b34">[35]</ref>, we no longer conform to the traditional notion of "layers" in a deep network.</p><p>The combination of a separate 3×3 convolution for each channel (depthwise convolution) followed by a 1 × 1 convolution (pointwise convolution) is often referred to as a depthwise seperable convolution, and is essential in efficient network design <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22]</ref>. With a static neural graph this process may be interpreted equivalently as a 3 × 3 convolution at each node followed by information flow on a complete bipartite graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Discovering a k-Edge neural graph</head><p>We now outline our method for discovering the edges of a static neural graph subject to the constraint that the total number of edges must not exceed k.</p><p>We consider a set of real edges E and a set of hallucinated edges E hal = V × V \ E. The real edge set is comprised of the k-edges which have the largest magnitude weight. As we allow the magnitude of the weights in both sets to change throughout training the edges in E hal may replace those in E.</p><p>Consider a hallucinated edge (u, v) ∈ E. If the gradient is pushing I v in a direction which aligns with Z u , then our update rule strengthens the magnitude of the weight w uv . If this alignment happens consistently then w uv will be eventually be strong enough to enter the real edge set E. As the total Initialize w uv by independently sampling from a uniform distribution. <ref type="bibr">3:</ref> for each training iteration do <ref type="bibr">4:</ref> Sample mini batch of data and labels (X , Y) = {(X i , Y i )} using p xy Sample data <ref type="bibr" target="#b4">5</ref>:</p><formula xml:id="formula_2">Algorithm 1 DNW-Train(V, V 0 , V E , g φ , h ψ ,</formula><formula xml:id="formula_3">E ← {(u, v) : |w uv | ≥ τ } where τ is chosen so that |E| = k Choose edges 6: Z v ← f θv (u,v)∈E w uv Z u v ∈ V \ V 0 g (v) φ (X ) v ∈ V 0 Forward pass 7:Ŷ = h ψ ({Z v } v∈V E ) Compute output 8: Update φ, {θ v } v∈V , ψ via SGD &amp; Backprop [26] using loss L Ŷ , Y 9:</formula><p>for each pair of nodes (u, v) such that u &lt; v do Update edge weights 10: number of edges is conserved, when (u, v) enters the edge set E another edge is removed and placed in E hal . This procedure is detailed by Algorithm 1, where V is the node set, V 0 , V E are the input and output node sets, g φ , h ψ and {f θv } v∈V are the input, output, and node functions, p xy is the data distribution, k is the number of edges in the graph and L is the loss.</p><formula xml:id="formula_4">w uv ← w uv + Z u , −α ∂L ∂Iv Recall I v = (u,v)∈E w uv Z u</formula><p>In practice we may also include a momentum and weight decay 2 term in the weight update rule (line 10 in Algorithm 1). In fact, the weight update rule looks nearly identical to that in traditional SGD &amp; Backprop but for one key difference: we allow the gradient to flow to edges which did not exist during the forward pass. Importantly, we do not allow the gradient to flow through these edges and so the rest of the parameters update as in traditional SGD &amp; Backprop. This gradient flow is illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>Under certain conditions we formally show that swapping an edge from E hal to E decreases the loss L.</p><p>We first consider the simple case where the hallucinated edge (i, k) replaces (j, k) ∈ E. In Section C we discuss the proof to a more general case.</p><p>We letw to denote the weight w after the weight update rulew uv = w uv + Z u , −α ∂L ∂Iv . We assume that α is small enough so that sign(w) = sign(w).</p><p>Claim: Assume L is Lipschitz continuous. There exists a learning rate α * &gt; 0 such that for α ∈ (0, α * ) the process of swapping (i, k) for (j, k) will decrease the loss on the mini-batch when the state of the nodes are fixed and |w ik | &lt; |w jk | but |w ik | &gt; |w jk |.</p><p>Proof. Let A be value of I k after the update rule if (j, k) is replaced with (i, k). Let B be the state of I k after the update rule if we do not allow for swapping. A and B are then given by</p><formula xml:id="formula_5">A =w ik Z i + (u,k)∈E, u =i,jw uk Z u , B =w jk Z j + (u,k)∈E, u =i,jw uk Z u .<label>(3)</label></formula><p>Additionally, let g = −α ∂L ∂I k be the direction in which the loss most steeply descends with respect to I k . By Lemma 1 (Section D of the Appendix) it suffices to show that moving I k towards A is more aligned with g then moving I k towards B. Formally we wish to show that</p><formula xml:id="formula_6">A − I k , g ≥ B − I k , g (4) which simplifies tow ik Z i , g ≥w jk Z j , g (5) ⇐⇒w ik (w ik − w ik ) ≥w jk (w jk − w jk ).</formula><p>(6) In the case wherew ik and (w ik −w ik ) have the same sign butw jk and (w jk −w jk ) have different signs the inequality immediately holds. This corresponds to the case where w ik increases in magnitude but w jk decreases in magnitude. The opposite scenario (w ik decreases in magnitude but w jk increases)</p><formula xml:id="formula_7">is impossible since |w ik | &lt; |w jk | but |w ik | &gt; |w jk |.</formula><p>We now consider the scenario where both sides of the inequality (equation 6) are positive. Simplifying further we obtain <ref type="bibr" target="#b6">7)</ref> and are now able to identify a range for α such that the inequality above is satisfied. By assumption the right hand side is less than 0 and sign(w) = sign(w) soww = |w||w|. Accordingly, it suffices to show that</p><formula xml:id="formula_8">(w jk w jk −w ik w ik ) ≥ w 2 jk −w 2 ik<label>(</label></formula><formula xml:id="formula_9">|w jk ||w jk | − |w ik ||w ik | ≥ 0. (8) If we let = |w jk | − |w ik | and α * = sup{α : |w ik | ≤ |w jk | + |w jk ||w ik | −1 }, then for α ∈ (0, α * ) |w jk ||w jk | − |w ik ||w ik | ≥ |w jk |   |w jk | − |w ik | = −   = 0<label>(9)</label></formula><p>the inequality (equation 7) is satisfied. Here we are implicitly using our assumption that the gradient is bounded and we may "tune" α to control the magnitude |w ik | − |w jk |. In the case where α = inf{α : |w ik | &gt; |w jk |} the right hand side of equation 7 becomes 0 while the left hand side is &gt; 0.</p><p>In Section E of the appendix we discuss the effect of θ v on w uv . In Section F of the Appendix, we show that the update rule is equivalently a straight-through estimator <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dynamic Neural Graph</head><p>We now consider a more general setting where the state of each node Z v (t) may vary through time.</p><p>We refer to this model as a dynamic neural graph.</p><p>The initial conditions of a dynamic neural graph are given by</p><formula xml:id="formula_10">Z v (0) = g (v) φ (X ) v ∈ V 0 0 v ∈ V \ V 0<label>(10)</label></formula><p>where V 0 is a designated set of input nodes, which may now have parents.</p><p>Discrete Time Dynamics: For a discrete time neural graph we consider times ∈ {0, 1, ..., L}.</p><p>The dynamics are then given by</p><formula xml:id="formula_11">Z v ( + 1) = f θv   (u,v)∈E w uv Z u ( ),  <label>(11)</label></formula><p>and the network output isŶ = h ψ (Z V E (L)). We may express equation 11 more succinctly as</p><formula xml:id="formula_12">Z V ( + 1) = f θ (A G Z V ( ), )<label>(12)</label></formula><p>where <ref type="figure">)</ref>) v∈V , and A G is the weighted adjacency matrix for graph G. Equation 12 suggests the following interpretation: At each time step we send information through the edges using A G then apply a function at each node.</p><formula xml:id="formula_13">Z V ( ) = (Z v ( )) v∈V , f θ (z, ) = (f θv (z v ,</formula><p>Continuous Time Dynamics: As in <ref type="bibr" target="#b2">[3]</ref>, we consider the case where t may take on a continuous range of values. We then arrive at dynamics given by</p><formula xml:id="formula_14">∇ Z V (t) = f θ (A G Z V (t), t) .<label>(13)</label></formula><p>Interestingly, if V 0 is a strict subset of V we uncover an Augmented Neural ODE <ref type="bibr" target="#b6">[7]</ref>.</p><p>The discrete time case is unifying in the sense that it may also express any static neural graph.</p><p>In <ref type="figure" target="#fig_0">Figure 1</ref> we illustrate than an MLP may also be expressed by a discrete time neural graph. Additionally, the discrete time dynamics are able to capture sequential models such as LSTMs <ref type="bibr" target="#b10">[11]</ref>, as long as we allow input to flow into V 0 at any time.</p><p>In continuous time it is not immediately obvious how to incorporate strided convolutions. One approach is to keep the same spatial resolution throughout and pad with zeros after applying strided convolutions. This design is illustrated by <ref type="figure">Figure 2</ref>.</p><p>We may also apply Algorithm 1 to learn the structure of dynamic neural graphs. One may use backpropogation through time <ref type="bibr" target="#b32">[33]</ref> and the adjoint-sensitivity method <ref type="bibr" target="#b2">[3]</ref> for optimization in the discrete and continuous time settings respectively. In Section 3.1, we demonstrate empirically that our method performs better than a random graph, though we do not formally justify the application of our algorithm in this setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Implementation details for Large Scale Experiments</head><p>For large scale experiments we do not consider the dynamic case as optimization is too expensive. Accordingly, we now present our method for constructing a large and efficient static neural graph.</p><p>With this model we may jointly learn the structure of the graph along with the parameters on ImageNet <ref type="bibr" target="#b4">[5]</ref>. As illustrated by <ref type="table" target="#tab_5">Table 5</ref> our model closely follows the structure of MobileNetV1 <ref type="bibr" target="#b11">[12]</ref>, and so we refer to it as MobileNetV1-DNW. We consider a separate neural graph for each spatial resolution -the output of graph G i is the input of graph G i+1 . For width multiplier <ref type="bibr" target="#b11">[12]</ref> d and spatial resolution s × s we constrain MobileNetV1-DNW to have the same number of edges for resolution s × s as the corresponding MobileNetV1 ×d. We use a slightly smaller width multiplier to obtain a model with similar FLOPs as we do not explicitly reduce the number of depthwise convolutions in MobileNetV1-DNW. However, we do find that neurons often die (have no output) and we may then skip the depthwise convolution during inference. Note that if we interpret a pointwise convolution with c 1 input channels and c 2 output channels as a complete bipartite graph then the number of edges is simply c 1 * c 2 .</p><p>We also constrain the longest path in graph G to be equivalent to the number of layers of the corresponding MobileNetV1. We do so by partitioning the nodes V into blocks B = {B 0 , ..., B L−1 } where B 0 is the input nodes V 0 , B L−1 is output nodes V E , and we only allow edges between nodes in B i and B j if i &lt; j. The longest path in a graph with L blocks is then L − 1. Splitting the graph into blocks also improves efficiency as we may operate on one block at a time. The structure of MobileNetV1 may be recovered by considering a complete bipartite graph between adjacent blocks.</p><p>The operation f θv at each non-output node is a batch-norm <ref type="bibr" target="#b13">[14]</ref> (2 parameters), ReLU <ref type="bibr" target="#b16">[17]</ref>, 3 × 3 convolution (9 parameters) triplet. There are no operations at the output nodes. When the spatial resolution decreases in MobileNetV1 we change the convolutional stride of the input nodes to 2.</p><p>In models denoted MobileNetV1-DNW-Small (×d) we also limit the last fully connected (FC) layer to have the same number of edges as the FC layer in MobileNetV1 (×d). In the normal setting of MobileNetV1-DNW we do not modify the last FC layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section we demonstrate the effectiveness of DNW for image classification in small and large scale settings. We begin by comparing our method with a random wiring on a small scale dataset  <ref type="table">Table 2</ref>: Other methods for discovering wirings (using the architecture described in <ref type="table" target="#tab_5">Table 5</ref>) tested on CIFAR-10 shown as mean and std over 5 runs. Models with † first require the complete graph to be trained. and model. This allows us to experiment in static, discrete time, and continuous settings. Next we explore the use of DNW at scale with experiments on ImageNet <ref type="bibr" target="#b4">[5]</ref> and compare DNW with other methods of discovering network structures. Finally we use our algorithm to effectively train sparse neural networks without retraining or fine-tuning.</p><p>Throughout this section we let RG denote our primary baseline -a randomly wired graph. To construct a randomly wired graph with k-edges we assign a uniform random weight to each edge then pick the k edges with the largest magnitude weights. As shown in <ref type="bibr" target="#b34">[35]</ref>, random graphs often outperform manually designed networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Small Scale Experiments For Static and Dynamic Neural Graphs</head><p>We begin by training tiny classifiers for the CIFAR-10 dataset <ref type="bibr" target="#b15">[16]</ref>. Our initial aim is not to achieve state of the art performance but instead to explore DNW in the static, discrete, and continuous time settings. As illustrated by <ref type="table" target="#tab_1">Table 1</ref>, our method outperforms a random graph by a large margin.</p><p>The image is first downsampled 3 then each channel is given as input to a node in a neural graph. The static graph uses 5 blocks and the discrete time graph uses 5 time steps. For the continuous case we backprop through the operation of an adaptive ODE solver <ref type="bibr" target="#b3">4</ref> . The models have 41k parameters. At each node we perform Instance Normalization <ref type="bibr" target="#b31">[32]</ref>, ReLU, and a 3 × 3 single channel convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ImageNet Classification</head><p>For large scale experiments on ImageNet <ref type="bibr" target="#b4">[5]</ref> we are limited to exploring DNW in the static case (recurrent and continuous time networks are more expensive to optimize due to lack of parallelization). Although our network follows the simple structure of MobileNetV1 <ref type="bibr" target="#b11">[12]</ref> we are able to achieve higher accuracy than modern networks which are more advanced and optimized. Notably, MobileNetV2 <ref type="bibr" target="#b26">[27]</ref> extends MobileNetV1 by adding residual connections and linear bottlenecks and ShuffleNet <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b21">22]</ref> introduces channel splits and channel shuffles. The results of the large scale experiments may be found in <ref type="table">Table 3</ref>.</p><p>As standard, we have divided the results of <ref type="table">Table 3</ref> to consider models which have similar FLOPs. In the more sparse case (∼ 41M FLOPs) we are able to use DNW to boost the performance of MobileNetV1 by 10%. Though random graphs perform extremely well we still observe a 7% boost in performance. In each experiment we train for 250 epochs using Cosine Annealing as the learning rate scheduler with initial learning rate 0.1, as in <ref type="bibr" target="#b34">[35]</ref>. Models using random graphs have considerably more FLOPs as nearly all depthwise convolutions must be performed. DNW allows neurons to die and we may therefore skip many operations. <ref type="table">Table 3</ref>: ImageNet Experiments (see Section 2.4 for more details). Models with * use the implementations of <ref type="bibr" target="#b21">[22]</ref>. Models with multiples asterisks use different image resolutions so that the FLOPs is comparable (see <ref type="table">Table 8</ref> in <ref type="bibr" target="#b21">[22]</ref> for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Params FLOPs Accuracy </p><formula xml:id="formula_15">MobileNetV1 (×0.5) 1.3M 149M 63.7% MobileNetV2 (×0.6) * - 141M 66.6% MobileNetV2 (×0.75) * * * - 145M 67.9% DenseNet (×1) * - 142M 54.8% Xception (×1) * - 145M 65.9% ShuffleNetV1 (×1, g = 3) - 140M 67.4% ShuffleNetV2 (×1) 2.3M 146M 69.4% MobileNetV1-RG(×0.49) 1.8M 170M 64.1% MobileNetV1-DNW(×0.49) 1.8M 154M 70.4%</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Related Methods</head><p>We compare DNW with various methods for discovering neural wirings. In <ref type="table">Table 2</ref> we use the structure of MobileNetV1-DNW but try other methods which find k-edge sub-networks. The experiments in <ref type="table">Table 2</ref> are conducted using CIFAR-10 <ref type="bibr" target="#b15">[16]</ref>. We train for 160 epochs using Cosine Annealing as the learning rate scheduler with initial learning rate α = 0.1 unless otherwise noted.</p><p>The Lottery Ticket Hypothesis: The authors of <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> offer an intriguing hypothesis: sparse subnetworks may be trained in isolation when reset to their initialization. However, their method for finding so-called winning tickets is quite expensive as it requires training the full graph from scratch. We compare with one-shot pruning from <ref type="bibr" target="#b8">[9]</ref>. One-shot pruning is more comparable in training FLOPS than iterative pruning <ref type="bibr" target="#b7">[8]</ref>, though both methods are more expensive in training FLOPS than DNW. After training the full network G full (i.e. no edges pruned) the optimal sub-network G k with k-edges is chosen by taking the weights with the highest magnitude. In the row denoted Lottery Ticket we retrain G k using the initialization of G full . We also initialize G k with the weights of G full after training -denoted by FT for fine-tune (we try different initial learning rates α). Though these experiments perform comparably with DNW, their training is more expensive as the full graph must initially be trained.</p><p>Exploring Randomly Wired Networks for Image Recognition: The authors of <ref type="bibr" target="#b34">[35]</ref> explore "a more diverse set of connectivity patterns through the lens of randomly wired neural networks." They achieve impressive performance on ImageNet <ref type="bibr" target="#b4">[5]</ref> using random graph algorithms to generate the structure of a neural network. Their network connectivity, however, is fixed during training. Throughout this section we have a random graph (denoted RG) as our primary baseline -as in <ref type="bibr" target="#b34">[35]</ref> we have seen that random graphs outperform hand-designed networks.</p><p>No Update Rule: In this ablation on DNW we do not apply the update rule to the hallucinated edges.</p><p>An edge may only leave the hallucinated edge set if the magnitude of a real edge is sufficiently decreased. This experiment demonstrates the importance of the update rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L1 + Anneal:</head><p>We experiment with a simple pruning technique -start with a fully connected graph and remove edges by magnitude throughout training until there are only k remaining. We found that accuracy was much better if we added an L1 regularization term. Targeted Dropout: The authors of <ref type="bibr" target="#b9">[10]</ref> present a simple and effective method for training a network which is robust to subsequent pruning. Their method outperforms variational dropout <ref type="bibr" target="#b22">[23]</ref> and L 0 pruning <ref type="bibr" target="#b20">[21]</ref>. We compare with Weight Dropout/Pruning from <ref type="bibr" target="#b9">[10]</ref>, which we denote as TD. Section B of the Appendix contains more information, experimental details, and hyperparameter trials for the Targeted Dropout experiments, though we provide the best result in <ref type="table">Table 2</ref>.</p><p>Neural Architecture Search: As illustrated by <ref type="table">Table 3</ref>, our network (with a very simple Mo-bileNetV1 like structure) is able to achieve comparable accuracy to an expensive method which performs neural architecture search using reinforcement learning <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Sparse Neural Networks</head><p>We may apply our algorithm for Discovering Neural Wirings to the task of training sparse neural networks. Importantly, our method requires no fine-tuning or retraining to discover a sparse subnetworks -the sparsity is maintained throughout training. This perspective was guided by the the work of Dettmers and Zettelmoyer in <ref type="bibr" target="#b5">[6]</ref>, though we would like to highlight some differences. Their work enables faster training, though our backwards pass is still dense. Moreover, their work allows for a redistribution of parameters across layers whereas we consider a fixed sparsity per layer.</p><p>Our algorithm for training a sparse neural network is similar to Algorithm 1, though we implicitly treat each convolution as a separate graph where each parameter is an edge. For each convolutional layer on the forwards pass, we use the top k% of the parameters chosen by magnitude. On the backwards pass we allow the gradient to flow to, but not through, all weights that were zeroed out on the forwards pass. All weights receive gradients as if they existed on the forwards pass, regardless of if they were zeroed out.</p><p>As in <ref type="bibr" target="#b5">[6]</ref> we leave the biases and batchnorm dense. We compare with the result in Appendix C of <ref type="bibr" target="#b5">[6]</ref>, as we also use a tuned version of a ResNet50 that uses modern optimization techniques such as cosine learning rate scheduling and warmup 5 . We train for 100 epochs and showcase our results in <ref type="table" target="#tab_4">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We present a novel method for discovering neural wirings. With a simple algorithm we demonstrate a significant boost in accuracy over randomly wired networks. We benefit from overparameterization during training even when the resulting model is sparse. Just as in <ref type="bibr" target="#b34">[35]</ref>, our networks are free from the typical constraints of NAS. This work suggests exciting directions for more complex and efficient methods of discovering neural wirings.</p><p>A Architecture dwconv denotes depthwise convolutions, pwconv denotes pointwise convolution, FC denotes a fully connected layer, d denotes width multiplier <ref type="bibr" target="#b11">[12]</ref>, c denotes the number of output channels, and s denotes stride. When omitted assume that stride is 1. Batch-norm <ref type="bibr" target="#b13">[14]</ref> and ReLU follow each convolution in MobileNetV1. When training on CIFAR-10 <ref type="bibr" target="#b15">[16]</ref> the first convolution has stride 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage Output MobilNetV1</head><p>MobileNetV1-DNW The method of Targeted Dropout is as follows: choose the bottom γ fraction of weights by magnitude and apply dropout with probability ρ. We use the same architecture as MobileNetV1-DNW (×0.225) and fix γ at each stage so that the network post pruning has the same number of edges per stage as MobileNetV1-DNW (×0.225).</p><p>In the setting we consider, unconstrained targeted weight dropout outperforms the targeted weight dropout presented in <ref type="bibr" target="#b9">[10]</ref> (which we refer to as regular). Accordingly, the results we present in <ref type="table">Table 2</ref> correspond to unconstrained targeted weight dropout. In regular targeted weight dropout, dropout is applied to the bottom γ fraction of incoming weights to each neuron. In unconstrained targeted weight dropout, we apply dropout to the bottom γ fraction of edges at a given spatial resolution. Accordingly, neurons may die and have no incoming or outgoing edges. We compare unconstrained and regular targeted dropout in <ref type="table">Table 6</ref>. <ref type="table">Table 6</ref>: Comparing variants of targeted weight dropout using the architecture described in <ref type="table" target="#tab_5">Table 5</ref> and tested on CIFAR-10 shown as mean and std over 5 runs. Model Accuracy Accuracy (Unconstrained) (Regular) TD ρ = 0.9 89.0 ± 0.2% 87.9 ± 0.5% TD ρ = 0.95 89.2 ± 0.4% 87.9 ± 0.2% TD ρ = 0.99 88.6 ± 0.2% 87.7 ± 0.3% TD ρ = γ 88.8 ± 0.2% 87.9 ± 0.2%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C A More General Case</head><p>We now consider the case where the hallucinated edge (i, ) replaces (j, k) ∈ E.</p><p>As before we usew to denote the weight w after the weight update rulew uv = w uv + Z u , −α ∂L ∂Iv . We assume that α is small enough so that sign(w) = sign(w).</p><p>Claim: Assume L is Lipschitz continuous. There exists a learning rate α * &gt; 0 such that for α ∈ (0, α * ) the process of swapping (i, ) for (j, k) will decrease the loss when the state of the nodes are fixed, there is no path from i to j, and |w i | &lt; |w jk | but |w i | &gt; |w jk |.</p><p>Proof. Let A k , A be value of I k and I after the update rule if (j, k) is replaced with (i, ). Let B k and B be the state of I k and I after the update rule if we do not allow for swapping. A k , A , B k and B are then given by</p><formula xml:id="formula_16">A k = (u,k)∈E, u =jw uk Z u , B k =w jk Z j + (u,k)∈E, u =jw uk Z u<label>(14)</label></formula><p>A</p><formula xml:id="formula_17">=w i Z i + (u, )∈E, u =iw u Z u , B = (u, )∈E, u =iw u Z u .<label>(15)</label></formula><p>Additionally, let g k = −α ∂L ∂I k and g = −α ∂L ∂I be the direction in which the loss most steeply descends with respect to I k and I . By Lemma 1 (Section D of the Appendix) it suffices to show that</p><formula xml:id="formula_18">A k − I k , g k + A − I , g ≥ B k − I k , g k + B − I , g<label>(16)</label></formula><p>which simplifies tow i Z i , g ≥w jk Z j , g k (17) ⇐⇒w i (w i − w i ) ≥w jk (w jk − w jk ).</p><p>We are now in the equivalent setting as equation 6 and may complete the proof as before.</p><p>In practice there may be a path from i and j the state of the nodes will never be the fixed due to stochasticity of mini-batches and updates to the rest of the parameters in the network. However, as the graph grows large the state of one node will have little effect on the state of another, even if there is a path between them. The proofs are done in an idealized case and the empirical results demonstrate that the method works in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Lemma 1</head><p>Here we show that for sufficiently small α,</p><formula xml:id="formula_20">γ 1 , −α ∂L ∂I v &gt; γ 2 , −α ∂L ∂I v<label>(19)</label></formula><p>implies that L (I v + αγ 1 ) &lt; L (I v + αγ 2 ) .</p><p>Note that for brevity we have written the loss as a function of I v . By taking a Taylor expansion we find that</p><formula xml:id="formula_22">L (I v + αγ) (21) = L (I v ) + αγ, ∂L ∂I v + O(α 2 )<label>(22)</label></formula><p>and so for sufficiently small α</p><formula xml:id="formula_24">L (I v ) − L (I v + αγ) ≈ γ, −α ∂L ∂I v<label>(24)</label></formula><p>which completes the lemma.</p><p>An equivalent argument holds for two dimensions.</p><formula xml:id="formula_25">γ 1 , −α ∂L ∂I v + ξ 1 , −α ∂L ∂I u &gt; γ 2 , −α ∂L ∂I v + ξ 2 , −α ∂L ∂I u<label>(25)</label></formula><p>implies that L (I v + αγ 1 , I u + αξ 1 ) &lt; L (I v + αγ 2 , I u + αξ 2 ) . </p><p>and so for sufficiently small α</p><formula xml:id="formula_28">L (I v , I u ) − L (I v + αγ, I u + αξ) ≈ γ, −α ∂L ∂I v + ξ, −α ∂L ∂I u .<label>(30)</label></formula><p>E Effect of θ v on w uv</p><p>One concern is that convolution or batch normalization <ref type="bibr" target="#b13">[14]</ref> in f θv would make it difficult to choose edges by magnitude. Consider the case where a convolutional kernel is scaled up arbitrarily in magnitude. Even if the incoming edges were important, their magnitudes would likely be small. As a consequence, they would not be chosen.</p><p>Here we argue that this is unlikely to occur. When batch normalization <ref type="bibr" target="#b13">[14]</ref> is present, the "energy" of the incoming weights are conserved. A formal treatment of this statement is provided by Thoerem 3 of <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Reformulation as a Straight-Through Estimator</head><p>We now reformulate the update rule as a straight-through estimator <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref>. We are equivalently computing the input to node v as</p><formula xml:id="formula_29">I v = u∈V h(w uv )Z u<label>(31)</label></formula><p>where h(w uv ) = w uv 1 {|wuv|&gt;τ } in the forward pass. Even though h has gradient 0 when |w uv | ≤ τ , we would still like a mechanism for updating w uv in the backward pass. Here we may use the "straightthrough" estimator <ref type="bibr" target="#b0">[1]</ref>, and let h be the identity in the backward pass (i.e. we go straight-through h).</p><p>Then I v = u∈V w uv Z u in the backward pass and we then compute</p><formula xml:id="formula_30">∂I v ∂w uv = Z u<label>(32)</label></formula><p>which, when using the chain rule and a standard SGD update, aligns with our update rule (line 10 in Algorithm 1). This is exactly how we implement the update rule in PyTorch <ref type="bibr" target="#b23">[24]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Dynamic Neural Graph: A 3-layer perceptron (left) can be expressed by a dynamic neural graph with 3 time steps (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Gradient flow: On the forward pass we use only on the real edges. On the backwards pass we allow the gradient to flow to but not through the hallucinated edges (as in Algorithm 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>0 112 × 5 × 3 × 7 × 7 3 ×</head><label>1125373</label><figDesc>112 3 × 3 conv, c = 32d , s = 2 g φ = (3 × 3 conv, c = 32 , s = 2)1 112 × 112 3 × 3 dwconv, c = 32d G 1 with |V| = 32 + 64 3 × 3 pwconv, c = 64d |V 0 | = 32, |V E | = 64, |B| = 2 2 56 × 56 3 × 3 dwconv, c = 64d, s = 2 G 2 with |V| = 64 + 2 * 128 3 × 3 pwconv, c = 128d |V 0 | = 64, |V E | = 128, |B| = 3 3 × 3 dwconv, c = 128d 3 × 3 pwconv, c = 128d 3 28 × 28 3 × 3 dwconv, c = 128d, s = 2 G 3 with |V| = 128 + 2 * 256 3 × 3 pwconv, c = 256d |V 0 | = 128, |V E | = 256, |B| = 3 3 × 3 dwconv, c = 256d 3 × 3 pwconv, c = 256d4 14 × 14 3 × 3 dwconv, c = 256d, s = 2 G 4 with |V| = 256 + 6 * 512 3 × 3 pwconv, c = 512d |V 0 | = 256, |V E | = 512, |B| = 7 3 dwconv, c = 512d 3 × 3 pwconv, c = 512d 5 3 dwconv, c = 512d, s = 2 G 5 with |V| = 512 + 2 * 1024 3 × 3 pwconv, c = 1024d |V 0 | = 512, |V E | = 1024, |B| = 3 3 × 3 dwconv, c = 1024d 3 × 3 pwconv, c = 1024d 6 1000 7 × 7 pool, 1024d × 1000 FC h ψ = (7 × 7 pool, 1024 × 1000 FC) B Targeted Dropout: Details, Regular vs. Unconstrained and Additional Hyperparameters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>{f θv } v∈V , p xy , k, L) 1: for each pair of nodes (u, v) such that u &lt; v do Initialize</figDesc><table /><note>2:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Testing a tiny (41k parameters) classifier on CIFAR-10<ref type="bibr" target="#b15">[16]</ref> in static and dynamic settings shown as mean and standard deviation (std) over 5 runs.</figDesc><table><row><cell>Model</cell><cell>Accuracy</cell></row><row><cell>Static (RG)</cell><cell>76.1 ± 0.5%</cell></row><row><cell>Static (DNW)</cell><cell>80.9 ± 0.6%</cell></row><row><cell>Discrete Time (RG)</cell><cell>77.3 ± 0.7%</cell></row><row><cell cols="2">Discrete Time (DNW) 82.3 ± 0.6%</cell></row><row><cell>Continuous (RG)</cell><cell>78.5 ± 1.2%</cell></row><row><cell>Continuous (DNW)</cell><cell>83.1 ± 0.3%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Training a tuned version of ResNet50 on ImageNet with modern optimization techniques, as in Appendix C of<ref type="bibr" target="#b5">[6]</ref>. For All Layers Sparse, every layer has a fixed sparsity. In contrast, we leave the very first convolution dense for First Layer Dense. The parameters in the first layer constitute only 0.04% of the total network.</figDesc><table><row><cell>Method</cell><cell cols="3">Weights (%) Top-1 Accuracy Top-5 Accuracy</cell></row><row><cell>Sparse Networks from Scratch [6]</cell><cell>10%</cell><cell>72.9%</cell><cell>91.5%</cell></row><row><cell>Ours -All Layers Sparse</cell><cell>10%</cell><cell>74.0%</cell><cell>92.0%</cell></row><row><cell>Ours -First Layer Dense</cell><cell>10%</cell><cell>75.0%</cell><cell>92.5%</cell></row><row><cell>Sparse Networks from Scratch [6]</cell><cell>20%</cell><cell>74.9%</cell><cell>92.5%</cell></row><row><cell>Ours -All Layers Sparse</cell><cell>20%</cell><cell>76.2%</cell><cell>93.0%</cell></row><row><cell>Ours -First Layer Dense</cell><cell>20%</cell><cell>76.6%</cell><cell>93.4%</cell></row><row><cell>Sparse Networks from Scratch [6]</cell><cell>30%</cell><cell>75.9%</cell><cell>92.9%</cell></row><row><cell>Ours -All Layers Sparse</cell><cell>30%</cell><cell>76.9%</cell><cell>93.4%</cell></row><row><cell>Ours -First Layer Dense</cell><cell>30%</cell><cell>77.1%</cell><cell>93.5%</cell></row><row><cell>Sparse Networks from Scratch [6]</cell><cell>100%</cell><cell>77.0%</cell><cell>93.5%</cell></row><row><cell>Ours -Dense Baseline</cell><cell>100%</cell><cell>77.5%</cell><cell>93.7%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>The general structure of MobileNetV1 vs. MobileNetV1-DNW for ImageNet experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>By taking a Taylor expansion we find that L (I v + αγ, I u + αξ)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(27)</cell></row><row><cell>= L (I v , I u ) + αγ,</cell><cell>∂L ∂I v</cell><cell>+ αξ,</cell><cell>∂L ∂I u</cell><cell>+ O(α 2 )</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We follow<ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b21">22]</ref> and define FLOPS as the number of Multiply Adds.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Weight decay<ref type="bibr" target="#b17">[18]</ref> may in fact be very helpful for eliminating dead ends.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We use two 3 × 3 strided convolutions. The first is standard while the second is depthwise-separable.<ref type="bibr" target="#b3">4</ref> We use a 5th order Runge-Kutta method<ref type="bibr" target="#b28">[29]</ref> as implemented by<ref type="bibr" target="#b2">[3]</ref> (from t = 0 to 1 with tolerance 0.001).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We adapt the code from https://github.com/NVIDIA/DeepLearningExamples/tree/master/ PyTorch/Classification/RN50v1.5, using the exact same hyperparameters but training for 100 epochs.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Sarah Pratt, Mark Yatskar and the Beaker team. We also thank Tim Dettmers for his assistance and guidance in the experiments regarding sparse networks. This work is in part supported by DARPA N66001-19-2-4031, NSF IIS-165205, NSF IIS-1637479, NSF IIS-1703166, Sloan Fellowship, NVIDIA Artificial Intelligence Lab, the Allen Institute for Artificial Intelligence, and the AI2 fellowship for AI. Computations on beaker.org were supported in part by credits from Google Cloud.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno>abs/1308.3432</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Sparse networks from scratch: Faster training without losing performance. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Augmented neural odes. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilien</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The lottery ticket hypothesis at scale. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karolina</forename><surname>Gintare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carbin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning sparse networks using targeted dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax. ArXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno>abs/1611.01144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A simple weight decay can improve generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Hertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Darts: Differentiable architecture search. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning sparse neural networks through l0 regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno>abs/1712.01312</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Variational dropout sparsifies deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsenii</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><forename type="middle">P</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep expander networks: Efficient deep networks from graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameya</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><forename type="middle">M</forename><surname>Namboodiri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>Mobilenetv2: Inverted residuals and linear bottlenecks</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning implicitly recurrent cnns through parameter sharing. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maire</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Some practical runge-kutta formulas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shampine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Comput</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">173</biblScope>
			<biblScope unit="page" from="135" to="150" />
			<date type="published" when="1986-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/1807.11626</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Luck matters: Understanding training dynamics of deep relu networks. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qucheng</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>abs/1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="1990-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03443</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Exploring randomly wired neural networks for image recognition. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
