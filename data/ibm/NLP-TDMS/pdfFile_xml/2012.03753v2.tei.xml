<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Pre-training for Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengpan</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Pre-training for Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a large scale unlabeled person re-identification (Re-ID) dataset "LUPerson" and make the first attempt of performing unsupervised pre-training for improving the generalization ability of the learned person Re-ID feature representation. This is to address the problem that all existing person Re-ID datasets are all of limited scale due to the costly effort required for data annotation. Previous research tries to leverage models pre-trained on ImageNet to mitigate the shortage of person Re-ID data but suffers from the large domain gap between ImageNet and person Re-ID data. LUPerson is an unlabeled dataset of 4M images of over 200K identities, which is 30× larger than the largest existing Re-ID dataset. It also covers a much diverse range of capturing environments (e.g., camera settings, scenes, etc.). Based on this dataset, we systematically study the key factors for learning Re-ID features from two perspectives: data augmentation and contrastive loss. Unsupervised pre-training performed on this largescale dataset effectively leads to a generic Re-ID feature that can benefit all existing person Re-ID methods. Using our pre-trained model in some basic frameworks, our methods achieve state-of-the-art results without bells and whistles on four widely used Re-ID datasets: CUHK03, Mar-ket1501, DukeMTMC, and MSMT17. Our results also show that the performance improvement is more significant on small-scale target datasets or under few-shot setting. Code is available at: https://github.com/DengpanFu/ LUPerson.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Model pre-training plays an indispensable role in person Re-identification (Re-ID). Compared to other vision tasks, as data collection and annotation for Re-ID is extremely difficult and expensive, existing public datasets all have a lim-* Corresponding author.  <ref type="figure" target="#fig_4">Figure 1</ref>: Person Re-ID performance comparison of applying different pre-trained models on two methods: IDE <ref type="bibr" target="#b44">[45]</ref> and MGN <ref type="bibr" target="#b37">[38]</ref>. We report the results on different dataset scales for Market1501 and DukeMTC with small-scale setting. IN sup. and LUP unsup. are the supervised model trained on ImageNet and the unsupervised model trained on LUPerson, respectively.</p><p>ited scale in terms of image number (largest MSMT17 <ref type="bibr" target="#b38">[39]</ref>, 126K images), person identities (largest Airport <ref type="bibr" target="#b24">[25]</ref>, 9,651 identities) and captured environments (&lt; 20 scenes, fixed cameras and resolution). To mitigate the shortage of person Re-ID data, previous research has tried to leverage models pre-trained on ImageNet and transfer the pre-trained feature to Re-ID tasks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b12">13]</ref>. However, it is arguable if using ImageNet for pre-training is optimal, due to the large domain gap between ImageNet and person Re-ID data. Inspired by the recent success of self-supervised learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19]</ref>, we make the first attempt towards large scale unsupervised pre-training for person Re-ID feature representation learning in this paper. Considering the limited scale of existing Re-ID datasets, we build a new Large-scale Unlabeled Person Re-ID dataset "LUPerson". It consists of 4M person images of over 200K identities extracted from 46K YouTube videos, which is 30× larger than the largest existing Re-ID dataset MSMT <ref type="bibr" target="#b38">[39]</ref>. Moreover, the collected videos cover a wide range of capturing environments (e.g., using fixed or moving cameras, under dynamic scenes, or having different resolutions), yielding a great data diversity which is essential for learning generic representation. We hope this study and the developed LUPerson dataset will serve as a solid baseline and motivate more feature representation learning researches for person re-identification.</p><p>Based on the LUPerson dataset, we systematically study the problem of unsupervised Re-ID feature learning. We find that directly applying the commonly used contrastive learning method such as MoCo v2 <ref type="bibr" target="#b8">[9]</ref> does not work well for the Re-ID task. After a careful investigation, we discover some unique factors when applying unsupervised pretraining to the Re-ID task: 1) As a common data augmentation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b0">1]</ref>, the color distortion (e.g., color jitter) is harmful to Re-ID feature learning. This is because the color information is a crucial clue for Re-ID. 2) To prevent contrastive learning from degrading to a trivial solution, a strong task-specific augmentation operation RandomErasing <ref type="bibr" target="#b48">[49]</ref> is proved as beneficial as in supervised Re-ID training. 3) How to use a proper temperature parameter in the contrastive loss plays an important role in finding a balance between maintaining the discriminativity and mining the hard negatives.</p><p>We demonstrate the effectiveness of our pre-trained model on various person Re-ID datasets. Upon the strong MGN <ref type="bibr" target="#b37">[38]</ref> baseline, our pre-trained model can improve the mAP by 3.5% on Market1501 <ref type="bibr" target="#b43">[44]</ref>, 2.7% on DukeMTMC <ref type="bibr" target="#b46">[47]</ref>, and 2.0% on MSMT17 <ref type="bibr" target="#b38">[39]</ref>; while achieving 2.9% mAP gain on CUHK03 <ref type="bibr" target="#b43">[44]</ref> based on another strong BDB <ref type="bibr" target="#b9">[10]</ref> baseline. These results are superior to all the state-of-the-art methods. Our results show that the performance improvement is even more significant with small-scale training samples for different datasets and baselines, as shown in <ref type="figure" target="#fig_4">Fig.1</ref>. Besides, our pre-trained model is also general to unsupervised Re-ID methods. Based on the strongest baseline SpCL <ref type="bibr" target="#b14">[15]</ref>, our pre-trained model consistently achieves remarkable improvements on various datasets. To the best of our knowledge, this is the first showing that large scale unsupervised pre-training can significantly benefit the person Re-ID task.</p><p>Our key contributions can be summarized as follows:</p><p>• We build a large-scale unlabeled dataset LUPerson, which consists of 4M images for over 200K identities, for unsupervised person Re-ID feature learning. This dataset is much larger than any existing public datasets and enables the first unsupervised pre-training for person Re-ID tasks.</p><p>• We make generic unsupervised pre-training possible for Re-ID tasks, by carefully investigating the crucial factors, such as data augmentation strategies and the temperature usage in the contrastive learning framework.</p><p>• The unsupervised representation learning is general to not only supervised Re-ID methods, but also unsupervised Re-ID methods, and helps significantly improve their performance on different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Supervised Person Re-ID. Most existing Re-ID approaches are based on supervised learning on labeled datasets. There are three typical categories of approaches: 1) learning a global feature from the whole image, with supervision imposed through a classification loss, e.g., IDE model <ref type="bibr" target="#b44">[45]</ref> and <ref type="bibr" target="#b32">[33]</ref>; 2) using a hard triplet loss on the global feature to ensure a smaller distance for features of the same person, such as <ref type="bibr" target="#b21">[22]</ref>; 3) learning a part-based feature instead. For example, Sun et al. <ref type="bibr" target="#b35">[36]</ref> proposed to partition an image feature into multiple horizontal strips each learned with a separate classification loss, and Suh et al. <ref type="bibr" target="#b33">[34]</ref> presented a part-aligned bi-linear representations. The MGN <ref type="bibr" target="#b37">[38]</ref>, known as one of the state-of-the-art (SOTA) methods, combined both a classification loss on the global feature and a triplet loss on the local features. Our pre-trained model can be used in the above three representative methods, and show better performance and generalization ability.</p><p>Unsupervised Person Re-ID. Recently, some works attempted to directly train an unsupervised Person Re-ID model without utilizing any labels on existing Re-ID datasets. BUC <ref type="bibr" target="#b26">[27]</ref> proposes a bottom-up hierarchical clustering method to jointly optimize a network and the relationship among samples. Mutual Mean-Teaching (MMT) <ref type="bibr" target="#b13">[14]</ref> adopts two networks and learns mutually to refine the hard and soft pseudo labels in the target domain to mitigate the effects of noisy pseudo labels. MMCL <ref type="bibr" target="#b36">[37]</ref> formulates unsupervised person Re-ID as a multi-label classification task to progressively seek true labels. SpCL <ref type="bibr" target="#b14">[15]</ref> is known as the SOTA approach in this literature. Its key idea is to finetune the network using pseudo labels generated from reliable clustering results trained with contrastive losses. In contrast to these methods, our work focuses on the unsupervised pre-training phase to learn a feature representation that can be generalized to either supervised or unsupervised Re-ID approaches. Unsupervised Representation Learning. Benefit from contrastive learning <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19]</ref>, unsupervised pre-training can learn feature representations with comparable quality to that learned from supervised approaches. Specifically, Wu et al. <ref type="bibr" target="#b39">[40]</ref> proposed to store representations within a memory bank. MoCo and MoCo v2 <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b8">9]</ref> introduced a dynamic queue to maintain slowly updated representations to generate negative samples. On the contrary, SimCLR and SimCLR v2 <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> proved that a projection head and rich data augmentations can also lead to advantageous visual representations even without these memory structures.</p><p>Recently, BYOL <ref type="bibr" target="#b16">[17]</ref> shows a good performance even without using any negative pairs. In this paper, we choose the MoCo v2 <ref type="bibr" target="#b7">[8]</ref> framework as our unsupervised pre-training method. Unfortunately, directly applying this framework on person Re-ID tasks does not work well, which requires an in-depth study to identify the key factors which make Re-ID tasks different from generic visual representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">LUPerson: Large-scale Re-ID Dataset</head><p>Data is the life-blood of training deep neural network models and ensuring their success. For the person Re-ID task, sufficient and high-quality data are also indispensable for increasing the model's generalization capability. A good Re-ID dataset requires not only a large amount of identities, each appearing in multiple cameras, but also a great diversity in terms of pose variation and capturing environments (e.g., camera angles, resolutions, scenes, etc.). Unfortunately, developing and annotating such a large-scale Re-ID dataset is extremely difficult and expensive.</p><p>All existing Re-ID datasets are of limited scale and diversity. <ref type="table" target="#tab_0">Table 1</ref> lists the statistics of existing popular Re-ID datasets, including VIPeR <ref type="bibr" target="#b15">[16]</ref>, GRID <ref type="bibr" target="#b27">[28]</ref>, CUHK03 <ref type="bibr" target="#b25">[26]</ref>, Market-1501 <ref type="bibr" target="#b43">[44]</ref>, Airport <ref type="bibr" target="#b24">[25]</ref>, DukeMTMC <ref type="bibr" target="#b46">[47]</ref>, and MSMT17 <ref type="bibr" target="#b38">[39]</ref>. As we can see, the largest dataset only consists of less than 0.2M images and the largest number of identities is less than 10K. Moreover, these datasets cover very limited scenes (&lt; 20) and camera settings. As a result, it is hard to use such datasets to learn a high-quality feature representation which is as good as that learned in the generic image classification task which can utilize 1.2M images (i.e., ImageNet-1k) or even 14M images (i.e., ImageNet-22k).</p><p>In this work, we ask the question: can we develop a person Re-ID dataset which is as large as ImageNet? To address this question, we build LUPerson: a Large-scale Unlabeled Person Re-ID dataset, consisting of 4M images for more than 200K identities collected from 46K scenes. To our best knowledge, this is the largest scale person Re-ID dataset.</p><p>The development of LUPerson is inspired by the recent success of unsupervised pre-training on generic image classification tasks. Although LUPerson is an unlabeled dataset, the large number of identities and diverse capturing environments included in the dataset offer a great potential for learning a high-quality person Re-ID feature representation that can be utilized to boost the performance of all kinds of Re-ID CNN models. We will make the dataset publicly available to motivate more research works to advance the state of the art of person re-identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Collection and Processing</head><p>To build the dataset, we crawled over 70K streetview videos from YouTube by using queries like "cityname + streeview (or scene)". To cover a large diversity, we chose To ensure a high quality, we further filtered out some invalid videos by checking the following cases: 1) duplicated videos with the same name (YouTube key); 2) videos containing less than 100 frames; 3) static videos (we evenly sample 5 frames and treat this video as a static video if these frames are the same); 4) virtual reality (VR) videos. In total, 50,534 videos are remained after filtering.</p><p>We follow the process of building existing person Re-ID datasets, which extract each person instance detected in every image. We use YOLO-v5 2 trained on MS-COCO to detect persons in every sampled frame. Considering that some instances only show partially visible body, we apply HRNet <ref type="bibr" target="#b34">[35]</ref> to detect the body key-points. Such key-points can be categorized into three types: head, upper body, lower body. In our specification, one person image is regarded as valid if it satisfies the following requirements: be larger than 1.5 and less than 5; 4) The detection confidence must be larger than 0.72; 5) The bounding box width must be larger than 48 pixels. Besides, we extract person every 100 frames. After applying these rules, we finally get 4,180,243 person images. To make an estimation of the person number, we adopt the following strategy: we search all the frames in a video to find the frame with the max number of persons and take the max number as the number of person of this video, then we sum up the persons of all the videos and get the number 219, 848. Some example images are given in <ref type="figure" target="#fig_1">Figure 2</ref>, it shows that our LUPerson has very diverse backgrounds and pose variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Comparison with Existing Datasets</head><p>Compared with existing datasets <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b38">39]</ref>, LUPerson is superior in the following aspects.</p><p>1) The numbers of images and identities. To the best of our knowledge, LUPerson is the largest person Re-ID dataset and contains over 4M images of 200K identities, which is 30× larger than MSMT17 <ref type="bibr" target="#b38">[39]</ref>.</p><p>2) Diverse environment. LUPerson is collected from much diverse environments (containing 46,260 scenes) with both static and dynamic camera views of street, campus, supermarket, sport fields, etc., while existing datasets were normally collected from a few fixed environments (e.g., campus, or no more than 15 scenes <ref type="bibr" target="#b38">[39]</ref>) and limited camera views. Diverse environments covered by LUPerson are essential for learning a generic Re-ID feature that can be generalized to real applications.</p><p>3) Complex devices. The raw videos of LUPerson are captured by diverse devices, such as variant types of video recorders, vlogs, smart phones, and surveillance cameras, in contrast to existing datasets which often share the same hardware property.   <ref type="bibr" target="#b18">[19]</ref>.</p><p>also has large pose variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Unsupervised Pre-training for Person Re-ID</head><p>Based on the LUPerson dataset, we attempt to pre-train an unsupervised model for improving the generalization ability of the learned person Re-ID feature representation. Without loss of generality, we leverage the widely used contrastive learning method MoCoV2 <ref type="bibr" target="#b8">[9]</ref> as a simple baseline. However, we find such a general setting does not work well on the person Re-ID task. It requires an in-depth study and re-design.</p><p>In this section, we will firstly introduce the contrastive learning method in section 4.1, and then present the key changes specifically for the person Re-ID task in section 4.2. Finally, we will show how the learned representation benefits existing person Re-ID methods in section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Contrastive Learning</head><p>Contrastive learning can be regarded as a dictionary look-up problem. Given an encoded query q and a set of encoded key samples {k 1 , ..., k K } in the dictionary, the contrastive learning is essentially to encourage q to be similar to the positive samples {k + i } and dissimilar to the negative samples {k − i }. Since no label information is available in unspervised learning, we do not know which are positive or negative. Hence, existing methods (e.g., MoCo <ref type="bibr" target="#b18">[19]</ref>) adopt a self-learning strategy. Specifically, as shown in <ref type="figure" target="#fig_3">Figure 3,</ref> given an image sample x, two different augmentations T 1 (x), T 2 (x) are used to generate two different samples. Then one sample is regarded as query q and the other is regarded as the positive sample k + . All the samples not augmented from the same image of q are then regarded as the negative samples k − . Formally, the contrastive loss is defined as:</p><formula xml:id="formula_0">L c = −log exp(q · k + /τ ) exp(q · k + /τ ) + K−1 i=0 exp(q · k − i /τ ) ,<label>(1)</label></formula><p>where τ is a temperature hyper-parameter, K is the number of negative samples.</p><p>To obtain (q, k), separated encoders f q and f k are used respectively, and both of them consist of a base feature extractor network (e.g., ResNet50 backbone) and an extra projection head (e.g., two-layer MLP). Intuitively, the above contrastive loss in Equation 1 can be viewed as a (K + 1)way classification loss. As demonstrated in existing methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b39">40]</ref>, a large K is beneficial because a rich set of negative samples are covered. One key idea of MoCo is using a queue to maintain a large dictionary and progressively updating the samples in the dictionary with the latest ones. However, updating the key encoder with a large queue by back-propagation is intractable. To solve this issue, another key idea is using momentum updating scheme for f k .</p><formula xml:id="formula_1">θ k := mθ k + (1 − m)θ q ,<label>(2)</label></formula><p>where θ k , θ q are the parameters of f k , f q , and m is a momentum coefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Key Factors to Study</head><p>Compared with general image recognition tasks, person Re-ID is a more challenging task since it needs to seek more fine-grained details for robust person association. Empirically, we observe that directly applying MoCoV2 <ref type="bibr" target="#b8">[9]</ref> cannot learn a good representation. To improve the performance, we systematically study some key factors: data augmentation and temperature tuning strategy in the contrastive loss.</p><p>Data Augmentation. Data augmentation is crucial to selfsupervised contrastive learning. Though the effects of different augmentation operators have been extensively studied for general representation learning <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19]</ref>, we find the recommended augmentations are not optimal to the person Re-ID task.</p><p>Firstly, we re-investigate the effect of each augmentation operator used in MoCoV2 with respect to the transfer performance on the CUHK03 dataset. Here, we mainly focus on color distortion augmentations (including random grayscale, Gaussian blurring and color jitter), since common augmentations like cropping, resizing and flipping are still indispensable to the Re-ID task. The results are shown in <ref type="table" target="#tab_2">Table 2</ref>. Compared with the "default" strategy in Mo-CoV2, we exclude one augmentation at each time. For random grayscale and Gaussian blurring, removing each has limited impact to the performance. Thus, we will not study them in our augmentation. For color jitter, the performance is boosted by 0.6% in terms of mAP and 0.6% in terms of cmc1 after it is disabled. This is because person Re-ID heavily relies on color information (e.g. color of clothes, color of bags) for association, while color jitter will harm its performance.</p><p>Then, we revisit the task-specific augmentation Ran-domErasing <ref type="bibr" target="#b48">[49]</ref>, which randomly selects a rectangle region in an image and erases its pixels with random values during training. It has been widely used in person Re-ID methods. As we can see in <ref type="table" target="#tab_2">Table 2</ref>, adding RandomErasing can achieve about 0.8% gain in terms of mAP. Besides, relatively large strength of RandomErasing can help achieve a better result, as shown in <ref type="table" target="#tab_3">Table 3</ref>. Note that the optimal RandomErasing strength on existing person Re-ID method is 0.4, which is smaller than the optimal value (0.6) in our unsupervised pre-training. This indicates that unsupervised pre-training benefits more from stronger data augmentation than vanilla person Re-ID training.</p><p>As a summary, we make two important changes in data augmentation for Re-ID contrastive learning: adding Ran-domErasing with high strength and removing color jitter.</p><p>Temperature Tuning Strategy. In the contrastive loss, the temperature hyper-parameter τ effectively weights different examples as shown in Equation 1. A too large value of τ will reduce the discriminativity between positive samples k + and negative samples k − since it makes exp(q · k + /τ ) close to exp(q · k − /τ ). By contrast, a too small value of τ would be harmful for the model to learn from hard negative samples, since it makes softmax output more spike towards the positive pair and cannot help the model learn from hard negatives. Hence, an appropriate temperature is critical to help the model learn a discriminative representation.</p><p>For this purpose, we search for an optimal τ with respect to the transfer performance on existing person Re-ID datasets (e.g., CUHK03 <ref type="bibr" target="#b25">[26]</ref>). The result is shown in Table 4. Using the default τ = 0.2 leads to mAP = 71.1; switching to a smaller τ , the accuracy increases to mAP = 74.7 (τ = 0.07). The phenomena consistently appears in other target Re-ID datasets. More interestingly, in general image recognition <ref type="bibr" target="#b8">[9]</ref>, the accuracy is reduced from 66.2 to 62.9 when τ varies from 0.2 to 0.07. One possible reason is that person Re-ID data is inherently more fine-grained compared with general image recognition data, like Ima-geNet. In other words, Re-ID data has smaller inter-class variations, which make positive samples close to negative samples. To avoid reducing the discriminitivity, a small temperature is better. It may suggest that a smaller temperature value is more appropriate for contrast learning on    fine-grained recognition problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transferring Features</head><p>The unsupervised pre-training performed on this largescale dataset effectively learns a generic person Re-ID feature representation that can benefit existing supervised and unsupervised person Re-ID methods. As mentioned in <ref type="bibr" target="#b18">[19]</ref>, features produced by unsupervised pre-training can have different distributions compared with ImageNet supervised pre-training. Besides, the training hyper-parameters (e.g., learning rate) of existing Re-ID methods are also tuned for supervised pre-training models, and thus may not be optimal for unsupervised pre-training models.</p><p>To address this issue, we add an extra batch normalization (BN) layer for re-calibration after anywhere the pretrained feature is used. For example, in MGN <ref type="bibr" target="#b37">[38]</ref>, as three heads are appended after the backbone features, we add three extra BN layers before each head. With this strategy, we can use the same hyper-parameters as the ImageNet supervised counterpart. GPUs for 200 epochs. The initial learning rate is 0.3 with batch size 2560. If not specified, the backbone network we used is ResNet50. Dataset. To demonstrate the superiority of our pre-training models, we conduct extensive experiments on four target person Re-ID datasets, including CUHK03, Market, DukeMTMC and MSMT17. For CUHK03, following the new protocols proposed in <ref type="bibr" target="#b47">[48]</ref>, we split this dataset into two parts: 7,365 images with 767 identities for training and 6,732 images with 700 identities for testing, and we only use its labeled sub-set. For the other three datasets, we use their official settings. Evaluation Protocol. In all the experiments, we follow the standard evaluation metrics: mean Average Precision (mAP) and the Cumulated Matching Characteristics top-1 (cmc1) metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Improving Supervised Re-ID Methods</head><p>In this section, we show the performance improvement by replacing the supervised pre-trained model on ImageNet with our unsupervised pre-trained model on LUPerson in three representative supervised Re-ID baselines: Trip <ref type="bibr" target="#b21">[22]</ref>, IDE <ref type="bibr" target="#b44">[45]</ref> and MGN <ref type="bibr" target="#b37">[38]</ref>. The Trip and IDE are our reimplementation based on open source and have comparable or better performance compared with the original papers' claim. For MGN, we use its popular implementation in fast-reid <ref type="bibr" target="#b19">[20]</ref> 3 , which can obtain considerable higher performance than what is reported in MGN <ref type="bibr" target="#b37">[38]</ref>. <ref type="table" target="#tab_6">Table 5</ref> shows the detailed improvements over 4 popular person Re-ID datasets. It can be seen that, equipped with our new per-trained model, all the three methods can have more than 4.2%, 3.5%, 2.7%, 2% improvement in terms of mAP on CUHK03, Market1501, DukeMTMC and MSMT17 respectively. As reference, we also compare to the baseline "unsupervised pre-trained model on Ima-geNet". In most cases, the unsupervised ImageNet model is slightly better than the supervised one but much less than our model. On the one hand, it demonstrates the potential of unsupervised pre-training. On the other hand, pre-training on ImageNet is not compatible with pre-training on LUPerson, which further emphasizes the importance of building person-related dataset for person Re-ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison on Small-scale and Few-shot</head><p>We further study how our pre-trained model benefits the cases where the target dataset has a smaller scale or just a few labels. This is especially important for real applications in which collecting a large labelled Re-ID dataset is so difficult. In this paper, we simulate two typical small dataset settings on the three target datasets: DukeMTMC, Market1501 and MSMT17. CUHK03 is not involved because it is too small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>pre-train</head><p>Trip <ref type="bibr" target="#b21">[22]</ref> IDE <ref type="bibr" target="#b44">[45]</ref> MGN <ref type="bibr" target="#b37">[38]</ref>     <ref type="table">Table 6</ref>: Performance for small-scale and few-shot setting with MGN method for Market1501, DukeMTMC and MSMT17 respectively.</p><p>• Small-scale. We randomly select a certain percentage of IDs and all the images belonging to the sampled IDs would be included.</p><p>• Few-shot. We keep all the identities and randomly sample a certain percentage of images for each ID. During sampling, we try to ensure that each ID has a similar number of images.</p><p>We use MGN as the baseline method and show the results in <ref type="table">Table 6</ref> by varying the percentage from 10% ∼ 100%. As we can see, our pre-training model significantly boosts the performance of MGN in most cases when the training set is small, no matter the identities is less or each identity has few images.</p><p>Specifically, for the "small-scale" setting on Mar-ket1501, which contains only 1,170 images for 75 persons (percentage= 10%, more details in supplementary materials), MGN with our pre-training model achieves 64.6 mAP on the testing set, which is 11.5 mAP higher than the Ima-geNet supervised counterpart.</p><p>For the "few-shot" setting, our per-training model also boosts the performance of MGN with a remarkable margin, but the performance gain becomes a bit saturated with the decrease of image amounts. One possible reason is that quite limited images for each identity may weaken the ability of hard triplet loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Pre-training Data Scale</head><p>In generic image recognition tasks, more powerful models often rely on more high-quality data. Here, we study   <ref type="table" target="#tab_8">Table 7</ref>, the learned representation is much stronger with the increase of the pre-training data scale. But the performance tends to saturate, when the data scale is large enough. A larger network capacity would be necessary to leverage more data for further improving the performance.  <ref type="table">Table 8</ref>: Improvement for unsupervised Re-ID method SpCL. M and D refer to Market1501 and DukeMTMC. Note that, we use the official released code and the performance obtained is slightly lower than the original paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Improving Unsupervised Re-ID Methods</head><p>Our pre-training model not only benefits the supervised person Re-ID methods, but also is indispensable in existing unsupervised person Re-ID methods. To verify it, we use state-of-the-art unsupervised baseline methods SpCL <ref type="bibr" target="#b14">[15]</ref> with its two settings: unsupervised learning (USL) and unsupervised domain adaptation (UDA). We follow the common setting as SpCL <ref type="bibr" target="#b14">[15]</ref>, MMT <ref type="bibr" target="#b13">[14]</ref> and MMCL <ref type="bibr" target="#b36">[37]</ref>, etc., and evaluate on Market1501 and DukeMTMC. The results are shown in 8. As we can see, our pre-training model can improve 3.8 mAP and 2.2 mAP on Market1501 and DukeMTMC respectively, which is a new state-of-the-art for USL in person Re-ID. For the UDA setting, our model can also boost M → D and D → M by 2.8% and 1.2% on mAP. It further demonstrates the superiority and generality of our pre-training model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Comparison with State-of-the-Arts</head><p>We compare our results with existing state-of-the-art Re-ID methods in <ref type="table">Table 9</ref>. Note that we do not apply any post-processing method like Re-Rank <ref type="bibr" target="#b47">[48]</ref> in our approach. As we can see, we achieve state-of-the-art performance on Market1501, DukeMTMC and MSMT17 with considerable advantages, by simply applying our pre-training  <ref type="table">Table 9</ref>: Comparison with state of the arts. MGN* refers to the re-implementation of MGN in fast-reid. The best is marked as bold and the second is underlined.</p><p>ResNet50 model on MGN. On CUHK03, there is a noticeable gap between the baseline MGN and the state-of-theart SONA <ref type="bibr" target="#b40">[41]</ref>. Thus, we also apply our pre-trained model upon BDB <ref type="bibr" target="#b9">[10]</ref>, which achieves 2.9 mAP gains over BDB and helps beat SONA <ref type="bibr" target="#b40">[41]</ref>.</p><p>We also test our method on a large backbone ResNet101. Compared to results of ResNet50, we can see that a stronger backbone can learn better representation features, yielding better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper releases a new large-scale person Re-ID dataset "LUPerson", which may address the issue of limited scale and diversities in all existing Re-ID datasets. Based on such an unlabelled dataset, we make the first attempt to utilize unsupervised pre-training to learn a general person Re-ID feature representation. Experiments demonstrated the effectiveness and generalization ability of our pre-training model on both supervised and unsupervised Re-ID approaches. Furthermore, it helps achieve bigger gains in small-scale or few-shot Re-ID datasets. Certainly, there are some interesting topics motivated in this direction, including leveraging the temporal information of videos into the pre-training and developing an end-to-end unsupervised Re-ID model which can beat supervised ones.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Some example images from our LUPerson dataset, which shows a strong diversity in terms of environment, scene, camera view, lighting, human pose, race, and age. the names of top 100 big cities in the world 1 and collected about 730 (680 ∼ 760) raw videos on average for each city.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4 )</head><label>4</label><figDesc>Lighting variance. The crawled videos span over a large range of time in a day, including morning, noon, and night, thus causing different lighting changes. 5) Ethnic and pose diversity. Since the raw videos are collected from 100 cities across the world, LUPerson has ethnic diversity. Due to the variance of camera views, it</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the Momentum Contrast mechanism for contrastive learning in MoCo</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5. 1 .</head><label>1</label><figDesc>Implementation Training details. We train MoCoV2 with Pytorch, and the images are resized to 256 × 128. For image normalization, we use [0.3525, 0.3106, 0.3140], [0.2660, 0.2522, 0.2505] as mean and std, which are calculated from our LUPerson dataset. The pre-training models are trained with 8 × V 100</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>25.5/51.1 44.6/71.4 53.0/77.7 59.5/81.8 63.7/85.0 17.0/36.0 49.0/73.6 57.4/80.5 62.9/83.5 65.0/85.1 (c) MSMT17</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The statistics comparison between existing popular Re-ID datasets and our large scale LUPerson dataset. It shows that LUPerson is the current largest Re-ID dataset and has much better diversity.</figDesc><table><row><cell>1) head and</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Transfer performance on the CUHK03 dataset with different data augmentations. "+, -"mean with and without, and "RE, GS, GB, CJ" mean RandomErasing, GrayScale, GaussianBlurring, and ColorJitter respectively.</figDesc><table><row><cell>Max area</cell><cell>0.0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell></row><row><cell>mAP</cell><cell>73.2</cell><cell>74.1</cell><cell>74.4</cell><cell>74.7</cell><cell>73.3</cell></row><row><cell>cmc1</cell><cell>73.8</cell><cell>74.3</cell><cell>75.3</cell><cell>75.4</cell><cell>73.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Transfer performance on the CUHK03 dataset with different RandomErasing strength, i.e., maximum erasing area.</figDesc><table><row><cell>τ</cell><cell>0.03</cell><cell>0.05</cell><cell>0.07</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell></row><row><cell>mAP</cell><cell>72.5</cell><cell>73.5</cell><cell>74.7</cell><cell>74.1</cell><cell>71.1</cell><cell>67.3</cell></row><row><cell>cmc1</cell><cell>73.4</cell><cell>74.0</cell><cell>75.4</cell><cell>73.9</cell><cell>71.5</cell><cell>67.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Transfer performance on the CUHK03 dataset with different temperature τ in the contrastive loss.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Improvement by using different pre-trained models on three representative supervised Re-ID baselines. "IN sup.", "IN unsup." refer to supervised and unsupervised pre-trained model on ImageNet, "LUP unsup." refers to unsupervised pre-trained model on LUPerson. The first number is mAP and the second is cmc1.</figDesc><table><row><cell>pre-train</cell><cell>10%</cell><cell>30%</cell><cell>small-scale 50%</cell><cell>70%</cell><cell>90%</cell><cell>10%</cell><cell>30%</cell><cell>few-shot 50%</cell><cell>70%</cell><cell>90%</cell></row><row><cell>IN sup.</cell><cell cols="10">53.1/76.9 75.2/90.8 81.5/93.5 84.8/94.5 86.9/95.2 21.1/41.8 68.1/87.6 80.2/92.8 84.2/94.0 86.7/94.6</cell></row><row><cell>IN unsup.</cell><cell cols="10">58.4/81.7 76.6/91.9 82.0/94.1 85.4/94.5 87.4/95.5 18.6/36.1 69.3/87.8 78.3/90.9 84.4/94.1 87.1/95.2</cell></row><row><cell cols="11">LUP unsup. 64.6/85.5 81.9/93.7 85.8/94.9 88.8/95.9 90.5/96.4 26.4/47.5 78.3/92.1 84.2/93.9 88.4/95.5 90.4/96.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(a) Market1501</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>pre-train</cell><cell>10%</cell><cell>30%</cell><cell>small-scale 50%</cell><cell>70%</cell><cell>90%</cell><cell>10%</cell><cell>30%</cell><cell>few-shot 50%</cell><cell>70%</cell><cell>90%</cell></row><row><cell>IN sup.</cell><cell cols="10">45.1/65.3 64.7/80.2 71.8/84.6 75.5/86.8 78.0/88.3 31.5/47.1 65.4/79.8 73.9/85.7 77.2/87.8 79.1/88.8</cell></row><row><cell>IN unsup.</cell><cell cols="10">48.1/66.9 65.8/80.2 72.5/84.4 76.3/86.9 78.5/88.7 32.4/48.0 65.3/80.2 73.7/85.1 77.7/87.8 79.4/89.0</cell></row><row><cell cols="11">LUP unsup. 53.5/72.0 69.4/81.9 75.6/86.7 78.9/88.2 81.1/90.0 35.8/50.2 72.3/83.8 77.7/87.4 80.8/89.2 82.0/90.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) DukeMTMC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>pre-train</cell><cell>10%</cell><cell>30%</cell><cell>small-scale 50%</cell><cell>70%</cell><cell>90%</cell><cell>10%</cell><cell>30%</cell><cell>few-shot 50%</cell><cell>70%</cell><cell>90%</cell></row><row><cell>IN sup.</cell><cell cols="10">23.2/50.2 41.9/70.8 50.3/76.9 56.9/81.2 61.9/84.2 14.7/34.1 44.5/71.1 56.2/79.5 60.9/82.8 63.4/84.5</cell></row><row><cell>IN unsup.</cell><cell cols="10">22.6/48.8 40.4/68.7 49.0/75.0 55.7/79.9 60.9/83.0 13.2/29.2 41.4/67.1 53.3/77.6 59.1/81.5 62.4/83.8</cell></row><row><cell>LUP unsup.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Comparison for different pre-training data scale, and the baseline method is MGN.the impact of pre-training data scale. Specifically, we involve various percentages (12.5%,25%,50%, 100%) of LU-Person into unsupervised pre-training and then evaluate the finetuing performance on the target datasets. As shown in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>LUP unsup. 76.2/90.2 67.1/81.6 79.2/91.7 69.1/83.2</figDesc><table><row><cell>pre-train</cell><cell>M</cell><cell>USL</cell><cell>D</cell><cell>UDA D → M M → D</cell></row><row><cell>IN sup.</cell><cell cols="4">72.4/87.8 64.9/80.3 76.4/90.1 67.9/82.3</cell></row><row><cell>IN unsup.</cell><cell cols="4">72.9/88.6 62.6/78.8 77.1/90.6 66.3/81.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Innovation City Index 2019: Top 100 Cities https : / / www . innovation-cities.com/worlds-top-100-cities-forinnovation-2019/18841/ 2 YOLO-v5 https://github.com/ultralytics/yolov5</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">fast-reid: https://github.com/JDAI-CV/fast-reid</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work is partially supported by the National Natural Science Foundation of China (NSFC, 61836011).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixed highorder attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="371" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Group consistent similarity learning via deep crf for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8649" to="8658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Self-critical attention learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunze</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9637" to="9646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abdnet: Attentive but diverse person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8351" to="8361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch dropblock network for person reidentification and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3691" to="3701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Serge Belongie, and Pietro Perona. Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Appel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving person re-identification with iterative impression aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengpan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9559" to="9571" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mutual meanteaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-paced contrastive learning with hybrid memory for domain adaptive object re-id</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beyond human parts: Dual partaligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Ge</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3642" to="3651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Fastreid: A pytorch toolbox for general instance re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02631</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Guided saliency feature learning for person re-identification in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="357" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Interaction-and-aggregation network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinqian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9317" to="9326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantics-aligned representation learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11173" to="11180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Angels Rates-Borras, Octavia Camps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengran</forename><surname>Srikrishna Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard J Radke</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09653</idno>
	</analytic>
	<monogr>
		<title level="m">A comprehensive evaluation and benchmark for person reidentification: Features, metrics, and datasets</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A bottom-up clustering approach to unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8738" to="8745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Person re-identification by manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Chen Change Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A strong baseline and batch normalization neck for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyang</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Relation network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11839" to="11847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Auto-reid: Searching for a part-aware convnet for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijie</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3750" to="3759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Person re-identification with deep similarity-guided graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="486" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Part-aligned bilinear representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumin</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised person reidentification via multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongkai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="10981" to="10990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Second-order non-local attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Bryan Ning Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poellabauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3760" to="3769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Densely semantically aligned person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Person re-identification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoyan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1367" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2138" to="2147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Reranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3702" to="3712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Identity-guided human semantic parsing for person re-identification. ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
