<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A MultiPath Network for Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A MultiPath Network for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>ZAGORUYKO et al.: A MULTIPATH NETWORK FOR OBJECT DETECTION 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recent COCO object detection dataset presents several new challenges for object detection. In particular, it contains objects at a broad range of scales, less prototypical images, and requires more precise localization. To address these challenges, we test three modifications to the standard Fast R-CNN object detector: (1) skip connections that give the detector access to features at multiple network layers, (2) a foveal structure to exploit object context at multiple object resolutions, and (3) an integral loss function and corresponding network adjustment that improve localization. The result of these modifications is that information can flow along multiple paths in our network, including through features from multiple network layers and from multiple object views. We refer to our modified classifier as a 'MultiPath' network. We couple our MultiPath network with DeepMask object proposals, which are well suited for localization and small objects, and adapt our pipeline to predict segmentation masks in addition to bounding boxes. The combined system improves results over the baseline Fast R-CNN detector with Selective Search by 66% overall and by 4× on small objects. It placed second in both the COCO 2015 detection and segmentation challenges.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object classification <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref> and object detection <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref> have rapidly progressed with advancements in convolutional neural networks (CNNs) <ref type="bibr" target="#b19">[20]</ref> and the advent of large visual recognition datasets <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21]</ref>. Modern object detectors predominantly follow the paradigm established by <ref type="bibr">Girshick et al.</ref> in their seminal work on Region CNNs <ref type="bibr" target="#b10">[11]</ref>: first an object proposal algorithm <ref type="bibr" target="#b16">[17]</ref> generates candidate regions that may contain objects, second, a CNN classifies each proposal region. Most recent detectors follow this paradigm <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25]</ref> and they have achieved rapid and impressive improvements in detection performance.</p><p>Except for concurrent work (e.g. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15]</ref>), most previous object detection work has focused on the PASCAL <ref type="bibr" target="#b6">[7]</ref> and ImageNet <ref type="bibr" target="#b4">[5]</ref> detection datasets. Recently, the COCO dataset <ref type="bibr" target="#b20">[21]</ref> was introduced to push object detection to more challenging settings. The dataset contains 300,000 images of fully segmented object instance in 80 categories, with an average of 7 object instances per image. COCO introduces a number of new challenges compared to previous object detection datasets: <ref type="bibr" target="#b0">(1)</ref> it contains objects at a broad range of  <ref type="figure">Figure 1</ref>: Proposed MultiPath architecture. The COCO dataset <ref type="bibr" target="#b20">[21]</ref> contains objects at multiple scales, in context and among clutter, and under frequent occlusion. Moreover, the COCO evaluation metric rewards high quality localization. To addresses these challenges, we propose the MultiPath network pictured above, which contains three key modifications: skip connections, foveal regions, and and an integral loss function. Together these modifications allow information to flow along multiple paths through the network, enabling the classifier to operate at multiple scales, utilize context effectively, and perform more precise object localization. Our MultiPath network, coupled with DeepMask object proposals <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, achieves major gains on COCO detection. scales, including a high percentage of small objects, <ref type="bibr" target="#b1">(2)</ref> objects are less iconic, often in nonstandard configurations and amid clutter or heavy occlusion, and (3) the evaluation metric encourages more accurate object localization.</p><p>In this paper, we revisit recent improvements in object detection by performing extensive experiments on the COCO dataset. In particular, we begin with the Fast R-CNN object detector <ref type="bibr" target="#b9">[10]</ref>, and test a number of intuitive modifications to explicitly address the unique challenges of this dataset, including small object detection, detection of objects in context, and improved localization. Our goal is to adapt the highly successful Fast R-CNN object detector to perform better in these settings, and we use COCO to drive our experiments.</p><p>Inspired by recent advances in object detection, we implement three network modifications: (1) a multi-stage feature aggregator that implements skip connections in intermediate network layers to more accurately detect objects at multiple scales, (2) a foveal structure in the classifier network that helps improve localization by looking at multiple image contexts, and (3) a novel loss function and corresponding network adjustment that optimize an integral of localization overlaps and encourage higher-precision localization. These three modifications allow information to flow along multiple paths in our network, including through features from multiple network layers and from multiple object views, see <ref type="figure">Figure 1</ref>. We therefore refer to our approach as a 'MultiPath' network.</p><p>We train our MultiPath detector using the recently proposed DeepMask object proposals <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, which, like our model, are well adapted to the COCO dataset. Our combined system, using DeepMask proposals and our MultiPath classifier, achieves a detection score of 33.5 average precision (AP) for detection with an ensemble of 6 models. Compared to the baseline Fast R-CNN detector <ref type="bibr" target="#b9">[10]</ref> with Selective Search proposals <ref type="bibr" target="#b31">[32]</ref>, which achieves an AP of 19.3, this represents a 66% improvement in performance. Moreover, for small objects we improve AP by nearly 4×. We also adopt our system to generate segmentation masks, and achieve an AP of 25.1 on the segmentation task.</p><p>Our system placed second in the 2015 COCO Detection Challenge in both the bounding box and segmentation tracks. Only the deeper ResNet classifier <ref type="bibr" target="#b14">[15]</ref> outperformed our approach. Potentially, ResNet could be used as the feature extractor in our MultiPath network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Object detection is a fundamental and heavily-researched task in computer vision. Until recently, the sliding window paradigm was dominant <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33]</ref>, especially for face and pedestrian detection. Deformable part models <ref type="bibr" target="#b7">[8]</ref> followed this framework but allowed for more object variability and thus found success across general object categories; likewise, Sermanet et al. <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> showcased the use of CNNs for general object detection in a sliding window fashion. More recent detectors follow the region-proposal paradigm established by Girshick et al. <ref type="bibr" target="#b10">[11]</ref> in which a CNN is used to classify regions generated by an object proposal algorithm <ref type="bibr" target="#b16">[17]</ref>. Many recent detectors follow this setup <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>, including our own work, which uses the Fast R-CNN detector as its staring point <ref type="bibr" target="#b9">[10]</ref>. We next discuss in more detail specific innovations in this paradigm and how they relate to our approach.</p><p>Context: Context is known to play an important role in visual recognition <ref type="bibr" target="#b30">[31]</ref>. Numerous ideas for exploiting context in CNNs have been proposed. Sermanet et al. <ref type="bibr" target="#b25">[26]</ref> used two contextual regions centered on each object for pedestrian detection. In <ref type="bibr" target="#b28">[29]</ref>, in addition to region specific features, features from the entire image are used to improve region classification. He et al. <ref type="bibr" target="#b13">[14]</ref> implement context in a more implicit way by aggregating CNN features prior to classification using different sized pooling regions. More recently, <ref type="bibr" target="#b8">[9]</ref> proposed to use ten contextual regions around each object with different crops. Our approach is most related to <ref type="bibr" target="#b8">[9]</ref>, however, we use just four contextual regions organized in a foveal structure and importantly our classifier is trained jointly end-to-end.</p><p>Skip connections: Sermanet et al. <ref type="bibr" target="#b25">[26]</ref> proposed to use a 'multi-stage' classifier that used features at many convolutional layers for pedestrian detection, showing improved results. Such 'skip' architectures have recently become popular for semantic segmentation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref>. Concurrently with our work, Bell et al. <ref type="bibr" target="#b2">[3]</ref> proposed to revisit skip connections for general object detection. Our own implementation of skip connections closely resembles <ref type="bibr" target="#b2">[3]</ref>.</p><p>Object Proposals: When originally introduced, object proposals were based on lowlevel grouping cues, edges, and superpixels <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref>. More recently, large gains in proposal quality have been achieved through use of CNNs <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>. In this work we use DeepMask segmentation proposals <ref type="bibr" target="#b22">[23]</ref>. Specifically, we used an early version of the improved variant of DeepMask described in <ref type="bibr" target="#b23">[24]</ref> that includes top-down refinement but is based on the VGG-A architecture <ref type="bibr" target="#b27">[28]</ref>, not the later ResNet architecture presented in <ref type="bibr" target="#b14">[15]</ref>. Overall, we obtain substantial improvements in detection accuracy on COCO by using DeepMask in place of the Selective Search <ref type="bibr" target="#b31">[32]</ref> proposals used in the original work on Fast R-CNN <ref type="bibr" target="#b9">[10]</ref>.</p><p>Classifier: The CNN used for classification forms an integral part of the detection pipeline and is key in determining final detector accuracy. The field has witnessed rapid progress in CNNs in recent years. The introduction of AlexNet <ref type="bibr" target="#b18">[19]</ref> reinvigorated the use of deep learning for visual recognition. The much deeper VGG <ref type="bibr" target="#b27">[28]</ref> and GoogleNet <ref type="bibr" target="#b29">[30]</ref> models further pushed accuracy. In our work we use variants of the VGG network <ref type="bibr" target="#b27">[28]</ref>, specifically VGG-A for DeepMask and VGG-D for our MultiPath network. In concurrent work, He at al. <ref type="bibr" target="#b14">[15]</ref> introduced the even deeper Residual Networks (ResNet) that have greatly improved the state of the art and have also proven effective for object detection. We expect that integration of ResNet into our system could further boost accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>A high-level overview of our detection model is shown in <ref type="figure">Figure 1</ref>. Our system is based on the Fast R-CNN framework <ref type="bibr" target="#b9">[10]</ref>. As in Fast R-CNN, the VGG-D network <ref type="bibr" target="#b27">[28]</ref> (pretrained on ImageNet <ref type="bibr" target="#b4">[5]</ref>) is applied to each input image and RoI-pooling is used to extract features for each object proposal. Using these features, the final classifier outputs a score for each class (plus the background) and predicts a more precise object localization via bounding box regression. We refer readers to <ref type="bibr" target="#b9">[10]</ref> for details.</p><p>We propose the following modifications to this basic setup. First, instead of a single classifier head, our model has four heads that observe different-sized context regions around the bounding box in a 'foveal' structure. Second, each of these heads combines features from the conv3, conv4, and conv5 layers. Finally, the outputs of the four classifiers are concatenated and used to compute a score based on our proposed integral loss. Similar to Fast R-CNN, the network also performs bounding box regression using these same features.</p><p>As information can flow through several parallel pathways of our network we name it a MultiPath CNN. We describe details of each modification next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Foveal Structure</head><p>Fast R-CNN performs RoI-pooling on the object proposal bounding box without explicitly utilizing surrounding information. However, as discussed, context is known to play an important role in object recognition <ref type="bibr" target="#b30">[31]</ref>. We also observed that given only cropped object proposals, identification of small objects is difficult even for humans without context.</p><p>To integrate context into our model, we looked at the promising results from the 'multiregion' model <ref type="bibr" target="#b8">[9]</ref> for inspiration. The multiregion model achieves improved localization results by focusing on 10 separate crops of an object with varying context. We hypothesized that this mainly improves localization from observing the object at multiple scales with increasing context, rather than by focusing on different parts of the object.</p><p>Therefore, to incorporate context, we add four region crops to our model with 'foveal' fields of view of 1×, 1.5×, 2× and 4× of the original proposal box all centered on the object proposal. In each case we use RoI-pooling to generate features maps of the same spatial dimensions given each differently-sized foveal region. The downstream processing shares an identical structure for each region (but with separate parameters), and the output features from the four foveal classifiers are concatenated into a single long vector. This feature vector is used for both classification and bounding box regression. See <ref type="figure">Figure 1</ref> for details.</p><p>Our foveal model can be interpreted as a simplified version of the multiregion model that only uses four regions instead of the ten in <ref type="bibr" target="#b8">[9]</ref>. With the reduced number of heads, we can train the network end-to-end rather than each head separately as in <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Skip Connections</head><p>Fast R-CNN performs RoI-pooling after the VGG-D conv5 layer. At this layer, features have been downsampled by a factor of 16. However, 40% of COCO objects have area less than 32×32 pixels and 20% less than 16×16 pixels, so these objects will have been downsampled to 2 × 2 or 1 × 1 at this stage, respectively. RoI-pooling will upsample them to 7 × 7, but most spatial information will have been lost due to the 16× downsampling of the features.</p><p>Effective localization of small objects requires higher-resolution features from earlier layers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref>. Therefore, we concatenate the RoI-pooled normalized features from conv3, conv4, and conv5 layers in the same manner as described in <ref type="bibr" target="#b2">[3]</ref> and provide this as input to each foveal classifier, as illustrated in <ref type="figure">Figure 1</ref>. A 1 × 1 convolution is used to reduce the dimension of the concatenated features to the classifier input dimension. The largest foveal features will not need as fine-grained features, so as an optimization, we sparsify these connections slightly. Specifically, we only connect conv3 to the 1× classifier head and conv4 to the 1×, 1.5×, and 2× heads. Overall, these skip connections give the classifier access to information from features at multiple resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Integral Loss</head><p>In PASCAL <ref type="bibr" target="#b6">[7]</ref> and ImageNet <ref type="bibr" target="#b4">[5]</ref>, the scoring metric only considers whether the detection bounding box has intersection over union (IoU) overlap greater than 50 with the ground truth. On the other hand, the COCO evaluation metric <ref type="bibr" target="#b20">[21]</ref> averages AP across IoU thresholds between 50 and 95, awarding a higher AP for higher-overlap bounding boxes <ref type="bibr" target="#b0">1</ref> . This incentivizes better object localization. Optimizing AP 50 has resulted in models that perform basic object localization well but often fail to return tight bounding boxes around objects.</p><p>For training, Fast R-CNN uses an IoU threshold of 50. We observed that changing this foreground/background threshold u during training improves AP u during testing, but can decrease AP at other IoU thresholds. To target the integral AP, we propose a loss function that encourages a classifier to perform well at multiple IoU thresholds.</p><p>The original loss L used in Fast R-CNN <ref type="bibr" target="#b9">[10]</ref> is given by:</p><formula xml:id="formula_0">L(p, k * ,t,t * ) = L cls (p, k * ) + λ [k * ≥ 1]L loc (t,t * ),<label>(1)</label></formula><p>for predicted class probabilities p, true class k * , predicted bounding box t, and true bounding box t * . The first term L cls (p, k) = − log p k * is the classification log loss for true class k * . The second term, L loc (t,t * ), encourages the class-specific bounding box prediction to be as accurate as possible. The combined loss is computed for every object proposal. If the proposal overlaps a ground truth box with IoU greater than 50, the true class k * is given by the class of the ground truth box, otherwise k * = 0 and the second term of the loss is ignored.</p><p>Observe that in the original R-CNN loss, the classification loss L cls does not prefer object proposals with high IoU: all proposals with IoU greater than 50 are treated equally. Ideally, proposals with higher overlap to the ground truth should be scored more highly. We thus propose to modify L cls to explicitly measure integral loss over all IoU thresholds u:</p><formula xml:id="formula_1">100 50 L cls (p, k * u )du,<label>(2)</label></formula><p>where k * u is the true class at overlap threshold u. We approximate this integral as a sum with du = 5 and modify our network to output multiple corresponding predictions p u . Specifically, our modified loss can be written as:</p><formula xml:id="formula_2">L(p, k * ,t,t * ) = 1 n ∑ u L cls (p u , k * u ) + λ [k * u ≥ 1]L loc (t,t * ) .<label>(3)</label></formula><p>We use n = 6 thresholds u ∈ {50, 55, . . . , 75}. Note that in this formulation each object proposal actually has n ground truth labels k * u , one label per threshold u. In our model, each term p u is predicted by a separate head, see <ref type="figure">Figure 1</ref>. Specifically, for each u, we train a separate linear classifier (using shared features) to predict the true class k * u of a proposal (where the ground truth label is defined using threshold u). At inference time, the output softmax probabilities p u of each of the n classifiers are averaged to compute the final class probabilities p. The modified loss function and updated network encourages object proposals with higher overlap to the ground truth to be scored more highly.</p><p>During training, each head has progressively fewer total positive training samples as there are fewer proposals overlapping the ground truth as u is increased. To keep the ratio of sampled positive and negative examples constant for each head, each minibatch is constructed to train a single head in turn. We restrict the heads to the range u ≤ 75, otherwise the proposals contain too few total positive samples for training. Finally, note that for bounding box regression, our network is unchanged and predicts only a single bounding box output t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section we perform a detailed experimental analysis of our MultiPath network. For all following experiments, Fast R-CNN <ref type="bibr" target="#b9">[10]</ref> serves as our baseline detector (with VGG-D <ref type="bibr" target="#b27">[28]</ref> features pre-trained on ImageNet <ref type="bibr" target="#b4">[5]</ref>). We use DeepMask object proposals <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> and focus exclusively on the recent COCO dataset <ref type="bibr" target="#b20">[21]</ref> which presents novel challenges for detection.</p><p>We begin by describing the training and testing setup in §4.1. Next, in §4.2 we study the impact of each of our three core network modifications, including skip connections, foveal regions, and the integral loss. We analyze the gain from DeepMask proposals in §4.3 and compare with the state of the art in §5. Finally, in the appendix we analyze a number of key parameters and also additional modifications that by and large did not improve accuracy.</p><p>Our system is written using the Torch-7 framework. All source code for reproducing the methods in this paper will be released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training and Testing Setup</head><p>For all experiments in this section we report both the overall AP (averaged over multiple IoU thresholds) and AP 50 . All our models are trained on the 80K images in COCO 2014 train set and tested on the first 5K images from the val set. We find that testing on these 5K images correlates well with the full 40K val set and 20K test-dev set, making these 5K images a good proxy for model validation without the need to test over the full val or test-dev sets.</p><p>Training is performed for 200K iterations with 4 images per batch and 64 object proposals per image. We use an initial learning rate of 10 −3 and reduce it to 10 −4 after 160K iterations. Training the full model takes ∼3 days on 4 NVIDIA Titan X GPUs. Unless noted, in testing we use a non maximal suppression threshold of 30, 1000 proposals per image, an image scale of 800 pixels, and no weight decay (we analyze all settings in the appendix). Both data and model parallelism are used in training <ref type="bibr" target="#b17">[18]</ref>. First, 4 images are propagated through the VGG-D network trunk, in parallel with 1 image per GPU. The features are then concatenated into one minibatch and subsequently used by each of the 4 foveal regions. Each foveal region resides in a separate GPU. Note that the prevalence of 4 GPU machines helped motivate our choice of using 4 foveal regions due to ease of parallelization.</p><p>Our network requires 150ms to compute the features and 350ms to evaluate the foveal regions, for a total of about 500ms per COCO image. We time with a scale of 800px and 400 proposals (see appendix and <ref type="figure" target="#fig_1">Figure 3</ref>). Fast R-CNN with these settings is about 2× faster.   <ref type="bibr">27.9</ref>. Right: Our 4-region foveal setup versus the 10 regions used in multiregion <ref type="bibr" target="#b8">[9]</ref>. Surprisingly, our approach outperforms <ref type="bibr" target="#b8">[9]</ref> despite using fewer regions. See text for details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MultiPath Network Analysis</head><p>Our implementation of Fast R-CNN <ref type="bibr" target="#b9">[10]</ref> with DeepMask object proposals <ref type="bibr" target="#b22">[23]</ref> achieves an overall AP of 25.2 and an AP 50 of 43.4. This is already a large improvement over the original Fast R-CNN results that used Selective Search proposals <ref type="bibr" target="#b31">[32]</ref>, we will return to this shortly. A breakdown of how each of our three core network modifications affects AP and AP 50 over our strong baseline is shown in <ref type="table" target="#tab_2">Table 1</ref>, left. Results are shown for each combination of modifications enabled including skip connections, foveal regions, and the integral loss (except skip connections were implemented only for foveal regions). Altogether, AP increases 2.7 points to 27.9, with each modification contributing ∼1 point to final performance. AP 50 improves 1.4 points to 44.8; however, not surprisingly, the best AP 50 of 46.4 is achieved without the integral loss. We carefully analyze the foveal structure and integral loss next.</p><p>Foveal structure: A breakdown of the gains from using foveal regions is shown in Table 1, right, both with and without the integral loss but without skip connections. Gains from foveal regions are amplified when using the integral loss, resulting in an AP improvement of 1.3 points. We also compare our foveal approach to the multiregion network <ref type="bibr" target="#b8">[9]</ref> which used 10 regions (for a fair comparison, we re-implement it in our setup). Surprisingly, it performs slightly worse than our foveal setup despite having more regions. This may be due to the higher number of parameters or it's possible that this requires more iterations to converge.</p><p>Integral Loss: <ref type="figure" target="#fig_0">Figure 2</ref>, left, shows AP at various IoU thresholds for models trained with different IoU cutoffs u as well as our integral loss. Each standard model tends to perform best at the IoU for which it was trained. Integral loss improves overall AP by ∼1 over the u = 50 model, and does so while maintaining a slightly higher AP 50 than simply increasing u (e.g. our AP 50 is 0.6 points higher than the u = 60 model). <ref type="figure" target="#fig_0">Figure 2</ref>, right, shows AP and AP 50 for varying number of heads. Using 6 heads (u ≤ 75) achieves the highest AP. For the experiments in <ref type="figure" target="#fig_0">Figure 2</ref> we trained for 280K iterations as we found the integral loss requires somewhat longer to converge (we used 200K iterations for all other ablations studies).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">DeepMask Proposals</head><p>Object proposals play a central role in determining detector accuracy. The original implementation of Fast R-CNN with Selective Search proposals <ref type="bibr" target="#b31">[32]</ref> has an AP of 19.3. Our Mul-tiPath network improves this to 22.8 AP using these same proposals. Switching to DeepMask proposals <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> increases accuracy by a further very substantial 5.1 points to 27.9 AP. <ref type="figure" target="#fig_1">Figure 3</ref> shows AP 50 and AP for varying number and type of proposals. Not only is accuracy substantially higher using DeepMask, fewer proposals are necessary to achieve top performance. Our results saturate with around 400 DeepMask proposals per image and using just 50 DeepMask proposals matches accuracy with 2000 Selective Search proposals.</p><p>Interestingly, our setup substantially reduces the benefits provided by bounding box regression. With the original Fast R-CNN and Selective Search proposals, box regression increases AP by 3.5 points, but with our MultiPath model and DeepMask proposals, box regression only increases AP by 1.1 points. See <ref type="table" target="#tab_4">Table 2</ref>, left, for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">COCO 201Results</head><p>To maximize accuracy prior to submitting to the COCO leaderboard, we added validation data to training, employed horizontal flip and fractional max pooling <ref type="bibr" target="#b11">[12]</ref> at inference, and ensembled 6 models. Together, these four enhancements boosted AP from 27.9 to 33.2 on the held-out validation images, see <ref type="table" target="#tab_4">Table 2</ref>, right. More details are given in the appendix. Finally, to obtain segmentation results, we simply fed the bounding box regression outputs back to the DeepMask segmentation system. Note that as discussed in §4.3, box regression only improved accuracy slightly. In principle, we could have used the original DeepMask segmentation proposals without box regression; however, we did not test this variant. AP AP 50 AP 75 AP S AP M AP L AR 1 AR 10 AR 100 AR S AR M AR L ResNet <ref type="bibr" target="#b14">[15]</ref> 27.9 51. <ref type="bibr" target="#b1">2</ref>    <ref type="bibr" target="#b24">[25]</ref>, but results between splits tend to be quite similar.</p><p>We submitted our results the COCO 2015 Object Detection Challenge. Our system placed second in both the bounding box and segmentation tracks. <ref type="table" target="#tab_6">Table 3</ref> compares our results to the top COCO 2015 challenge systems and additional baselines. Only the deeper ResNet classifier <ref type="bibr" target="#b14">[15]</ref> outperformed our approach (and potentially ResNet could be integrated as the feature extractor in our MultiPath network, leading to further gains). Compared to the baseline Fast R-CNN, our system showed the largest gains on small objects and localization, improving AP on small objects by 4× and AP 75 by 82%. <ref type="figure" target="#fig_2">Figure 4</ref> and <ref type="figure" target="#fig_3">Figure 5</ref> show selected detection results from our system. <ref type="figure" target="#fig_5">Figure 6</ref> shows a breakdown of errors of our system. Most of the overall error comes from false positives and negatives, with little inter-class classification error. Despite our improvements on small objects, small object detection remains quite challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed three modifications to Fast R-CNN: (1) skip connections to give the network access to multi-scale features, (2) foveal regions to provide context, and (3) the integral loss to improve localization. We coupled our resulting MultiPath classifier with DeepMask proposals and achieved a 66% improvement over the baseline Fast R-CNN with Selective Search. All source code for our approach will be released. Our hope is that our model can serve as a baseline system on COCO and prove useful to the detection community.    (f) Books are an incredibly difficult category due to their small size and highly inconsistent annotation in COCO. (g,h,i) Accuracy broken down by scale; not unexpectedly, small objects (area &lt; 32 2 ) are quite difficult, while accuracy on large objects (area &gt; 96 2 ) is much higher. While there is a practical limit to the performance on small objects which are often ambiguous or poorly-labeled, there is still substantial opportunity for improvement. We expect better proposals, more accurate filtering of false positives, and stronger reasoning about context can all improve small object detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Additional Analysis</head><p>In the appendix we describe our additional enhancements reported in <ref type="table" target="#tab_4">Table 2</ref> and analyze a number of key parameters. We also report additional modifications that did not improve accuracy; we hope that sharing our negative results will prove beneficial to the community. train+val: Adding validation data to training (minus the 5K held-out images from the validation set we use for testing) improved accuracy by 2.3 points AP, see <ref type="table" target="#tab_4">Table 2</ref>. We trained for 280K iterations in this case. We note that the DeepMask proposals were only trained using the train set, so retraining these on train+val could further improve results.</p><p>hflip: Fast R-CNN is not invariant to horizontal image flip (hflip) even though it is trained with hflip data augmentation. Thus, we average the softmax scores from the original and flipped images and also average the box regression outputs (directly, not in log space). AP improves by 0.6 points, see <ref type="table" target="#tab_4">Table 2</ref>.</p><p>FMP: Inspired by Fractional Max Pooling <ref type="bibr" target="#b11">[12]</ref>, we perform multiple RoI-pooling operations with perturbed pooling parameters and average the softmax outputs (note that the network trunk is computed only once). Specifically, we perform two ROI-poolings: the first follows <ref type="bibr" target="#b13">[14]</ref> and uses the floor and ceil operations for determining the RoI region, the second uses the round operation. As shown in <ref type="table" target="#tab_4">Table 2</ref>, FMP improves AP 0.7 points.</p><p>Ensembling: Finally, we trained an ensemble of 6 similar models. Each model was initialized with the same ImageNet pre-trained model, only the order of COCO training images changed. This ensemble boosted AP 1.7 points to 33.2, see <ref type="table" target="#tab_4">Table 2</ref>.</p><p>Scale: <ref type="figure" target="#fig_6">Figure 7</ref>, left, shows accuracy as a function of image scale (minimum image dimension in pixels with maximum fixed to 1000px). Increasing scale improves accuracy up to ∼800px, but at increasing computation time. We set the scale to 800px which improves AP by 0.5 points over the 600px scale used by <ref type="bibr" target="#b9">[10]</ref> for PASCAL.</p><p>NMS threshold: <ref type="figure" target="#fig_6">Figure 7</ref>, right, shows accuracy as a function of the NMS threshold. Fast R-CNN <ref type="bibr" target="#b9">[10]</ref> used a threshold of 30. For our model, an NMS threshold of 50 performs best, improving AP by 0.4 points, possibly due to the higher object density in COCO.</p><p>Dropout &amp; Weight Decay: Dropout helped regularize training and we keep the same dropout value of 0.5 that was used for training VGG-D. On the other hand, setting weight decay to 0 for fine-tuning improved results by 1.1 AP 50 and 0.5 AP. Note that <ref type="bibr" target="#b2">[3]</ref> used weight decay but not dropout, so perhaps it is sufficient to have just one form of regularization.</p><p>Iterative Localization: Bounding box voting with iterative localization as proposed in <ref type="bibr" target="#b8">[9]</ref> did not substantially improve the AP of our model, again probably due to the higher quality of DeepMask proposals and the improved localization ability of our MultiPath network.</p><p>ImageNet Data Augmentation: As there are some under-represented classes in COCO with few annotations, we tried to augment the training set with ImageNet 2012 detection training data. Surprisingly, this only improved performance on the most underrepresented class: hair dryer; for all other classes, accuracy remained unchanged or suffered.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Left: Each standard model performs best at the threshold used for training while using the integral loss yields good results at all settings. Right: Integral loss achieves best AP with 6 heads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>AP 50 and AP versus number and type of proposals. Accuracy saturates using 400 DeepMask proposals per image and using ∼50 DeepMask proposals matches 2000 Selective Search proposals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Selected detection results on COCO. Only high-scoring detections are shown. While there are missed objects and false positives, many of the detections and segmentations are quite good.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Selected detection results on COCO. Only high-scoring detections are shown. While there are missed objects and false positives, many of the detections and segmentations are quite good.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Detailed analysis of detector performance on unseen COCO validation images at select settings (plots in style of [16] generated by COCO API code). (a) Removing localization errors would lead to an AP 10 of 58.8 on COCO ('Loc'). Removing similar and other class confusion ('Sim' and 'Oth') would only lead to slight improvements in accuracy. The remaining errors are all based on background confusions ('BG') and false negatives ('FN'). (b,c) Our detector performs similarly on cats and dogs, achieving high overall accuracy with some class and background confusions but few missed detections. (d) Zebras are quite distinct, however, localization of overlapping zebras can be difficult due to their striped patterns. (e) People are the dominant category on COCO and have average difficulty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Effect of scale (left) and NMS threshold (right) on detection performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Left: Model improvements of our MultiPath network. Results are shown for various combinations of modifications enabled. Each contributes roughly equally to final accuracy, and in total AP increases 2.7 points to</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Left</figDesc><table /><note>: Bounding box regression is key when using Selective Search (SS) proposals and the Fast R-CNN classifier (our implementation). However, with DeepMask (DM) proposals and our MultiPath network, box regression increases AP by only 1.1 points (and AP 50 by 0.3) as our pipeline already outputs well-localized detections. Right: Final enhancements to our model. Use of additional training data, horizontal flip at inference, fractional max pooling (FMP), and ensembling gave a major cumulative boost. These are common approaches for maximizing accuracy, see appendix for details.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Top: COCO test-standard segmentation results. Bottom: COCO test-standard bounding box results (top methods only). Leaderboard snapshot from 01/01/2016.</figDesc><table /><note>*Note: Fast R-CNN and Faster R-CNN results are on test-dev as reported in</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Going forward, we use the notation introduced by the COCO dataset<ref type="bibr" target="#b20">[21]</ref>. Specifically, we use AP to denote AP averaged across IoU values from 50 to 95, and AP u to denote AP at IoU threshold u (e.g., the PASCAL metric is denoted by AP 50 ). Note also that we use the convention that IoU ranges from 0 to 100.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements:</head><p>We would like to thank Ross Girshick, Rob Fergus, Bharath Hariharan, Spyros Gidaris, Nikos Komodakis, and Francisco Massa for helpful discussion and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inside-outside net: Detecting objects in context with skip pooling and recurrent neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The PASCAL visual object classes (VOC) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object detection via a multi-region and semantic segmentation-aware cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fractional max-pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6071</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Diagnosing error in object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chodpathumwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">What makes for effective detection proposals?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5997</idno>
		<title level="m">One weird trick for parallelizing convolutional neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.0312</idno>
		<title level="m">Microsoft COCO: Common objects in context</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multi-stage feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scalable</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1441</idno>
		<title level="m">high-quality object detection</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Contextual priming for object detection. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<title level="m">Selective search for object recog. IJCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
