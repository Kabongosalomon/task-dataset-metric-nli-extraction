<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HMOR: Hierarchical Multi-Person Ordinal Relations for Monocular Multi-Person 3D Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
							<email>wangcan@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
							<email>liuwentao@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
							<email>qianchen@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
							<email>lucewu@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HMOR: Hierarchical Multi-Person Ordinal Relations for Monocular Multi-Person 3D Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D human pose</term>
					<term>ordinal relations</term>
					<term>integrated model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Remarkable progress has been made in 3D human pose estimation from a monocular RGB camera. However, only a few studies explored 3D multi-person cases. In this paper, we attempt to address the lack of a global perspective of the top-down approaches by introducing a novel form of supervision -Hierarchical Multi-person Ordinal Relations (HMOR). The HMOR encodes interaction information as the ordinal relations of depths and angles hierarchically, which captures the body-part and joint level semantic and maintains global consistency at the same time. In our approach, an integrated top-down model is designed to leverage these ordinal relations in the learning process. The integrated model estimates human bounding boxes, human depths, and root-relative 3D poses simultaneously, with a coarse-to-fine architecture to improve the accuracy of depth estimation. The proposed method significantly outperforms state-of-the-art methods on publicly available multi-person 3D pose datasets. In addition to superior performance, our method costs lower computation complexity and fewer model parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Estimating 3D human poses from a monocular RGB camera is fundamental and challenging. It has found applications in robotics <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b71">72]</ref>, activity recognition <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b31">32]</ref>, human-object interaction detection <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>, and content creation for graphics <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b0">1]</ref>. With deep neural networks <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref> and large scale publicly available datasets <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b32">33]</ref>, significant improvement has been achieved in the field of 3D pose estimation. Most of the works <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b8">9]</ref> focus on estimating the single-person pose. Recently, some methods <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b38">39]</ref> Denotes equal contribution. Cewu Lu is the corresponding author. He is the member of Qing Yuan Research Institute and MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China. arXiv:2008.00206v2 [cs.CV] 10 Aug 2020 start to deal with multi-person cases. However, recovering absolute 3D poses in the camera-centered coordinate system is quite a challenge. Since multi-person activities take place in cluttered scenes, inherent depth ambiguity and occlusions make it still difficult to estimate the absolute position of multiple instances.</p><p>Recently, top-down approaches <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b38">39]</ref> achieve noticeable improvements in estimating multi-person 3D poses. These approaches first perform human detection and estimate the 3D pose of each person by a single-person pose estimator. However, the pose estimator is applied to each bounding box separately, which raises the doubt that the top-down models are not able to understand multiperson relationships and handle complex scenes. Without a broad view of the input scenario, it is challenging to get rid of inherent depth ambiguity and occlusion problems. In this paper, the relationship among multiple persons is fully considered to address this limitation of top-down approaches.</p><p>We propose a novel form of supervision for 3D pose estimation -Hierarchical Multi-person Ordinal Relations (HMOR). HMOR explicitly encodes the interaction information as ordinal relations, supervising the networks to output 3D poses in the correct order. Different from previous works <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b53">54]</ref> that only use relative depth information, HMOR considers both depths and angles relations and expresses the ordinal information hierarchically, i.e., instance → part → joint, which makes up for the lack of a global perspective of the top-down approaches.</p><p>Further, we propose an integrated top-down model to learn this knowledge by encoding it into the learning process. The integrated model can be end-to-end trained with back-propagation and performs human detection, pose estimation, and human-depth estimation simultaneously. Since metric depth from a single image is fundamentally ambiguous, estimating absolute 3D pose suffers from inaccurate human-depth estimation. To improve the accuracy, we take a coarseto-fine approach to estimate human depth: i) initializes a global depth map, and ii) finetunes the human depths by estimating the correction residual.</p><p>We evaluate our method on two multi-person <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b22">23]</ref> and one single-person 3D pose datasets <ref type="bibr" target="#b20">[21]</ref>. Our method significantly outperforms previous multi-person 3D pose estimation methods <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39]</ref> by 12.3 PCK abs improvement on the MuPoTS-3D <ref type="bibr" target="#b37">[38]</ref> dataset, and 20.5 mm improvement on CMU Panoptic <ref type="bibr" target="#b22">[23]</ref> dataset, with lower computation complexity and fewer model parameters. Compared to state-of-the-art single-person methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b67">68]</ref>, our method does not need ground-truth bounding-box in the inference phase and still achieves comparable performance. Additionally, our proposed method is compatible with 2D pose annotations, which allows the 2D-3D mixed training strategy.</p><p>The contributions of this paper can be summarized as follows:</p><p>• We propose HMOR, a novel form of supervision, to explicitly leverage the relationship among multiple persons for pose estimation. HMOR divides human relations into three levels: instance, part and joint. This hierarchical manner ensures both the global consistency and the fine-grained accuracy of the predicted results.</p><p>• An integrated end-to-end top-down model is proposed for multi-person 3D pose estimation from a monocular RGB input. We design a coarse-to-fine architecture to improve the accuracy of human-depth estimation. Our model jointly performs human detection, human-depth estimation, and 2D/3D pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multi-person 2D Pose Estimation. Most of the multi-person 2D pose estimation methods can be divided into two categories: bottom-up and top-down approaches. Bottom-up approaches localize the body joints and group them into different persons. Traditional top-down approaches first detect human bounding boxes in the image and then estimate single-person 2D poses separately. Representative works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b21">22]</ref> of the bottom-up approaches are reviewed. Cao et al. <ref type="bibr" target="#b4">[5]</ref> propose part affinity fields (PAFs) to model human bones. Complete skeletons are assembled by detected joints with PAFs. Newell et al. <ref type="bibr" target="#b41">[42]</ref> introduce a pixel-wise tag to assign joints to a specific person. Kocabas et al. <ref type="bibr" target="#b24">[25]</ref> assign joints to detected persons by a pose residual network.</p><p>Top-down approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b57">58]</ref> achieve impressive accuracy in multi-person 2D pose estimation. Mask R-CNN <ref type="bibr" target="#b17">[18]</ref> is an end-to-end model to estimate multiple human poses but still process multiple persons separately. Fang et al. <ref type="bibr" target="#b15">[16]</ref> propose a two-stage framework (RMPE) to reduce the effect of the inaccurate human detector. Sun et al. <ref type="bibr" target="#b57">[58]</ref> propose the HRNet that maintains high-resolution representations through the whole process.</p><p>Single-person 3D Pose Estimation. There are two approaches to the problem of single-person 3D pose estimation from monocular RGB: single-stage and two-stage approaches.</p><p>Single-stage approaches <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b59">60]</ref> directly locate 3D human joints from the input image. For example, Pavlakos et al. <ref type="bibr" target="#b46">[47]</ref> propose a coarse-to-fine approach to estimate a 3D heatmap for pose estimation. Kanazawa et al. <ref type="bibr" target="#b23">[24]</ref> recover 3D pose and body mesh by minimizing the reprojection loss. Sun et al. <ref type="bibr" target="#b59">[60]</ref> operate an integral operation as soft-argmax to obtain 3D pose coordinates in a differentiable manner.</p><p>Two-stage approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b63">64]</ref> first estimate 2D pose or utilize the off-the-shelf accurate 2D pose estimator, and then lift them to the 3D space. Martinez et al. <ref type="bibr" target="#b34">[35]</ref> propose a simple baseline to regress 3D pose from 2D coordinates directly. Moreno-Noguer <ref type="bibr" target="#b39">[40]</ref> obtains more precise pose estimation by the distance matrix representation. Yang et al. <ref type="bibr" target="#b63">[64]</ref> utilize a multi-source discriminator to generate anthropometrically valid poses.</p><p>Multi-person 3D Pose Estimation. A few works explore the problem of multi-person 3D pose estimation from a monocular RGB. Rogez et al. <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref> propose LCR-Net and LCR-Net++. They locate human bounding boxes and classify those boxes into a set of K anchor-poses. A regression module is proposed to refine the anchor-pose to the final prediction. Instead of using a learningbased manner, they obtain the human depth by minimizing the distance between the projected 3D pose and the estimated 2D pose. Mehta et al. <ref type="bibr" target="#b37">[38]</ref> propose a bottom-up method. Their proposed occlusion-robust pose-map (ORPM) enables full body pose inference even under strong partial occlusions. Zanfir et al. <ref type="bibr" target="#b66">[67]</ref> propose MubyNet, a bottom-up model. MubyNet integrates a limb scoring model and formulates the person grouping problem as an integer program. Moon et al. <ref type="bibr" target="#b38">[39]</ref> propose a top-down two-stage model. They utilize the off-theshelf human detection model and then perform single-person 3D pose estimation and root-joint localization. Those top-down approaches are not able to utilize multi-person relations since they estimate individual 3D pose separately. The bottom-up approaches are still suffering from limited accuracy. Our method combines the advantages of both approaches and boosts multi-person absolute 3D pose estimation by leveraging the multi-person relations in the integrated end-to-end top-down model.</p><p>Ordinal Relations. In the context of computer vision, several works learn ordinal apparent depth <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b7">8]</ref> or reflectance <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b68">69]</ref> relationship as weak supervision. They motivated by the fact that ordinal relations are easier for humans to annotate. In the case of single-person 3D pose estimation, <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55</ref>] use depth relations of body joints to generate 3D pose from 2D pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We propose a novel representation, Hierarchical Multi-person Ordinal Relation (HMOR), to explicitly leverage ordinal relations among multiple persons and improve the performance of 3D pose estimation. Compared with previous works <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b53">54]</ref> that use ordinal relation in 3D pose estimation, HMOR extends this idea in three dimensions: i) single-person to multi-persons, ii) joint level to hierarchical instance-part-joint levels, iii) depth relations to angle relations. Further, we develop an integrated model to aggregate HMOR into the end-to-end training process. In this section, we first describe the unified representation of the absolute multi-person 3D pose recovery under the top-down framework ( §3.1). Then we detail the encoding and training schemes of the proposed HMOR ( §3.2). Finally, the integrated model with a coarse-to-fine depth estimation design is elaborated ( §3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Representation</head><p>Our task is to recover multiple absolute 3D human poses P = {P abs m } N m=1 in the camera-centered coordinate system, where N denotes the number of persons in the input RGB image. We assume that there are J joints in a single 3D pose skeleton. The m th absolute 3D pose can be formulated as:   </p><formula xml:id="formula_0">P abs m = {k m,j : (x abs m,j , y abs m,j , z abs m,j ) T } J j=1 ,<label>(1)</label></formula><formula xml:id="formula_1">B m = (û top m ,v top m ,ŵ m ,ĥ m ) T ,<label>(2)</label></formula><formula xml:id="formula_2">P rel m = {(û m,j ,v m,j ,ẑ rel m,j ) T } J j=1 ,<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchical Multi-person Ordinal Relations</head><p>Our initial goal is to leverage multi-person interaction relations to improve the performance of 3D pose estimation. Traditional top-down methods <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b38">39</ref>] lack a global perspective because they estimate single human poses in each bounding box separately. Therefore, they are vulnerable to truncation, self-occlusions, and inter-person occlusions. Here, we develop a novel form of supervision named Hierarchical Multi-person Ordinal Relations (HMOR) to model human relations explicitly. Basically, given an image of human activities, we divide the relationship into three levels: i) instance-level depth relations, ii) part-level angle relations, iii) joint-level depth relations. In each level, HMOR formulates pair-wise ordinal relations and punishes the wrong-order pairs. In the following, we detail our HMOR formulations that reflect interpretable relations of human activities.</p><p>Instance-Level Depth Relations. In a given camera view, for two persons (p 1 , p 2 ), we denote the instance depth-relation function as R ins (p 1 , p 2 ; n ⊥ ), taking the value:</p><formula xml:id="formula_4">• +1, if p 1 is closer than p 2 in the n ⊥ direction, • −1, if p 2 is closer than p 1 in the n ⊥ direction, • 0, if the depths of two person are equal,</formula><p>where n ⊥ is the camera normal vector. We define the position of a person as the arithmetic mean of its body joints, i.e. p m = 1 J J jk m,j . The ordinal error of a pair of instances is denoted as:</p><formula xml:id="formula_5">err ins (p 1 ,p 2 ) = log(1 + max(0, R ins (p 1 ,p 2 ; n ⊥ ) * [(p 1 −p 2 ) · n ⊥ ])).<label>(5)</label></formula><p>This differentiable instance ranking expression will punish the wrong-order instance pairs and ignore the correct results. For example, if p 1 is closer than p 2 , and the prediction relation is correct, i.e., (p 1 −p 2 ) · n ⊥ &lt; 0, the multiplication result will be smaller than 0 and ignored by the maximum operation. Supervising the instance-level depth relations is to help the network build a global understanding of the input scenario. Ablative study in §4.4 reveals that the accuracy of human-depth estimation benefits a lot from instance-level depth relations.</p><p>Part-Level Angle Relations. As shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, we divide the body skeleton into S = 14 parts according to the kinematically connected joints. Each part t is a vector defined by start-joint k start and end-joint k end , i.e., t = k end − k start . Since body-parts are a set of 3D vectors with direction and length values, we can not directly compare their depths. Here, we utilize a unique attribute of body-part -direction, and compare their angle relations. To simplify the ordinal relation of angles, we first project the body-part vector t m,s onto the camera plane:</p><formula xml:id="formula_6">t n ⊥ m,s = t m,s − (t m,s · n ⊥ )n ⊥ ,<label>(6)</label></formula><p>where m is the person index, and s is the body-part index. In a given camera view, for a pair of body parts (t m1,s1 , t m2,s2 ), we denote the angle-relation function as R arg (t m1,s1 , t m2,s2 ; n ⊥ ), taking the value:</p><formula xml:id="formula_7">• +1, if Arg(t n ⊥ m1,s1 ) &lt; Arg(t n ⊥ m2,s2 ), • −1, if Arg(t n ⊥ m1,s1 ) &gt; Arg(t n ⊥ m2,s2 ), • 0, if Arg(t n ⊥ m1,s1 ) = Arg(t n ⊥ m2,s2 ),</formula><p>where Arg(t n ⊥ ) computes the principal value of the argument of the projection vector. The ordinal error of a pair of body-parts is:</p><formula xml:id="formula_8">err part (t m1,s1 ,t m2,s2 ) = [R arg (t m1,s1 ,t m2,s2 ; n ⊥ ) * [(t m1,s1 ×t m2,s2 ) · n ⊥ ]] + .<label>(7)</label></formula><p>With the cross-product operation ×, we supervise the direction of the angle between a pair of body-parts. If the angle betweent m1,s1 andt m2,s2 is in the correct direction, the projection of the cross-product (t m1,s1 ×t m2,s2 ) · n ⊥ will have an opposite sign of R arg (·). Therefore, the negative multiplication results will be ignored by the [·] + operation. Another intuitive way is to express body-parts as particles and supervise their depth relations, using the average position of its two endpoints. To compare vector and particle representations, we conduct ablative experiments and find out vector is superior to particle representation. We suspect this is because the depth relations have been fully utilized in the other two levels, supervising depths of body-part is redundant. More experimental details are reported in §4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint-Level Depth</head><p>Relations. The definition of body joint depth-relation function R jt (k m1,s1 , k m2,s2 ; n ⊥ ) is similar to R ins :</p><p>• +1, if k m1,s1 is closer than k m2,s2 in the n ⊥ direction, • −1, if k m2,s2 is closer than k m1,s1 in the n ⊥ direction, • 0, if the depths of two joints are equal.</p><p>The ordinal error of a pair of joints is denoted as: err jt (k m1,s1 ,k m2,s2 ) = log(1+[R jt (k m1,s1 ,k m2,s2 ; n ⊥ ] + * [(k m1,s1 −k m2,s2 )·n ⊥ ])).</p><p>(8) Denoting the set of estimated persons, body-parts, and joints pairs as I ins , I part , and I jt , respectively, the HMOR loss is computed as follows:</p><formula xml:id="formula_9">L HMOR = 1 |I ins | p 1,p2 err ins + 1 |I part | t 1,t2 err part + 1 |I jt | k 1,k2 err jt<label>(9)</label></formula><p>Augmented Training Scheme. As mentioned before, HMOR computes the ordinal relations with respect to a vector n ⊥ . Initially, this vector is set as the camera normal vector. However, we notice that annotations from 3D human pose datasets (Human3.6M, MuPoTS-3D, and CMU Panoptic) are mostly captured in an laboratory environment, limited to the fixed viewing angle. To alleviate camera restrictions, we sample virtual views to improve the generalization ability.</p><p>In the training phase, we generate a virtual view vector n v by rotating the camera normal vector n ⊥ randomly. We adapt the uniform sphere sampling strategy from Marsaglia et al. <ref type="bibr" target="#b33">[34]</ref>:</p><formula xml:id="formula_10">n v = ( 1 − u 2 cos θ, 1 − u 2 sin θ, u) T ,<label>(10)</label></formula><p>where θ ∼ U [0, 2π) and u ∼ U [0, 1]. In this way, HMOR can calculate the ordinal relations with respect to an arbitrary viewing angle. The effectiveness of the sampled view is validated in §4.4. Additionally, a mixed datasets training strategy is utilized for a fair comparison with previous methods in experiments. HMOR is compatible with 2D pose datasets and single-person 3D pose datasets. Given an image only with 2D pose annotations, we can define the part-level angle relations, since the 2D pose skeletons are the projections of body-parts with respect to n ⊥ . As for single-person cases, HMOR only supervises the joint and body-part relations of an individual person and ignore instance-level relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PoseHead</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Root-relative 3D Poses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RoI Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Integrated End-to-end Model</head><p>In our approach, an integrated end-to-end top-down model is designed to aggregate HMOR into the end-to-end training process. Although the disjoint model <ref type="bibr" target="#b38">[39]</ref> can use different strong networks for different tasks (e.g., human detection, pose estimation, depth estimation), an integrated model has three advantages over the disjoint learning model: 1) Fewer model parameters. 2) Only an integrated model can leverage the multi-person relations since the disjoint learning methods train their model with single person annotations separately. 3) The multi-task training strategy of the integrated model can benefit each task. In our experiments, the integrated model is found to have much better performance than the disjoint learning methods.</p><p>The overall architecture of our model is summarized in <ref type="figure" target="#fig_2">Fig. 2</ref>. Our model consists of two stages. In the first stage, the backbone network extracts RoIs and the initial depth map. PoseHead and DetHead estimate root-relative 3D poses and human bounding boxes from RoIs, respectively. In the second stage, we retrieve the initial depths of root-joints from the depth map. The DepthHead takes the RoI features and initial depth as input and outputs the correction residual ∆ẑ. The residual is added to the initial depth to obtain refined human depths. Aggregating the outputs from DetHead, PoseHead, and DepthHead, the absolute 3D posesP abs are estimated via back-projection as Eq. 4.</p><p>Human Detection. The architecture for human detection and the loss function L det follow the design in Mask R-CNN <ref type="bibr" target="#b17">[18]</ref>. Region Proposal Network (RPN) proposes candidate human bounding boxes, and the DetHead predicts class labels and bounding-box offsets. RoiAlign is used to extract feature maps from each RoI.</p><p>Root-Relative 3D Pose Estimation. PoseHead is proposed to estimate the root-relative 3D poseP rel from an input RoI feature. We use 3D heatmaps as the representations of 3D poses. The soft-argmax operation <ref type="bibr" target="#b59">[60]</ref> is adopted to extractP rel from the 3D heatmap. 1 regression loss is applied to root-relative coordinatesP rel m :</p><formula xml:id="formula_11">L pose = 1 N 1 J N m P rel m − P rel m 1 .<label>(11)</label></formula><p>RoIAlign extracts 14 × 14 RoI features, which are fed into the PoseHead subsequently. We adopt a simple network as PoseHead, including three residual blocks for feature extraction, a transposed convolution <ref type="bibr" target="#b13">[14]</ref> for upsampling, a batch normalization layers <ref type="bibr" target="#b19">[20]</ref>, a ReLU activation function, and a 1 × 1 convolution. The size of an output heatmap is 28 × 28 × 28.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Depth Estimation. Direct human-depth regression from an input</head><p>RoI is challenging. Part of the challenges comes from the variety of camera parameters and human body shapes. Furthermore, the inputs of DepthHead are fixed-size RoI features, which erase the information of projected body shapes and sizes. Inspired by the idea of iterative error feedback (IEF) from previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24]</ref>, we design a coarse-to-fine estimation approach to enhance the accuracy of human-depth regression. The model will first predict an initial depth of root-jointẑ init . Then the DepthHead takes the RoI features and the initial depthẑ init as an input and outputs the residual ∆z. Ideally, the refined depth is updated by adding this residual to the initial estimateẑ refine =ẑ init + ∆z.</p><p>Depth Initialize. To estimate the initial depths of root-joints, we directly regress an initial depth map. During training, we first normalize the absolute depth value by focal lengths and then calculate the loss L init between the ground truth and the initial depth map in the area around the root-joint's 2D pixel location:</p><formula xml:id="formula_12">z norm R = z abs R / f x · f y .<label>(12)</label></formula><formula xml:id="formula_13">L init = 1 N N m z norm m,R −ẑ init m,R 1 ,<label>(13)</label></formula><p>Depth Refinement. In the refinement step, we retrieve the initial-depth values of root-joints from the depth map according to their 2D pixel locations. Because the input features are resized by RoIAlign, we first need to transfer the original depth to the equivalent depth of the resized person. According to the pinhole camera model:</p><formula xml:id="formula_14">z eq,norm R = z norm R · A Box A RoI ,<label>(14)</label></formula><p>z eq,init</p><formula xml:id="formula_15">R =ẑ init R · A Box A RoI ,<label>(15)</label></formula><p>where A Box denotes the area of the bounding box, and A RoI denotes the area of RoI. DepthHead extracts 1D features from RoIs. Then the equivalent initialdepth valuesẑ eq,init m,R are concatenated with the extracted features and fed into an fc layer to predict the residual ∆ẑ. The loss function of the refinement step L refine is defined as:</p><formula xml:id="formula_16">L refine = 1 N N m z eq,norm m,R −ẑ eq,init m,R − ∆ẑ m 1 .<label>(16)</label></formula><p>In the testing phase, we can recover the absolute depth of root-jointẑ abs m,R as:</p><formula xml:id="formula_17">z abs m,R = (∆ẑ m +ẑ eq,init m,R ) · f x · f y · A RoI A Box .<label>(17)</label></formula><p>The DepthHead uses three residual blocks (following ResNet <ref type="bibr" target="#b18">[19]</ref>) and an average pooling layer to extract 1D features. The FC layer contains 512 neurons.</p><p>The end-to-end training loss is formulated as:</p><formula xml:id="formula_18">L = L det + L pose + L init + L refine + L HMOR .<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we first introduce the datasets employed for quantitative evaluation and elaborate implementation details. Then we report our results and compare the proposed method with state-of-the-art methods. Finally, ablation experiments are conducted to evaluate our contributions and show how each choice contributes to our state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>MuCo-3DHP and MuPoTS-3D: MuCo-3DHP is a multi-person composited 3D human pose training dataset. MuPoTS-3D is the real-world scenes test set. Following <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>, 400K composited frames are utilized for training. CMU Panoptic. CMU Panoptic <ref type="bibr" target="#b22">[23]</ref> is a multi-person 3D pose dataset captured in an indoor dome with multiple cameras. Here we follow the evaluation protocol of <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b66">67]</ref>. 3DPW. 3D Poses in the Wild (3DPW) <ref type="bibr" target="#b32">[33]</ref> is a recent challenging dataset, captured mostly in outdoor conditions. It contains 60 video sequences (24 train, 24 test, and 12 validation). Human3.6M. Human3.6M <ref type="bibr" target="#b20">[21]</ref> is an indoor benchmark for single-person 3D pose estimation. A total of 11 professional actors (6 male, 5 female) perform 15 activities in a laboratory environment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Our method is implemented in PyTorch. We adopt a ResNet-50 <ref type="bibr" target="#b18">[19]</ref> based FPN <ref type="bibr" target="#b29">[30]</ref> as our model backbone. The backbone is initialized with the Ima-geNet <ref type="bibr" target="#b10">[11]</ref> pre-trained model. The settings of each network head are reported in §3.3. We resize the image to 1333 × 800 and feed into the network. SGD is used for optimization, with a mini-batch size of 32. All tasks are trained simultaneously. We adopt the linear learning rate warm-up policy. The learning rate is set to 0.2/3 at first and gradually increases to 0.2 after 2.5k iterations. We reduce the learning rate by a factor of 10 at the 10th and 20th epochs. In each experiment, our model is trained for 30 epochs with 16 NVIDIA 1080 Ti GPUs. We perform data augmentations including horizontal flip and multi-scale training. Additional COCO <ref type="bibr" target="#b30">[31]</ref> 2D pose estimation data are used in the training phase. For evaluation, we report the flip-test results. All reported numbers have been obtained with a single model without ensembling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Compare with Prior Art</head><p>MuPoTS-3D. We compare our method against state-of-the-art methods under three protocols. PCK abs is used to evaluate absolute camera-centered coordinates of 3D poses. Additionally, PCK rel and AUC rel are used to evaluate root-relative 3D poses after root alignment. Quantitative results are reported in <ref type="table" target="#tab_1">Table 1</ref>. Without bells and whistles, our method surpasses state-of-the-art meth- CMU Panoptic. Following previous works <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b66">67]</ref>, we evaluate our method under MPJPE after root alignment. <ref type="table">Table 2</ref> provides experimental results. In this dataset, the activities take place in a small room. Thus, the scenarios are severely affected by the occlusion problem. Our method effectively reduces the interference of occlusion and outperforms state-of-the-art methods by 20.5 mm MPJPE (28.4% relative improvement).</p><p>Human3.6M. We conduct experiments on Human3.6M dataset to evaluate the performance of root-relative 3D pose estimation. Two experimental protocols are widely used. Protocol 1 uses PA MPJPE and Protocol 2 uses MPJPE as evaluation metrics. As most of the previous methods use the ground-truth bounding box, our method does not require any ground-truth information at inference time. Quantitative results are reported in <ref type="table" target="#tab_2">Table 3</ref>. Our method achieves comparable performance with single-person methods and outperforms previous multi-person 3D pose estimation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In this study, we evaluate the effectiveness of the proposed HMOR and integrated model. We evaluate on 3DPW dataset that contains in-the-wild complex scenes to demonstrate the strength of our model. We further propose ABS-MPJPE to evaluate the absolute 3D pose estimation results without root alignment. Effect of Hierarchical Multi-person Ordinal Relations. In this experiment, we study the effectiveness of using HMOR supervision. We first implement a vanilla baseline without HMOR supervision. Moreover, we implement another baseline by directly supervising the predicted absolute 3D poses with an 1 loss L abs . Intuitively, since the human poses are evaluated in the camera coordinate system, the local optimum for L abs is consistent with the evaluation metrics.</p><p>The experimental results are shown in <ref type="table" target="#tab_3">Table 4</ref>(a). The model trained with L abs supervision has better performance than the vanilla baseline, but is still inferior to HMOR supervision. HMOR supervision brings 7.4 mm MPJPE improvement. By removing three types of relations separately, we can observe that instance relation affects the absolute pose accuracy (ABS-MPJPE) most, while part and joint relations mainly affect the root-relative pose accuracy.</p><p>Variants of HMOR. In this experiment, we examine a variant of HMOR. When handling the part relations, we represent a body part as a particle rather than a vector. The position of a body part is defined as the average of its two endpoints. Similar to joint and instance, we supervise the depth relations of the particle body-parts. The experimental results are shown in <ref type="table" target="#tab_3">Table 4</ref>(b). The particle representation produces inferior performance than the vector representation.</p><p>Effect of Sampled Views.  <ref type="table" target="#tab_3">Table 4</ref>(a) that only use the original camera normal vector n ⊥ , sampled views provide 0.6 mm MPJPE improvement.</p><p>Effect of Coarse-to-Fine Depth Surpervision. In this experiment, we study the effectiveness of the coarse-to-fine design for human depth estimation. We remove the refinement step and output the initial value directly. The experimental results are shown in <ref type="table" target="#tab_3">Table 4</ref>(d). We observe that the coarse-to-fine design is necessary to produce accurate human depth.  Computation Complexity. The experimental results of computation complexity and model parameters are listed in <ref type="table" target="#tab_5">Table 5</ref>. We compare our method with Moon et al. <ref type="bibr" target="#b38">[39]</ref>, which is the only open-source multi-person 3D pose estimation method. Our approach obtains superior results to the state-of-the-art 3D pose estimation method (both absolute pose and root-relative pose), with significantly lower computation complexity and fewer model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a novel form of supervision -HMOR, to learn multiperson 3D poses from a monocular RGB image. HMOR supervises the multiperson ordinal relations in a hierarchical manner, which captures fine-grained semantics and maintains global consistency at the same time. To end-to-end learn the ordinal relations, we further proposed an integrated model with a coarse-to-fine depth-estimation architecture. We demonstrate the effectiveness of our proposed method on standard benchmarks. The proposed method surpasses state-of-the-art multi-person 3D pose estimation methods, with lower computation complexity and fewer model parameters. We believe the idea of leveraging multi-person relations can be further explored to improve 3D pose estimation, e.g., exploit the relations via network design.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of the proposed HMOR. (a) Definition of skeletal parts. (b) Monocular input image. (c-e) Hierarchical Multi-person Ordinal Relations. HMOR supervises the ordinal relations among multiple persons where k m,j is the j th joint position of the m th absolute pose. Human bounding boxes {B m } N m=1 , root-relative 3D poses {P rel m } N m=1 , and absolute depth of the root-joint {ẑ abs m,R } N m=1 are needed to estimate the absolute 3D poses. We term root-joint's absolute depth as human depth, corresponding to the pelvis bone position (the R th joint of the body skeleton). We useˆto denote the predicted values. The m th human bounding boxB m and root-relative 3D poseP rel m are formulated as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>whereû m,j andv m,j represent pixel coordinates of the estimated body joint with respect to the bounding box.ẑ rel m,j denotes the estimated depth of joint j relative to the root-joint.û top m ,v top m ,ŵ m , andĥ m are the top left corner coordinates, the width, and the height of the predicted bounding box, respectively. With the intrinsic matrix M, the final absolute 3D poseP abs m can be obtained via back-projection, where each joint is calculated by: ẑ rel m,j +ẑ abs m,R )M −1  û m,j +û top m v m,j +v top m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Architecture of the integrated model. The ResNet-50 based backbone network extract RoI features and initial depth map. PoseHead and DetHead perform rootrelative 3D pose estimation and human detection, respectively. DepthHead retrieves initial depths from the depth map and predicts refined human depths by correction residual ∆ẑ. This architecture allows the 2D-3D mixed training strategy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Qualitative results of our proposed method on COCO validation set (left) and MuPoTS-3D test set (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparisons with state-of-the-art methods on the MuPoTS-3D dataset. "-" shows the results that are not availableMethod AUC rel ↑ 3DPCK rel ↑ 3DPCK abs ↑</figDesc><table><row><cell cols="2">LCRNet [52]</cell><cell>-</cell><cell>53.8</cell><cell>-</cell><cell></cell></row><row><cell cols="2">Single Shot [38]</cell><cell>-</cell><cell>66.0</cell><cell>-</cell><cell></cell></row><row><cell cols="2">LCRNet++ [53]</cell><cell>-</cell><cell>70.6</cell><cell>-</cell><cell></cell></row><row><cell>Xnect [37]</cell><cell></cell><cell>-</cell><cell>70.4</cell><cell>-</cell><cell></cell></row><row><cell cols="2">Moon et al. [39]</cell><cell>39.8</cell><cell>81.8</cell><cell>31.5</cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell>43.5</cell><cell>82.0</cell><cell cols="2">43.8</cell></row><row><cell cols="6">Table 2. Quantitative comparisons of MPJPE on the CMU Panoptic dataset</cell></row><row><cell>Method</cell><cell cols="5">Haggling Mafia Ultimatum Pizza Mean↓</cell></row><row><cell>Popa [49]</cell><cell>217.9</cell><cell>187.3</cell><cell>193.6</cell><cell cols="2">221.3 203.4</cell></row><row><cell>Zanfir [66]</cell><cell>140.0</cell><cell>165.9</cell><cell>150.7</cell><cell cols="2">156.0 153.4</cell></row><row><cell>Zanfir [67]</cell><cell>72.4</cell><cell>78.8</cell><cell>66.8</cell><cell>94.3</cell><cell>72.1</cell></row><row><cell>Ours</cell><cell>50.9</cell><cell>50.5</cell><cell>50.7</cell><cell>68.2</cell><cell>51.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Quantitative comparisons on the Human3.6M dataset PCK rel and 43.5 AUC rel . Note that the PCK result relies on the threshold value. AUC can reflect a more reliable result since it computes the area under the PCK curve from various thresholds. Our method outperforms the previous methods by 3.7 AUC rel .</figDesc><table><row><cell></cell><cell cols="2">Single-Person</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Multi-Person</cell></row><row><cell>Method</cell><cell>Moreno [40]</cell><cell>Zhou [70]</cell><cell>Martinez [35]</cell><cell>Sun [59]</cell><cell>Fang [17]</cell><cell>Sun [60]</cell><cell>Zhou [68]</cell><cell>Rogez [53]</cell><cell>Moon [39]</cell><cell>Ours</cell></row><row><cell>PA MPJPE↓</cell><cell>76.5</cell><cell>55.3</cell><cell cols="8">47.7 48.3 45.7 40.6 27.9 42.7 35.2 30.5</cell></row><row><cell></cell><cell cols="2">Single-Person</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Multi-Person</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Martinez [35]</cell><cell>Fang [17]</cell><cell>Sun [59]</cell><cell>Sun [60]</cell><cell>Zhou [68]</cell><cell>Rogez [52]</cell><cell>Metha [38]</cell><cell>Rogez [53]</cell><cell>Moon [39]</cell><cell>Ours</cell></row><row><cell>MPJPE↓</cell><cell>62.9</cell><cell>60.4</cell><cell cols="8">59.1 49.6 39.9 87.7 69.9 63.5 54.4 48.6</cell></row><row><cell cols="11">ods by 12.3 PCK abs (39.0% relative improvement). Our method demonstrates</cell></row><row><cell cols="8">a clear advantage for handling multi-person 3D poses.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">As for root-relative results, our method achieves 82.0</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablative study on the effects of HMOR</figDesc><table><row><cell></cell><cell>Settings</cell><cell></cell><cell>3DPW</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">MPJPE↓ PA-↓ ABS-↓</cell></row><row><cell></cell><cell>baseline</cell><cell>95.7</cell><cell>63.6</cell><cell>169.3</cell></row><row><cell></cell><cell>+ L abs</cell><cell>94.6</cell><cell>61.1</cell><cell>158.2</cell></row><row><cell></cell><cell>+ jt</cell><cell>89.9</cell><cell>59.7</cell><cell>132.8</cell></row><row><cell></cell><cell>+ part</cell><cell>90.2</cell><cell>60.3</cell><cell>143.2</cell></row><row><cell>(a)</cell><cell>+ instance</cell><cell>93.3</cell><cell>61.2</cell><cell>128.3</cell></row><row><cell></cell><cell>+ jt + part</cell><cell>89.1</cell><cell>58.3</cell><cell>125.9</cell></row><row><cell></cell><cell>+ jt + instance</cell><cell>89.2</cell><cell>58.5</cell><cell>122.3</cell></row><row><cell></cell><cell>+ part + instance</cell><cell>89.5</cell><cell>59.5</cell><cell>123.6</cell></row><row><cell></cell><cell>+ jt + part + instance</cell><cell>88.3</cell><cell>57.8</cell><cell>119.6</cell></row><row><cell cols="2">(b) + jt + particle-part + instance</cell><cell>89.0</cell><cell>58.2</cell><cell>119.5</cell></row><row><cell cols="2">(c) + jt + part + instance + sample views (Final)</cell><cell>87.7</cell><cell cols="2">57.4 118.5</cell></row><row><cell cols="2">(d) w/o refine depth</cell><cell>88.4</cell><cell>58.1</cell><cell>133.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>(c) reports the result of training with sampled views. Compare with the results in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Ablative study on computation complexity and model parametersMethod#Params↓ GFLOPs↓ AUC rel ↑ PCK rel ↑ PCK abs ↑</figDesc><table><row><cell cols="2">Moon [39] 167.7M</cell><cell>547.8</cell><cell>39.8</cell><cell>81.8</cell><cell>31.5</cell></row><row><cell>Ours</cell><cell>45.0M</cell><cell>320.2</cell><cell>43.5</cell><cell>82.0</cell><cell>43.8</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work is supported in part by the National Key R&amp;D Program of China, No. 2017YFA0700800, National Natural Science Foundation of China under Grants 61772332Shanghai Qi Zhi Institute.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Creation of 3D human avatar using kinect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aitpayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gaber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ATFECM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Pose-conditioned joint angle limits for 3D human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Integration of motion control techniques for virtual human and avatar real-time animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Boulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bécheiraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Emering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>VRST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">3D human pose estimation = 2D pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Single-image depth perception in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Weakly-supervised discovery of geometry-aware representation for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Cascaded pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Markerless human-robot interface for dual robot manipulators using kinect sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ROBOT CIM-INT MANUF</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A guide to convolution arithmetic for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07285</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pairwise body-part attention for recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning pose grammar to encode human body configuration for 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<editor>Mask R-CNN</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Human3.6m: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multi-person articulated tracking with spatial and temporal embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Panoptic studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">MultiPoseNet: Fast multi-person pose estimation using pose residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Crowdpose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Detailed 2d-3d joint representation for human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<title level="m">Pastanet: Toward human activity knowledge engine</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Microsoft COCO: Common objects in context</title>
		<imprint>
			<publisher>ECCV</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning features combination for human action recognition from skeleton sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Choosing a point from the surface of a sphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marsaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Monocular 3D human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Xnect: Real-time multi-person 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.00837</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Single-shot multi-person 3D pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Camera distance-aware top-down approach for 3D multi-person pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">3D human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning lightness from human judgement on relative reflectance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Narihira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep rnn framework for visual sequential applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">3d human pose estimation using convolutional neural networks with 2D pose information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Ordinal depth supervision for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Domes to drones: Self-supervised active triangulation for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pirinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Deep multitask architecture for integrated 2D and 3D human sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">3d skeleton-based human action classification: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Presti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>La Cascia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Lcr-net: Localization-classificationregression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Lcr-net++: Multi-person 2D and 3D pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">It&apos;s all relative: Monocular 3D human pose estimation from weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Eng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Monocular 3D human pose estimation by generation and ordinal ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Varigonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<title level="m">Drpose3d: Depth ranking in 3D human pose estimation. IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00977</idno>
		<title level="m">Pose flow: Efficient online pose tracking</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">3D human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">A dual-source approach for 3D pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Monocular 3D pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Deep network for the integrated 3D sensing of multiple people in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Hemlets pose: Learning part-centric heatmap triplets for accurate 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Learning data-driven reflectance priors for intrinsic image decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Monocap: Monocular human motion capture using a cnn coupled with a geometric prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Towards 3D human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">3D human pose estimation in rgbd images for robotic task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Welschehold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dornhege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICRA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Learning ordinal relationships for mid-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
