<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly-supervised Temporal Action Localization by Uncertainty Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilhyeon</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
							<email>yanlu@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
							<email>hrbyun@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Graduate School of AI</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly-supervised Temporal Action Localization by Uncertainty Modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>THU-MOS&apos;14 and ActivityNet (1.2 &amp; 1.3). Our code is available at https://github.com/Pilhyeon/WTAL-Uncertainty-Modeling.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly-supervised temporal action localization aims to learn detecting temporal intervals of action classes with only videolevel labels. To this end, it is crucial to separate frames of action classes from the background frames (i.e., frames not belonging to any action classes). In this paper, we present a new perspective on background frames where they are modeled as out-of-distribution samples regarding their inconsistency. Then, background frames can be detected by estimating the probability of each frame being out-of-distribution, known as uncertainty, but it is infeasible to directly learn uncertainty without frame-level labels. To realize the uncertainty learning in the weakly-supervised setting, we leverage the multiple instance learning formulation. Moreover, we further introduce a background entropy loss to better discriminate background frames by encouraging their in-distribution (action) probabilities to be uniformly distributed over all action classes. Experimental results show that our uncertainty modeling is effective at alleviating the interference of background frames and brings a large performance gain without bells and whistles. We demonstrate that our model significantly outperforms state-of-the-art methods on the benchmarks,</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Temporal action localization (TAL) is a very challenging problem of finding and classifying action intervals in untrimmed videos, which plays an important role in video understanding and analysis. To tackle the problem, many works have been done in the fully-supervised manner and achieved impressive progress <ref type="bibr" target="#b44">(Zeng et al. 2019;</ref><ref type="bibr" target="#b15">Lin et al. 2019</ref><ref type="bibr" target="#b14">Lin et al. , 2020</ref><ref type="bibr" target="#b20">Xu et al. 2020)</ref>. However, they suffer from the extremely high cost of acquiring precise annotations, i.e., labeling the start and end timestamps of each action instance.</p><p>To relieve the high-cost issue and enlarge the scalability, researchers direct their attention to the same task with weak supervision, namely, weakly-supervised temporal action localization (WTAL). Among the various levels of weak supervision, thanks to the cheap cost, video-level action labels are widely employed . In this setting, each video is labeled as positive for action classes if it contains corresponding action frames and as negative otherwise. Note that a video may have multiple action classes as its label.</p><p>Existing approaches commonly cast WTAL as frame-wise classification, and adopt attention mechanism <ref type="bibr" target="#b25">(Nguyen et al. 2018)</ref> or multiple instance learning <ref type="bibr" target="#b27">(Paul, Roy, and Roy-Chowdhury 2018)</ref> to learn from video-level labels. Nonetheless, they still show highly inferior performances when compared to fully-supervised counterparts. According to the literature , the performance degradation mainly comes from the false alarms of background frames, since video-level labels do not have any clue for background. To bridge the gap, there appear several studies attempting explicit background modeling in the weakly-supervised manner. <ref type="bibr" target="#b17">Liu et al. (Liu, Jiang, and Wang 2019)</ref> merges static frames to synthesize pseudo background videos, but they overlook dynamic background frames <ref type="figure" target="#fig_0">(Fig. 1a</ref>). Meanwhile, some work <ref type="bibr" target="#b26">(Nguyen, Ramanan, and Fowlkes 2019;</ref><ref type="bibr" target="#b12">Lee, Uh, and Byun 2020)</ref> tries to classify background frames as a separate class. However, it is undesirable to force all background frames to belong to one specific class, as they do not share any common semantics <ref type="figure" target="#fig_0">(Fig. 1b</ref>).</p><p>In this paper, we embrace the observation on inconsistency of background frames, and propose to formulate them as out-of-distribution samples <ref type="bibr" target="#b13">(Liang, Li, and Srikant 2018;</ref><ref type="bibr" target="#b5">Dhamija, Günther, and Boult 2018)</ref>. They can be identified by estimating the probability of each sample coming from out-of-distribution, also known as uncertainty <ref type="bibr" target="#b1">(Bendale and Boult 2016;</ref><ref type="bibr" target="#b11">Lakshminarayanan, Pritzel, and Blundell 2017)</ref>. To model uncertainty, we propose to utilize the magnitude of an embedded feature vector. Generally, features of action frames have larger magnitudes compared to ones from background frames, as shown in <ref type="figure" target="#fig_1">Fig. 2a</ref>. This is because action frames need to produce high logits for ground-truth action classes. Although the features magnitudes show the correlation with the discrimination between background and action frames, directly using them for discrimination is insufficient since the distributions of action and background are close to each other. Therefore, to further encourage the discrepancy in feature magnitudes, we propose to separate the distributions by enlarging magnitudes of action features and reducing those of background features close to zero <ref type="figure" target="#fig_1">(Fig. 2b)</ref>.</p><p>In order to learn uncertainty only with video-level supervision, we leverage the formulation of multiple instance learning <ref type="bibr" target="#b22">(Maron and Lozano-Pérez 1998;</ref><ref type="bibr" target="#b48">Zhou 2004)</ref>, where a model is trained with a bag (i.e., untrimmed video) instead of instances (i.e., frames). Specifically, from each untrimmed video, we select top-k and bottom-k frames based on the feature magnitude and consider them as pseudo action and background frames, respectively. Thereafter, we design an uncertainty modeling loss to separate their magnitudes, through which our model is able to indirectly model uncertainty without frame-level labels and provides better separation between action and background frames. Moreover, we introduce a background entropy loss to force pseudo background frames to have uniform probability distribution over action classes. This prevents them from leaning toward a certain action class and helps to reject them by maximizing the entropy of their action class distribution. To validate the effectiveness of our method, we perform experiments on two standard benchmarks, THUMOS'14 and Ac-tivityNet. By jointly optimizing the proposed losses along with a general action classification loss, our model successfully distinguishes action frames from background frames. Furthermore, our method achieves the new state-of-the-art performances on the both benchmarks.</p><p>Our contributions are three-fold: 1) We propose to formulate background frames as out-of-distribution samples, overcoming the difficulty in modeling background due to their inconsistency. 2) We design a new framework for weaklysupervised action localization, where uncertainty is modeled and learned only with video-level labels via multiple instance learning. 3) We further encourage separation between action and background with a loss maximizing the entropy of action probability distribution from background frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Fully-supervised action localization. The goal of temporal action localization is to find temporal intervals of action instances from long untrimmed videos and classify them. For the task, many approaches depend on accurate temporal annotations for each training video. Most of them utilize the two-stage approach, i.e., they first generate proposals, and then classify them. To generate proposals, earlier methods adopt the sliding window technique <ref type="bibr" target="#b31">(Shou, Wang, and Chang 2016;</ref><ref type="bibr" target="#b42">Yuan et al. 2016;</ref><ref type="bibr" target="#b29">Shou et al. 2017;</ref><ref type="bibr" target="#b40">Yang et al. 2018;</ref><ref type="bibr" target="#b36">Xiong et al. 2017;</ref><ref type="bibr" target="#b4">Chao et al. 2018)</ref>, while recent models predict the start and end frames of action <ref type="bibr" target="#b16">(Lin et al. 2018</ref><ref type="bibr" target="#b15">(Lin et al. , 2019</ref><ref type="bibr" target="#b14">(Lin et al. , 2020</ref>. Meanwhile, there appear some attempts to leverage graph structural information <ref type="bibr" target="#b44">(Zeng et al. 2019;</ref><ref type="bibr" target="#b20">Xu et al. 2020)</ref>. Moreover, there are also a gaussian modeling of each action instance <ref type="bibr" target="#b19">(Long et al. 2019</ref>) and an efficient method without proposal generation step (Alwassel, Caba Heilbron, and Ghanem 2018).</p><p>Weakly-supervised action localization. Recently, many attempts have been made to solve temporal action localization with weak supervision, mostly video-level labels. ) first tackle the problem by selecting relevant segments on soft and hard ways. <ref type="bibr" target="#b25">(Nguyen et al. 2018</ref>) propose a sparsity regularization, while <ref type="bibr" target="#b32">(Singh and Lee 2017)</ref> and ) extend small discriminative parts. <ref type="bibr" target="#b27">(Paul, Roy, and Roy-Chowdhury 2018)</ref>, <ref type="bibr" target="#b24">(Narayan et al. 2019)</ref> and <ref type="bibr" target="#b23">(Min and Corso 2020)</ref> employ deep metric learning to force features from the same action to get closer to themselves than those from different classes.  and  learn attention weights using a variational auto-encoder and an expectation-maximization strategy, respectively. <ref type="bibr" target="#b45">(Zhai et al. 2020)</ref> pursue the consensus between different modalities. Meanwhile, <ref type="bibr" target="#b30">(Shou et al. 2018)</ref> and ) attempt to regress the intervals of action instances, instead of performing hard thresholding. Recently, <ref type="bibr" target="#b21">(Ma et al. 2020)</ref> propose to exploit the intermediate level of supervision (i.e., single frame supervision). Apart from the methods above, Some work <ref type="bibr" target="#b17">(Liu, Jiang, and Wang 2019;</ref><ref type="bibr" target="#b26">Nguyen, Ramanan, and Fowlkes 2019;</ref><ref type="bibr" target="#b12">Lee, Uh, and Byun 2020)</ref> seeks to explicitly model background. However, as aforementioned, they have innate limitations in that background frames could be dynamic and inconsistent. In contrast, we consider background as out-of-distribution and propose to learn uncertainty as well as action class scores. In Sec. 4, the efficacy of our approach is verified.  <ref type="figure">Figure 3</ref>: Overview of the proposed method. The main pipeline serves the standard process for weakly-supervised action localization. We discriminate background frames from action frames by modeling uncertainty, i.e., probability of being out-ofdistribution. In the uncertainty modeling part, pseudo action/background segments are selected based on features magnitudes, which are in turn used to derive two proposed losses for background discrimination: (a) uncertainty modeling loss which enlarges and reduces the feature magnitudes of the pseudo action and background segments respectively; and (b) background entropy loss forcing the pseudo background segments to have uniform action probability distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we first set up the baseline network for weakly-supervised temporal action localization (Sec. 3.1). Thereafter, we cast the problem of identifying background frames as out-of-distribution detection and tackle it by modeling uncertainty (Sec. 3.2). Lastly, the objective functions for training our model (Sec. 3.3) and how the inference is performed (Sec. 3.4) are elaborated. The overview of our method is illustrated in <ref type="figure">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Main pipeline</head><p>Feature extraction. Due to the memory limit, we split each video into multi-frame segments, i.e., v n = {s n,l } Ln l=1 , where L n denotes the number of segments in v n . To handle the large variation in video lengths, a fixed number of T segments {s n,t } T t=1 are sampled from each original video. From the sampled RGB and flow segments, spatio-temporal features x RGB n,t ∈ R D and x flow n,t ∈ R D are extracted by a pretrained feature extractor, respectively. Next, we concatenate the RGB and flow features into the complete feature vectors x n,t ∈ R 2D , which are then stacked to build a feature map of length T , i.e., X n = [x n,1 , ..., x n,T ] ∈ R 2D×T .</p><p>Feature embedding. In order to embed the extracted features into the task-specific space, we employ a single 1-D convolutional layer followed by a ReLU function. Formally, F n = g embed (X n ; φ embed ), where g embed denotes the convolution operator with the activation and φ embed is the trainable parameters of the layer. Concretely, the dimension of an embedded feature is the same as that of an input feature, i.e.,</p><formula xml:id="formula_0">F n = [f n,1 , ..., f n,T ] ∈ R 2D×T .</formula><p>Segment-level classification. From the embedded features, we predict segment-level class scores, which are later used for action localization. Given the features F n , the segment-wise class scores are derived by the action classifier, i.e., A n = g cls (F n ; φ cls ), where g cls represents the linear classifier with its parameters φ cls . In specific, A n ∈ R C×T , where C is the number of action classes.</p><p>Action score aggregation. Following the previous work (Paul, Roy, and Roy-Chowdhury 2018), we aggregate top k act scores along all segments for each action class and average them to build a video-level raw class score:</p><formula xml:id="formula_1">a c (v n ) = 1 k act max An;c⊂An[c,:] ∀a∈Ân;c a,<label>(1)</label></formula><p>whereÂ n;c is a subset containing k act action scores for the c-th class (i.e., Â n;c = k act ), and k act is a hyper-parameter controlling the number of the aggregated segments. Thereafter, we obtain the video-level action probability for each action class by applying the softmax function to the aggregated scores:</p><formula xml:id="formula_2">p c (v n ) = exp(a c (v n )) C c =1 exp(a c (v n )) ,<label>(2)</label></formula><p>where p c (v n ) represents the softmax score for the c-th action of v n , which is guided by video-level weak labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Uncertainty modeling</head><p>From the main pipeline, we can obtain the action probabilities for each segment, but the essential component for action localization, i.e., background discrimination, is not carefully considered. Regarding the unconstraint and inconsistency of background frames, we treat background as outof-distribution <ref type="bibr" target="#b7">(Hendrycks and Gimpel 2017)</ref> and model uncertainty (probability of being background) for WTAL.</p><p>Considering the probability that the segments n,t belongs to the c-th action, it can be decomposed into two parts with a chain rule, i.e., the in-distribution action probability and the uncertainty. Let d ∈ {0, 1} denote the variable for the background discrimination, i.e., d = 1 if the segment belongs to any action class, d = 0 otherwise (belongs to background). Then, the posterior probability for class c ofs n,t is given by:</p><formula xml:id="formula_3">P (yn,t = c|sn,t) = P (yn,t = c, d = 1|sn,t) = P (yn,t = c|d = 1,sn,t)P (d = 1|sn,t),<label>(3)</label></formula><p>where y n,t is the label of the segment, i.e., ifs n,t belongs to the c-th action class, then y n,t = c, while y n,t = 0 for background segments. We describe the single-label case for readability. Without loss of generality, this can be generalized to the multi-label.</p><p>Uncertainty formulation. In Eq. 3, the probability for indistribution action classification, P (y n,t = c|d = 1,s n,t ), is estimated with the softmax function as in general classification task. Additionally, it is necessary to model the probability that a segment belongs to any action class, i.e., P (d = 1|s n,t ), to tackle the background discrimination problem. Observing that features of action frames generally have larger magnitudes than those of background frames <ref type="figure" target="#fig_1">(Fig. 2)</ref>, we formulate uncertainty by employing magnitudes of feature vectors. Specifically, background features have small magnitudes close to 0, while action features have large ones. Then the probability that the t-th segment in the n-th video (s n,t ) is an action segment is defined by:</p><formula xml:id="formula_4">P (d = 1|s n,t ) = min(m, f n,t ) m ,<label>(4)</label></formula><p>where f n,t is the corresponding feature vector ofs n,t , · is a norm function (we use L-2 norm here), and m is the predefined maximum feature magnitude. From the formulation, it is ensured that the probability falls between 0 and 1, i.e., 0 ≤ P (d = 1|s n,t ) ≤ 1.</p><p>Uncertainty learning via multiple instance learning. To learn uncertainty only with video-level labels, we borrow the concept of multiple instance learning <ref type="bibr" target="#b22">(Maron and Lozano-Pérez 1998)</ref>, where a model is trained with a bag (video) instead of instances (segments). In this setting, considering that each untrimmed video contains both action and background frames, we select pseudo action/background segments representing the video. Specifically, the top k act segments in terms of the feature magnitude are treated as the pseudo action segments {s n,i |i ∈ S act }, where S act indicates the set of pseudo action indices. Meanwhile, the bottom k bkg segments are considered the pseudo background segments {s n,j |j ∈ S bkg }, where S bkg denotes the set of indices for pseudo background. k act and k bkg represent the number of segments sampled for action and background, respectively.</p><p>Then the pseudo action/background segments serve as the representatives of the input untrimmed video, and they are used for training via multiple instance learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training objectives</head><p>Our model is jointly optimized with three losses: 1) videolevel classification loss L cls for action classification of each input video, 2) uncertainty modeling loss L um for separating the magnitudes of action and background feature vectors, and 3) background entropy loss L be which forces background segments to have uniform probability distribution for action classes. The overall loss function is as follows:</p><formula xml:id="formula_5">L total = L cls + αL um + βL be ,<label>(5)</label></formula><p>where α and β are balancing hyper-parameters.</p><p>Video-level classification loss. For multi-label action classification, we use the binary cross entropy loss with normalized video-level labels  as follows:</p><formula xml:id="formula_6">L cls = 1 N N n=1 C c=1 −y n;c log p c (v n ),<label>(6)</label></formula><p>where p c (v n ) represents the video-level softmax score for the c-th class of the n-th video (Eq. 2), and y n;c is the normalized video-level label for the c-th class of the n-th video.</p><p>Uncertainty modeling loss. To learn uncertainty, we train the model to produce large feature magnitudes for pseudo action segments but ones with small ones for pseudo background segments, as illustrated in <ref type="figure">Fig. 3 (a)</ref>. Formally, uncertainty modeling loss takes the form:</p><formula xml:id="formula_7">L um = 1 N N n=1 (max(0, m − f act n ) + f bkg n ) 2 ,<label>(7)</label></formula><p>where f act n = 1 k act i∈S act f n,i and f bkg n = 1 k bkg j∈S bkg f n,j are the mean features of the pseudo action and background segments of the n-th video, respectively. · is the norm function, and m is the pre-defined maximum feature magnitude, the same in Eq. 4.</p><p>Background entropy loss. Although uncertainty modeling loss encourages background segments to produce low logits for all actions, softmax scores for some action classes could be high due to the relativeness of softmax function. To prevent background segments from having a high softmax score for any action class, we define a loss function which maximizes the entropy of action probability from background segments, i.e., background segments are forced to have uniform probability distribution for action classes as described in <ref type="figure">Fig. 3 (b)</ref>. The loss is calculated as follows:</p><formula xml:id="formula_8">L be = 1 N C N n=1 C c=1 − log(p c (s bkg n )),<label>(8)</label></formula><p>where p c (s bkg n ) = 1 k bkg j∈S bkg p c (s n,j ) is the averaged action probability for the c-th class of pseudo background segments, and p c (s n,j ) is the softmax score for class c ofs n,j .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inference</head><p>At the test time, for an input video, we first obtain the videolevel softmax score and threshold on it with θ vid to determine which action classes are to be localized. For the remaining action classes, we calculate the segment-wise posterior probability by multiplying the segment-level softmax score and the probability of being an action segment following Eq. 3. Afterwards, the segments whose posterior probabilities are larger than θ seg are selected as the candidate segments. Finally, consecutive candidate segments are grouped into a single proposal, whose scores are calculated following <ref type="bibr" target="#b17">(Liu, Jiang, and Wang 2019)</ref>. To enrich the proposal pool, we use multiple thresholds for θ seg and non-maxium suppression (NMS) is performed for overlapped proposals. Evaluation metrics. We evaluate our method with mean average precisions (mAPs) under several different intersection of union (IoU) thresholds, which are the standard evaluation metrics for temporal action localization. The official evaluation code of ActivityNet 1 is used for measurement.</p><p>Implementation details. As the feature extractor, we employ I3D networks (Carreira and Zisserman 2017) pretrained on Kinetics (Carreira and Zisserman 2017), which take input segments with 16 frames. It should be noted that we do not finetune the feature extractor for fair comparison. TVL1 algorithm <ref type="bibr">(Wedel et al. 2009</ref>) is used to extract optical flow from videos. We fix the number of segments T as 750 and 50 for THUMOS'14 and ActivityNet, respectively.  <ref type="formula" target="#formula_1">(2019)</ref> 28.3 17.0 3.5 17.1 RPN <ref type="bibr" target="#b38">(2020)</ref> 37.6 23.9 5.4 23.3 BaS-Net <ref type="bibr" target="#b38">(2020)</ref> 38.5 24.2 5.6 24.3 DGAM <ref type="bibr" target="#b38">(2020)</ref> 41.0 23. <ref type="bibr">5 5.3 24.4 Gong et al. (2020)</ref> 40.0 25.0 4.6 24.6 EM-MIL <ref type="bibr" target="#b38">(2020)</ref> 37.4 --20.3 TSCN <ref type="bibr" target="#b38">(2020)</ref> 37.6 23.7 5.7 23.6</p><p>Ours 41.2 25.6 6.0 25.9  <ref type="table">Table 3</ref>: Ablation study on THUMOS'14. AVG represents the average mAP at IoU thresholds 0.1:0.1:0.7. "Score" indicates the way of calculating the final scores. To derive final scores, the softmax method uses the segment-level softmax scores, while the fusion method fuses the segment-level softmax scores and uncertainty as in Eq. 3.</p><p>The sampling method is the same as STPN <ref type="bibr" target="#b25">(Nguyen et al. 2018)</ref>. The number of the pseudo action/background frames is determined by the ratio parameters, i.e., k act = T /r act and k bkg = T /r bkg . All hyper-parameters are set by grid search; m = 100, r act = 9, r bkg = 4, α = 5 × 10 −4 , β = 1, and θ vid = 0.2. Multiple thresholds from 0 to 0.25 with a step size 0.025 are used as θ seg , then we perform non-maxium suppression (NMS) with an IoU threshold of 0.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with state-of-the-art methods</head><p>We compare our method with the existing fully-supervised and weakly-supervised methods under several IoU thresholds. We separate the entries by horizontal lines regarding the levels of supervision. For readability, all results are reported on the percentage scale. <ref type="table" target="#tab_1">Table 1</ref> demonstrates the results on THUMOS'14. As shown, our method achieves a new state-of-the-art performance on weakly-supervised temporal action localization. Notably, our method significantly outperforms the existing background modeling approaches, <ref type="bibr" target="#b17">Liu et al. (Liu, Jiang, and Wang 2019)</ref>, <ref type="bibr" target="#b26">Nguyen et al. (Nguyen, Ramanan, and Fowlkes 2019)</ref> and BaS-Net <ref type="bibr" target="#b12">(Lee, Uh, and Byun 2020)</ref>, by large margins of 9.5 %, 5.6 % and 6.6 % in terms of average mAP, respectively. This confirms the effectiveness of our uncer-  <ref type="bibr" target="#b38">(2020)</ref> 34.5 22.5 4.9 22.2 A2CL-PT <ref type="bibr" target="#b38">(2020)</ref> 36.8 22. <ref type="bibr">5 5.2 22.5 TSCN (2020)</ref> 35.3 21.4 5.3 21.7</p><p>Ours 37.0 23.9 5.7 23.7  <ref type="table">Table 5</ref>: Analysis on the maximum feature magnitude m on THUMOS'14. We report the average mAPs under IoU thresholds 0.1:0.1:0.7 with varying m from 10 to 250. tainty modeling. Moreover, with a much lower level of supervision, our method beats several fully-supervised methods, following the latest approaches with the least gap. The performances on ActivityNet 1.2 are demonstrated in <ref type="table" target="#tab_4">Table 2</ref>. Consistent with the results on THUMOS'14, our method outperforms all weakly-supervised approaches by obvious margins, following SSN ) with a small gap less than 1 %. We also summarize the results on ActivityNet 1.3 in <ref type="table" target="#tab_6">Table 4</ref>. We see that our method surpasses existing weakly-supervised methods including those which use external information. Furthermore, our method performs better than the supervised method TAL-Net <ref type="bibr" target="#b4">(Chao et al. 2018)</ref> in terms of average mAP, demonstrating the potential of weakly-supervised action localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation study</head><p>Effects of loss components and score calculation. In Table 3, we investigate the contribution of each component on THUMOS'14. Firstly, the baseline is set as the main pipeline only with video-level classification loss (L cls ). We try two types of score calculation methods for the baseline: (1) softmax and (2) fusion of softmax and uncertainty (as in Eq. 3). For the second, as the original feature magnitudes are unconstrained, it is inappropriate to use Eq. 4. Instead, we perform min-max normalization on feature magnitudes to estimate the probability of a segment being in-distribution. Surprisingly, the fusion method largely improves the average mAP by 9.9 %, as the relatively low magnitudes of background segments offset their softmax scores, which re- (1) a sparse action case with GolfSwing and (2) a frequent action case with VolleyballSpking. There are five plots with sampled frames for each example. The first and second plot show the final scores and the detection results of the corresponding action class from BaS-Net respectively, while the third and fourth plot represent those from our method respectively. The last plot is the ground truth action intervals. The horizontal axis in each plot means the timesteps of the video, while the vertical axes are the score values from 0 to 1. In the red boxes, while BaS-Net fails to cover complete action instances and splits them into multiple detection results, our method precisely localizes them. The black dashed lines indicate the action frames that are misclassified by BaS-Net but detected by our method.</p><p>duces false positives. This confirms both the correlation between feature magnitudes and action/background segments and the importance of background modeling. Thereafter, the proposed losses, i.e., uncertainty modeling loss (L um ) and background entropy loss (L be ), are added subsequently. We note that background entropy loss cannot stand alone, as it is calculated with the pseudo background segments which are selected based on uncertainty. As a result, uncertainty modeling boosts the performance to 40.2 % in terms of the average mAP, which already achieves a new state-of-the-art. In addition, background entropy loss further improves the performance, widening the gap with the existing methods.</p><p>Analysis on the maximum feature magnitude m. The maximum feature magnitude m in Eq. 4 determines how different the feature magnitudes from action frames and those from background frames are. We investigate the effect of the maximum feature magnitude m in <ref type="table">Table 5</ref>, where m is altered from 10 to 250. We notice that the performance differences are insignificant when m is large enough. Meanwhile, the performance decreases with a too small m, because the separation between action and background is insufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative results</head><p>To confirm the superiority of our background modeling, we qualitatively compare our method with BaS-Net (Lee, Uh, and Byun 2020) which uses the background class. In <ref type="figure" target="#fig_3">Fig. 4</ref>, two examples from THUMOS'14 are illustrated: sparse and frequent action cases. In the both cases, we see that our model detects the action instances more precisely. More specifically, in the red boxes, it can be noticed that BaS-Net splits one action instance into multiple incomplete detection results. We conjecture that this problem is because BaS-Net strongly forces inconsistent background frames to belong to one class, which makes the model misclassify confusing parts of action instances as the background class (See the black dashed lines). On the contrary, our model provides better separation between action and background via uncertainty modeling instead of using a separate class. Consequently, our model successfully localizes the complete action instances without splitting them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we identified the inherent limitations of existing background modeling approaches, with the observation that background frames may be dynamic and inconsistent. Thereafter, based on their inconsistency, we proposed to formulate background frames as out-of-distribution samples and model uncertainty with feature magnitudes. In order to train the model to identify background frames without frame-level annotations, we designed a new architecture, where uncertainty is learned via multiple instance learning. Furthermore, background entropy loss was introduced to prevent background segments from leaning toward any specific action class. The ablation study verified that our uncertainty modeling and background entropy loss both are beneficial for the localization performance. Through the experiments on two most popular benchmarks -THUMOS'14 and ActivityNet, our method achieved a new state-of-the-art with a large margin on weakly-supervised temporal action localization. We believe it would be a promising future direction to adopt the out-of-distribution formulation of background to the fully-supervised setting or other related tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) dynamic background frames from a SoccerPenalty video (b) inconsistent background frames from a GolfSwing video time Challenges in modeling background: dynamism and inconsistency. (a) The background frames in red boxes showing soccer players celebrating are very dynamic. (b) The background frames are inconsistent. The frames in green boxes show empty scenes with subtitles and the one in a blue box shows a golfer preparing to shoot. These two types do not have consistent appearances and semantics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Normalized histograms of feature magnitudes. (a) The magnitudes of original features show correlation with action/background, but the centers of two distributions are close. (b) The separated feature magnitudes by our method have better discrimination of action and background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative comparison with BaS-Net (Lee, Uh, and Byun 2020) on THUMOS'14. We provide two different cases of examples:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison on THUMOS'14. AVG is the average mAP under the thresholds 0.1:0.1:0.7, while † indicates the use of additional information, such as action frequency or human pose.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="9">: Results on ActivityNet 1.2. AVG is the averaged</cell></row><row><cell cols="9">mAP at the thresholds 0.5:0.05:0.95, while  † means the use</cell></row><row><cell cols="2">of action counts.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Score</cell><cell>Loss</cell><cell></cell><cell></cell><cell></cell><cell cols="2">mAP@IoU (%)</cell><cell></cell></row><row><cell cols="2">softmax fusion Lum Lbe</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7 AVG</cell></row><row><cell></cell><cell></cell><cell cols="7">42.3 35.1 27.7 21.7 16.4 10.5 4.6</cell><cell>22.6</cell></row><row><cell></cell><cell></cell><cell cols="7">58.8 50.0 40.5 31.7 23.6 15.5 7.2</cell><cell>32.5</cell></row><row><cell></cell><cell></cell><cell cols="7">65.3 59.2 50.8 42.3 32.4 21.1 10.4 40.2</cell></row><row><cell></cell><cell></cell><cell cols="7">67.5 61.2 52.3 43.4 33.7 22.9 12.1 41.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="5">: Comparison on ActivityNet 1.3.  † means the use</cell></row><row><cell cols="5">of external data and AVG is the averaged mAP under the</cell></row><row><cell cols="3">thresholds 0.5:0.05:0.95.</cell><cell></cell><cell></cell></row><row><cell>m</cell><cell>10</cell><cell>25</cell><cell>50</cell><cell>100 150 200 250</cell></row><row><cell cols="5">mAP@AVG 39.6 41.1 41.8 41.9 41.7 41.9 41.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/activitynet/ActivityNet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>This project was partly supported by the National Research </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Action search: Spotting actions in videos and its application to temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="251" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards open set deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1563" to="1572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Reducing Network Agnostophobia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Dhamija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Günther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
		<idno>NeurIPS. 2</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning Temporal Co-Attention Models for Unsupervised Video Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno>ICLR. 4</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Relational Prototypical Network for Weakly Supervised Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Action-Bytes: Learning From Trimmed Videos to Localize Actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<idno>CVPR. 5</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/.5" />
		<title level="m">THUMOS Challenge: Action Recognition with a Large Number of Classes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6402" to="6413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Background Suppression Network for Weakly-supervised Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI. 1, 2, 5</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<idno>ICLR. 2</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast Learning of Temporal Action Proposal via Dense Boundary Generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11499" to="11506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BMN: Boundary-Matching Network for Temporal Action Proposal Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Completeness Modeling and Context Separation for Weakly Supervised Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weakly Supervised Temporal Action Localization Through Contrast Based Evaluation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gaussian Temporal Awareness Networks for Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="344" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weakly-Supervised Action Localization with Expectation-Maximization Multi-Instance Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guillory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">SF-Net: Single-Frame Supervision for Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<idno>ECCV. 2</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Framework for Multiple-Instance Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adversarial Background-Aware Loss for Weakly-supervised Temporal Activity Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3C-Net: Category Count and Center Loss for Weakly-Supervised Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weakly supervised action localization by sparse temporal pooling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weakly-Supervised Action Localization With Background Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">W-talc: Weakly-supervised temporal activity localization and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Weakly-Supervised Action Localization by Generative Attention Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5734" to="5743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Autoloc: Weakly-supervised temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>1, 2, 4, 5</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An improved algorithm for tv-l 1 optical flow</title>
	</analytic>
	<monogr>
		<title level="m">Statistical and geometrical approaches to visual motion analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="23" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02716.2</idno>
		<title level="m">A pursuit of temporal accuracy in general activity detection</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sub-Graph Localization for Temporal Action Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-Tad</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. 1, 2, 5</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Segregated Temporal Assembly Recurrent Networks for Weakly Supervised Multiple Action Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<idno>AAAI. 1</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Exploring temporal preservation networks for precise temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dou</surname></persName>
		</author>
		<idno>AAAI. 2</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Temporal Structure Mining for Weakly Supervised Action Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Temporal action localization with pyramid of score distribution features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3093" to="3102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Marginalized average attentional network for weakly-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-H</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph Convolutional Networks for Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Two-Stream Consensus Network for Weakly-Supervised Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-instance Multi-label Action Recognition and Localization Based on Spatio-Temporal Pre-trimming for Untrimmed Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-instance learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science &amp; Technology, Nanjing University</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Tech. Rep .</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
