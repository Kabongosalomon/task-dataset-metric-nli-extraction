<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Semantically Enhanced Feature for Fine-Grained Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020">2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Hengmin</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
						</author>
						<title level="a" type="main">Learning Semantically Enhanced Feature for Fine-Grained Image Classification</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE SIGNAL PROCESSING LETTERS</title>
						<imprint>
							<biblScope unit="volume">27</biblScope>
							<biblScope unit="page">1</biblScope>
							<date type="published" when="2020">2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Image classification</term>
					<term>visual categorization</term>
					<term>fea- ture learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We aim to provide a computationally cheap yet effective approach for fine-grained image classification (FGIC) in this letter. Unlike previous methods that rely on complex part localization modules, our approach learns fine-grained features by enhancing the semantics of sub-features of a global feature. Specifically, we first achieve the sub-feature semantic by arranging feature channels of a CNN into different groups through channel permutation. Meanwhile, to enhance the discriminability of sub-features, the groups are guided to be activated on object parts with strong discriminability by a weighted combination regularization. Our approach is parameter parsimonious and can be easily integrated into the backbone model as a plug-and-play module for end-to-end training with only image-level supervision. Experiments verified the effectiveness of our approach and validated its comparable performance to the state-of-the-art methods. Code is available at https:// github.com/ cswluo/ SEF</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>F INE-grained image classification (FGIC) concerns the task of distinguishing subordinate categories of some base classes such as dogs <ref type="bibr" target="#b0">[1]</ref>, birds <ref type="bibr" target="#b1">[2]</ref>, cars <ref type="bibr" target="#b2">[3]</ref>, aircraft <ref type="bibr" target="#b3">[4]</ref>. Due to the large intra-class pose variation and high inter-class appearance similarity, as well as the scarcity of annotated data, it is challenging to efficiently solve the FGIC problem.</p><p>Recent studies have shown great interests in tackling the FGIC problem by unifying part localization and feature learning in an end-to-end CNN <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. For example, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, and <ref type="bibr" target="#b4">[5]</ref> first crop object parts on input images and feed them into models for feature extraction, where the part locations are obtained by taking as input the imagelevel features extracted by a convolutional network. To make the model optimization more easier, <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b13">[14]</ref> produce parts features by weighting feature channels using soft attentions. Another line of research divides feature channels into several groups with each corresponding to a semantic part of the input image <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>. However, these methods usually make the model optimization more difficult, since they either rely on complex part localization modules, or introduce a large number of parameters into their backbone models, or require a separate module to guide the learning of the feature channel grouping.</p><p>In this letter, we propose a computationally cheap yet effective approach that learns fine-grained features by improving the sub-features' semantics and discriminability. It includes two core components: 1) a semantic grouping module that arranges feature channels with similar properties into the same group to represent a semantic part of the input image; 2) a feature enhancement module that improves the discriminability of the grouped features by guiding them to be activated on object parts with strong discriminability. The two components can be easily implemented as a plug-and-play module in modern CNNs without the need of guided initialization or modifying the backbone network structure. By coupling the two components together, our approach ensures the generation of powerful and distinguishable fine-grained features without requiring complex part localization modules.</p><p>Concretely, we construct semantic groups in the last convolutional layer of a CNN by arranging feature channels through a permutation matrix, which is learned implicitly through regularizing the relationship between feature channels, i.e., maximizing the correlations between feature channels in the same predefined group and decorrelating those in different predefined groups. Thus, our feature channel grouping does not introduce any additional parameters into the backbone model. Compared to <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b5">[6]</ref>, our strategy groups feature channels more consistently and requires no guided initialization. To guide the semantic groups to be activated on object parts with strong discriminability, we introduce a regularization method that employs a weighted combination of maximum entropy learning and knowledge distillation. The strategy of weighted combination is derived from matching prediction distributions between the outputs of the global feature and its group-wise sub-features. This regularization method introduces only a small number of parameters into the backbone models due to the output of prediction distributions of sub-features. Overall, by coupling the two components together, our approach can efficiently obtain fine-grained features with strong discriminability. Besides, our approach can be easily integrated into the backbone model as a plug-and-play module for end-to-end training with only image-level supervision. Our contributions are summarized as follows:</p><p>• We propose a computationally cheap FGIC approach that achieves comparable performance to the state-of-theart methods with only 1.7% parameters more than its ResNet-50 backbone on the Birds dataset. guided initialization and extra parameters. • We propose to enhance feature discriminability by guiding its sub-features to be extracted from object parts with strong discriminability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROPOSED APPROACH</head><p>Our approach involves two main components: 1) A semantic grouping module that arranges feature channels with different properties into different groups. 2) A feature enhancement module that elevates feature performance by improving its subfeatures discriminability. <ref type="figure" target="#fig_0">Fig. 1</ref> is an overview of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Semantic Grouping</head><p>Previous work <ref type="bibr" target="#b15">[15]</ref> has verified that bunches of filters in the high-level layers of CNNs are required to represent a semantic concept. Therefore, we develop a regularization method that arranges filters with different properties into different groups to capture semantic concepts. Specifically, given a convolutional feature X L ∈ R C×W H of layer L, where a single feature channel is represented by</p><formula xml:id="formula_0">X L i ∈ R W H , i ∈ [1, · · · , C].</formula><p>We first arrange its feature channels through a permutation operation X L = AX L , where A ∈ R C×C is a permutation matrix, and then divide the channels into G groups. Since X L is obtained by convolving the filters of layer L with the features of layer L − 1. The convolution operation can be formulated as</p><formula xml:id="formula_1">X L = BX L−1 ,<label>(1)</label></formula><p>where B ∈ R C×Ω and X L−1 ∈ R Ω×Ψ are respectively the reshaped filters of layer L and the reshaped feature of layer L − 1. Thus X L can be rewritten as</p><formula xml:id="formula_2">X L = AX L = ABX L−1 = WX L−1 ,<label>(2)</label></formula><p>where W is a permutation of B.</p><p>To achieve the groups with semantic meaning, A should be learned to discover the similarities between the filters (rows) of B. It is, however, nontrivial to learn the permutation matrix straightforwardly. Therefore, we instead learn W directly by constraining the relationships between feature channels of X L , thus circumventing the difficulty of learning A. To effect, we maximize the correlation between feature channels in the same group while decorrelating those in different groups.</p><p>Concretely, letX L i ← X L i /||X L i || 2 be a normalized channel. The correlation between channels is then defined as</p><formula xml:id="formula_3">d ij =X L T iX L j ,<label>(3)</label></formula><p>where T is the transpose operation. Let D ∈ R G×G be the correlation matrix with element D mn = 1</p><p>CmCn i∈m,j∈n d ij corresponding to the average correlation of feature channels from groups m and n, where m, n ∈ 1, · · · , G and C m is the number of channels in group m. Then the semantic groups can be achieved by minimizing</p><formula xml:id="formula_4">L group = 1 2 ( D 2 F − 2 diag(D) 2 2 ),<label>(4)</label></formula><p>where diag(·) extracts the main diagonal of a matrix. Generally, Eq. 2 can be implemented as a convolutional operation in CNNs. Eq. 4 can be regularized on feature channels without introducing any additional parameters. Different from <ref type="bibr" target="#b10">[11]</ref> in which a semantic mapping matrix is explicitly learned to reduce the redundancy of bilinear features, our strategy focuses on improving the semantics of sub-features and does not modify the backbone network structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Enhancement</head><p>Semantic grouping can drive features of different groups to be activated on different semantic (object) parts. However, the discriminability of those parts may not be guaranteed. We, therefore, need to guide these semantic groups to be activated on object parts with strong discriminability. A simple way to achieve this effect is to match the prediction distributions between the object and its parts as implemented in <ref type="bibr" target="#b13">[14]</ref>. However, it is unclear about the reasons why matching distributions can improve performance. Here, we provide an analysis to understand its principles and make improvements to achieve better performance.</p><p>Let P w and P a be the prediction distributions of an object and its part, respectively. Then, matching their distributions can be achieved by minimizing the KL divergence between them <ref type="bibr" target="#b13">[14]</ref>,</p><formula xml:id="formula_5">L KL(Pw||Pa) = −H(P w ) + H(P w , P a ),<label>(5)</label></formula><p>where H(P w ) = − P w log P w and H(P w , P a ) = − P w log P a . Thus, the optimization objective of a classification task can be generally written as</p><formula xml:id="formula_6">L = L cr + λL KL ,<label>(6)</label></formula><p>where L cr is the cross entropy loss and λ is the balance weight. Substituting Eq. 5 into Eq. 6 we have L = L cr − λH(P w ) + λH(P w , P a ).</p><p>Eq. 7 implies that minimizing matching prediction distributions can be decomposed into a maximum entropy term and a knowledge distillation term. Both of them are powerful regularization methods <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref>. In FGIC, maximum entropy learning can effectively reduce the confidence of classifiers, thus leading to better generalization in low data-diversity scenarios <ref type="bibr" target="#b18">[18]</ref>. The last term in Eq. 7 distills knowledge from the global feature to the local feature, thus enhancing the discriminability of local features. To this end, we can regulate the importance of the two terms separately for better performance. Put all together, our optimization objective can be formulated as</p><formula xml:id="formula_8">L = E x L cr −λH(P w )+ γ G G g=1 H(P w , P g a )+φL group . (8)</formula><p>Here, λ, γ, φ and G are hyper-parameters and we omit the dependence on x for clarity (see <ref type="table" target="#tab_1">Table III</ref> for evaluation). In our implementation, the last-layer feature channels are first pooled averagely and then simultaneously fed into G + 1 neural networks (NNs) to output class distributions, in which one takes as input the global feature and the others take as input only features in corresponding groups. Only the NN taking the global feature is used for prediction (see <ref type="figure" target="#fig_0">Fig. 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>We employ ResNet-50 <ref type="bibr" target="#b19">[19]</ref> as the backbone of our approach in PyTroch and experiment on CUB-Birds <ref type="bibr" target="#b1">[2]</ref>, Stanford Cars <ref type="bibr" target="#b2">[3]</ref>, Stanford Dogs <ref type="bibr" target="#b0">[1]</ref>, and FGVC-Aircraft <ref type="bibr" target="#b3">[4]</ref> datasets (see supplementary materials (SMs) for data statistics). We initialize our model using the pretrained weights on Ima-geNet <ref type="bibr" target="#b20">[20]</ref> and fine-tune all layers on batches of 32 images of size 448 × 448 by SGD <ref type="bibr" target="#b21">[21]</ref> with momentum of 0.9. Random flip is employed only in training. The G+1 NNs are all singlelayer networks. The initial learning rate, lr, is 0.01 except on Dogs where 0.001 is used and decays by 0.1 every 20 epochs with a total of 50 training epochs. λ, γ and φ are validated on the Birds validation set, which contains 10% of the training samples, and are correspondingly set to 1, 0.05 and 1 across all datasets. G is determined by the performance of models with different values and respectively set to 3, 4, and 2 on Aircraft, Birds, and the other two datasets in this letter.</p><p>Metrics. Except for classification accuracy, we also employ scores to rank the overall performance of a method across datasets. Given the performance of method m on L datasets, the score of method m is S m = 1 L L l=1 R m l , where R m l is the rank of method m on the l-th dataset based on its classification accuracy. The closer the score is to 1, the better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with the State-of-the-Art</head><p>To be fair, we only compare to weakly-supervised methods employing the ResNet-50 backbone, due to its popularity and state-of-the-art performance in recent FGIC work. Notice that we are not attending to achieve the best performance but to emphasize the advantage brought by our simple construction.</p><p>Complexity analysis. Our approach introduces additional parameters into the backbone network only in knowledge distillation, where the total number of additional parameters is constrained by the multiplication of feature dimensionality and the number of classes. For example, 409, 600 new parameters are introduced on the Birds dataset, which accounts for only 1.7% of the parameters of ResNet-50. Since there are only negligible additional parameters in our approach, the network is efficient to train. Compared with computation-intensive methods such as S3N <ref type="bibr" target="#b9">[10]</ref>, TASN <ref type="bibr" target="#b8">[9]</ref>, API-Net <ref type="bibr" target="#b22">[22]</ref>, and DCL <ref type="bibr" target="#b23">[23]</ref> (requires 60, 90, 100, and 180 epochs for training, respectively), our approach can be optimized in 50 epochs.</p><p>During testing, only the backbone network is activated with all additional modules removed. Compared with its ResNet-50 backbone, our approach boosts performance +1.65% on average with the same time cost at inference, which indicates the practical value of our approach. , † and ‡ represent methods with separated initialization, multicropping operations, and results from our re-implementation, respectively. The closer the score is to 1, the better.</p><p>Performance comparison. <ref type="table" target="#tab_1">Table I</ref> shows that there is no single method that can achieve the best performance on all datasets. Our approach (SEF) attains a score of 4.8 ranking 5th in overall performance, which is comparable to the state-ofthe-art methods, especially considering its simple construction. The methods in the second group are closely related to ours. Compared to ResNet-50 and MaxEnt-CNN, SEF boosts performance on all datasets. Besides, SEF achieves a more robust performance than DBT-Net. Among other methods, S3N, API-Net, DCL, and Cross-X rank before ours. However, they are more expensive than ours such as Cross-X using twice as many parameters as ResNet-50. <ref type="table" target="#tab_1">Table II</ref> shows the performance of our approach on different backbones. It is worth noting that 1) we did not cross-validate the hyper-parameters for these new backbones, but to use those validated for ResNet-50, and 2) the hyper-parameters are determined through cross-validation on Birds and applied to all datasets without modification. Thus better results can be expected than here. However, the performance demonstrates that our approach can be robustly generalized to different backbones and datasets (Please refer to SMs for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Studies</head><p>For simplicity, the rest experiments and analyses are performed on ResNet-18 unless otherwise clarified.</p><p>Effectiveness of individual module. <ref type="table" target="#tab_1">Table III</ref> shows the results of our approach with different configurations. It reveals that learning semantic groups (row 2) or matching distributions (row 3) independently can slightly improve the performance. Combining both directly (row 4) brings some advantages, but the improvement is not systematically consistent. The performance, however, can be significantly improved by decomposing matching distributions into separate regularizers (row 5), which indicates the effectiveness of feature enhancement that guiding semantic groups to be activated on object parts with strong discriminability. Number of semantic groups. The group semantic is strongly correlated with its discriminability. However, too many groups may break the correlation, resulting in weak group features. <ref type="figure">Fig. 2</ref> shows the correlations between feature channels of the last convolutional layer. <ref type="figure" target="#fig_1">Fig. 3</ref> depicts the discriminability of group features of models with varying groups. They illustrate that 1) our semantic grouping module can effectively achieve the function of grouping correlated feature channels and separating uncorrelated feature channels <ref type="figure">(Fig. 2)</ref>; 2) feature channels should not be divided into too  many groups, which on the one hand causes the difficulty in optimization ( <ref type="figure">Fig. 2)</ref> and, on the other hand, reduces the semantic as well as the discriminability of each group <ref type="figure" target="#fig_1">(Fig. 3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Visualization</head><p>The discriminability of semantic groups (sub-features) can be visualized by highlighting corresponding areas on input images. Ideally, every group should be activated by a set of proximity pixels due to the similar property of neurons in the group. <ref type="figure" target="#fig_2">Fig. 4</ref> shows that different groups can be activated by different semantic parts of images, which are highly identifiable such as the wing and engine nacelles of the aircraft. It signifies the success of our approach to enhance feature semantically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this letter, we proposed a computationally cheap yet effective approach that involves a semantic grouping and a feature enhancement module for FGIC. We empirically studied the effectiveness of each individual module and their combining effects through ablation studies, as well as the relationship between the number of groups and the semantic integrity in each group. Comparable performance to the state-of-the-art methods and low computational cost make it possible to be widely employed in FGIC applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•Fig. 1 .</head><label>1</label><figDesc>We propose to achieve part localization by learning semantic groups of feature channels, which does not require arXiv:2006.13457v3 [cs.CV] 26 Aug 2020 Overview of our approach. The last-layer convolutional feature channels (depicted by the mixed color block) of the CNN are arranged into different groups (represented by different colors) by our semantic grouping module. The global and its sub-features (group-wise features) are obtained from the arranged feature channels by average pooling. The light yellow block in the gray block denotes the predicted class distributions from corresponding sub-features, which are regularized by the output of the global feature through knowledge distillation. All gray blocks are only effective in the training stage while removed in the testing stage. The details of the CNN are omitted for clarity. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Discriminability of the 1st group features of models at different learning epochs tested on the Birds validation set. ng means the number of groups used in the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Group-wise activation maps of models with 2 semantic groups superimposed on original images. The 1st, 2nd, and 3rd rows are respectively the original images, activation maps of the 1st and 2nd semantic groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Submitted date: 07/05/2020. This work was supported in part by NSFC under grant 61702197, in part by NSFGD under grant 2020A151501813 and 2017A030310261.Wei Luo is with the South China Agricultural University, Guangzhou, 510000, China (email:cswluo@gmail.com).</figDesc><table /><note>Hengmin Zhang is with the East China University of Science and Technol- ogy, Shanghai, 200237, China (email:zhanghengmin@126.com). Jun Li and Xiu-Shen Wei are with the Nanjing University of Science and Technology, Nanjing, 210094, China (email:{junli,weixs}@njust.edu.cn).* indicates equal contribution. Wei Luo is the corresponding author.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I COMPARISON</head><label>I</label><figDesc>WITH STATE-OF-THE-ART METHODS (%).</figDesc><table><row><cell></cell><cell>Birds</cell><cell>Cars</cell><cell>Dogs</cell><cell>Aircraft</cell><cell>Scores</cell></row><row><cell>Kernel-Pooling [24]</cell><cell>84.7</cell><cell>92.4</cell><cell>−</cell><cell>86.9</cell><cell>10.3</cell></row><row><cell>MAMC-CNN [8]</cell><cell>86.2</cell><cell>93.0</cell><cell>84.8</cell><cell>−</cell><cell>7.7</cell></row><row><cell>DFB-CNN [7]</cell><cell>87.4</cell><cell>93.8</cell><cell>−</cell><cell>92.0</cell><cell>6.3</cell></row><row><cell>NTS-Net  † [13]</cell><cell>87.5</cell><cell>93.9</cell><cell>−</cell><cell>91.4</cell><cell>6.0</cell></row><row><cell>S3N  † [10]</cell><cell>88.5</cell><cell>94.7</cell><cell>−</cell><cell>92.8</cell><cell>1.7</cell></row><row><cell>API-Net [22]</cell><cell>87.7</cell><cell>94.8</cell><cell>88.3</cell><cell>93.0</cell><cell>2.3</cell></row><row><cell>DCL [23]</cell><cell>87.8</cell><cell>94.5</cell><cell>−</cell><cell>93.0</cell><cell>2.7</cell></row><row><cell>TASN  † [9]</cell><cell>87.9</cell><cell>93.8</cell><cell>−</cell><cell>−</cell><cell>5.0</cell></row><row><cell>Cross-X [14]</cell><cell>87.7</cell><cell>94.6</cell><cell>88.9</cell><cell>92.6</cell><cell>2.8</cell></row><row><cell>ResNet-50  ‡ [19]</cell><cell>84.5</cell><cell>92.9</cell><cell>88.1</cell><cell>90.3</cell><cell>8.3</cell></row><row><cell>MaxEnt-CNN  ‡ [18]</cell><cell>83.4</cell><cell>92.8</cell><cell>88.0</cell><cell>90.7</cell><cell>8.8</cell></row><row><cell>DBT-Net [11]</cell><cell>87.5</cell><cell>94.1</cell><cell>−</cell><cell>91.2</cell><cell>5.7</cell></row><row><cell>SEF (ours)</cell><cell>87.3</cell><cell>94.0</cell><cell>88.8</cell><cell>92.1</cell><cell>4.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>ON DIFFERENT BACKBONES (%).</figDesc><table><row><cell></cell><cell cols="3">VGG16 (+SEF) ResNet-50 (+SEF) ResNeXt-50 (+SEF)</cell></row><row><cell>Birds</cell><cell>77.7 (81.1)</cell><cell>84.5 (87.3)</cell><cell>86.8 (87.8)</cell></row><row><cell>Cars</cell><cell>87.3 (88.3)</cell><cell>92.9 (94.0)</cell><cell>93.7 (94.2)</cell></row><row><cell>Dogs</cell><cell>71.1 (75.4)</cell><cell>88.1 (88.8)</cell><cell>89.8 (90.8)</cell></row><row><cell>Aircraft</cell><cell>87.0 (88.5)</cell><cell>90.3 (92.1)</cell><cell>92.0 (92.6)</cell></row><row><cell></cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell></cell><cell>0.8</cell><cell>0.8</cell><cell>0.8</cell></row><row><cell></cell><cell>0.6</cell><cell>0.6</cell><cell>0.6</cell></row><row><cell></cell><cell>0.4</cell><cell>0.4</cell><cell>0.4</cell></row><row><cell></cell><cell>0.2</cell><cell>0.2</cell><cell>0.2</cell></row><row><cell cols="4">Fig. 2. From left to right are correlation matrices of feature channels of</cell></row><row><cell cols="4">models with 3, 5, and 7 groups, averaged on 64 images randomly selected</cell></row><row><cell cols="3">from the Birds testing dataset, respectively.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III PERFORMANCE</head><label>III</label><figDesc>WITH DIFFERENT OPTIONS (%).</figDesc><table><row><cell></cell><cell>Birds</cell><cell>Cars</cell><cell>Dogs</cell><cell>Aircraft</cell></row><row><cell>ResNet-18 [19]</cell><cell>82.3</cell><cell>90.4</cell><cell>81.5</cell><cell>88.5</cell></row><row><cell>λ = 0, γ = 0, φ = 1</cell><cell>82.7</cell><cell>90.9</cell><cell>82.1</cell><cell>88.9</cell></row><row><cell>λ = 0.05, γ = 0.05, φ = 0</cell><cell>82.2</cell><cell>90.3</cell><cell>81.8</cell><cell>88.8</cell></row><row><cell>λ = 0.05, γ = 0.05, φ = 1</cell><cell>83.2</cell><cell>90.1</cell><cell>81.8</cell><cell>89.0</cell></row><row><cell>λ = 1, γ = 0.05, φ = 1</cell><cell>84.8</cell><cell cols="2">91.8 83.1</cell><cell>89.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">#category #training #testing CUB-Birds [2] 200 5, 994 5, 794 Stanford Cars<ref type="bibr" target="#b2">[3]</ref> </note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A: DATA STATISTICS</head><p>We validate the effectiveness of our approach on 4 finegrained benchmark datasets. Detailed instructions of these datasets can be found on their homepages, respectively. The data statistics are presented in the following <ref type="table">Table.</ref>   <ref type="bibr" target="#b25">[25]</ref>. Due the the huge parameters of VGG16, we correspondingly modified the structure of VGG16 to make it suitable for our validation. To this end, we remove all the fully connected layers and employ the global average pooling on the last convolutional feature maps, then directly feed the pooled features into the output layer. This modification reduces about 89% parameters of the vanilla VGG16 model. Our approach introduces about 0.7% additional parameters to this modified VGG16 on the Birds dataset.</p><p>ResNeXt-50 <ref type="bibr" target="#b26">[26]</ref>. We use the default setting of ResNeXt-50, which has 32 paths in all blocks. The input and output channels in every path are 4-dimensional. This setting ensures the similar capacity of ResNeXt-50 and ResNet-50. Thus, our approach also introduces about 1.7% additional parameters to ResNeXt-50 on the Birds dataset.</p><p>Training details. We did not fine-tune hyper-parameters (including the number of groups, learning rate, regularization coefficients) for VGG16 and ResNeXt-50, but used the same hyper-parameters as ResNet-50. Thus, better results can be achieved if fine-tuning these hyper-parameters on target datasets. Both backbones were initialized by using pretrained weights on ImageNet <ref type="bibr" target="#b20">[20]</ref>. All other training criteria are the same as ResNet-50.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Workshop on Fine-Grained Visual Categorization (FGVC) at CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th IEEE Workshop on 3D Representation and Recognition at ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a cnn for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-attention multi-class constraint for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Selective sparse sampling for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning deep bilinear transformation for fine-grained image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fully convolutional attention localization networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06765</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-N</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cross-x learning for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Net2vec: Quantifying and explaining how concepts are encoded by filters in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A maximum entropy approach to natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A D</forename><surname>Pietra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="71" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Maximum entropy finegrained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gradient based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning attentive pairwise interaction for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Destruction and construction learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Kernel pooling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
