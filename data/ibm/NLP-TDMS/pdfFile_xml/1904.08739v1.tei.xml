<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cascaded Partial Decoder for Fast and Accurate Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
							<email>zhe.wu@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS)</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Key Lab of Big Data Mining and Knowledge Management</orgName>
								<orgName type="institution">UCAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS)</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Key Lab of Big Data Mining and Knowledge Management</orgName>
								<orgName type="institution">UCAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Key Lab of Intelligent Information Processing</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
							<email>qmhuang@usas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS)</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Key Lab of Big Data Mining and Knowledge Management</orgName>
								<orgName type="institution">UCAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Key Lab of Intelligent Information Processing</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cascaded Partial Decoder for Fast and Accurate Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing state-of-the-art salient object detection networks rely on aggregating multi-level features of pretrained convolutional neural networks (CNNs). Compared to high-level features, low-level features contribute less to performance but cost more computations because of their larger spatial resolutions. In this paper, we propose a novel Cascaded Partial Decoder (CPD) framework for fast and accurate salient object detection. On the one hand, the framework constructs partial decoder which discards larger resolution features of shallower layers for acceleration. On the other hand, we observe that integrating features of deeper layers obtain relatively precise saliency map. Therefore we directly utilize generated saliency map to refine the features of backbone network. This strategy efficiently suppresses distractors in the features and significantly improves their representation ability. Experiments conducted on five benchmark datasets exhibit that the proposed model not only achieves state-of-the-art performance but also runs much faster than existing models. Besides, the proposed framework is further applied to improve existing multi-level feature aggregation models and significantly improve their efficiency and accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, deep learning has achieved surprising performance in salient object detection for it providing abundant and discriminative image representations. The early deep saliency methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32]</ref> utilize CNNs to predict saliency scores of image regions and obtain accurate saliency maps with high computational complexity. In the following works, fully convolutional network (FCN) <ref type="bibr" target="#b23">[24]</ref>  based encoder-decoder architecture is widely applied for salient object detection. The encoder is the pre-trained image classification model (e.g. VGG <ref type="bibr" target="#b28">[29]</ref> and ResNet <ref type="bibr" target="#b7">[8]</ref>) which provides multi-level deep features: the high-level features with low resolutions represent semantic information, and the low-level features with high resolutions represent spatial details. In the decoder, these features are combined to generate accurate saliency maps. Researchers have developed various decoders <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref> to integrate low-level and high-level features. However, two drawbacks exist in these deep aggregation methods. On the one hand, compared to high-level features, low-level features contribute less to the performance of deep aggregation methods. In <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, we present performances of different side outputs of the DSS <ref type="bibr" target="#b8">[9]</ref> model. It is obvious that the performance tends to saturate quickly when gradually aggregating features from high-level to lowlevel. On the other hand, due to the large resolutions of low-level features, integrating them with high-level features obviously enlarges the computational complexity as shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. However, detecting and segmenting salient objects should be fast since this process is often a preprocessing stage to more complex operations <ref type="bibr" target="#b2">[3]</ref>. In consequence, it is essential to design a mechanism to eliminate the impact of low-level features on computational complexity while ensuring the performance. When CNNs go deep, feature gradually changes from low-level representation to high-level representation. Hence deep aggregation models may recover spatial details of saliency maps when only integrating features of deeper layers. In <ref type="figure" target="#fig_1">Fig. 2</ref>, we show examples of multi-level feature maps of VGG16 <ref type="bibr" target="#b28">[29]</ref>. Compared to low-level features of Conv1 2 and Conv2 2 layers, the feature of Conv3 3 layer also reserve edge information. Besides, background regions in feature maps may result in inaccuracy of saliency maps. Previous works make use of adaptive attention mechanism <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b40">41]</ref> to solve this problem. However, the effect of this mechanism relies on the accuracy of the attention map. Since fusing features of deeper layers will generate relatively precise saliency map, we can directly use this map to refine features.</p><p>In this paper, we propose a novel cascaded partial decoder framework, which discards features of shallower layers to ensure high computational efficiency and then refine features of deeper layers to improve their representation ability. We modify the standard straight backbone network to a bifurcated one. This new backbone network contains two branches with the same architecture. We construct partial decoder which only aggregates features in each branch. In order to further accelerate the model, we design a fast and efficient context module to abstract discriminative features and integrate them in an upsampling-concatenating way. Then we propose a cascaded optimization mechanism which utilizes initial saliency map of the first branch to re-fine features of the second branch. In order to uniformly segment the whole salient objects, we propose a holistic attention module to allow the initial saliency map cover more useful information. In addition, the proposed framework can be utilized to improve existing deep aggregation models. When embedding their decoders in our framework, the accuracy and efficiency will be significantly improved. Our contributions are summarized as follows:</p><p>(1) We propose a novel cascaded partial decoder framework, which discards low-level features to reduce the complexity of deep aggregation models, and utilizes generated relatively precise attention map to refine high-level features to improve the performance.</p><p>(2) Experimental results on five benchmark datasets demonstrate that the proposed model not only achieves state-of-the-art performance but also runs much faster than existing models.</p><p>(3) Our framework can be applied to improve existing deep aggregation models. The efficiency and accuracy of improved models will both be significantly improved compared to the original models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Over the past two decades, researchers have developed a large amount of saliency detection algorithms. Traditional models extract hand-crafted features and are based on various saliency assumptions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b45">46]</ref>. More details about traditional methods are concluded in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Here we mainly discuss deep learning based saliency detection models.</p><p>Early works utilize CNNs to determine whether image regions are salient or not <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44]</ref>. Although these models have achieved much better performance than traditional methods, it is time-consuming to predict saliency scores for image regions. Then researchers develop more effective models based on the successful fully convolutional network <ref type="bibr" target="#b23">[24]</ref>. Li et al. <ref type="bibr" target="#b17">[18]</ref> set up a unified framework for salient object detection and semantic segmentation to effectively learn the semantic properties of salient objects. Wang et al. <ref type="bibr" target="#b33">[34]</ref> leverage cascaded fully convolutional networks to continuously refine previous prediction maps.</p><p>Recently, researchers have proved that fusing multi-level features further improves the performance of dense prediction tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27]</ref>. In CNNs, high-level features provide semantic information, and low-level features contains spatial details which are helpful for refining object boundaries. Many works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref> follow this strategy and precisely segment salient objects. Li et al. <ref type="bibr" target="#b16">[17]</ref> directly integrate multi-level features to obtain more advanced feature representation. Liu and Han <ref type="bibr" target="#b19">[20]</ref> first make a coarse global prediction, and then hierarchically and progressively refine the details of saliency maps step by step <ref type="figure">Figure 3</ref>: (a) Traditional encoder-decoder framework, (b) The proposed cascaded partial decoder framework. We use VGG16 <ref type="bibr" target="#b28">[29]</ref> as the backbone network. Traditional framework generates saliency map S by adopting full decoder which integrates all level features. The proposed framework adopts partial decoder, which only integrates features of deeper layers, and generates an initial saliency map S i and the final saliency map S d .</p><p>via integrating local context information. Hou et al. <ref type="bibr" target="#b8">[9]</ref> introduce short connections to the skip-layer structures within the HED <ref type="bibr" target="#b37">[38]</ref> architecture. Luo et al. <ref type="bibr" target="#b24">[25]</ref> segment salient objects by combining local contrast feature and global information through a multi-resolution 4 × 5 grid network. Zhang et al. <ref type="bibr" target="#b41">[42]</ref> first integrate multi-level feature maps into multiple resolutions, which simultaneously incorporate semantic information and spatial details. Then this work predicts the saliency map in each resolution and fuses them to generate the final saliency map. In <ref type="bibr" target="#b40">[41]</ref>, the work extracts context-aware multi-level features and then utilizes a bidirectional gated structure to pass messages between them. Liu et al. <ref type="bibr" target="#b20">[21]</ref> leverage global and local pixel-wise contextual attention network to capture global and local context information. Then these modules are incorporated with U-Net architecture to segment salient objects. In this paper, we argue that low-level features always contribute less than highlevel features. However, they need more computation cost than high-level features owing to their larger spatial resolutions. Hence we propose a novel cascaded partial decoder framework for salient object detection, which does not consider low-level features and utilizes generated saliency map to refine high-level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Framework</head><p>In this paper, we propose a novel cascaded partial decoder framework which contains two branches. In each branch, we design a fast and effective partial decoder. The first branch generates an initial saliency map which is utilized to refine the features of the second branch. Besides, we propose a holistic attention module to segment the whole objects uniformly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Mechanism of the Proposed Framework</head><p>We design the proposed model on the basis of VGG16 network, which is the most widely utilized backbone network in deep salient object detection models. For an input image I with size H × W , we can abstract features at five levels, which are denoted as {f i , i = 1, ..., 5} with res-</p><formula xml:id="formula_0">olutions [ H 2 i−1 , W 2 i−1 ]</formula><p>. The decoders proposed in previous works <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>, which are called full decoder in this paper, integrate all features to generate saliency map S. A unified architecture of the full decoder is shown in <ref type="figure">Fig. 3</ref>(a) and it can be represented by</p><formula xml:id="formula_1">D T = g(f 1 , f 2 , f 3 , f 4 , f 5 ),</formula><p>where g(·) denotes a multi-level feature aggregation algorithm. Previous works focus on how to develop a more effective integration strategy.</p><p>In <ref type="figure">Fig. 3</ref>(b), we show the architecture of the proposed cascaded partial decoder framework. Since that the features of shallower layers contribute less to performance, we construct a partial decoder that only integrates features of deeper layers. In order to utilize generated saliency map to refine features, we design a bifurcated backbone network. We set the Conv3 3 layer as an optimization layer, and use the last two convolutional blocks to construct two branches (an attention one and a detection one). In the attention branch, we design a partial decoder to integrate three-level features which are denoted as {f a i = f i , i = 3, 4, 5}. Hence the partial decoder is represented by D a = g a (f a 3 , f a 4 , f a 5 ) and it generates an initial saliency map S i . After processing of the proposed holistic attention module, we obtain an enhanced attention map S h which is utilized to refine the feature f 3 . Because we can obtain relatively precise saliency map via integrating features of three top layers, the attention map S h effectively eliminates distractors in feature f 3 . Then we obtain the refined feature f d 3 for detection branch via element-wise multiplying the feature and the attention map: f d 3 = f 3 S h . Hence the following two-level features of the detection branch are denoted as</p><formula xml:id="formula_2">{f d 4 , f d 5 }. Through constructing another partial decoder D d = g d (f d 3 , f d 4 , f d 5 )</formula><p>for the detection branch, the proposed model outputs the final saliency map S d . For convenience, we set g a = g d in this paper. The details of the proposed holistic attention module and the partial decoder are described in Section 3.2 and Section 3.3 respectively.</p><p>We jointly train the two branches with ground truth. The parameters of the two branches are not shared. Given {S i , S d } and the corresponding label l, the total loss L total is formulated as:</p><formula xml:id="formula_3">L total = L ce (S i , l|Θ i ) + L ce (S d , l|Θ d ).</formula><p>(1)</p><p>L ce is the sigmoid cross entropy loss:</p><formula xml:id="formula_4">L ce (Θ) = − N j=1 c∈{0,1} δ(l j = c) log p(S j = c|Θ),<label>(2)</label></formula><p>where N is the pixel number, δ is the indicator function, j denotes pixel coordinate and Θ = {Θ i , Θ d } are parameter sets corresponding to the saliency maps S = {S i , S d }. It is obvious that Θ i is a proper subset of Θ d , which indicates that the two branches work in an alternating way. On the one hand, the attention branch provides precise attention map for the detection branch, which leads to that the detection branch segments more accurate salient objects. On the other hand, the detection branch could be considered as an auxiliary loss of the attention branch, which also helps the attention branch to focus on salient objects. Joint training the two branches makes our model uniformly highlights salient objects while suppressing distractors.</p><p>In addition, we can leverage the proposed framework to improve existing deep aggregation models when we integrate the features of each branch by using the aggregation algorithms of these works. Even though we raise the computation cost of the backbone network and add one more decoder when compared to the traditional encoder-decoder architecture, the total computation complexity is still significantly reduced because of discarding low-level features in decoders. Moreover, the cascaded optimization mechanism of the proposed framework promotes the performance, and the experiments show that the two branches both outperform the original models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Holistic Attention Module</head><p>Given the feature map from the optimization layer and the initial saliency map from attention branch, we can use a initial attention strategy which means directly multiplying the feature map with the initial saliency map. When we obtain an accurate saliency map from the attention branch, this strategy will efficiently suppress distractors of the feature. On the contrary, if distractors are classified as salient regions, this strategy results in abnormal segmentation results. As a result, we need to improve the effectiveness of the initial saliency map. More specially, the edge information of salient objects may be filtered out by the initial saliency map because it is difficult to be precisely predicted. In addition, some objects in complex scenes are hard to be completely segmented. Therefore we propose a holistic attention module which aims to enlarge the coverage area of the initial saliency map, and it is defined as follows:</p><formula xml:id="formula_5">S h = M AX(f min max (Conv g (S i , k)), S i )<label>(3)</label></formula><p>Image GT Initial Attention Holistic Attention <ref type="figure">Figure 4</ref>: GT is the ground truth. As we can see, the proposed holistic attention module is helpful for segmenting the whole salient objects and refining more precise boundaries.</p><p>where Conv g is a convolution operation with a Gaussian kernel k and zero bias, f min max (·) is a normalization function to make the blurred map ranges in [0, 1], and M AX(·) is a maximum function which tends to increase the weight coefficient of salient regions of S i because that the convolution operation will blur S i . Compared to the initial attention, the proposed holistic attention mechanism hardly increases computation cost, and it further highlights the whole salient objects as shown in <ref type="figure">Fig. 4</ref>. Moreover, the size and standard deviation of Gaussian kernel k are initialized with 32 and 4. Then it is jointly trained with the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The Proposed Decoder</head><p>Since that the proposed framework consists of two decoders, we need to construct a fast integration strategy to ensure low complexity. Meanwhile, we need to generate saliency map as accurate as possible. Firstly, in order to capture global contrast information, we design an effective context module which is inspired by the receptive field block (RFB) <ref type="bibr" target="#b21">[22]</ref>. Compared to the original RFB, we add one more branch to enlarge the receptive field further. Our context module consists of four branches {b m , m = 1, ..., 4}. For acceleration, in each branch, we use a 1 × 1 convolutional layer to reduce channel number to 32. For {b m , m &gt; 1}, we add two layers: a (2m−1)×(2m−1) convolutional layer and a 3 × 3 convolutional layer with (2m − 1) dilation <ref type="bibr" target="#b4">[5]</ref>. We concatenate the outputs of these branches and reduce the channel to 32 by an additional 1×1 convolutional layer. Then a short connection is added as the original RFB. In general, given features {f c i , i ∈ [l, ..., L], c ∈ [a, d]} from the bifurcated backbone network, we obtain discriminative features {f c1 i } from the context module. Then we use multiplication operation to reduce the gap between multilevel features. Especially, for the top-most feature (i = L), we set f c2 L = f c1 L . For feature {f c1 i , i &lt; L}, we update it to f c2 i via element-wise multiplying itself with all features of deeper layers. This operation is defined as follows:</p><formula xml:id="formula_6">f c2 i = f c1 i Π L k=i+1 Conv(U p(f c1 k )), i ∈ [l, ..., L−1],<label>(4)</label></formula><p>where U p(·) is upsampling feature by a factor 2 k−j , and Conv is a 3 × 3 convolutional layer. At last, we utilize an upsampling-concatenating strategy to integrate multi-level features. When we construct a partial decoder and set the Conv3 3 layer as the optimization layer (l = 3, L = 5), we obtain a feature map with  <ref type="bibr" target="#b32">[33]</ref>, DUT-OMRON <ref type="bibr" target="#b39">[40]</ref>. Evaluation Metrics. We adopt two metrics: mean absolute error (MAE) and F-measure (maxF). We adopt mean absolute error (MAE) and F-measure as our evaluation metrics. According to the different ways for saliency map binarization, there exist two ways to compute F-measure <ref type="bibr" target="#b3">[4]</ref>. One is maximum F-measure (denoted as maxF), which is adopted in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41]</ref>. The other is average F-measure (denoted as avgF), which is adopted in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>. For fairly comparison, we compute both maxF and avgF. Implementation Details. We implement the proposed model based on the Pytorch 1 framework and a GTX 1080Ti GPU is used for acceleration. Following previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43]</ref>, we train the proposed model on the training set of DUTS <ref type="bibr" target="#b32">[33]</ref> dataset. The parameters of the bifurcated backbone network are initialized by VGG16 <ref type="bibr" target="#b28">[29]</ref>. We initialize the other convolutional layers using the default setting of the Pytorch. All training and test images are resized to 352 × 352. Any postprocessing procedure (e.g. CRF <ref type="bibr" target="#b13">[14]</ref>) is not applied in this paper. The proposed model is trained by Adam optimizer <ref type="bibr" target="#b12">[13]</ref>. The batch size is set as 10 and the initial learning rate is set as 10 −4 and decreased by 10% when training loss reaches a flat. It takes nearly six hours for training the proposed model. The code is available at https: //github.com/wuzhe71/CPD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Comparison with State-of-the-arts</head><p>We compare the proposed model with eight state-of-theart deep salient object detection algorithms, including NLDF <ref type="bibr" target="#b24">[25]</ref>, Amulet <ref type="bibr" target="#b41">[42]</ref>, DSS <ref type="bibr" target="#b8">[9]</ref>, SRM <ref type="bibr" target="#b34">[35]</ref>, BMPM <ref type="bibr" target="#b40">[41]</ref>, PAGR <ref type="bibr" target="#b42">[43]</ref>, DGRL <ref type="bibr" target="#b35">[36]</ref> and PiCANet <ref type="bibr" target="#b20">[21]</ref>. We implement 1 https://pytorch.org/ these models with available source codes or directly evaluate saliency maps provided by authors. Especially, NLDF, Amulet and DSS are originally trained on MSRA10K <ref type="bibr" target="#b5">[6]</ref> dataset or MSRA-B <ref type="bibr" target="#b22">[23]</ref> dataset (there is a large overlap between these two datasets). Hence we re-train these three models on DUTS dataset as other models for fairly comparison. We find that training on DUTS dataset will make deep models work better in complex scenes. Besides, we also train the proposed model on MSRA-B dataset to compare with these three original models, and the results are reported in supplementary material.</p><p>In <ref type="table">Table.</ref> 1, we show the quantitative comparison results. Considering some works use ResNet50 as the backbone, we also train the proposed model on the basis of this backbone network. ResNet50 contains four convolutional blocks, and we set the last layer of the second block as the optimization layer. Then we utilize the last two blocks to design the two branches. In <ref type="table">Table.</ref> 1, the results of the attention branch (denoted as "-A") of the proposed model are also reported. Moreover, we compare the average execution time with the other models on DUTS dataset, and all scores are tested on our platform (PAGR only provides saliency maps). It is obvious that the proposed model outperforms all other models in most cases and it runs much faster than existing models. Only PiCANet-R obtains higher maxF score than the proposed model on DUT-OMRON dataset. However, our model runs about 12 times faster than PiCANet-R. More specially, compared to the improvements on maxF and MAE, we obtain a larger improvement on avgF. This demonstrates that the proposed model works much better in uniformly highlighting salient objects. In addition, we can find that the results of our attention branch also achieve comparable results with other models. Meanwhile, the proposed model only with the attention branch runs faster. This indicates that the proposed model provides two-level saliency maps for real-time applications.</p><p>In <ref type="figure" target="#fig_3">Fig. 5</ref>, we show the qualitative comparison on some challenging cases: small object, complex scenes, multiple objects and large object. Even though we discard the lowlevel features of backbone network, our model still recovers precise boundaries of salient objects, and the small object is still accurately segmented. Moreover, the proposed model segments more uniform salient objects than the compared models. It is consistency with the results in <ref type="table">Table.</ref> 1 that our model achieves more improvement in avgF score than MAE and maxF. This phenomenon is owing to the joint training strategy of the proposed model. On one hand, the supervised attention map of the attention branch makes the detection branch further concentrate on salient objects. On the other hand, when training the proposed model, the gradient of the detection branch also back propagates to the attention branch. This training mechanism gradually promotes the proposed model to focus on salient objects. More visual   <ref type="table">Table 1</ref>: Comparison of different methods on five benchmark datasets and four metrics including FPS, MAE (lower is better), max F-measure (higher is better) and average F-measure. The comparison is under two settings (with VGG <ref type="bibr" target="#b28">[29]</ref> and ResNet50 <ref type="bibr" target="#b7">[8]</ref> backbone netowrk). The best result of each setting is shown in Red. "-R" means using ResNet50 as the backbone. "-A" means the results of the attention branch. All method are the trained on training set of DUTS <ref type="bibr" target="#b32">[33]</ref>. There is not available code of PAGR <ref type="bibr" target="#b42">[43]</ref> and the author only provides the saliency maps. comparison results are shown in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Application in Existing Models</head><p>Through integrating features of each branch via using aggregation algorithms proposed in existing models, our framework can be utilized to improve these works. In this paper, we apply the proposed framework in three deep aggregation models (BMPM, Amulet, NLDF). NLDF adopts a typical U-Net architecture, BMPM proposes a bidirectional decoder with gate function and Amulet integrates multi-level feature maps in multiple resolutions. We implement the improved models in their respectively default deep learning library (tensorflow <ref type="bibr" target="#b0">[1]</ref> for BMPM and NLDF, caffe <ref type="bibr" target="#b11">[12]</ref> for Amulet). For BMPM and NLDF, we train the improved models (denoted as BMPM-CPD and NLDF-CPD) by using default settings, and it only needs to change the learning rate from the original 10 −6 to 10 −5 . For Amulet, we train the improved model (denoted as Amulet-CPD) by using the completely same settings as the original model. In <ref type="table">Table.</ref> 2, we show the quantitative results of the original models and the improved models (-CPD-A, -CPD) on five benchmark datasets. We can see that each improved model outperform its original model. More specially, the improved models obtain a large improvement on the two most challenging DUT-OMRON and DUTS datasets. In addition, the improved models (-CPD and -CPD-A) runs about 2 and 3 times faster than the original models respectively. In conclusion, the proposed cascaded partial decoder framework can be used to improve deep aggregation models with different kinds of decoders. In <ref type="figure" target="#fig_5">Fig. 6</ref>, we show the qualitative results on challenge cases: multiple objects, small object, large object and complex scene. The upper two rows show that the improved models further focus on target regions and suppress distractions. The under two rows show   <ref type="table">Table 2</ref>: Comparison of the original models and the improved models (-CPD-A and -CPD). that the improved model further highlights the whole objects.</p><formula xml:id="formula_7">Image GT BMPM BMPM-CPD-A BMPM-CPD Amulet Amulet-CPD-A Amulet-CPD NLDF NLDF-CPD-A NLDF-CPD</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Analysis of the Proposed Framework</head><p>Effectiveness of holistic attention. Here we demonstrate the effectiveness of the proposed holistic attention model in the proposed model and the three improved models. We compare these models with holistic attention and the models with initial attention, and the results are shown in <ref type="table">Table.</ref>4. It is shown that holistic attention outperforms initial attention. Selection of Optimization Layer. In the proposed model, we set Conv3 3 layer as the optimization layer. Here we compared the proposed model with different optimization layers <ref type="figure" target="#fig_1">(Conv2 2 and conv4 3)</ref>. In addition, we also report the results of no optimization layer, which means integrating all-level features via the proposed decoder. We do not test the proposed model with Conv1 2 optimization layer because this setting will increase the computation cost via adding one more full decoder; thus requirements of reducing computation cost will not be achieved. The comparison results on five benchmark datasets are shown in <ref type="table">Table.</ref> 3.</p><p>In conclusion, we set the conv3 3 layer as the optimization layer considering its best performance. When we refine the shallower feature (Conv2 2), the computation complexity increases and the performance decreases. The reason might be that the feature of shallower layer has not been enough trained. When we refine the deep feature (Conv4 3), the computation cost and the performance both decrease. This is because that resolution of the feature in the Conv4 3 layer is smaller. The accuracy and efficiency of settings (Conv2 2 and Conv4 3) both outperform the full decoder, which validates the effectiveness of the proposed framework. Failure Examples. The performance of the proposed model relies on the accuracy of the attention branch. When the attention branch detects clutters as target regions, our model will obtain wrong results. In <ref type="figure">Fig. 7</ref>, we show some failure examples of our model. When a large target region is not segmented correctly, the proposed model is unable to segment the whole objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Application in Other Tasks</head><p>In this paper, we also evaluate the proposed model on other two binary segmentation tasks: shadow detection and portrait segmentation. Shadow Detection. We re-train our model on the training set of SBU <ref type="bibr" target="#b29">[30]</ref> dataset and test the model on three public    <ref type="table">Table 4</ref>: Comparison of initial attention (ia) and holistic attention (ha) in four models (the proposed model and three improved models).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>GT CPD-A CPD <ref type="figure">Figure 7</ref>: Some Failure examples of the proposed model. When the attention branch only localizes a small part of target regions, our model performs poorly.</p><p>shadow detection datasets: test set of SBU, ISTD <ref type="bibr" target="#b30">[31]</ref> and UCF <ref type="bibr" target="#b44">[45]</ref>. Moreover, we apply the widely-used metric BER (balanced error rate) for quantitative comparison. We compare our method with five deep shadow detection methods: JDR <ref type="bibr" target="#b30">[31]</ref>, DSC <ref type="bibr" target="#b9">[10]</ref>, DC-DSPF <ref type="bibr" target="#b36">[37]</ref>, scGAN <ref type="bibr" target="#b25">[26]</ref>, Stacked-CNN <ref type="bibr" target="#b29">[30]</ref>. In addition, we re-train three salient object detection models for shadow detection: NLDF <ref type="bibr" target="#b24">[25]</ref>, DSS <ref type="bibr" target="#b8">[9]</ref>, BMPM <ref type="bibr" target="#b40">[41]</ref>. The results are shown in <ref type="table">Table.</ref> 5, and the proposed model outperforms the other models in all cases.</p><p>Portrait Segmentation. We use the data from <ref type="bibr" target="#b27">[28]</ref>. And we re-train NLDF, DSS, BMPM on this dataset. The results are shown in <ref type="table" target="#tab_7">Table 6</ref>. It can be seen that the proposed model outperforms existing algorithms.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel cascaded partial decoder framework for fast and accurate salient object detection. When constructing decoders, the proposed framework discards features of shallower layers to improve the computational efficiency, and utilizes generated saliency map to refine features to improve the accuracy. We also propose a holistic attention module to further segment the whole salient objects and an effective decoder to abstract discriminative features and quickly integrate multi-level features. The experiments show that our model achieves state-of-theart performance on five benchmark datasets and runs much faster than existing deep models. To prove the generalization of the proposed framework, we apply it to improve existing deep aggregation models and significantly improve their accuracy and efficiency. Besides, we validate the effectiveness of the proposed model in two tasks of shadow detection and portrait segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Maximum F-measure of six side outputs of the original DSS [9] model in PASCAL-S [19] dataset. (b) We set inference time of backbone network as 1, and show inference time of each side output here. The performance growth is getting slower and the inference time rapidly increases when gradually integrating features from high-level 6 to low-level 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The original image and five-level feature maps from VGG16<ref type="bibr" target="#b28">[29]</ref>. The Conv3 3 feature still retains edge information. Hence the Conv1 2 and Conv2 2 features with large resolutions are not under consideration in the proposed framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>0.885 0.046 0.921 0.870 0.042 0.794 0.710 0.068 0.851 0.749 0.054 0.862 0.796 0.076 CPD-A (ours) VGG16 105 0.928 0.906 0.045 0.918 0.884 0.037 0.781 0.721 0.061 0.854 0.787 0.047 0.859 0.814 0.077 CPD (ours) VGG16 66 0.936 0.915 0.040 0.924 0.896 0.033 0.794 0.745 0.057 0.864 0.813 0.043 0.866 0.825 0.074 SRM [0.903 0.043 0.914 0.882 0.037 0.779 0.709 0.063 0.834 0.764 0.051 0.853 0.807 0.074 PiCANet-R [21] ResNet50 5 0.935 0.886 0.046 0.919 0.870 0.043 0.803 0.717 0.065 0.860 0.759 0.051 0.863 0.798 0.075 CPD-RA (ours) ResNet50 104 0.934 0.907 0.043 0.918 0.882 0.038 0.783 0.725 0.059 0.852 0.776 0.048 0.855 0.807 0.077 CPD-R (ours) ResNet50 62 0.939 0.917 0.037 0.925 0.891 0.034 0.797 0.747 0.056 0.865 0.805 0.043 0.864 0.824 0.072</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visual comparisons of the proposed model and existing state-of-the-art algorithms in some challenging cases: small object, complex scene, multiple objects and large object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>21 0.915 0.886 0.051 0.908 0.871 0.041 0.759 0.694 0.071 0.830 0.759 0.055 0.840 0.792 0.083 NLDF-CPD-A 75 0.918 0.889 0.049 0.914 0.873 0.039 0.775 0.710 0.061 0.837 0.773 0.050 0.841 0.793 0.083 NLDF-CPD 48 0.922 0.896 0.044 0.916 0.880 0.036 0.781 0.721 0.060 0.842 0.786 0.048 0.843 0.800 0.080 Amulet [42] 21 0.922 0.881 0.057 0.909 0.863 0.047 0.791 0.699 0.072 0.832 0.738 0.062 0.839 0.780 0.095 Amulet-CPD-A 61 0.925 0.889 0.053 0.910 0.864 0.045 0.790 0.708 0.070 0.832 0.747 0.060 0.842 0.784 0.091 Amulet-CPD 45 0.934 0.901 0.047 0.920 0.878 0.040 0.805 0.735 0.063 0.845 0.771 0.055 0.851 0.801 0.085</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Visual comparisons of original models (BMPM, Amulet, NLDF) with improved models (-CPD-A, -CPD).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>074 Amulet-CPD (with ia) 0.842 0.763 0.056 0.849 0.794 0.087 Amulet-CPD (with ha) 0.845 0.771 0.055 0.851 0.801 0.085 BMPM-CPD (with ia) 0.865 0.791 0.045 0.867 0.818 0.072 BMPM-CPD (with ha) 0.870 0.808 0.044 0.868 0.822 0.072 NLDF-CPD (with ia) 0.838 0.777 0.051 0.840 0.793 0.084 NLDF-CPD (with ha) 0.842 0.786 0.048 0.843 0.800 0.080</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>H 4 , W 4 ] size and 96 channel number. With 3 × 3 layer and 1 × 1 convolutional layers, we obtain the final feature map and resize it to [H, W ].</figDesc><table><row><cell>4. Experiment</cell></row><row><cell>4.1. Salient Object Detection</cell></row><row><cell>4.1.1 Experimental Setup</cell></row><row><cell>Evaluation Datasets. We evaluate the proposed model</cell></row><row><cell>on five benchmark datasets: ECSSD [39], HKU-IS [16],</cell></row><row><cell>PASCAL-S [19], DUTS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>MAE maxF avgF MAE maxF avgF MAE maxF avgF MAE maxF avgF MAE BMPM [41] 28 0.928 0.894 0.044 0.920 0.875 0.039 0.775 0.693 0.063 0.850 0.768 0.049 0.862 0.803 0.074 BMPM-CPD-A 82 0.932 0.901 0.046 0.920 0.882 0.037 0.796 0.731 0.057 0.864 0.799 0.046 0.861 0.817 0.074 BMPM-CPD 47 0.935 0.907 0.043 0.925 0.888 0.035 0.804 0.740 0.056 0.870 0.808 0.044 0.868 0.822 0.072 NLDF<ref type="bibr" target="#b24">[25]</ref> </figDesc><table><row><cell>Method</cell><cell>FPS</cell><cell>ECSSD [39] maxF avgF</cell><cell>HKU-IS [16]</cell><cell>DUT-OMRON [40]</cell><cell>DUTS [33]</cell><cell>PASCAL-S [19]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>MAE maxF avgF MAE maxF avgF MAE maxF avgF MAE maxF avgF MAE Conv2 2 38 0.936 0.903 0.042 0.925 0.884 0.036 0.792 0.720 0.063 0.861 0.778 0.048 0.865 0.810 0.076 Conv3 3 66 0.936 0.915 0.040 0.924 0.896 0.033 0.794 0.745 0.057 0.864 0.813 0.043 0.866 0.825 0.074 Conv4 3 90 0.931 0.910 0.041 0.920 0.890 0.034 0.787 0.737 0.059 0.855 0.801 0.045 0.863 0.824 0.072 Full Decoder 30 0.922 0.891 0.051 0.911 0.873 0.042 0.758 0.692 0.070 0.843 0.766 0.050 0.853 0.807 0.077</figDesc><table><row><cell>Settings</cell><cell>FPS</cell><cell>ECSSD [39] maxF avgF</cell><cell>HKU-IS [16]</cell><cell>DUT-OMRON [40]</cell><cell>DUTS [33]</cell><cell>PASCAL-S [19]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of the proposed model with different optimization layers and no optimization layer (full decoder).</figDesc><table><row><cell>Settings</cell><cell>DUTS [33] maxF avgF MAE maxF avgF MAE PASCAL-S [19]</cell></row><row><cell>CPD (with ia)</cell><cell>0.862 0.803 0.045 0.862 0.821 0.075</cell></row><row><cell>CPD (with ha)</cell><cell>0.864 0.813 0.043 0.866 0.825 0.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparing the proposed method with state-of-thearts for shadow detection (DSC, DC-DSPF, JDR, Stacked-CNN, scGAN), and for salient object detection (Amulet, NLDF, BMPM, DSS).</figDesc><table><row><cell cols="6">Methods PFCN+ [28] NLDF [25] DSS [9] BMPM [42] CPD (Ours)</cell></row><row><cell>Mean IoU</cell><cell>95.90%</cell><cell>95.60%</cell><cell>96.20%</cell><cell>96.20%</cell><cell>96.60%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Quantitative Comparison on Portrait Segmentation.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.5878</idno>
		<title level="m">Salient object detection: A survey</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="815" to="828" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Directionaware spatial context features for shadow detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7454" to="7462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep saliency with encoded low level distance map and high level features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="660" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep contrast learning for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepsaliency: Multi-task deep neural network model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3919" to="3930" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dhsnet: Deep hierarchical saliency network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="678" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Picanet: Learning pixelwise contextual attention for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3089" to="3098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Non-local deep features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Achkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Eichel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6593" to="6601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Shadow detection with conditional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F Y</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4520" to="4528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic portrait segmentation for image stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sachs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="93" to="102" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Large-scale training of shadow detectors with noisilyannotated shadow examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F Y</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1788" to="1797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep networks for saliency detection via local estimation and global search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3183" to="3192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Saliency detection with recurrent fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="825" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A stagewise refinement model for detecting salient objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4019" to="4028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Detect globally, refine locally: A novel approach to saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3127" to="3135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Densely cascaded shadow detection network via deeply supervised parallel fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cripac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1007" to="1013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A bidirectional message passing model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1741" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Progressive attention guided recurrent network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="714" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to recognize shadows in monochromatic natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Masood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="223" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Saliency optimization from robust background detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2814" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
