<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">All SMILES Variational Autoencoder</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaccary</forename><forename type="middle">Alperstein</forename><surname>Quadrant</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Vancouver Prostate Centre</settlement>
									<country>UBC</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Cherkasov</surname></persName>
							<email>acherkasov@prostatecentre.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Vancouver Prostate Centre</settlement>
									<country>UBC</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Tyler</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Vancouver Prostate Centre</settlement>
									<country>UBC</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolfe</forename><surname>Quadrant</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Vancouver Prostate Centre</settlement>
									<country>UBC</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">All SMILES Variational Autoencoder</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Variational autoencoders (VAEs) defined over SMILES string and graph-based representations of molecules promise to improve the optimization of molecular properties, thereby revolutionizing the pharmaceuticals and materials industries. However, these VAEs are hindered by the non-unique nature of SMILES strings and the computational cost of graph convolutions. To efficiently pass messages along all paths through the molecular graph, we encode multiple SMILES strings of a single molecule using a set of stacked recurrent neural networks, pooling hidden representations of each atom between SMILES representations, and use attentional pooling to build a final fixed-length latent representation. By then decoding to a disjoint set of SMILES strings of the molecule, our All SMILES VAE learns an almost bijective mapping between molecules and latent representations near the high-probability-mass subspace of the prior. Our SMILES-derived but moleculebased latent representations significantly surpass the state-of-the-art in a variety of fully-and semi-supervised property regression and molecular property optimization tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The design of new pharmaceuticals, OLED materials, and photovoltaics all require optimization within the space of molecules <ref type="bibr" target="#b49">[50]</ref>. While well-known algorithms ranging from gradient descent to the simplex method facilitate efficient optimization, they generally assume a continuous search space and a smooth objective function. In contrast, the space of molecules is discrete and sparse. Molecules correspond to graphs, with each node labeled by one of ninety-eight naturally occurring atoms, and each edge labeled as a single, double, or triple bond. Even within this discrete space, almost all possible combinations of atoms and bonds do not form chemically stable molecules, and so must be excluded from the optimization domain, yet there remain as many as 10 60 small molecules to consider <ref type="bibr" target="#b50">[51]</ref>. Moreover, properties of interest are often sensitive to even small changes to the molecule [62], so their optimization is intrinsically difficult.</p><p>Efficient, gradient-based optimization can be performed over the space of molecules given a map between a continuous space, such as R n or the n-sphere, and the space of molecules and their properties <ref type="bibr" target="#b56">[57]</ref>. Initial approaches of this form trained a variational autoencoder (VAE) [31, 52] on SMILES string representations of molecules [66] to learn a decoder mapping from a Gaussian prior to the space of SMILES strings <ref type="bibr" target="#b15">[16]</ref>. A sparse Gaussian process on molecular properties then facilitates Bayesian optimization of molecular properties within the latent space <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b55">56]</ref>, or a neural network regressor from the latent space to molecular properties can be used to perform gradient descent on molecular properties with respect to the latent space <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b45">46]</ref>. Alternatively, semisupervised VAEs condition the decoder on the molecular properties <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b40">41]</ref>, so the desired properties can be specified directly. Recurrent neural networks have also been trained to model SMILES strings directly, and tuned with transfer learning, without an explicit latent space or encoder <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b57">58]</ref>. SMILES, the simplified molecular-input line-entry system, defines a character string representation of a molecule by performing a depth-first pre-order traversal of a spanning tree of the molecular graph, Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>emitting characters for each atom, bond, tree-traversal decision, and broken cycle <ref type="bibr" target="#b65">[66]</ref>. The resulting character string corresponds to a flattening of a spanning tree of the molecular graph, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The SMILES grammar is restrictive, and most strings over the appropriate character set do not correspond to well-defined molecules. Rather than require the VAE decoder to explicitly learn this grammar, context-free grammars <ref type="bibr" target="#b34">[35]</ref>, and attribute grammars <ref type="bibr" target="#b8">[9]</ref> have been used to constrain the decoder, increasing the percentage of valid SMILES strings produced by the generative model. Invalid SMILES strings and violations of simple chemical rules can be avoided entirely by operating on the space of molecular graphs, either directly <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b58">59]</ref> or via junction trees <ref type="bibr" target="#b26">[27]</ref>.</p><p>Every molecule is represented by many well-formed SMILES strings, corresponding to all depth-first traversals of every spanning tree of the molecular graph. The distance between different SMILES strings of the same molecule can be much greater than that between SMILES strings from radically dissimilar molecules <ref type="bibr" target="#b26">[27]</ref>, as shown in <ref type="figure">Figure 8</ref> of Appendix A. A generative model of individual SMILES strings will tend to reflect this geometry, complicating the mapping from latent space to molecular properties and creating unnecessary local optima for property optimization <ref type="bibr" target="#b64">[65]</ref>. To address this difficulty, sequence-to-sequence transcoders <ref type="bibr" target="#b62">[63]</ref> have been trained to map between different SMILES strings of a single molecule <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b67">68]</ref>.</p><p>Reinforcement learning, often combined with adversarial methods, has been used to train progressive molecule growth strategies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b71">72]</ref>. While these approaches have achieved state-ofthe-art optimization of simple molecular properties that can be evaluated quickly in silico, critic-free techniques generally depend upon property values of algorithm-generated molecules (but see <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b47">48]</ref>), and so scale poorly to real-world properties requiring time-consuming wet-lab experiments. Molecular property optimization would benefit from a generative model that directly captures the geometry of the space of molecular graphs, rather than SMILES strings, but efficiently infers a latent representation sensitive to spatially distributed molecular features. To this end, we introduce the All SMILES VAE, which uses recurrent neural networks (RNNs) on multiple SMILES strings to implicitly perform efficient message passing along and amongst many flattened spanning trees of the molecular graph in parallel. A fixed-length latent representation is distilled from the variablelength RNN output using attentional mechanisms. From this latent representation, the decoder RNN reconstructs a set of SMILES strings disjoint from those input to the encoder, ensuring that the latent representation only captures features of the molecule, rather than its SMILES realization. Simple property regressors jointly trained on this latent representation surpass the state-of-the-art for molecular property prediction, and facilitate exceptional gradient-based molecular property optimization when constrained to the region of prior containing almost all probability. We further demonstrate that the latent representation forms a near-bijection with the space of molecules, and is smooth with respect to molecular properties, facilitating effective optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Efficient molecular encoding with multiple SMILES strings</head><p>A variational autoencoder (VAE) defines a generative model over an observed space x in terms of a prior distribution over a latent space p(z) and a conditional likelihood of observed states given the latent configuration p(x|z) <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b51">52]</ref>. The true log-likelihood p(x) = log z p(z)p(x|z) is intractable, so the evidence lower bound (ELBO), based upon a variational approximation q(z|x) to the posterior distribution, is maximized instead: L = E q(z|x) [log p(x|z)] − KL [q(z|x)||p(z)] . The ELBO implicitly defines a stochastic autoencoder, with encoder q(z|x) and decoder p(x|z).</p><p>Many effective molecule encoders rely upon graph convolutions: local message passing in the molecular graph, between either adjacent nodes or adjacent edges <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43]</ref>. To maintain permutation symmetry, the signal into each node is a sum of messages from the adjacent nodes, but may be a function of edge type, or attentional mechanisms dependent upon the source and destination nodes <ref type="bibr" target="#b54">[55]</ref>. This sum of messages is then subject to a linear transformation and a pointwise nonlinearity. Messages are sometimes subject to gating <ref type="bibr" target="#b38">[39]</ref>, like in long short-term memories (LSTM) <ref type="bibr" target="#b20">[21]</ref> and gated recurrent units (GRU) <ref type="bibr" target="#b6">[7]</ref>, as detailed in Appendix B.1.</p><p>Message passing on molecular graphs is analogous to a traditional convolutional neural network applied to images <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37]</ref>, with constant-resolution hidden layers <ref type="bibr" target="#b19">[20]</ref> and two kernels: a 3 × 3 average-pooling kernel that sums messages from adjacent pixels (corresponding to adjacent nodes in a molecular graph), and a trainable 1 × 1 kernel that transforms the message from each pixel (node) independently, before a pointwise nonlinearity. While convolutional networks with such small kernels are now standard in the visual domain, they use hundreds of layers to pass information throughout the image and achieve effective receptive fields that span the entire input <ref type="bibr" target="#b63">[64]</ref>. In contrast, molecule encoders generally use between three and seven rounds of message passing <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b69">70]</ref>. This limits the computational cost, since molecule encoders cannot use highly-optimized implementations of spatial 2D convolutions, but each iteration of message passing only propagates information a geodesic distance of one within the molecular graph. <ref type="bibr" target="#b0">1</ref> In the case of the commonly used dataset of 250,000 drug-like molecules <ref type="bibr" target="#b15">[16]</ref>, information cannot traverse these graphs effectively, as their average diameter is 11.1, and their maximum diameter is 24, as shown in Appendix A.</p><p>Non-local molecular properties, requiring long-range information propagation along the molecular graph, are of practical interest in domains including pharmaceuticals, photovoltaics, and OLEDs. The pharmacological efficacy of a molecule generally depends upon high binding affinity for a particular receptor or other target, and low binding affinity for other possible targets. These binding affinities are determined by the maximum achievable alignment between the molecule's electromagnetic fields and those of the receptor. Changes to the shape or charge distribution in one part of the molecule affect the position and orientation at which it fits best with the receptor, inducing shifts and rotations that alter the binding of other parts of the molecule, and changing the binding affinity <ref type="bibr" target="#b7">[8]</ref>. Similarly, efficient next-generation OLEDs depend on properties, such as the singlet-triple energy gap, that are directly proportional to the strength of long-range electronic interactions across the molecule <ref type="bibr" target="#b22">[23]</ref>. The latent representation of a VAE can directly capture these non-local, nonlinear properties only if the encoder passes information efficiently across the entire molecular graph.</p><p>Analogous to graph convolutions, gated RNNs defined directly on SMILES strings effectively pass messages, via the hidden state, through a flattened spanning tree of the molecular graph (see <ref type="figure" target="#fig_0">Figure 1</ref>). The message at each symbol in the string is a weighted sum of the previous message and the current input, followed by a pointwise nonlinearity and subject to gating, as reviewed in Appendix B.1. This differs from explicit graph-based message passing in that the molecular graph is flattened into a chain corresponding to a depth-first pre-order traversal of a spanning tree, and the set of adjacent nodes that affect a message only includes the preceding node in this chain. Rather than updating all messages in parallel, RNNs on SMILES strings move sequentially down the chain, so earlier messages influence all later messages, and information can propagate through all branches of a flattening of a spanning tree in a single pass. With a well-chosen spanning tree, information can pass the entire width of the molecular graph in a single RNN update. The relationship between RNNs on SMILES strings and graph-based architectures is further explored in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model architecture</head><p>To marry the latent space geometry induced by graph convolutions to the information propagation efficiency of RNNs on SMILES strings, the All SMILES encoder combines these architectures. It takes multiple distinct SMILES strings of the same molecule as input, and applies RNNs to them in parallel. This implicitly realizes a representative set of message passing pathways through the molecular graph, corresponding to the depth-first pre-order traversals of the spanning trees underlying the SMILES strings. Between each layer of RNNs, the encoder pools homologous messages between parallel representations, so that information flows along the union of the implicit SMILES pathways.</p><p>The characters of the multiple SMILES strings are linearly embedded, and each string is preprocessed by a BiGRU <ref type="bibr" target="#b6">[7]</ref>, followed by a linear transformation, to produce the initial hidden representation h 0 i for each SMILES string i. The encoder then applies a stack of modules, each of which pools between homologous atoms in the parallel representations, followed by layer norm, concatenation with the linearly embedded SMILES input, and a GRU applied to the parallel representations independently, as shown in <ref type="figure" target="#fig_2">Figures 2 and 3</ref>. Each such parallel representation comprises a sequence of vectors, one for each character in the original SMILES string.  Multiple SMILES strings representing a single molecule need not have the same length, and syntactic characters indicating branching and ring closures rather than atoms and bonds do not generally match. However, the set of atoms is always consistent, and a bijection can be defined between homologous atom characters. At the beginning of each encoder module <ref type="figure">(Figure 2</ref>), the parallel inputs corresponding to a single, common atom of the original molecule are pooled, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. This harmonized representation of the atom replaces the original in each of the input streams for the subsequent layer normalizations and GRUs, reversing the information flow of <ref type="figure" target="#fig_2">Figure 3</ref>. While we experimented with average and max pooling, we found element-wise sigmoid gating to be most effective <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b54">55]</ref></p><formula xml:id="formula_0">: a = 1 k k a k σ W a k , 1 k k a k + b , where [x, y]</formula><p>is the concatenation of vectors x and y, and the logistic function σ(x) = (1 + e −x ) −1 is applied elementwise. The pooling effectively sums messages propagated from many adjacent nodes in the molecular graph, analogous to a graph convolution, but the GRUs efficiently transfer information through many edges in each layer, rather than just one. The hidden representations associated with non-atom, syntactic input characters, such as parentheses and digits, are left unchanged by the pooling operation.</p><p>The approximating posterior distills the resulting variable-length encodings into a fixed-length hierarchy of autoregressive Gaussian distributions <ref type="bibr" target="#b53">[54]</ref>. The mean and log-variance of the first layer of the approximating posterior, z 1 , is parametrized by max-pooling the terminal hidden states of the final encoder GRUs, followed by batch renormalization <ref type="bibr" target="#b23">[24]</ref> and a linear transformation, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Succeeding hierarchical layers use Bahdanau-style attention <ref type="bibr" target="#b2">[3]</ref> (reviewed in Appendix B.2) over the pooled final atom vectors, with the query vector defined by a one-hidden-layer network of rectified linear units (ReLUs) given the concatenation of the previous latent layers as input. This is analogous to the order-invariant encoding of set2set, but an output is produced at each step, and processing is not gated <ref type="bibr" target="#b64">[65]</ref>. The attentional mechanism is also effectively available to property regressors that take the fixed-length latent representation as input, allowing them to aggregate contributions from across the molecule. The output of the attentional mechanism is subject to batch renormalization and a linear transformation to compute the conditional mean and log-variance of the layer. The prior has a similar autoregressive structure, but uses neural networks of ReLUs in place of Bahdanau-style attention, since it does not have access to the atom vectors. For molecular optimization tasks, we usually scale up the term KL [q(z|x)||p(z)] in the ELBO by the number of SMILES strings in the decoder, analogous to multiple single-SMILES VAEs in parallel; we leave this KL term unscaled for property prediction.</p><formula xml:id="formula_1">GRU 1 GRU 2 h T 1 h T 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pool</head><p>Lin z 1</p><p>Pool atoms The decoder is a single-layer LSTM, for which the initial cell state is computed from the latent representation by a neural network, and a linear transformation of the latent representation is concatenated onto each input. It is trained with teacher forcing to reconstruct a set of SMILES strings disjoint from those provided to the encoder, but representing the same molecule. Grammatical constraints <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b34">35]</ref> can naturally be enforced within this LSTM by parsing the unfolding character sequence with a pushdown automaton, and constraining the final softmax of the LSTM output at each time step to grammatically valid symbols. This is detailed in Appendix D, although we leave the exploration of this technique to future work.</p><formula xml:id="formula_2">NN Att z 2 NN Att z 3 k k µ, σ q µ, σ q µ, σ</formula><p>Since the SMILES inputs to the encoder are different from the targets of the decoder, the decoder is effectively trained to assign equal probability to all SMILES strings of the encoded molecule. The latent representation must capture the molecule as a whole, rather than any particular SMILES input to the encoder. To accommodate this intentionally difficult reconstruction task, facilitate the construction of a bijection between latent space and molecules, and following prior work <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b66">67]</ref>, we use a width-5 beam search decoder to map from the latent representation to the space of molecules at test-time. Further architectural details are presented in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Latent space optimization</head><p>Unlike many models that apply a sparse Gaussian process to fixed latent representations to predict molecular properties <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b55">56]</ref>, the All SMILES VAE jointly trains property regressors with the generative model <ref type="bibr" target="#b41">[42]</ref>. <ref type="bibr" target="#b1">2</ref> We use linear regressors for the log octanol-water partition coefficient (logP) and molecular weight (MW), which have unbounded values; and logistic regressors for the quantitative estimate of drug-likeness (QED) <ref type="bibr" target="#b3">[4]</ref> and twelve binary measures of toxicity <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b44">45]</ref>, which take values in [0, 1]. We then perform gradient-based optimization of the property of interest with respect to the latent space, and decode the result to produce an optimized molecule.</p><p>Naively, we might either optimize the predicted property without constraints on the latent space, or find the maximum a posteriori (MAP) latent point for a conditional likelihood over the property that assigns greater probability to more desirable values. However, the property regressors and decoder are only accurate within the domain in which they have been trained: the region assigned high probability mass by the prior. For a n-dimensional standard Gaussian prior, almost all probability mass lies in a practical support comprising a thin spherical shell of radius √ n − 1 [6, Gaussian Annulus Theorem]. With linear or logistic regressors, predicted property values increase monotonically in the direction of the weight vector, so unconstrained property maximization diverges from the origin of the latent space. Conversely, MAP optimization with a Gaussian prior is pulled towards the origin, where the density of the prior is greatest. Both unconstrained and MAP optimization thus deviate from the practical support in each layer of the hierarchical prior, resulting in large prediction errors and poor optimization.</p><p>We can use the reparametrization trick <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b51">52]</ref> to map our autoregressive prior back to a standard Gaussian. The image of the thin spherical shell through this reparametrization still contains almost all of the probability mass. We therefore constrain optimization to the reparametrized n − 1 dimensional sphere of radius √ n − 1 for each n-dimensional layer of the hierarchical prior by optimizing the angle directly. <ref type="bibr" target="#b2">3</ref> Although the reparametrization from the standard Gaussian prior to our autoregressive prior is not volume preserving, this hierarchical radius constraint holds us to the center of the image of the thin spherical shell. The distance to which the image of the thin spherical shell extends away from the n − 1 dimensional sphere at its center is a highly nonlinear function of the previous layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We evaluate the All SMILES VAE on standard 250,000 and 310,000 element subsets <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref> of the ZINC database of small organic molecules <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b60">61]</ref>. We also evaluate on the Tox21 dataset <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b44">45]</ref> in the DeepChem package <ref type="bibr" target="#b68">[69]</ref>, comprising binarized binding affinities of 7831 compounds against 12 proteins. For further details, see Appendix A. Additional experiments, including ablations of novel model components, are described in Appendix C.</p><p>The full power of continuous, gradient-based optimization can be brought bear on molecular properties given a bijection between molecules and contractible regions of a latent space, along with a regressor from the latent space to the property of interest that is differentiable almost everywhere. Such a bijection is challenging to confirm, since it is difficult to find the full latent space preimage of a molecule implicitly defined by a mapping from latent space to SMILES strings, such as our beam search decoder. As a necessary condition, we confirm that it is possible to map from the space of molecules to latent space and back again, and that random samples from the prior distribution in the latent space map to valid molecules. The former is required for injectivity, and the latter for surjectivity, of the mapping from molecules to contractible regions of the latent space.</p><p>Using the approximating posterior as the encoder, but always selecting the mean of each conditional Gaussian distribution (the maximum conditional a posteriori point), and a using beam search over the conditional likelihood as the decoder, 87.4% ± 1% of a held-out test set of ZINC250k (80/10/10 train/val/test split) is reconstructed accurately. With the same beam search decoder, 98.5% ± 0.1% of samples from the prior decode to valid SMILES strings. We expect that enforcing grammatical constraints in the decoder LSTM, as described in Appendix D, would further increase these rates. All molecules decoded from a set of 50,000 independent samples from the prior were unique, 99.958% were novel relative to the training dataset, and their average synthetic accessibility score <ref type="bibr" target="#b12">[13]</ref> was 2.97 ± 0.01, compared to 3.05 in the ZINC250k dataset used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Property prediction</head><p>Ultimately, we would like to optimize molecules for complicated physical properties, such as fluorescence quantum yield, solar cell efficiency, binding affinity to selected receptors, and low toxicity. Networks can only be trained to predict such physical properties if their true values are known on an appropriate training dataset. While proxy properties can be accurately computed from first principles, properties like drug efficacy arise from highly nonlinear, poorly characterized processes, and can only be accurately determined through time-consuming and expensive experimental measurements.</p><p>Since such experiments can only be performed on a tiny fraction of the 10 60 drug-like molecules, we evaluate the ability of the All SMILES VAE to perform semi-supervised property prediction.</p><p>As <ref type="figure">Figure 5</ref> and <ref type="table" target="#tab_2">Table 4</ref> in Appendix C demonstrate, we significantly improve the state-of-the-art in the semi-supervised prediction of simple molecular properties, including the log octanol-water partition coefficient (logP), molecular weight (MW), and quantitative estimate of drug-likeness (QED) <ref type="bibr" target="#b3">[4]</ref>, against which many algorithms have been benchmarked. We achieve a similar improvement in fully supervised property prediction, as shown in <ref type="table" target="#tab_0">Table 1</ref>, where we compare to extended connectivity fingerprints (ECFP) <ref type="bibr" target="#b52">[53]</ref>, the character VAE (CVAE) <ref type="bibr" target="#b15">[16]</ref>, and graph convolutions <ref type="bibr" target="#b11">[12]</ref>. <ref type="table">Table 3</ref> in Appendix C documents an even larger improvement compared to models that use a sparse Gaussian process for property prediction. We also surpass the state-of-the-art in toxicity prediction on the Tox21 dataset <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b44">45]</ref>, as shown in <ref type="table" target="#tab_0">Table 1</ref>, despite refraining from ensembling our model, or engineering features using expert chemistry knowledge, as in previous state-of-the-art methods <ref type="bibr" target="#b70">[71]</ref>.</p><p>Accurate property prediction only facilitates effective optimization if the true property value is smooth with respect to the latent space. In <ref type="figure" target="#fig_5">Figure 6a</ref>, we plot the true (not predicted) logP over a densely sampled 2D slice of the latent space, where the y axis is aligned with the logP linear regressor. Fraction labeled (c) QED <ref type="figure">Figure 5</ref>: Semi-supervised mean absolute error (MAE) ± the standard deviation across ten replicates for the log octanol-water partition coefficient (a), molecular weight (b), and the quantitative estimate drug-likeness <ref type="bibr" target="#b3">[4]</ref> (c) on the ZINC310k dataset. Plots are log-log; the All SMILES MAE is a fraction of that of the SSVAE <ref type="bibr" target="#b27">[28]</ref> and graph convolutions <ref type="bibr" target="#b28">[29]</ref>. Semi-supervised VAE (SSVAE) and graph convolution results are those reported by Kang &amp; Cho <ref type="bibr" target="#b27">[28]</ref>.  <ref type="bibr" target="#b37">[38]</ref> 0.854 POTENTIALNET <ref type="bibr" target="#b13">[14]</ref> 0.857 ± 0.006 TOXICBLEND <ref type="bibr" target="#b70">[71]</ref> 0.862 All SMILES 0.871</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Molecular optimization</head><p>We maximize the output of our linear and logistic property regressors, plus a log-prior regularizer, with respect to the latent space, subject to a hierarchical radius constraint. After optimizing in the latent space with ADAM, we project back to a SMILES representation of a molecule with the decoder. Following prior work, we optimize QED and logP penalized by the synthetic accessibility score and the number of large rings <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b71">72]</ref>. <ref type="figure" target="#fig_5">Figure 6b</ref> depicts the predicted and true logP over an optimization trajectory, while <ref type="table" target="#tab_1">Table 2</ref> compares the top three values found amongst 100 such trajectories to the previous state-of-the-art. <ref type="bibr" target="#b3">4</ref> The molecules realizing these property values are shown in <ref type="figure" target="#fig_6">Figure 7</ref>. Leaving the KL term in the ELBO unscaled by the number of SMILES strings in the decoder reduces the regularization of the latent space embeddings, allowing latent space optimization to search a wider space of molecules that are less similar to the training set, as shown in <ref type="figure" target="#fig_0">Figure 14</ref> of Appendix C.3. Unlike reinforcement learning methods that progressively evaluate the properties of novel molecules generated during training, including Graph Convolutional Policy Networks (GCPN) and Molecule Deep Q-Networks (MolDQN) <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b71">72]</ref>, the All SMILES VAE only requires a fixed training dataset. This is critical when optimizing properties, including pharmacological efficacy, toxicity, and OLED efficiency, for which accurate in silico approximations do not exist.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>For each molecule, the All SMILES encoder uses stacked, pooled RNNs on multiple SMILES strings to efficiently pass information throughout the molecular graph. The decoder targets a disjoint set of SMILES strings of the same molecule, forcing the latent space to develop a consistent representation for each molecule. Attentional mechanisms in the approximating posterior summarize spatially diffuse features into a fixed-length, non-factorial approximating posterior, and construct a latent representation on which linear regressors achieve state-of-the-art semi-and fully-supervised property prediction. Gradient-based optimization of these regressor outputs with respect to the latent representation, constrained to a subspace near almost all probability in the prior, produces state-of-the-art optimized molecules when coupled with a simple RNN decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets</head><p>SMILES strings, as well as the true values of the log octanol-water partition coefficient (logP), molecular weight (MW), and the quantitative estimate of drug-likeness (QED), are computed using RDKit <ref type="bibr" target="#b35">[36]</ref>.</p><p>(a) COCOC1CNCC(C)N1 (b) CCCCC(CCCC)CCCC CC1NC(CNC1)OCOC <ref type="figure">Figure 8</ref>: Multiple SMILES strings of a single molecule may be more dissimilar than SMILES strings of radically dissimilar molecules. The top SMILES string for molecule (a) is 30% similar to the bottom SMILES string by string edit distance, but 60% similar to the SMILES string for molecule (b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 ZINC</head><p>For molecular property optimization and fully supervised property prediction, we train the All SMILES VAE on the ZINC250k dataset of 250,000 organic molecules with between 6 and 38 heavy atoms, and penalized logPs 5 from -13 to 5 <ref type="bibr" target="#b15">[16]</ref>. This dataset is curated from the full ZINC12 dataset <ref type="bibr" target="#b24">[25]</ref>, and available from https://github.com/aspuru-guzik-group/chemical_vae. The distribution of molecular diameters in ZINC250k is shown in <ref type="figure">Figure 9</ref>.</p><p>For semi-supervised property prediction on logP, MW, and QED, we train on the ZINC310k dataset of 310,000 organic molecules with between 6 and 38 heavy atoms <ref type="bibr" target="#b27">[28]</ref>. This dataset is curated from the full ZINC15 dataset <ref type="bibr" target="#b60">[61]</ref>, and available from https://github.com/nyu-dl/conditional-molecular-design-ssvae. <ref type="figure">Figure 9</ref>: Histogram of molecular diameters in the ZINC250k dataset. The diameter is defined as the maximum eccentricity over all atoms in the molecular graph. The mean is 11.1; the maximum is 24. Typical implementations of graph convolution use only three to seven rounds of message passing <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b69">70]</ref>, and so cannot propagate information across most molecules in this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Tox21</head><p>For the semi-supervised prediction of twelve forms of toxicity, we train on the Tox21 dataset <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b44">45]</ref>, accessed through the DeepChem package <ref type="bibr" target="#b68">[69]</ref>, with the provided random train/validation/test set split. This dataset contains binarized binding affinities against up to 12 proteins for 6264 training, 783 validation, and 784 test molecules. Tox21 contains molecules with up to 140 atoms, ranging from large peptides to lanthanide, actinide and other metals. Many of these metal atoms are not present in any of the standard molecular generative modeling datasets, and there are metal atoms in the validation and test set that never appear in the training set. To address these difficulties, we curated an unsupervised dataset of 1.5 million molecules from the PubChem database <ref type="bibr" target="#b29">[30]</ref>, which we will make available upon publication. To maintain commensurability with prior work, this additional unsupervised dataset is only used on the Tox21 prediction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Extended model architecture</head><p>The full All SMILES VAE architecture is summarized in <ref type="figure" target="#fig_0">Figure 10</ref>. The evidence lower bound (ELBO) of the log-likelihood,</p><formula xml:id="formula_3">L = E q(z|x) [log p(x|z)] − KL [q(z|x)||p(z)] ,<label>(1)</label></formula><p>is the sum of the conditional log-likelihoods of x i in <ref type="figure" target="#fig_0">Figure 10</ref>, minus the Kullback-Leibler divergence between the approximating posterior, q(z|x), computed by node AP in <ref type="figure" target="#fig_0">Figure 10</ref>, and the prior depicted in <ref type="figure" target="#fig_0">Figure 11</ref>.  <ref type="figure" target="#fig_3">Figure 4</ref> then produces a sample from the latent state z, which is decoded into SMILES strings by LSTMs. Note that all SMILES strings, in both the input and the output, are distinct. The encoder blocks also receive a linear embedding of the original SMILES strings as input.</p><formula xml:id="formula_4">z 1 NN z 2 NN z 3 NN z 4</formula><p>µ, σ µ, σ µ, σ <ref type="figure" target="#fig_0">Figure 11</ref>: The prior distribution over z = [z 1 , z 2 , · · · ] is a hierarchy of autoregressive Gaussians. The conditional prior distribution of hierarchical layer i given layers 1 through i−1, p(z i |z 1 , z 2 , · · · z i−1 ), is a Gaussian with mean µ and log-variance log σ 2 determined by a neural network with input</p><formula xml:id="formula_5">[z 1 , z 2 , · · · , z i−1 ].</formula><p>In all experiments, we use encoder stacks of depth three, with 512 hidden units in each GRU. The approximating posterior uses four layers of hierarchy, with 128 hidden units in the one-hidden-layer neural network that computes the attentional query vector. In practice, separate GRUs were used to produce the final hidden state for z 1 and the atom representations for z &gt;1 . The single-layer LSTM decoder has 2048 hidden units. Training was performed using ADAM, with a decaying learning rate and KL annealing. In all multiple SMILES strings architectures, we use 5 SMILES strings for encoding and decoding.</p><p>In contrast to many previous molecular VAEs, we do not scale down the term KL [q(z|x)||p(z)] in the ELBO by the number of latent units <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b34">35]</ref>. However, our loss function does include separate reconstructions for multiple SMILES strings of a single molecule. For molecular optimization tasks, we usually scale up this KL term by the number of SMILES strings in the decoder, analogous to multiple single-SMILES VAEs in parallel; we leave the KL term unscaled for property prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Gated recurrent neural networks</head><p>Convolutional neural networks on images <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37]</ref> leverage the inherent geometry of the visual domain to perform local message passing. At every spatial location of each layer, a convolutional network computes a message comprising the weighted sum of messages from the surrounding region in the preceding layer, followed by a point-wise nonlinearity. Each round of messages propagates information a distance equal to the convolutional kernel diameter multiplied by the current spatial resolution.</p><p>Recurrent neural networks, such as long short-term memories (LSTMs) <ref type="bibr" target="#b20">[21]</ref> and gated recurrent units (GRUs) <ref type="bibr" target="#b6">[7]</ref>, model text, audio, and other one-dimensional sequences in an analogous manner. The kernel, comprising the weights on the previous hidden state and the current input, has a width of only two. However, the messages (i.e., the hidden states) are updated consecutively along the sequence, so information can propagate through the entire network in a single pass, substantially reducing the number of layers required. LSTMs and GRUs are ubiquitous in natural language processing tasks, and efficient GPU implementations have been developed <ref type="bibr" target="#b0">[1]</ref>.</p><p>Gated recurrent units (GRUs) are defined by <ref type="bibr" target="#b6">[7]</ref>:</p><formula xml:id="formula_6">[r, z] = σ (x t [W r , W z ] + h t−1 [U r , U z ] + [b r , b z ]) h t = (1 − z) h t−1 + z tanh (x t W + (r h t−1 ) U + b h )</formula><p>where r, z, and h are row-vectors, [x, y] denotes the column-wise concatenation of x and y, and the logistic function σ(x) = (1 + e −x ) −1 and hyperbolic tangent are applied element-wise to vector argument x. The hidden state h t , comprising the message from node t, is a gated, weighted sum of the previous message h t−1 and the current input x t , both subject to an element-wise linear transformation and nonlinear (sigmoid) transformation. Specifically, the sum of the message from the input, x t W U −1 , and the gated message from the previous node, r h t−1 , is subject to a linear transformation U and a pointwise nonlinearity. This is then gated and added to a gated residual connection from the previous node.</p><p>Long short-term memories (LSTMs) are defined similarly <ref type="bibr" target="#b20">[21]</ref>:</p><formula xml:id="formula_7">[f t , i t , o t ] = σ (x t [W f , W i , W o ] + h t−1 [U f , U i , U o ] + [b f , b i , b o ]) c t = f t c t−1 + i t tanh (x t W c + h t−1 U c + b c ) h t = o t tanh (c t )</formula><p>where f is the forget gate, i is the input gate, and o is the output gate. LSTMs impose a second hyperbolic tangent and gating unit on the nonlinear recurrent message, but nevertheless still follow the form of applying width-two kernels and pointwise nonlinearities to the input and hidden state.</p><p>In contrast, message passing in graphs is defined by <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39]</ref>:</p><formula xml:id="formula_8">h (n) t = f     m∈N (n) h (m) t−1   W t  </formula><p>where N (n) is the set of neighbors of node n, for which there is an edge between n and m ∈ N (n), and f (x) is a pointwise nonlinearity such as a logistic function or rectified linear unit. This message passing, also called graph convolutions, can be understood as a first-order approximation to spectral convolutions on graphs <ref type="bibr" target="#b18">[19]</ref>. Kipf &amp; Welling <ref type="bibr" target="#b31">[32]</ref> additionally normalize each message by the square root of the degree of each node before and after the sum over neighboring nodes. Kearnes et al. <ref type="bibr" target="#b28">[29]</ref> maintain separate messages for nodes and edges, with the neighborhood of a node comprising the connected edges and the neighborhood of an edge comprising the connected nodes. Li et al. <ref type="bibr" target="#b38">[39]</ref> add gating analogous to a GRU.</p><p>An LSTM, taking a SMILES string as input, can realize a subset of the messages passed by graph convolutions. For instance, input gates and forget gates can conspire to ignore open-parentheses, which indicate the beginning of a branch of the depth-first spanning tree traversal. If they similarly ignore the digits that close broken rings, the messages along each branch of the flattened spanning tree are not affected by the extraneous SMILES syntax. Input and forget gates can then reset the LSTM's memory at close-parentheses, which indicate the end of a branch of the depth-first spanning tree traversal, and the return to a previous node, ensuring that messages only propagate along connected paths in the molecular graph.</p><p>A set of LSTMs on multiple SMILES strings of a single molecule, with messages exchanged between the LSTMs, can generate all of the messages produced by a graph convolution. Atom-based pooling between LSTMs on multiple SMILES strings of the same molecule combines the messages produced in each flattened spanning tree, allowing every LSTM to access all messages produced by a graph convolution. While an LSTM decoder generating SMILES strings faces ambiguity regarding which set of SMILES strings representing a molecule to produce, this is analogous to the problem faced by graph-based decoders, as discussed in Appendix D.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Bahdanau-style attention</head><p>The layers of the hierarchical approximating posterior after the first define a conditional Gaussian distribution, the mean and log-variance of which are parametrized by an attentional mechanism of the form proposed by Bahdanau et al. <ref type="bibr" target="#b2">[3]</ref>. The final encoder hidden vectors for each atom comprise the key vectors k, whereas the query vector q is computed by a one-hidden-layer network of rectified linear units given the concatenation of the previous latent layers as input. The final output of the attentional mechanism, c, is computed via:</p><formula xml:id="formula_9">e i = tanh (qW a + k i U a ) v α i = exp(e i ) j exp(e j ) c = i α i k i</formula><p>The output of the attentional mechanism is subject to batch renormalization and a linear transformation to compute the conditional mean and log-variance of the layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Latent space optimization</head><p>To further ensure that the optimization is constrained to well-trained regions of latent space, we add λ log(p(z)) to the objective function, where λ is a hyperparameter. Finally, to moderate the strictly monotonic nature of linear regressors, we apply an element-wise hard tanh to all latent variables before the regressor, with a linear region that encompasses all values observed in the training set.</p><p>To compare with previous work as fairly as possible, we optimize 1000 random samples from the prior to convergence, collecting the last point from each trajectory with a valid SMILES decoding. From these 1000 points, we evaluate the true molecular property on the 100 points for which the predicted property value is largest. Of these 100 values, we report the three largest. However, optimization within our latent space is computationally inexpensive, and requires no additional property measurement data. We could somewhat improve molecular optimization at minimal expense by constructing additional optimization trajectories in latent space, and evaluating the true molecular properties on the best points from this larger set.</p><p>Molecular optimization is quite robust to hyperparameters. We considered ADAM learning rates in {0.1, 0.01, 0.001, 0.0001} and λ ∈ {0.1, 0.01, 0.001, 0.0001}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Summary of novel contributions</head><p>Starting with the work of Gómez-Bombarelli et al. <ref type="bibr" target="#b15">[16]</ref>, previous molecular variational autoencoders have used one consistent SMILES string as both the input to the RNN encoder and the target of the RNN decoder. Any single SMILES string explicitly represents only a subset of the pathways in the molecular graph. Correspondingly, the recurrent neural networks in these encoders implicitly propagated information through only a fraction of the possible pathways. Kipf &amp; Welling <ref type="bibr" target="#b32">[33]</ref>, Liu et al. <ref type="bibr" target="#b41">[42]</ref>, and Simonovsky &amp; Komodakis <ref type="bibr" target="#b58">[59]</ref>, amongst others, trained molecular VAEs with graph convolutional encoders, which pass information through all graph pathways in parallel, but at considerable computational expense. None of these works used enough layers of graph convolutions to transfer information across the diameter of the average molecule in standard drug design datasets. The All SMILES VAE introduces the use of multiple SMILES strings of a single, common molecule as input to a RNN encoder, with pooling of homologous messages amongst the hidden representations associated with different SMILES strings. This allows information to flow through all pathways of the molecular graph, but can efficiently propagate information across the entire width of the molecule in a single layer.</p><p>Bjerrum &amp; Sattarov <ref type="bibr" target="#b4">[5]</ref> and Winter et al. <ref type="bibr" target="#b66">[67]</ref> trained sequence-to-sequence transcoders to map between different SMILES strings of the same molecule. These transcoders do not define an explicit generative model over molecules, and their latent spaces have no prior distributions. The All SMILES VAE extends this approach to variational autoencoders, and thereby learns a SMILES-derived generative model of molecules, rather than SMILES strings. The powerful, learned, hierarchical prior of the All SMILES VAE regularizes molecular optimization and property prediction. To ensure that molecular property optimization searches within the practical support of the prior, containing almost all of its probability mass, we introduce a hierarchical radius constraint on optimization with respect to the latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Extended results</head><p>We compare the performance of the All SMILES VAE to a variety of state-of-the-art algorithms that have been evaluated on standard molecular property prediction and optimization tasks. In particular, we compare to previously published results on the character/chemical VAE (CVAE) <ref type="bibr" target="#b15">[16]</ref> (with results reported in <ref type="bibr" target="#b34">[35]</ref>), grammar VAE (GVAE) <ref type="bibr" target="#b34">[35]</ref>, syntax-directed VAE (SD-VAE) <ref type="bibr" target="#b8">[9]</ref>, junction tree VAE (JT-VAE) <ref type="bibr" target="#b26">[27]</ref>, NeVAE <ref type="bibr" target="#b55">[56]</ref>, semisupervised VAE (SSVAE) <ref type="bibr" target="#b27">[28]</ref>, graph convolutional policy network (GCPN) <ref type="bibr" target="#b69">[70]</ref>, molecule deep Q-network (MolDQN) <ref type="bibr" target="#b71">[72]</ref>, and the DeepChem <ref type="bibr" target="#b68">[69]</ref> implementation of extended connectivity fingerprints (ECFP) <ref type="bibr" target="#b52">[53]</ref> and graph convolutions (GraphConv) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b68">69]</ref>. Extended connectivity fingerprints are a fixed-length hash of local fragments of the molecule, used as input to conventional machine learning techniques such as random forests, support vector machines, and non-convolutional neural networks <ref type="bibr" target="#b68">[69]</ref>. For toxicity prediction, we also compare to PotentialNet <ref type="bibr" target="#b13">[14]</ref>, ToxicBlend <ref type="bibr" target="#b70">[71]</ref>, and the results of Li et al. <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Reconstruction accuracy and validity</head><p>Previous molecular variational autoencoders have been evaluated using the percentage of molecules that are correctly reconstructed when sampling from both the approximating posterior q(z|x) and the conditional likelihood p(x|z) (reconstruction accuracy), and the percentage of samples from the prior p(z) and conditional likelihood p(x|z) that are valid SMILES strings (validity). While these measure have intuitive appeal, they reflect neither the explicit training objective (the ELBO), nor the requirements of molecular optimization. In particular, when optimizing molecules via the latent space, a deterministic decoder ensures that each point in latent space is associated with a single set of well-defined molecular properties.</p><p>The All SMILES VAE is trained on a more difficult task than previous molecular VAEs, since the reconstruction targets are different SMILES encodings than those input to the approximating posterior. This ensures that the latent representation only captures the molecule, rather than its particular SMILES encoding, but it requires the decoder LSTM to produce a complex, highly multimodal distribution over SMILES strings. As a result, samples from the decoder distribution are less likely to correspond to the input to the encoder, either due to syntactic or semantic errors.</p><p>To compensate for this unusually difficult decoding task, we evaluate the All SMILES VAE using a beam search over the decoder distribution. <ref type="bibr" target="#b5">6</ref> That is, we decode to the single SMILES string estimated to be most probable under the conditional likelihood p(x|z). This has the added benefit of defining an unambiguous decoding for every point in the latent space, simplifying the interpretation of optimization in the latent space (Section 4.2). However, it renders the reconstruction and validity reported in Section 4 incommensurable with much prior work, which use stochastic encoders and decoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Property prediction</head><p>Rather than jointly modeling the space of molecules and their properties, some earlier molecular variational autoencoders first trained an unsupervised VAE on molecules, extracted their latent representations, and then trained a sparse Gaussian process over molecular properties as a function of these fixed latent representations <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b55">56]</ref>. Sparse Gaussian processes are parametric regressors, with the location and value of the inducing points trained based upon the entire supervised dataset <ref type="bibr" target="#b59">[60]</ref>. They have significantly more parameters, and are corresponding more powerful, than linear regressors.</p><p>Molecular properties are only a smooth function of the VAE latent space when the property regressor is trained jointly with the generative model <ref type="bibr" target="#b15">[16]</ref>. Results using a sparse Gaussian process on the latent space of an unsupervised VAE are very poor compared to less powerful regressors trained jointly with the VAE. Our property prediction is two orders of magnitude more accurate than sparse Gaussian process regression on an unsupervised VAE latent representation, as shown in <ref type="table">Table 3</ref>. <ref type="table">Table 3</ref>: Root-mean-square error of the log octanol-water partition coefficient (logP) on the ZINC250k dataset. Results other than the All SMILES VAE are those reported in the cited papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MODEL RMSE</head><p>CHARACTER VAE (CVAE) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35]</ref> 1.504 GRAMMAR VAE (GVAE) <ref type="bibr" target="#b34">[35]</ref> 1.404 SYNTAX-DIRECTED VAE (SD-VAE) <ref type="bibr" target="#b8">[9]</ref> 1.366 JUNCTION TREE VAE (JT-VAE) <ref type="bibr" target="#b26">[27]</ref> 1.290 NEVAE <ref type="bibr" target="#b55">[56]</ref> 1.23 All SMILES 0.011 ± 0.001</p><p>We report numerical results on semi-supervised property prediction, as well as comparisons from Kang &amp; Cho <ref type="bibr" target="#b27">[28]</ref>, in <ref type="table" target="#tab_2">Table 4</ref>. Our mean absolute error is at least three times smaller than comparison algorithms on the log octanol-water partition coefficient (logP) and molecular weight (MW).</p><p>As a visual demonstration of the accuracy of property prediction, in <ref type="figure" target="#fig_0">Figure 12</ref> we show the predicted logP of a 2D slice of latent space subject to the hierarchical radius constraint, alongside the true logP of the molecules decoded from this slice (identical to <ref type="figure" target="#fig_5">Figure 6a</ref>).  Pathways on which activity (active or inactive) is assessed for the Tox21 dataset include seven nuclear receptor signaling pathways: androgen receptor, full (NR-AR) androgen receptor, LBD (NR-AR-LBD); aryl hydrocarbon receptor (NR-AHR); aromatase (NR-AROMATASE); estrogen receptor alpha, LBD (NR-ER-LBD); estrogen receptor alpha, full (NR-ER); and peroxisome proliferatoractivated receptor gamma (NR-PPAR-GAMMA). The Tox21 dataset also includes activity assessments for five stress response pathways: nuclear factor (erythroid-derived 2)-like 2/antioxidant responsive element (SR-ARE); ATAD5 (SR-ATAD5); heat shock factor response element (SR-HSE); mitochondrial membrane potential (SR-MMP); and p53 (SR-p53). We report the area under the receiver operating characteristic curve (AUC-ROC) on each assay independently in <ref type="table" target="#tab_3">Table 5</ref>. The average of these AUC-ROCs is reported in <ref type="table" target="#tab_0">Table 1</ref>. We do not include the result of Kearnes et al. <ref type="bibr" target="#b28">[29]</ref> in <ref type="table" target="#tab_0">Table 1</ref>, since it is not evaluated on the same train/validation/test split of the Tox21 dataset, and so is not commensurable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Molecular optimization</head><p>We present an optimization trajectory for the quantitative estimate of drug-likeness (QED) in <ref type="figure" target="#fig_0">Figure 13</ref>. For the molecules depicted in <ref type="figure" target="#fig_6">Figure 7</ref>, we scaled KL(q(z|x)||p(z))) in the ELBO (Equation 1) of the All SMILES VAE by the number of SMILES strings in the decoder. This renders the loss function analogous to that of many parallel single-SMILES VAEs, but with message passing between encoders leading to a shared latent representation. If we leave the KL term unscaled, latent space embeddings are subject to less regularization forcing them to match the prior distribution. Optimization of molecular properties with respect to the latent space therefore searches over a wider space of molecules, which are less similar to the training set. In <ref type="figure" target="#fig_0">Figure 14</ref>, we show that such an optimization for penalized log P finds very long aliphatic chains, with penalized log P values as large as 42.46. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Ablation of model components</head><p>The All SMILES VAE passes messages along and between the flattened spanning trees of molecular graphs encoded by multiple SMILES strings of a shared molecule. In the base implementation, this is realized via a stack of GRUs, with pooling across SMILES strings amongst the hidden representation of each atom in each layer, and further max pooling of the final hidden states. In <ref type="table" target="#tab_4">Table 6</ref>, we progressively ablate model components to demonstrate that this computational architecture builds a powerful fixed-length representation of molecules, rather than their particular SMILES string instantiations. We evaluate the effect of these model ablations on the mean absolute error of predictions of the log octanol-water partition coefficient (logP) and the quantitative estimate of druglikeness (QED), as well as the percentage of samples from the prior that decode to valid SMILES strings (Val) and the percentage of molecules in a held-out test set that are reconstructed accurately by the encoder and decoder (Rec acc). In all cases, we use the layer-wise maximum a posterior configuration of the encoder (the mean of each conditional Gaussian distribution), and a beam-search decoder.</p><p>NO ATOM-BASED POOLING removes the pooling between each instance of an atom across SMILES strings, depicted in <ref type="figure" target="#fig_2">Figure 3</ref>. As a result, the multiple SMILES inputs are processed independently until the final max pooling over GRU hidden states, which serves as input to the first layer of the latent variable hierarchy. A random SMILES string is chosen to serve as input to the attentional mechanism for the remaining layers of the latent variable hierarchy. The effect of this ablation on toxicity prediction for the Tox21 dataset is reported in <ref type="table" target="#tab_5">Table 7</ref>. We extend this process in ONE SMILES ENC by only feeding a single SMILES string to the encoder, although the decoder still reconstructs multiple disjoint SMILES strings. ONE SMILES ENC/DEC ( =) further reduces the size of the decoder set to one, but the encoded and decoded SMILES strings are distinct representations of the molecule. Finally, ONE SMILES ENC/DEC (=) encodes and decodes a single, shared SMILES string.</p><p>Except for ONE SMILES ENC/DEC (=), all of these ablations primarily disrupt the flow of messages between the flattened spanning trees, and induce a similar, significant decay in performance. ONE SMILES ENC/DEC (=) further permits the latent representation to encode the details of the particular SMILES string, rather than forcing the representation of only the underlying molecule, and causes a further reduction in performance.</p><p>We also observe a meaningful contribution from the hierarchical approximating posterior, with its attentional pooling mechanism. In NO POSTERIOR HIERARCHY, we move all latent variables to the first layer of the hierarchy, removing the succeeding layers. The remaining prior is a standard Gaussian, and there is no attentional pooling over the atom representations. <ref type="table" target="#tab_6">Table 8</ref> shows that the hierarchical radius constraint significantly improves molecular optimization. In contrast to <ref type="table" target="#tab_1">Table 2</ref>, optimization is performed on penalized logP alone, without a log prior regularizer. This produces better results without the radius constraint, and so constitutes a more conservative ablation experiment.    <ref type="bibr" target="#b34">[35]</ref> is equivalent to the context-free grammar shown in <ref type="figure" target="#fig_0">Figure 15</ref>. This subset does not include the ability to represent multiple disconnected molecules in a single SMILES string, multiple fragments that are only connected by ringbonds, or wildcard atoms. element_symbols includes symbols for every element in the periodic table, including the aliphatic_organic symbols.</p><p>Productions generally begin with a unique, defining symbol or set of symbols. Exceptions include bond and charge (both can begin with -), and aromatic_organic and aromatic_symbols (both include c, n, o, s, and p), but these pairs of productions never occur in the same context, and so cannot be confused. The particular production for chiral can only be resolved by parsing characters up to the next production, but the end of chiral and the identity of the subsequent production can be inferred from its first symbol of the production after chiral. Alternatively, the strings of chiral can be encoded as monolithic tokens.</p><p>Whenever there is a choice between productions, the true production is uniquely identified by the next symbols. The only aspect of the SMILES grammar that requires more than a few bits of memory chain → branched_atom rest_of_chain rest_of_chain → | bond? chain bond → '-' | '=' | '#' | '$' | ':' | '/' | '\' branched_atom → atom ringbond* branch* ringbond → bond digit? digit branch → '(' bond? chain ')' atom → aliphatic_organic | aromatic_organic | bracket_atom <ref type="figure" target="#fig_0">Figure 15</ref>: Context-free grammar of SMILES strings is the matching of parentheses, which can be performed in a straightforward manner with a pushdown automaton. As a result, parse trees <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b34">35]</ref> need not be explicitly constructed by the decoder to enforce the syntactic restrictions of SMILES strings. Rather, the SMILES grammar can be enforced with a pushdown automaton running in parallel with the decoder RNN. The state of the pushdown automaton tracks progress within the representation of each atom, and the sequence of atoms and bonds. The set output symbols available to the decoder RNN is restricted to those consistent with the current state of the pushdown automaton. ( and [ are pushed onto the stack when are emitted, and must be popped from the top of the stack in order to emit ) or ] respectively.</p><formula xml:id="formula_10">aliphatic_organic → 'B' | 'C' | 'N' | 'O' | 'S' | 'P' | 'F' | 'Cl' | 'Br' | 'I' aromatic_organic → 'b' | 'c' | 'n' | 'o' | 's' | 'p' bracket_atom → '[' isotope? symbol chiral? hcount? charge? class? ']' isotope → digit? digit? digit symbol → element_symbols | aromatic_symbols aromatic_symbols → 'c' | 'n' | 'o' | 'p' | 's' | 'se' | 'as' chiral → '@' | '@@' | '@TH1' | '@TH2' | '@AL1' | '@AL2' | '@SP1' | '@SP2' | '@SP3' | '@TB1' | '@TB2' · · · '@TB30' | '@OH1' | '@OH2' · · · '@OH30' hcount → 'H' digit? charge → '-' digit? | '+' digit? class → ':' digit? digit? digit? digit → '0' | '1' | '2' | '3' | '4' | '5' | '6' | '7' | '8' | '9'</formula><p>For example, in addition to simple aliphatic organic (B, C, N, O, S, P, F, Cl, Br, or I) or aromatic organic (b, c, n, o, s, or p) symbols, an atom may be represented by a pair of brackets (requiring parentheses matching) containing a sequence of isotope number, atom symbol, chiral symbol, hydrogen count, charge, and class. With the exception of the atom symbol, each element of the sequence is optional, but is easily parsed by a finite state machine. isotope, symbol, chiral, hcount, charge, and class can all be distinguished based upon their first character, so the position in the progression can be inferred trivially. <ref type="bibr" target="#b6">7</ref> When parsing branched_atom, all productions after the initial atom are ringbonds until the first (, which indicates the beginning of a branch. After observing a ), and popping the complementary ( off of the stack, the SMILES string is necessarily in the third component of a branched_atom, since only a branched_atom can emit a branch, and only branch produces the symbol ). The next symbol must be a (, indicating the beginning of another branch, or one of the first symbols of rest_of_chain, since this must follow the branched_atom in the chain production.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Ringbond and valence shell semantic constraints</head><p>Similarly, the semantic restrictions of ringbond matching and valence shell constraints can be enforced during feedforward production of a SMILES string using a pushdown stack and a small (100-element) random access memory. Our approach depends upon the presence of matching bond labels at both sides of a ringbond, which is allowed but not required in standard SMILES syntax. We assume the trivial extention of the SMILES grammar to include this property.</p><p>ringbonds are constrained to come in pairs, with the same bond label on both sides. Whenever a given ringbond is observed, flip a bit in the random access memory corresponding to the ring number (the set of digits after the bond). When the ringbond bit is flipped on, record the associated bond in the random access memory associated with the ring number; when the ringbond bit is flipped off, require that the new bond matches the recorded bond, and clear the random access memory of the bond. The molecule is only allowed to terminate (rest_of_chain produces rather than bond? chain) when all ringbond bits are off (parity is even). The decoder may receive as input which ringbonds are open, and the associated bond type, so it can preferentially close them.</p><p>The set of nested atomic contexts induced by chain, branched_atom, and branch can be arbitrarily deep, corresponding to the depth of branching in the spanning tree realized by a SMILES string. As a result, the set of SMILES symbols describing bonds to a single atom can be arbitrarily far away from t=he associated atom. However, once a branch is entered, it must be traversed in its entirety before the SMILES string can return to the parent atom. For each atom, it is sufficient to push the valence shell information onto the stack as it is encountered. If the SMILES string enters a branch while processing an atom, simply push on a new context, with a new associated root atom. Once the branch is completed, pop this context off the stack, and return to the original atom.</p><p>More specifically, each atom in the molecule is completely described by a single branched_atom and the bond preceding it (from the rest_of_chain that produced the branched_atom). Within each successive pair of bond and branched_atom, track the sum of the incoming rest_of_chainbond, the internal ringbond and branch bonds, and outgoing rest_of_chain bond (from the succeeding rest_of_chain) on the stack. That is, each time a new bond is observed from the atom, pop off the old valence shell count and push on the updated count. Require that the total be less than a bound set by the atom; any remaining bonds are filled by implicity hydrogen atoms. Provide the number of available bonds as input to the decoder RNN, and mask additional ringbonds and branches once the number of remaining available bonds reaches one (if there are still open ringbonds) or zero (if all ringbonds are closed). Mask the outgoing bond, or require that rest_of_chain produce , based upon the number of remaining available bonds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Redundancy in graph-based and SMILES representations of molecules</head><p>To avoid the degeneracy of SMILES strings, for which there are many encodings of each molecule, some authors have advocated the use of graph-based representations <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b58">59]</ref>. While graphbased processing may produce a unique representation in the encoder, it is not possible to avoid degeneracy in the decoder. Parse trees <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b34">35]</ref>, junction trees <ref type="bibr" target="#b26">[27]</ref>, lists of nodes and edges <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b55">56]</ref>, and vectors/matrices of node/edge labels <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b58">59]</ref> all imply an ordering amongst the nodes and edges, with many orderings describing the same graph. Canonical orderings can be defined, but unless they are obvious to the decoder, they make generative modeling harder rather than easier, since the decoder must learn the canonical ordering rules. Graph matching procedures can ensure that probability within a generative model is assigned to the correct molecule, regardless of the order produced by the decoder <ref type="bibr" target="#b58">[59]</ref>. However, they do not eliminate the degeneracy in the decoder's output, and the generative loss function remains highly multimodal.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The molecular graph of the amino acid Tryptophan (a). To construct a SMILES string, all cycles are broken, forming a spanning tree (b); a depth-first traversal is selected (c); and this traversal is flattened (d). The beginning and end of intermediate branches in the traversal are denoted by ( and ) respective. The ends of broken cycles are indicated with matching digits. The full grammar is listed in Appendix D. A small set of SMILES strings can cover all paths through a molecule (e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 Figure 2 :</head><label>22</label><figDesc>In each layer of the encoder after the initial BiGRU and linear transformation, hidden states corresponding to each atom are pooled across encodings of different SMILES strings for a common molecule, followed by layer norm and a GRU on each SMILES encoding independently.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>To pass information between distinct paths implicit in multiple SMILES representations of a molecule, the encoder pools the representation of each atom across multiple SMILES strings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The approximating posterior is an autoregressive set of Gaussian distributions. The mean (µ) and log-variance (log σ 2 ) of the first subset of latent variables z 1 is a linear transformation of the max-pooled final hidden state of GRUs fed the encoder outputs. Succeeding subsets z i are produced via Bahdanau-style attention with the pooled atom outputs of the GRUs as keys (k), and the query (q) computed by a neural network on z &lt;i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>(a) True logP over a 2D slice of latent space (b) Predicted and true logP over optimization Dense decodings of true logP along a local 2D sheet in latent space, with the y axis aligned with the regressor (a), and predicted and true penalized logP across steps of optimization (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Molecules produced by gradient-based optimization in the All SMILES VAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Multiple SMILES strings representing a single, common molecule are preprocessed by a BiGRU and a linear transformation, followed by multiple encoder blocks as in Figures 2 and 3. The approximating posterior depicted in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Dense decodings of true logP (a) and predicted logP (b) along a local 2D sheet in latent space, with the y axis aligned with the trained logP regressor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :Figure 14 :</head><label>1314</label><figDesc>Predicted (red line) and true (blue x's) quantitative estimate of drug-likeness (QED) over the optimization trajectory resulting in the molecule with the maximum observed true QED (0.948). Molecules with the top three true penalized LogP values produced by gradient-based optimization subject to the hierarchical radius constraint in the All SMILES VAE, but with the KL term unscaled by the number of SMILES strings in the decoder. Molecules are shown as SMILES strings, wrapped across multiple lines, as they are too large to be properly rendered into an image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Fully supervised regression on ZINC250k (a), evaluated using the mean absolute error; and Tox21 (b), evaluated with the area under the receiver operating characteristic curve (AUC-ROC), averaged over all 12 toxicity types. Aside from All SMILES, results in (a) are those reported in<ref type="bibr" target="#b15">[16]</ref>.</figDesc><table><row><cell></cell><cell>(a) ZINC250k</cell><cell></cell><cell>(b) Tox21</cell><cell></cell></row><row><cell>MODEL</cell><cell cols="2">MAE LOGP MAE QED</cell><cell>MODEL</cell><cell>AUC-ROC</cell></row><row><cell>ECFP [53]</cell><cell>0.38</cell><cell>0.045</cell><cell>GRAPHCONV [69]</cell><cell>0.829 ± 0.006</cell></row><row><cell>CVAE [16]</cell><cell>0.15</cell><cell>0.054</cell><cell>LI, CAI, &amp; HE</cell><cell></cell></row><row><cell>CVAE ENC [16]</cell><cell>0.13</cell><cell>0.037</cell><cell></cell><cell></cell></row><row><cell cols="2">GRAPHCONV [12] 0.05</cell><cell>0.017</cell><cell></cell><cell></cell></row><row><cell>All SMILES</cell><cell cols="2">0.005 ± 0.0006 0.0052 ± 0.0001</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Properties of the top three optimized molecules trained on ZINC250k. All SMILES(KL unscaled)  42.46, 42.42, 41.54    </figDesc><table><row><cell>MODEL</cell><cell>PENALIZED LOGP</cell><cell>MODEL</cell><cell>QED</cell></row><row><cell>JT-VAE [27]</cell><cell>5.30, 4.93, 4.49</cell><cell>JT-VAE [27]</cell><cell>0.925, 0.911, 0.910</cell></row><row><cell>GCPN [70]</cell><cell>7.98, 7.85, 7.80</cell><cell>CGVAE [42]</cell><cell>0.938, 0.931, 0.880</cell></row><row><cell>MOLDQN [72]</cell><cell>11.84, 11.84, 11.82</cell><cell>GCPN [70]</cell><cell>0.948, 0.947, 0.946</cell></row><row><cell>ALL SMILES</cell><cell>16.42, 16.32, 16.21</cell><cell cols="2">MolDQN [72] 0.948, 0.948, 0.948</cell></row><row><cell></cell><cell></cell><cell>All SMILES</cell><cell>0.948, 0.948, 0.948</cell></row><row><cell cols="2">(a) Molecules with the top three penalized logP values</cell><cell cols="2">(b) Molecules with the top three QED values</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Mean absolute error (MAE) of semi-supervised property prediction on the log octanol-water partition coefficient (logP), molecular weight (MW), and the quantitative estimate of drug-likeness (QED) on ZINC310k dataset. Results other than the All SMILES VAE are those reported by<ref type="bibr" target="#b27">[28]</ref>.</figDesc><table><row><cell>MODEL</cell><cell cols="2">% LABELED MAE LOGP</cell><cell>MAE MW</cell><cell>MAE QED</cell></row><row><cell>ECFP</cell><cell>50%</cell><cell cols="2">0.180 ± 0.003 9.012 ± 0.184</cell><cell>0.023 ± 0.000</cell></row><row><cell>GRAPHCONV</cell><cell>50%</cell><cell cols="2">0.086 ± 0.012 4.506 ± 0.279</cell><cell>0.018 ± 0.001</cell></row><row><cell>SSVAE</cell><cell>50%</cell><cell cols="2">0.047 ± 0.003 1.05 ± 0.164</cell><cell>0.01 ± 0.001</cell></row><row><cell cols="2">ALL SMILES 50%</cell><cell cols="2">0.007 ± 0.002 0.21 ± 0.07</cell><cell>0.0064 ±0.0002</cell></row><row><cell>ECFP</cell><cell>20%</cell><cell cols="3">0.249 ± 0.004 12.047 ± 0.168 0.033 ± 0.001</cell></row><row><cell>GRAPHCONV</cell><cell>20%</cell><cell cols="2">0.112 ± 0.015 4.597 ± 0.419</cell><cell>0.021 ± 0.002</cell></row><row><cell>SSVAE</cell><cell>20%</cell><cell cols="2">0.071 ± 0.007 1.008 ± 0.370</cell><cell>0.016 ± 0.001</cell></row><row><cell cols="2">ALL SMILES 20%</cell><cell cols="2">0.009 ± 0.002 0.33 ±0.06</cell><cell>0.0079 ±0.0003</cell></row><row><cell>ECFP</cell><cell>10%</cell><cell cols="3">0.335 ± 0.005 15.057 ± 0.358 0.045 ± 0.001</cell></row><row><cell>GRAPHCONV</cell><cell>10%</cell><cell cols="2">0.148 ± 0.016 5.255 ± 0.767</cell><cell>0.028 ± 0.003</cell></row><row><cell>SSVAE</cell><cell>10%</cell><cell cols="2">0.090 ± 0.004 1.444 ± 0.618</cell><cell>0.021 ± 0.001</cell></row><row><cell cols="2">ALL SMILES 10%</cell><cell cols="2">0.014 ± 0.002 0.30 ± 0.06</cell><cell>0.0126 ± 0.0006</cell></row><row><cell>ECFP</cell><cell>5%</cell><cell cols="3">0.380 ± 0.009 17.713 ± 0.396 0.053 ± 0.001</cell></row><row><cell>GRAPHCONV</cell><cell>5%</cell><cell cols="2">0.187 ± 0.015 6.723 ± 2.116</cell><cell>0.034 ± 0.004</cell></row><row><cell>SSVAE</cell><cell>5%</cell><cell cols="2">0.120 ± 0.006 1.639 ± 0.577</cell><cell>0.028 ± 0.001</cell></row><row><cell cols="2">ALL SMILES 5%</cell><cell cols="2">0.036 ± 0.004 0.4 ± 0.1</cell><cell>0.0217 ± 0.0003</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Area under the receiver operating characteristic curve (AUC-ROC) per assay on the Tox21 dataset.</figDesc><table><row><cell>NR-AR</cell><cell cols="2">NR-AR-LBD NR-AHR</cell><cell cols="2">NR-AROMATASE NR-ER</cell><cell>NR-ER-LBD</cell></row><row><cell>0.868</cell><cell>0.907</cell><cell>0.889</cell><cell>0.907</cell><cell>0.714</cell><cell>0.830</cell></row><row><cell cols="2">NR-PPAR-GAMMA SR-ARE</cell><cell cols="2">SR-ATAD5 SR-HSE</cell><cell cols="2">SR-MMP SR-p53</cell></row><row><cell>0.911</cell><cell>0.863</cell><cell>0.870</cell><cell>0.901</cell><cell>0.914</cell><cell>0.888</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Effect of model ablation on fully supervised property prediction and generative modeling using the ZINC250k dataset.</figDesc><table><row><cell>ABLATION</cell><cell>MAE LOGP</cell><cell>MAE QED</cell><cell>VAL</cell><cell>REC ACC</cell></row><row><cell>FULL MODEL</cell><cell cols="2">0.005 ± 0.0006 0.0052 ± 0.0001</cell><cell cols="2">98.5 ± 0.1 87.4 ± 1.0</cell></row><row><cell>NO ATOM-BASED POOLING</cell><cell>0.008 ± 0.004</cell><cell>0.0076 ± 0.0005</cell><cell cols="2">97.6 ± 0.2 84.0 ± 0.4</cell></row><row><cell>ONE SMILES ENC</cell><cell cols="2">0.008 ± 0.0005 0.0073 ± 0.0002</cell><cell cols="2">98.4 ± 0.1 82.3 ± 0.4</cell></row><row><cell cols="2">ONE SMILES ENC/DEC ( =) 0.009 ± 0.001</cell><cell>0.0091 ± 0.0003</cell><cell></cell><cell></cell></row><row><cell cols="2">ONE SMILES ENC/DEC (=) 0.025 ± 0.003</cell><cell>0.0115 ± 0.0004</cell><cell></cell><cell></cell></row><row><cell>NO POSTERIOR HIERARCHY</cell><cell>0.010 ± 0.003</cell><cell>0.0051 ± 0.0001</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Effect of model ablation on Tox21 toxicity prediction.</figDesc><table><row><cell>ABLATION</cell><cell>AUC-ROC</cell></row><row><cell>FULL MODEL</cell><cell>0.871</cell></row><row><cell cols="2">NO ATOM-BASED POOLING 0.864</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Effect of the hierarchical radius constraint on penalized logP optimization. Predicted penalized logP was evaluated on 1000 optimization trajectories. From these, the true logP was evaluated on the 100 best trajectories, and the top three true penalized logPs are reported. Each optimization was repeated 5 times. ABLATION 1ST BEST LOGP 2ND BEST LOGP 3RD BEST LOGP The subset of the SMILES grammar [66] captured by Dai et al. [9] and Kusner et al.</figDesc><table><row><cell>WITH RADIUS CONSTRAINT</cell><cell>17.0 ± 3.0</cell><cell>16.0 ± 2.0</cell><cell>14.8 ± 0.3</cell></row><row><cell cols="2">WITHOUT RADIUS CONSTRAINT 8.5044 ± 0.0</cell><cell>6.9526 ± 0</cell><cell>5.36 ± 0.05</cell></row><row><cell cols="4">D SMILES grammar can be enforced with a pushdown automaton</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">All-to-all connections allow fast information transfer, but computation is quadratic in graph size<ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29]</ref>. Lusci et al.<ref type="bibr" target="#b42">[43]</ref> considered a set of DAGs rooted at every atom, with full message propagation in a single pass.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Gómez-Bombarelli et al.<ref type="bibr" target="#b15">[16]</ref> jointly train a regressor, but still optimize using a Gaussian process.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">This is generalizes the slerp interpolations of Gómez-Bombarelli et al.<ref type="bibr" target="#b15">[16]</ref> to optimization.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Recently, Winter et al.<ref type="bibr" target="#b67">[68]</ref> reported molecules with penalized logP as large as 26.1, but train on an enormous, non-standard dataset of 72 million compounds aggregated from the ZINC15 and PubChem databases.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The log octanol-water partition coefficient minus the synthetic accessibility score and the number of rings with more than six atoms.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">The full decoder distribution is still used for training.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">symbol and hcount can both start with 'H', but symbol is mandatory, so there is no ambiguity.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Alla Pryyma for help in preparing <ref type="figure">Figures 1, 13</ref>, and 6b. Mani Ranjbar provided invaluable, unstinting assistance with the compute cluster. Juan Carrasquilla, Colin Collins, Faraz Hach, and William G. Macready contributed helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Optimizing performance of recurrent neural networks on gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Appleyard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01946</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Latent molecular optimization for targeted therapeutic design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aumentado-Armstrong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02032</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quantifying the chemical beauty of drugs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Bickerton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Besnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Hopkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature chemistry</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">90</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving chemical autoencoder latent space and molecular de novo generation diversity with heteroencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bjerrum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sattarov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomolecules</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">131</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Foundations of Data Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/publication/foundations-of-data-science-2/" />
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Organic chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clayden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Greeves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Warren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wothers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08786</idno>
		<title level="m">Syntax-directed variational autoencoder for structured data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Molgan: An implicit generative model for small molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11973</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ertl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schuffenhauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Potentialnet for molecular property prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Husic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS central science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1520" to="1530" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic chemical design using a data-driven continuous representation of molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sánchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sheberla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS central science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="268" to="276" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Objective-reinforced generative adversarial networks (organ) for sequence generation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Guimaraes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sanchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Outeiral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L C</forename><surname>Farias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10843</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative recurrent networks for de novo drug design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Huisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular informatics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">1700111</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tox21challenge to build predictive models of nuclear receptor and stress response pathways as mediated by exposure to environmental chemicals and drugs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakamuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Shahane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rossoshek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Simeonov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Environmental Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Molecular design strategy of organic thermally activated delayed fluorescence emitters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-A</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Yook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemistry of Materials</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1946" to="1963" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Batch renormalization: Towards reducing minibatch dependence in batch-normalized models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1945" to="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Zinc: a free tool to discover chemistry for biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Mysinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Bolstad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Coleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1768" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1645" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04364</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Conditional molecular design with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00108</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riley</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pubchem 2019 update: improved access to chemical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gindulyte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Shoemaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Thiessen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="1102" to="1109" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Variational graph auto-encoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01925</idno>
		<title level="m">Grammar variational autoencoder</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Rdkit: Open-source cheminformatics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Landrum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a back-propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="396" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning graph-level representation for drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03741</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning deep generative models of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03324</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Molecular generative model based on conditional variational autoencoder for de novo molecular design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Constrained graph variational autoencoders for molecule design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Gaunt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09076</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep architectures and deep learning in chemoinformatics: the prediction of aqueous solubility for drug-like molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lusci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pollastri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1563" to="1575" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Constrained generation of semantically valid graphs via regularizing variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7113" to="7124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deeptox: toxicity prediction using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Environmental Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">80</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sequence to better sequence: continuous revision of combinatorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gifford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Molecular de-novo design through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Olivecrona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blaschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Engkvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for de novo drug design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Isayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tropsha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science advances</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">7885</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Reinforced adversarial neural computer for de novo molecular design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Putin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asadulaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ivanenkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Aladinskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sanchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhavoronkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1194" to="1204" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">What is high-throughput virtual screening? a perspective from organic materials discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">O</forename><surname>Pyzer-Knapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Materials Research</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="195" to="216" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Chemical space as a source for new drugs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Reymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Deursen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruddigkeit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MedChemComm</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="38" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Extended-connectivity fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="742" to="754" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Rolfe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02200</idno>
		<title level="m">Discrete variational autoencoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Deeply learning molecular structure-property relationships using graph attention neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10988</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Samanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Chattaraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gomez-Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nevae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05283</idno>
		<title level="m">deep generative model for molecular graphs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Inverse molecular design using machine learning: Generative models for matter engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sanchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">361</biblScope>
			<biblScope unit="issue">6400</biblScope>
			<biblScope unit="page" from="360" to="365" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Generating focused molecule libraries for drug discovery with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Segler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kogej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tyrchan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Waller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS central science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="120" to="131" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Graphvae: Towards generation of small graphs using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="412" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Sparse gaussian processes using pseudo-inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Snelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Zinc 15-ligand discovery for everyone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2324" to="2337" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Exploring activity cliffs in medicinal chemistry: miniperspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stumpfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bajorath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2932" to="2942" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06391</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weininger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and computer sciences</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="36" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning continuous and data-driven molecular descriptors by translating equivalent chemical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Noé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Science</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Efficient multiobjective molecular optimization in a continuous latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steffen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Briem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Noe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Clevert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">MoleculeNet: A benchmark for molecular machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Graph convolutional policy network for goal-directed molecular graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02473</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Virtual screening of toxic compounds with ensemble predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaslavskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Tramel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wainrib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toxicblend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Toxicology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="81" to="88" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Optimization of molecules via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Zare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riley</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08678</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
