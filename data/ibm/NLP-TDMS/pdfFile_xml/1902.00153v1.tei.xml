<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Triplet Quantization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-10-22">2018. October 22-26. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
							<email>caoyue10@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
							<email>jimwang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Triplet Quantization</title>
					</analytic>
					<monogr>
						<title level="m">2018 ACM Multimedia Conference (MM &apos;18)</title>
						<meeting> <address><addrLine>Seoul</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="2018-10-22">2018. October 22-26. 2018</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3240508.3240543</idno>
					<note>ACM Reference Format: Republic of Korea. ACM, New York, NY, USA, 9 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Information systems → Image search</term>
					<term>• Computing method- ologies → Neural networks</term>
					<term>KEYWORDS Deep hashing, Quantization, Image search</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep hashing establishes efficient and effective image retrieval by end-to-end learning of deep representations and hash codes from similarity data. We present a compact coding solution, focusing on deep learning to quantization approach that has shown superior performance over hashing solutions for similarity retrieval. We propose Deep Triplet Quantization (DTQ), a novel approach to learning deep quantization models from the similarity triplets. To enable more effective triplet training, we design a new triplet selection approach, Group Hard, that randomly selects hard triplets in each image group. To generate compact binary codes, we further apply a triplet quantization with weak orthogonality during triplet training. The quantization loss reduces the codebook redundancy and enhances the quantizability of deep representations through back-propagation. Extensive experiments demonstrate that DTQ can generate high-quality and compact binary codes, which yields state-of-the-art image retrieval performance on three benchmark datasets, NUS-WIDE, CIFAR-10, and MS-COCO.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Approximate nearest neighbors (ANN) search has been widely applied to retrieve large-scale multimedia data in search engines and social networks. Due to the low storage cost and fast retrieval speed, learning to hash has been increasingly popular in the ANN research community, which transforms high-dimensional media data into compact binary codes and generates similar binary codes Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. MM <ref type="bibr">'18, October 22-26, 2018</ref>, Seoul, Republic of Korea © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5665-7/18/10. . . $15.00 https://doi.org <ref type="bibr">/10.1145/3240508.3240543</ref> for similar data items. This paper will focus on data-dependent hashing schemes for efficient image retrieval, which have achieved better performance than data-independent hashing methods, e.g. Locality-Sensitive Hashing (LSH) <ref type="bibr" target="#b10">[11]</ref>.</p><p>A rich line of hashing methods have been proposed to enable efficient ANN search using Hamming distance <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39]</ref>. Recently, deep hashing methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42]</ref> have shown that both image representation and hash coding can be learned more effectively using deep neural networks, resulting in state-of-the-art results on many benchmark datasets. In particular, it proves crucial to jointly preserve similarity and control quantization error of converting continuous representations to binary codes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b41">42]</ref>. However, a pivotal weakness of these deep hashing methods is that they first learn continuous deep representations, and then convert them into hash codes by a separated binarization step. By continuous relaxation, i.e. solving the original discrete optimization of hash codes with continuous optimization, the optimization problem deviates significantly from the original hashing objective. As a result, these methods cannot learn exactly compact binary hash codes in their optimization.</p><p>To address the limitation of continuous relaxation, Deep Quantization Network (DQN) <ref type="bibr" target="#b1">[2]</ref> and Deep Visual-Semantic Quantization (DVSQ) <ref type="bibr" target="#b0">[1]</ref> are proposed to integrate quantization method <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40]</ref> and deep learning. The quantization method represents each point by a short binary code formed by the index of the nearest center, which can generate natively binary codes and empirically achieve better performance than hashing methods for ANN search. However, previous deep quantization methods are either point-wise method that relies on expensive class-label information, or pairwise method that cannot capture the relative similarity between images, i.e. a pair of images should not be seen as absolutely similar or dissimilar. In other words, there should be a continuous spectrum from very similar to very dissimilar relations.</p><p>Recently, the triplet loss <ref type="bibr" target="#b27">[28]</ref> has been studied for computer vision problems. The triplet loss captures the relative similarity, which only brings anchor images closer to positive samples than to negative samples, hence it fits the ranking tasks naturally and achieves better performance than point-wise and pairwise losses for retrieval tasks. However, how to enable effective triplet training for deep learning to quantization with only pairwise similarity available still remains a challenge. Note that, without effective triplet selection, previous deep hashing method with triplet loss <ref type="bibr" target="#b18">[19]</ref>  Towards these open problems, this paper presents Deep Triplet Quantization (DTQ) for efficient and effective image retrieval, which introduces a novel triplet training strategy to deep quantization, offering superior retrieval performance. The proposed solution is comprised of four main components: 1) a novel triplet selection module, Group Hard, to mine good triplets for effective triplet training; 2) a standard deep convolutional neural network (CNN), e.g. AlexNet or ResNet, for learning deep representations; 3) a wellspecified triplet loss for pulling together similar pairs and pushing away dissimilar pairs; and 4) a novel triplet quantization loss with weak orthogonality constraint for converting the deep representations of different samples (such as the anchor, positive and negative samples) in the triplets into B-bit compact binary codes. The weakorthogonality reduces the redundancy of codebooks and controls the quantizability of deep representations. Comprehensive empirical evidence shows that the proposed DTQ can generate compact binary codes and yield state-of-the-art retrieval results on three image retrieval benchmarks, NUS-WIDE, CIFAR-10, and MS-COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Existing hashing methods can be categorized into unsupervised hashing and supervised hashing <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>. Please refer to <ref type="bibr" target="#b33">[34]</ref> for a comprehensive survey.</p><p>Unsupervised hashing methods learn hash functions to encode data points to binary codes by training from unlabeled data. Typical learning criteria include reconstruction error minimization <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29]</ref> and graph learning <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref>. Supervised hashing explores supervised information (e.g. given similarity or relevance feedback) to learn compact hash codes. Binary Reconstruction Embedding (BRE) <ref type="bibr" target="#b17">[18]</ref> pursues hash functions by minimizing the squared errors between the distances of data points and the distances of their corresponding hash codes. Minimal Loss Hashing (MLH) <ref type="bibr" target="#b26">[27]</ref> and Hamming Distance Metric Learning <ref type="bibr" target="#b27">[28]</ref> learn hash codes by minimizing the triplet loss functions based on similarity of data points. Supervised Hashing with Kernels (KSH) <ref type="bibr" target="#b22">[23]</ref> and Supervised Discrete Hashing (SDH) <ref type="bibr" target="#b29">[30]</ref> build discrete binary codes by minimizing the Hamming distances across similar pairs and maximizing the Hamming distances across dissimilar pairs.</p><p>As deep convolutional networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17]</ref> yield advantageous performance on many computer vision tasks, deep hashing methods have attracted wide attention recently. CNNH <ref type="bibr" target="#b36">[37]</ref> adopts a two-stage strategy in which the first stage learns binary hash codes and the second stage learns a deep-network based hash function to fit the codes. DNNH <ref type="bibr" target="#b18">[19]</ref> improved CNNH with a simultaneous feature learning and hash coding pipeline such that deep representations and hash codes are optimized by the triplet loss. DHN <ref type="bibr" target="#b41">[42]</ref> and HashNet <ref type="bibr" target="#b2">[3]</ref> improve DNNH by jointly preserving the pairwise semantic similarity and controlling the quantization error by simultaneously optimizing the pairwise cross-entropy loss and quantization loss via a multi-task approach.</p><p>Quantization methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> represent each point by a short code formed by the index of the nearest center, have been shown to give more powerful representation ability than hashing for approximate nearest neighbor search. To our best knowledge, Deep Quantization Network (DQN) <ref type="bibr" target="#b1">[2]</ref> and Deep Visual-Semantic Quantization (DVSQ) <ref type="bibr" target="#b0">[1]</ref> are the only two prior works on deep learning to quantization. DQN jointly learns deep representations via a pairwise cosine loss and a product quantization loss <ref type="bibr" target="#b15">[16]</ref> for generating compact binary codes. DVSQ proposes a pointwise adaptive-margin Hinge loss exploring class labels, and a visual-semantic quantization loss for inner-product search.</p><p>There are several key differences between our work and previous deep learning to quantization methods. 1) Our work introduces a novel triplet training strategy to deep quantization framework for efficient similarity retrieval. It is worth noting that DTQ can learn compact binary codes when only the relative similarity information is available, which is more general than the label-based quantization method DVSQ. 2) During the triplet learning procedure, DTQ proposes a novel triplet mining strategy, Group Hard, resulting in faster convergence and better search accuracy. 3) DTQ proposes a novel triplet quantization loss with weak orthogonality constraint to reduce coding redundancy. An end-to-end architecture to join the above three terms yield both efficient and effective image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DEEP TRIPLET QUANTIZATION</head><p>In similarity retrieval, we are given N training points X = {x i } N i=1 , where some pairs of points x i and x j are given with pairwise similarity labels s i j , where s i j = 1 if x i and x j are similar while s i j = 0 if x i and x j are dissimilar. The goal of deep learning to quantization is to learn a composite quantizer q : x → b ∈ {0, 1} B from input space to binary coding space {0, 1} B through deep networks, which encodes each point x into B-bit binary code b = q(x) such that the supervision in the training data can be maximally preserved. In supervised hashing, the similarity pairs {(x i , x j , s i j ) : s i j ∈ S} are readily available from semantic labels or relevance feedbacks from click-through data in many image search engines.</p><p>We propose Deep Triplet Quantization (DTQ), an end-to-end architecture to join deep learning and quantization, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>. DTQ has four key components: 1) a novel triplet selection module, Group Hard, to mine a appropriate number of good triplets for effective triplet training; 2) a standard deep convolutional neural network (CNN), e.g. AlexNet, VGG, or ResNet, for learning deep representations; 3) a well-specified triplet loss for pulling together similar pairs and pushing away dissimilar pairs; and 4) a novel triplet quantization loss with weak orthogonality constraint for converting deep representations of different samples (the anchor, positive and negative samples) in triplets into B-bit compact binary codes and controlling the quantizability of the deep representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Triplet Training</head><p>We train a convolutional network from image triplets</p><formula xml:id="formula_0">T ={t i } N t i=1 . Each triplet t i =⟨x a i , x p i , x n i ⟩ is constructed from pairwise similarity data {(x i , x j , s i j )</formula><p>: s i j ∈ S} as follows: for each anchor image x a i , we find a positive image x p i with s ap = 1 (x a i and x p i are similar), and a negative image x n i with s an = 0 (x a i and x n i are dissimilar).</p><formula xml:id="formula_1">Given a triplet t i =⟨x a i , x p i , x n i ⟩, the deep network maps the triplet t i into a learned feature space with f (t i )=⟨z a i , z p i , z n i ⟩.</formula><p>We ensure that an anchor image x a i is closer to all positive images x p i than to all negative images x n i . And the relative similarity between the images in triplets, x a i , x p i , x n i , are measured by the Euclidean distances between their deep features, z a i , z p i , z n i . Thus the triplet loss is  where δ is a margin that is enforced between positive and negative pairs, and T is the set of cardinality N t for all possible triplets in the training set. Compared to the widely-used pointwise and pairwise metric-learning losses <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> in previous deep quantization methods, the triplet loss (1) only requires anchor samples to be more similar to positive samples than to negative samples, by a specifically margin. This establishes a relative similarity relation between images, thus is much more reasonable than the absolute similarity relation used in previous pointwise or pairwise approaches.</p><formula xml:id="formula_2">L = N t i=1 L i = N t i=1 max 0, δ − z a i − z n i 2 2 + z a i − z p i 2 2 ,<label>(1)</label></formula><p>However, as the dataset gets larger, the number of triplets grows cubically, and generating all possible triplets would result in many easy triplets with L i = 0 in Eq. (1), which would not contribute to the training and suffer from slower convergence. Note that, without a sophisticated triplet selection procedure, previous deep hashing methods with the triplet loss <ref type="bibr" target="#b18">[19]</ref> cannot achieve superior performance. Consequently, it is crucial to mine good triplets for effective triplet training and faster convergence. In this paper, we propose a novel triplet selection module, Group Hard, to ensure the number of mined valid triplets is neither too big nor too small. The core idea is that we first randomly split the training data into several groups {G i } |G | i=1 , then randomly select one hard negative sample for each anchor-positive pair in one group. The proposed triplet selection method is formulated as</p><formula xml:id="formula_3">T = |G | i=1 a ∈G i p ∈G p i rand G n i ,<label>(2)</label></formula><p>where G p i = p ∈ G i : p a, s ap = 1 is the group of positive samples consisting of the samples similar to the anchor a in the ith group, rand(G n i ) is the random function that randomly chooses one negative sample from the group of hard negative samples</p><formula xml:id="formula_4">G n i = n ∈ G i : δ − z a i − z n i 2 2 + z a i − z p i 2 2 &gt; 0, s an = 0 .</formula><p>Here hard negative sample x n i is defined as having non-zero loss value for a triplet t i =⟨x a i , x p i , x n i ⟩. Note that, mining only the triplets with the hardest negative images would select the outliers in the dataset and make it unable to learn ground truth relative similarity. Thus in this paper, the proposed DTQ only selects the negative examples with moderate hardness, based on the random sampling rand(G n i ) instead of argmax(G n i ) in Eq. <ref type="formula" target="#formula_3">(2)</ref>. As the training proceeds, the average of triplet loss becomes smaller and the size of the hard triplets reduces. To ensure that there are enough hard triplets each epoch for effective triplet training, we design a decay strategy for the size of groups |G | as: if the actual number of valid hard triplets is lower than the minimum number of the valid hard triplets (the constant MIN_TRIPLETS in Algorithm 1), the size of the groups is halved until |G | = 1.</p><p>Complexity: Similar to previous work on triplet training <ref type="bibr" target="#b40">[41]</ref>, we can prune the triplets with zero losses (L i = 0), resulting a valid triplet set T whose size |T | is much smaller than the possible number N 3 of triplets. Through the proposed Group Hard selection strategy that chooses one negative sample for each anchor-positive pair in each group, the number of the candidate triplets for training is further reduced to |T |/|G |. Furthermore, all the selected triplets are hard triplets (L i &gt; 0 in Eq. (1)), and the total amount can be controlled in a suitable range by adjusting the number of groups G, resulting in effective triplet training and higher retrieval accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Weak-Orthogonal Quantization</head><p>While triplet training with Group Hard selection enables effective image retrieval, efficient image retrieval is enabled by a novel triplet quantization model. As each batch used for training the deep neural networks is comprised of triplets, the proposed quantization model should be compatible with the triplet training. For the ith</p><formula xml:id="formula_5">triplet, each image representation z * i , where * ∈ {a, p, n}, is quan- tized using a set of M codebooks C * = [C * 1 , . . . , C * M ], where each codebook C * m contains K codewords C * m = [C * m1 , .</formula><p>. . , C * mK ], and each codeword C * mk is a D-dimensional cluster-centroid vector as in K-means. Corresponding to the M codebooks, we partition the binary codewords assignment vector b</p><formula xml:id="formula_6">* i into M 1-of-K indicator vectors b * i = [b * 1i ; . . . ; b * Mi ]</formula><p>, and each indicator vector b * mi indicates which one (and only one) of the K codewords in the mth codebook is used to approximate the ith data point z * i . To enable knowledge sharing across the anchors, positive and negative samples in the triplets, we propose a triplet quantization approach by sharing the codebooks {C * m = C m } M m=1 across different samples in all triplets.</p><p>To mitigate the degeneration issue of K-means, we further propose a weak orthogonality penalty across the M codebooks, which potentially reduces the redundancy of the multiple codebooks and improves the compactness of the binary codes. The proposed triplet quantization model with weak-orthogonal constraint is defined as</p><formula xml:id="formula_7">Q = N t i=1 * ∈ {a,p,n } z * i − M m=1 C m b * mi 2 2 + γ M m=1 M m ′ =1 C T m C m ′ − I 2 F (3) where b * mi 0 = 1, b * mi ∈ {0, 1} K ,</formula><p>∥·∥ 0 is the ℓ 0 -norm that simply counts the number of the vector's nonzero elements, and γ is the hyper-parameter that controls the degree of orthogonality. The ℓ 0 constraint guarantees that only one codeword in each codebook can be activated to approximate the input data, which leads to compact binary codes. The underlying reason of using M codebooks instead of single codebook to approximate each input data point is to further minimize the quantization error, while single codebook yields significantly lossy compression and large performance drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Deep Triplet Quantization</head><p>We enable efficient and effective image retrieval in an end-to-end architecture by integrating the triplet training procedure (1), triplet selection module (2) and the weak-orthogonal quantization (3) in a unified deep triplet quantization (DTQ) model as</p><formula xml:id="formula_8">min Θ,C, B * L + λQ,<label>(4)</label></formula><p>where λ &gt; 0 is a hyper-parameter between the triplet loss L and the triplet quantization loss Q, and Θ denotes the set of learnable parameters of the deep network. Through joint optimization problem (4), we can learn the binary codes by jointly preserving the similarity via triplet learning procedure and controlling the quantization error of binarizing continuous representations to compact binary codes. A notable advantage of joint optimization is that we can improve the quantizability of the learned deep representations {z * i } such that they can be quantized more effectively by our weak-orthogonal quantizer (3), yielding more accurate binary codes.</p><p>Approximate nearest neighbor (ANN) search by maximum innerproduct similarity is a powerful tool for quantization methods <ref type="bibr" target="#b6">[7]</ref>. Given a database of N binary codes {b n } N n=1 , we follow <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> to adopt Asymmetric Quantizer Distance (AQD) as the metric, which computes the inner-product similarity between a given query q and the reconstruction of the database point x n as</p><formula xml:id="formula_9">AQD (q, x n ) = z T q M m=1 C m b mn ,<label>(5)</label></formula><p>Given query q and the deep representation z q , these inner-products between z q and all M codebooks {C m } M m=1 and all K possible values of b mn can be pre-computed and stored in a query-specific M × K lookup table, which is used to compute AQD between the query and all database points, each entails M table lookups and additions and is slightly more costly than computing the Hamming distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learning Algorithm</head><p>The DTQ optimization problem in Equation <ref type="formula" target="#formula_8">(4)</ref>   Θ, shared codebook C = [C 1 , . . . , C M ], and binary codes B * . We adopt an alternating optimization paradigm <ref type="bibr" target="#b25">[26]</ref> which iteratively updates one variable with the remaining variables fixed.</p><formula xml:id="formula_10">д = 0 to N /|G | do foreach x a , x p ∈ G д , s.t s ap = 1 do foreach x n ∈ G д , s.t s an = 0 do if δ − ∥z a − z n ∥ 2 2 + z a − z p 2 2 &gt; 0 then // Triplet &lt; x a , x p , x n &gt; is hard T ap ← T ap ∪ {&lt; x a , x p , x n &gt;} end end //</formula><p>Learning Θ. The network parameters Θ can be efficiently optimized via standard back-propagation (BP) algorithm. We adopt the automatic differentiation techniques in TensorFlow.</p><p>Learning C. We update codebook C by rewriting Equation <ref type="formula" target="#formula_8">(4)</ref> with C as the unknown variables in matrix formulation as follows,</p><formula xml:id="formula_11">min C * ∈ {a,p,n } Z * − CB * 2 F + γ C T C − I 2 F .<label>(6)</label></formula><p>We adopt the gradient descent to update C, C ← C − η Learning B. As each b * i is independent on the rest of {b * i ′ } i ′ i , the optimization for B * can be decomposed to 3N t subproblems,</p><formula xml:id="formula_12">min b * i z * i − M m=1 C m b * mi 2 s.t. b * mi 0 = 1, b * mi ∈ {0, 1} K .<label>(8)</label></formula><p>This is essentially a high-order Markov Random Field (MRF) problem. As the MRF problem is generally NP-hard, we resort to the Iterated Conditional Modes (ICM) algorithm <ref type="bibr" target="#b39">[40]</ref> that solves M indicators {b * mi } M m=1 alternatively. Specifically, given {b * m ′ i } m ′ m fixed, we update b * mi by exhaustively checking all the codewords in the codebook C m , finding the specific codeword with mimimal objective in <ref type="bibr" target="#b7">(8)</ref>, and setting the corresponding entry of b * mi as 1 and the rest as 0. The ICM algorithm is guaranteed to converge to local minima, and can be terminated if maximum iteration is reached. And the training procedure of DTQ is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We conduct extensive experiments to evaluate the efficacy of the proposed DTQ approach against several state-of-the-art shallow and deep hashing methods on three image retrieval benchmark datasets, NUS-WIDE, CIFAR-10, and MS-COCO. Project codes and detailed configurations will be available at https://github.com/thuml.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>The evaluation is conducted on three widely used image retrieval benchmark xdatasets: NUS-WIDE, CIFAR-10, and MS-COCO.</p><p>NUS-WIDE 1 <ref type="bibr" target="#b3">[4]</ref> is a public image dataset which contains 269,648 images in 81 ground truth categories. We follow similar experimental protocols in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, and randomly sample 5,000 images as query points, with the remaining images used as the database and randomly sample 10,000 images from the database for training.</p><p>CIFAR-10 2 is a public dataset with 60,000 tiny images in 10 classes. We follow the protocol in <ref type="bibr" target="#b1">[2]</ref> to randomly select 100 images per class as the query set, 500 images per class for training, and the rest images as the database.</p><p>MS-COCO 3 <ref type="bibr" target="#b20">[21]</ref> is a dataset for image recognition, segmentation and captioning. The current release contains 82,783 training images and 40,504 validation images, where each image is labeled by some of the 80 semantic concepts. We randomly sample 5,000 images as the query points, with the rest used as the database, and randomly sample 10,000 images from the database for training.</p><p>Following standard evaluation protocol as previous work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42]</ref>, the similarity information for hash function learning and for ground-truth evaluation is constructed from image labels: if two images i and j share at least one label, they are similar and s i j = 1, otherwise they are dissimilar and s i j = 0. Though we use the ground truth image labels to construct the similarity information, the proposed DTQ can learn compact binary codes when only the similarity information is available, more general than label-based hashing and quantization methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>We compare the retrieval performance of DTQ with ten stateof-the-art hashing methods, including supervised shallow hashing methods BRE <ref type="bibr" target="#b17">[18]</ref>, ITQ-CCA <ref type="bibr" target="#b12">[13]</ref>, KSH <ref type="bibr" target="#b22">[23]</ref>, SDH <ref type="bibr" target="#b29">[30]</ref> and supervised deep hashing methods CNNH <ref type="bibr" target="#b36">[37]</ref>, DNNH <ref type="bibr" target="#b18">[19]</ref>, DHN <ref type="bibr" target="#b41">[42]</ref>, DQN <ref type="bibr" target="#b1">[2]</ref>, HashNet <ref type="bibr" target="#b2">[3]</ref>, DVSQ <ref type="bibr" target="#b0">[1]</ref>. We evaluate retrieval quality based on three standard evaluation metrics: Mean Average Precision (MAP), Precision-Recall curves (PR), and Precision curves with respect to the numbers of top returned samples (P@N). To enable a direct comparison to the published results, all methods use identical training and test sets. We follow <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> and adopt MAP@5000 for NUS-WIDE dataset, MAP@5000 for MS-COCO dataset, and MAP@54000 for CIFAR-10 dataset.</p><p>Our implementation of DTQ is based on TensorFlow. For shallow hashing methods, we use as image features the 4096-dimensional DeCAF 7 features <ref type="bibr" target="#b5">[6]</ref>. For deep hashing methods, we use as input the original images, and adopt AlexNet <ref type="bibr" target="#b16">[17]</ref> as the backbone architecture. We fine-tune layers conv1-fc7 copied from the AlexNet model pre-trained on ImageNet and train the last hash layer via back-propagation. As the last layer is trained from scratch, we set its learning rate to be 10 times that of the lower layers. We use mini-batch stochastic gradient descent (SGD) with 0.9 momentum as the solver, and cross-validate the learning rate from 10 −5 to 10 −2 with a multiplicative step-size 10 1 2 . We fix K = 256 codewords for each codebook as <ref type="bibr" target="#b0">[1]</ref>. For each point, the binary code for all M codebooks requires B = M log 2 K = 8M bits (i.e. M bytes), where we set M = B/8 as B is a hyper-parameters. We fix the mini-batch size of triplets as 128 in each iteration and set the initial number of groups as |G | = 200 for NUS-WIDE and MS-COCO, and |G | = 10 for CIFAR-10. We select the hyper-parameters of the proposed method DTQ and all comparison methods using the three-fold cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>The MAP results of all methods are listed in <ref type="table" target="#tab_2">Table 1</ref>, showing that the proposed DTQ substantially outperforms all the comparison methods. Specifically, compared to SDH <ref type="bibr" target="#b29">[30]</ref>, the best shallow hashing method with deep features as input, DTQ achieves absolute increases of 11.1%, 33.0% and 20.7% in the average MAP on NUS-WIDE, CIFAR-10, and MS-COCO respectively. Compared to DVSQ <ref type="bibr" target="#b0">[1]</ref>, the state-of-the-art deep quantization method with class labels as supervised information, DTQ outperforms DVSQ by large margins of 0.8%, 6.2% and 4.9% in average MAP on the three datasets, NUS-WIDE, CIFAR-10, and MS-COCO, respectively.</p><p>The MAP results reveal several interesting insights. 1) Shallow hashing methods cannot learn discriminative deep representations and hash codes through end-to-end framework, which explains the fact that they are surpassed by deep hashing methods. 2) Deep quantization methods DQN and DVSQ learn less lossy binary codes by jointly preserving similarity information and controlling the quantization error, significantly outperforming pioneering methods CNNH and DNNH without reducing the quantization error.</p><p>The proposed DTQ improves substantially from the state-ofthe-art DVSQ by three important perspectives: 1) DTQ introduces a novel triplet training strategy to deep quantization framework for efficient similarity retrieval. It is worth noting that DTQ can learn compact binary codes when only the similarity information is available, which is more general than the label-based hashing   <ref type="figure" target="#fig_3">Figures 2 and 3</ref>, respectively. These metrics are widely used in deploying practical systems. The proposed DTQ significantly outperforms all the comparison methods by large margins under these two evaluation metrics. In particular, DTQ achieves much higher precision at lower recall levels or smaller number of top samples than all compared baselines. This is very desirable for precision-oriented retrieval, where people count more on the top-N returned results with a small N . This justifies the value of our model for practical retrieval systems.  <ref type="formula" target="#formula_2">(1)</ref> with the widely-used pairwise cross-entropy loss <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">42]</ref>; 2) DTQ-H is the DTQ variant without Group Hard to mine appropriate amount of good triplets for each epoch during the learning of the triplet loss as <ref type="bibr" target="#b18">[19]</ref>; 3) DTQ-2 is the two-step variant of DTQ which first learns the deep representations for all images and then generates compact binary codes via the weak-orthogonal quantization. 4) DTQ-Q is the DTQ variant which replaces the proposed Triplet Quantization to the Product Quantization <ref type="bibr" target="#b15">[16]</ref> used in DQN <ref type="bibr" target="#b1">[2]</ref>. 5) DTQ-O is the DTQ variant by removing the weak orthogonality penalty for redundancy reduction, i.e. γ = 0.</p><p>The MAP results for DTQ and it's five variants with respect to different code lengths on three benchmark datasets, NUS-WIDE, CIFAR-10, and MS-COCO are reported in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>Triplet Loss. DTQ outperforms DTQ-T by very large margins of 7.4%, 11.8% and 3.8% in the average MAP on the three datasets, NUS-WIDE, CIFAR-10, and MS-COCO, respectively. DTQ-T uses the widely-used pairwise cross-entropy loss <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">42]</ref> which achieves state-of-the-art results on previous similarity retrieval tasks. It is worth noting that the triplet loss is a learning to rank method, and tries to bring the anchor and the positive samples closer while also pushing away the negative samples. The DTQ with triplet loss is actually more suitable for the similarity retrieval tasks and naturally gives rise to much better performance than DTQ-T.</p><p>Quantizability. Another observation is that by jointly preserving similarity information in the deep representations of image triplets as well as controlling the quantization error of compact binary codes, DTQ outperforms DTQ-2 by 3.9%, 6.4% and 3.4% in the average MAP on the three datasets, NUS-WIDE, CIFAR-10, and MS-COCO. This shows that end-to-end quantization can improve the quantizability of deep feature representations and satisfactorily yield much more accurate retrieval results.</p><p>Triplet Quantization. After replacing the proposed Triplet Quantization to Product Quantization <ref type="bibr" target="#b15">[16]</ref> used in DQN <ref type="bibr" target="#b1">[2]</ref>, DTQ-Q yields significantly lossy compression and incur remarkable performance drop of 2.3%, 2.9%, 3.2% in the average MAP on the three datasets, NUS-WIDE, CIFAR-10, and MS-COCO datasets respectively. This proves that the proposed Triplet Quantization with weak orthogonality can effectively learn compact binary codes and enable more effective retrieval than Product Quantization.</p><p>Weak-Orthogonal Quantization. Finally, by removing the weak orthogonality penalty, DTQ-O incurs performance drop of 1.3%, 1.2%, 1.4% in the average MAP on the three datasets, NUS-WIDE, CIFAR-10, and MS-COCO datasets respectively. This proves the importance of removing the codebook redundancy and improving the compactness of binary codes for efficient image retrieval.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Triplet</head><p>Selection. By using the proposed triplet mining strategy, Group Hard, DTQ outperforms DTQ-H by large margins of 3.8%, 4.0% and 4.4% in the average MAP on three benchmark datasets, NUS-WIDE, CIFAR-10, and MS-COCO, respectively. As shown in <ref type="figure" target="#fig_6">Figure 4</ref>, without mining the appropriate amount of hard triplets, the Group All training of triplet loss will quickly stagnate, leading to suboptimal convergence quality and MAP results. The proposed triplet mining strategy, Group Hard, randomly samples proper amount of useful triplets with hard examples from several randomly partitioned group, resulting in effective training and faster convergence as well as more accurate retrieval performance.  <ref type="table" target="#tab_4">Table 3</ref>. Due to the low ratio of the valid hard triplets in each batch for triplet training, DTQ-online (with online triplet selection) fails to achieve satisfactory retrieval results compared with the proposed DTQ. As online triplet selection cannot achieve satisfactory results, we adopt offline triplet selection, which selects the valid hard triplets at the beginning of each epoch. However, the offline strategy may generate too many candidate triplets and need a huge number of batches per epoch, leading to hard triplets outdated for training and potentially wasting most batches of each epoch. To alleviate the outdated effect of hard triplets in offline selection, we split the data into specific groups and select hard triplets within each group, reducing the training triplets from |T | to |T |/|G |.</p><p>We conduct an experiment to count the number of outdated hard triplets during training, shown in <ref type="figure" target="#fig_8">Figure 6</ref>. By splitting training data into |G | specific groups, the number of outdated hard triplets is significantly reduced, leading to much better MAP results than the original offline triplet selection (i.e. |G | = 1). This validates the effectiveness of the proposed offline selection strategy, Group Hard. Visualization of Representations. <ref type="figure" target="#fig_7">Figure 5</ref> shows the t-SNE visualizations <ref type="bibr" target="#b30">[31]</ref> of the deep representations learned by DVSQ <ref type="bibr" target="#b0">[1]</ref>, DTQ-2, and DTQ on CIFAR-10 dataset. The deep representations of the proposed DTQ exhibit clear discriminative structures with data points in different categories well separated, while the deep representations by DVSQ <ref type="bibr" target="#b0">[1]</ref> exhibit relative vague structures. This validates that by introducing the triplet training to deep quantization, the deep representations generated by our DTQ are more discriminative than that generated by DVSQ, enabling more accurate image retrieval. Also, the deep representations of DTQ are more discriminative than that of the two-step variant DTQ-2, showing the efficacy of jointly preserving similarity information in the deep representations of image triplets and controlling the quantization error of compact binary codes via back-propagation.  <ref type="figure" target="#fig_9">Figure 7</ref> illustrates the top 10 returned images of DTQ and the best deep hashing baseline DVSQ <ref type="bibr" target="#b0">[1]</ref> for three query images on the three datasets NUS-WIDE, CIFAR-10, and MS-COCO, respectively. DTQ yields much more relevant and user-desired retrieval results than the state-of-the-art method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This paper proposed Deep Triplet Quantization (DTQ) for efficient image retrieval, which introduces a triplet training strategy to deep quantization framework. Through a novel triplet selection module, Group Hard, an appropriate number of hard triplets are selected for effective triplet training and faster convergence. To enable efficient image retrieval, DTQ can learn compact binary codes by jointly optimizing a novel triplet quantization loss with weak orthogonality. Comprehensive experiments justify that DTQ generates compact binary encoding and yields state-of-the-art retrieval performance on three benchmark datasets NUS-WIDE, CIFAR-10, and MS-COCO.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The proposed Deep Triplet Quantization (DTQ) model consists of four main components: 1) a novel triplet selection module, Group Hard, to mine good triplets for effective triplet training and faster convergence; 2) a standard deep convolutional neural network (CNN), e.g. AlexNet, VGG or ResNet, for learning deep representations; 3) a well-specified triplet loss for pulling together similar pairs and pushing away dissimilar pairs; and 4) a novel triplet quantization loss with weak orthogonality constraint for converting the deep representations of different samples (the anchor, positive and negative samples) in the triplets into B-bit compact binary codes and controlling the quantizability of the deep representations. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Randomly choose a hard negative sample from T ap T ← T ∪ rand(T ap ) end end for i = 0 to |T |/BATCH_SIZE do Train the model using the i-th batch of triplets end Update C and B with Eqn. (7) and Eqn. (8) respectively if |T | &lt; MIN_TRIPLETS and |G | &gt; 1 then // Halve the size of the groups The trained deep neural networks of DTQ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>= 2 *</head><label>2</label><figDesc>∈ {a,p,n } CB * B * T −2 * ∈ {a,p,n } Z * B * T +2γC 2C T C − I(<ref type="bibr" target="#b6">7)</ref> where η is a learning rate. We can further speed up computation by first solving C with γ = 0, which leads to an analytic solution C = * ∈ {a,p,n } Z * B * T * ∈ {a,p,n } B * B * T −1 , then updating C with this solution as the starting point of gradient descent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Precision-recall curves on the NUS-WIDE, CIFAR-10 and MS-COCO datasets with binary codes @ 32 bits. Precision@top-N curves on the NUS-WIDE, CIFAR-10 and MS-COCO datasets with binary codes @ 32 bits. method DVSQ. 2) During the learning of triplet loss, DTQ adopts a novel triplet mining strategy, Group Hard, that mines appropriate amount of good triplets for each epoch, resulting in effective triplet training and better performance. 3) DTQ is the first method to apply weak-orthogonal quantization during triplet training. And backpropagating the triplet quantization loss can remarkably enhance the quantizability of the deep representations.The retrieval performance in terms of Precision-Recall curves (PR) and Precision curves with respect to different numbers of top returned samples (P@N) are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Triplet Loss and MAP curves w.r.t. #iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>The t-SNE visualizations of deep representations learned by DVSQ, DTQ-2, and DTQ on CIFAR-10 dataset respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>(left) mAP and ratio of non-outdated hard triplets w.r.t. iterations; (right) #groups and #triplets w.r.t. #epochs. 4.3.3 Visualization. We show t-SNE visualization of binary codes and the illustration of top 10 returned images for better understanding the impressive performance improvement of DTQ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>The top 10 images returned by DVSQ and DTQ. Illustration of Top 10 Results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>cannot achieve superior results. Hence, how to select good triplets for effective training in deep quantization also remains an open problem.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>consists of three sets of variables: deep convolutional neural network parameters Algorithm 1: Deep Triplet Quantization (DTQ) Training Input: N training images X = {x i } N i=1 ; Input: Similarity pairs S = {s i j } N i, j=1 . for epoch = 0 to MAX_EPOCH do Run the model to update {z i } N i=1 for N training images if epoch == 0 then Initialize B and C via Product Quantization [16] end Split the N training images to N /|G | groups randomly T ← ∅ for group</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Mean Average Precision (MAP) Results for Different Number of Bits on the Three Benchmark Image Datasets bits 24 bits 32 bits 8 bits 16 bits 24 bits 32 bits 8 bits 16 bits 24 bits 32 bits</figDesc><table><row><cell cols="4">NUS-WIDE 0.575 8 bits 16 ITQ-CCA 0.526 Method 0.572</cell><cell>0.594</cell><cell>0.315</cell><cell cols="2">CIFAR-10 0.354 0.371</cell><cell>0.414</cell><cell>0.501</cell><cell cols="2">MS-COCO 0.566 0.563</cell><cell>0.562</cell></row><row><cell>BRE</cell><cell>0.550</cell><cell>0.607</cell><cell>0.605</cell><cell>0.608</cell><cell>0.306</cell><cell>0.370</cell><cell>0.428</cell><cell>0.438</cell><cell>0.535</cell><cell>0.592</cell><cell>0.611</cell><cell>0.622</cell></row><row><cell>KSH</cell><cell>0.618</cell><cell>0.651</cell><cell>0.672</cell><cell>0.682</cell><cell>0.489</cell><cell>0.524</cell><cell>0.534</cell><cell>0.558</cell><cell>0.492</cell><cell>0.521</cell><cell>0.533</cell><cell>0.534</cell></row><row><cell>SDH</cell><cell>0.645</cell><cell>0.688</cell><cell>0.704</cell><cell>0.711</cell><cell>0.356</cell><cell>0.461</cell><cell>0.496</cell><cell>0.520</cell><cell>0.541</cell><cell>0.555</cell><cell>0.560</cell><cell>0.564</cell></row><row><cell>CNNH</cell><cell>0.586</cell><cell>0.609</cell><cell>0.628</cell><cell>0.635</cell><cell>0.461</cell><cell>0.476</cell><cell>0.476</cell><cell>0.472</cell><cell>0.505</cell><cell>0.564</cell><cell>0.569</cell><cell>0.574</cell></row><row><cell>DNNH</cell><cell>0.638</cell><cell>0.652</cell><cell>0.667</cell><cell>0.687</cell><cell>0.525</cell><cell>0.559</cell><cell>0.566</cell><cell>0.558</cell><cell>0.551</cell><cell>0.593</cell><cell>0.601</cell><cell>0.603</cell></row><row><cell>DHN</cell><cell>0.668</cell><cell>0.702</cell><cell>0.713</cell><cell>0.716</cell><cell>0.512</cell><cell>0.568</cell><cell>0.594</cell><cell>0.603</cell><cell>0.607</cell><cell>0.677</cell><cell>0.697</cell><cell>0.701</cell></row><row><cell>HashNet</cell><cell>0.613</cell><cell>0.662</cell><cell>0.687</cell><cell>0.699</cell><cell>0.621</cell><cell>0.643</cell><cell>0.660</cell><cell>0.667</cell><cell>0.625</cell><cell>0.687</cell><cell>0.699</cell><cell>0.718</cell></row><row><cell>DQN</cell><cell>0.721</cell><cell>0.735</cell><cell>0.747</cell><cell>0.752</cell><cell>0.527</cell><cell>0.551</cell><cell>0.558</cell><cell>0.564</cell><cell>0.649</cell><cell>0.653</cell><cell>0.666</cell><cell>0.685</cell></row><row><cell>DVSQ</cell><cell>0.780</cell><cell>0.790</cell><cell>0.792</cell><cell>0.797</cell><cell>0.715</cell><cell>0.727</cell><cell>0.730</cell><cell>0.733</cell><cell>0.704</cell><cell>0.712</cell><cell>0.717</cell><cell>0.720</cell></row><row><cell>DTQ</cell><cell>0.795</cell><cell>0.798</cell><cell>0.799</cell><cell>0.801</cell><cell>0.785</cell><cell>0.789</cell><cell>0.790</cell><cell>0.792</cell><cell>0.758</cell><cell>0.760</cell><cell>0.764</cell><cell>0.767</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Mean Average Precision (MAP) Results of DTQ and Its Variants DTQ-H, DTQ-T, DTQ-2, and DTQ bits 24 bits 32 bits 8 bits 16 bits 24 bits 32 bits 8 bits 16 bits 24 bits 32 bits</figDesc><table><row><cell cols="4">NUS-WIDE 0.758 8 bits 16 DTQ-H Method 0.753 0.763</cell><cell>0.769</cell><cell>0.741</cell><cell cols="2">CIFAR-10 0.747 0.751</cell><cell>0.754</cell><cell>0.708</cell><cell cols="2">MS-COCO 0.714 0.722</cell><cell>0.729</cell></row><row><cell>DTQ-T</cell><cell>0.719</cell><cell>0.722</cell><cell>0.727</cell><cell>0.731</cell><cell>0.663</cell><cell>0.670</cell><cell>0.672</cell><cell>0.679</cell><cell>0.714</cell><cell>0.720</cell><cell>0.728</cell><cell>0.734</cell></row><row><cell>DTQ-2</cell><cell>0.752</cell><cell>0.757</cell><cell>0.761</cell><cell>0.768</cell><cell>0.718</cell><cell>0.722</cell><cell>0.726</cell><cell>0.731</cell><cell>0.717</cell><cell>0.725</cell><cell>0.733</cell><cell>0.739</cell></row><row><cell>DTQ-Q</cell><cell>0.769</cell><cell>0.773</cell><cell>0.777</cell><cell>0.781</cell><cell>0.750</cell><cell>0.761</cell><cell>0.763</cell><cell>0.765</cell><cell>0.721</cell><cell>0.727</cell><cell>0.734</cell><cell>0.740</cell></row><row><cell>DTQ-O</cell><cell>0.785</cell><cell>0.787</cell><cell>0.780</cell><cell>0.788</cell><cell>0.771</cell><cell>0.777</cell><cell>0.779</cell><cell>0.781</cell><cell>0.739</cell><cell>0.745</cell><cell>0.750</cell><cell>0.758</cell></row><row><cell>DTQ</cell><cell>0.795</cell><cell>0.798</cell><cell>0.799</cell><cell>0.801</cell><cell>0.785</cell><cell>0.789</cell><cell>0.790</cell><cell>0.792</cell><cell>0.758</cell><cell>0.760</cell><cell>0.764</cell><cell>0.767</cell></row><row><cell>4.3 Analysis</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">4.3.1 Ablation Study. We investigate five variants of DTQ: 1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">DTQ-T is the DTQ variant by replacing the triplet loss in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>MAP on CIFAR-10 for Different Number of Bits Selecting all batch samples as negative is also known as online triplet selection in the literature. Here we conduct a new experiment which uses online triplet selection and selects all hard negative samples in a batch (samples per batch = 192) for each anchor-positive pair. The results are reported in</figDesc><table><row><cell>Method</cell><cell>8 bits</cell><cell>16 bits</cell><cell>24 bits</cell><cell>32 bits</cell></row><row><cell>DTQ-online</cell><cell>0.703</cell><cell>0.708</cell><cell>0.710</cell><cell>0.713</cell></row><row><cell>DTQ</cell><cell>0.785</cell><cell>0.789</cell><cell>0.790</cell><cell>0.792</cell></row><row><cell>Online Selection.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm 2 http://www.cs.toronto.edu/kriz/cifar.html 3 http://mscoco.org</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGEMENTS</head><p>This work is supported by National Key R&amp;D Program of China (2016YFB1000701), and NSFC grants (61772299, 61672313, 71690231).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep visualsemantic quantization for efficient image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep Quantization Network for Efficient Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingfu</forename><surname>Wen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">HashNet: Deep Learning to Hash by Continuation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">NUS-WIDE: A Real-World Web Image Database from National University of Singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tat-Seng Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Tao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<editor>ICMR. ACM</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to hash with binary deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Toan</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh-Dzung</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai-Man</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Inner Product Similarity Search using Compositional Codes. CoRR abs/1406</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">4966</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep Hashing for Compact Binary Codes Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fast search in Hamming space with multi-index hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Punjani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<editor>CVPR. CVPR</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiezheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifa</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optimized Product Quantization. TPAMI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristides</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning binary codes for high-dimensional data using bilinear projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudhakar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="484" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Iterative quantization: A procrustean approach to learning binary codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SUBIC: A supervised, structured binary code for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><surname>Zepeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Product Quantization for Nearest Neighbor Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to hash with binary reconstructive embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1042" to="1050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Simultaneous Feature Learning and Hash Coding with Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjiang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<editor>CVPR. IEEE</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature learning based deep supervised hashing with pairwise labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu-Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep supervised hashing for fast image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haomiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2064" to="2072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Supervised hashing with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<editor>CVPR. IEEE</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Hashing with Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<editor>ICML. ACM</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Hash bit selection: a unified solution for selection problems in hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<editor>CVPR. IEEE</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Composite Correlation Quantization for Efficient Multimodal Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Minimal loss hashing for compact binary codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. ACM</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hamming distance metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1061" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning a nonlinear embedding by preserving class neighbourhood structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="412" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Supervised Discrete Hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
		<editor>CVPR. IEEE</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing High-Dimensional Data Using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semi-supervised hashing for large-scale search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2393" to="2406" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optimized Cartesian K-Means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Shun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Heng Tao Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2014.2324592</idno>
		<ptr target="https://doi.org/10.1109/TKDE.2014.2324592" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="180" to="192" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Survey on Learning to Hash</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="769" to="790" />
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Supervised quantization for similarity search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<title level="m">Spectral Hashing. In NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Supervised hashing for image retrieval via image representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongkai</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjiang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Circulant binary embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. ACM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Supervised hashing with latent factor models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu-Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR. ACM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Composite Quantization for Approximate Nearest Neighbor Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<editor>ICML. ACM</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Deep Hashing Network for Efficient Similarity Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
