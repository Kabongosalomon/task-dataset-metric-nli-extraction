<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UniDrop: A Simple yet Effective Technique to Improve Transformer without Extra Cost</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
							<email>lijun.wu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Meng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
							<email>yingce.xia@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shufang</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
							<email>taoqin@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Dai</surname></persName>
							<email>daixinyu@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
							<email>tyliu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">UniDrop: A Simple yet Effective Technique to Improve Transformer without Extra Cost</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer architecture achieves great success in abundant natural language processing tasks. The over-parameterization of the Transformer model has motivated plenty of works to alleviate its overfitting for superior performances. With some explorations, we find simple techniques such as dropout, can greatly boost model performance with a careful design. Therefore, in this paper, we integrate different dropout techniques into the training of Transformer models. Specifically, we propose an approach named UniDrop to unite three different dropout techniques from fine-grain to coarse-grain, i.e., feature dropout, structure dropout, and data dropout. Theoretically, we demonstrate that these three dropouts play different roles from regularization perspectives. Empirically, we conduct experiments on both neural machine translation and text classification benchmark datasets. Extensive results indicate that Transformer with UniDrop can achieve around 1.5 BLEU improvement on IWSLT14 translation tasks, and better accuracy for the classification even using strong pre-trained RoBERTa as backbone.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, Transformer <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref> has been the dominant structure in natural language processing (NLP), such as neural machine translation <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref>, language modeling <ref type="bibr" target="#b4">(Dai et al., 2019)</ref> and text classification <ref type="bibr" target="#b6">(Devlin et al., 2019;</ref>. To further improve the model performance, there has been much effort in designing better architectures or introducing external knowledge into Transformer models <ref type="bibr" target="#b26">Lu et al., 2019;</ref><ref type="bibr" target="#b21">Kitaev et al., 2020;</ref><ref type="bibr" target="#b0">Ahmed et al., 2017;</ref><ref type="bibr" target="#b12">Hashemi et al., 2020)</ref>, which increases computational costs or requires extra resources.</p><p>Despite the effectiveness of above strategies, the over-parameterization and overfitting is still a crucial problem for Transformer. Regularization methods such as weight decay <ref type="bibr" target="#b22">(Krogh and Hertz, 1992)</ref>, data augmentation <ref type="bibr" target="#b32">(Sennrich et al., 2016a)</ref>, dropout <ref type="bibr" target="#b34">(Srivastava et al., 2014)</ref>, parameter sharing <ref type="bibr" target="#b5">(Dehghani et al., 2018;</ref> are all widely adopted to address overfitting. Among these regularization approaches, dropout <ref type="bibr" target="#b34">(Srivastava et al., 2014)</ref>, which randomly drops out some hidden units during training, is the most popular one and various dropout techniques have been proposed for Transformer. For example, <ref type="bibr" target="#b7">Fan et al. (2020a)</ref> propose LayerDrop, a random structured dropout, to drop certain layers of Transformer during training.  alternatively propose DropHead as a structured dropout method for regularizing the multi-head attention mechanism. Both of them achieved promising performances. One great advantage of dropout is that it is free of additional computational costs and resource requirements. Hence we ask one question: can we achieve stronger or even state-of-the-art (SOTA) results only relying on various dropout techniques instead of extra model architecture design or knowledge enhancement?</p><p>To this end, in this paper, we propose UniDrop to integrate three different-level dropout techniques from fine-grain to coarse-grain, feature dropout, structure dropout, and data dropout, into Transformer models. Feature dropout is the conventional dropout <ref type="bibr" target="#b34">(Srivastava et al., 2014</ref>) that we introduced before, which is widely applied on hidden representations of networks. Structure dropout is a coarse-grained control and aims to randomly drop some entire substructures or components from the whole model. In this work, we adopt the aforementioned LayerDrop <ref type="bibr" target="#b7">(Fan et al., 2020a)</ref> as our structure dropout. Different from the previous two dropout methods, data dropout <ref type="bibr" target="#b17">(Iyyer et al., 2015)</ref> is performed on the input data level, which serves as a data augmentation method by randomly dropping out some tokens in an input sequence.  We first theoretically analyze different regularization roles played by the three dropout techniques, and we show they can improve the generalization ability from different aspects. Then, we provide empirical evaluations of the UniDrop approach. We conduct experiments on neural machine translation with 8 translation datasets, and text classification task with 8 benchmark datasets. On both sequence generation and classification tasks, experimental results show that the three dropouts in UniDrop can jointly improve the performance of Transformer.</p><p>The contributions of this paper can be summarized as follows:</p><p>• We introduce UniDrop, which unites three different dropout techniques into a robust one for Transformer, to jointly improve the performance of Transformer without additional computational cost and prior knowledge.</p><p>• We theoretically demonstrate that the three dropouts, i.e., feature dropout, structure dropout, and data dropout play different roles in preventing Transformer from overfitting and improving the robustness of the model.</p><p>• Extensive results indicate that Transformer models with UniDrop can achieve strong or even SOTA performances on sequence generation and classification tasks. Specifically, around 1.5 BLEU improvement on IWSLT14 translation tasks, and better accuracy for classification even using strong pre-trained model RoBERTa as backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Feature dropout (FD) and structure dropout (SD) are highly coupled with model architecture. Therefore, we briefly recap Transformer and refer the readers to <ref type="bibr" target="#b37">Vaswani et al. (2017)</ref> for details. As shown in <ref type="figure" target="#fig_1">Figure 1a</ref>, Transformer is stacked by several identical blocks, and each block contains two sub-layers, which are multi-head selfattention layer and position-wise fully connected feed-forward layer. Each sub-layer is followed by an AddNorm operation that is a residual connection Add <ref type="bibr" target="#b13">(He et al., 2016)</ref> and a layer normalization LN <ref type="bibr">(Ba et al., 2016)</ref>. Multi-head Attention sub-layer consists of multiple parallel attention heads, and each head maps the query Q and a set of key-value pairs K, V to an output through a scale dot-product attention:</p><formula xml:id="formula_0">Attn(Q, K, V) = softmax( QK √ d k )V,<label>(1)</label></formula><p>where d k is the dimension of query and key, and</p><formula xml:id="formula_1">1 √ d k</formula><p>is a scaling factor. The outputs of these heads are then concatenated and projected again to result in the final values. Position-wise Feed-Forward sub-layer applies two linear transformations with an inner ReLU <ref type="bibr" target="#b29">(Nair and Hinton, 2010)</ref> activation:</p><formula xml:id="formula_2">FFN(x) = max(0, xW1 + b1)W2 + b2,<label>(2)</label></formula><p>where W and b are parameters. The output of each sub-layer is then followed with AddNorm: AddNorm(x) = LN(Add(x)).</p><p>In this section, we first introduce the details of the three different levels of dropout techniques we study, feature dropout, structure dropout and data dropout. Then we provide the theoretical analysis of these dropout methods on the regularization perspectives. Finally, we present our proposed UniDrop approach for training Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Dropout</head><p>The feature dropout (FD), as a well-known regularization method, is proposed by <ref type="bibr" target="#b34">Srivastava et al. (2014)</ref>, which is to randomly suppress neurons of neural networks during training by setting them to 0 with a pre-defined probability p.</p><p>In practice, dropout is applied to the output of each sub-layer by default. Besides, Transformer also contains two specific feature dropouts for multi-head attention and activation layer of feedforward network. In this work, we also explore their effects on the performance of Transformer.</p><p>• FD-1 (attention dropout): according to Equation (1), we can obtain attention weight matrix A = QK towards value sequence V. Our FD-1 is applied to the attention weight A.</p><p>• FD-2 (activation dropout): FD-2 is employed after the activation function between the two linear transformations of FFN sub-layer.</p><p>In addition to the above FDs for Transformer, we still find the risk of overfitting in pre-experiments. Therefore, we further introduce another two feature dropouts into the model architecture:</p><p>• FD-3 (query, key, value dropout): FD-1 is used to improve generalization of multi-head attention. However, it is directly applied to the attention weights A, where drop value A(i, j) means ignore the relation between token i and token j, thus a larger FD-1 means a larger risk of losing some critical information from sequence positions. To alleviate this potential risk, we add dropout to query, key, and value before the calculation of attention.</p><p>• FD-4 (output dropout): we also apply dropout to the output features before linear transformation for softmax classification. Specifically, when dealing with sequence-to-sequence tasks such as machine translation, we add FD-4 to the output features of the last layer in the Transformer decoder, otherwise the last layer of the Transformer encoder.</p><p>The positions of each feature dropout applied in Transformer 1 are shown in <ref type="figure" target="#fig_1">Figure 1b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Structure Dropout</head><p>There are three structure dropouts, respectively LayerDrop <ref type="bibr" target="#b7">(Fan et al., 2020a)</ref>, DropHead  and HeadMask <ref type="bibr" target="#b35">(Sun et al., 2020)</ref>, which are specifically designed for Transformer.</p><p>Some recent studies <ref type="bibr" target="#b38">(Voita et al., 2019;</ref><ref type="bibr" target="#b28">Michel et al., 2019)</ref> show multi-head attention mechanism is dominated by a small portion of attention heads. To prevent domination and excessive coadaptation between different attention heads,  and <ref type="bibr" target="#b35">Sun et al. (2020)</ref> respectively propose structured DropHead and HeadMask that drop certain entire heads during training. In contrast, LayerDrop <ref type="bibr" target="#b7">(Fan et al., 2020a)</ref> is a higher-level and coarser-grained structure dropout. It drops some entire layers at training time and directly reduces the Transformer model size.</p><p>In this work, we adopt LayerDrop as the structure dropout to incorporate it into our UniDrop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data Dropout</head><p>Data dropout aims to randomly remove some words in the sentence with a pre-defined probability. It is often used as a data augmentation technique <ref type="bibr" target="#b43">(Wei and Zou, 2019;</ref>. However, directly applying vanilla data dropout is hard to keep the original sequence for training, which leads to the risk of losing high-quality training samples. To address this issue, we propose a twostage data dropout strategy. Specifically, given a sequence, with probability p k (a hyperparameter lies in (0, 1)), we keep the original sequence and do not apply data dropout. If data dropout is applied, for each token, with another probability p (another hyperparameter lies in (0, 1)), we will drop the token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Theoretical Analysis</head><p>In this section, we provide theoretical analysis for feature dropout, structure dropout and data dropout, to show their different regularization effects. We first re-formulate the three dropout methods. For some probability p and layer representation h ∈ R d (i.e., h is the vector of outputs of some layer), we randomly sample a scaling vector ξ ∈ R d with each independent coordinate as follows:</p><formula xml:id="formula_3">ξ i = −1 with probability p p 1 − p with probability 1-p.</formula><p>(3)</p><formula xml:id="formula_4">Here, i indexes a coordinate of ξ, i ∈ [1, ..., d].</formula><p>Then feature dropout can be applied by computing</p><formula xml:id="formula_5">h f d = (1 + ξ) h,</formula><p>where denotes element-wised product and 1 = (1, 1, · · · , 1) . We use F (h f d (x)) to denote the output of a model after dropping feature from a hidden layer and L to denote the loss function. Similar to , we apply Taylor expansion to L and take expectation to ξ:</p><formula xml:id="formula_6">E ξ L(F (h f d (x))) = E ξ L(F ((1 + ξ) h(x))) ≈ L(F (h(x)) + 1 2 E ξ (ξ h(x)) T D 2 h L(x)(ξ h(x)) = L(F (h(x)) + p 2(1 − p) d j=1 D 2 h j ,h j L(x) · hj(x) 2 ,<label>(4)</label></formula><p>where D 2 h L is the Hessian matrix of loss with respect to hidden output h and</p><formula xml:id="formula_7">D 2 h j ,h j L(x) is the j-th diagonal element of D 2 h L. Expect the orig- inal loss L(F (h(x))</formula><p>), the above formula shows that feature dropout implicitly regularize the term d j=1 D 2 h j ,h j L(x) · h j (x) 2 , which relates to the trace of the Hessian.</p><p>For structure dropout, we use a 1-dim random scalar η ∈ R whose distribution is: η = −1 with probability p, and η = 0 with probability 1−p. The structure dropout is similarly applied by computing</p><formula xml:id="formula_8">h sd = (1 + η) · h.</formula><p>For input data x ∈ R m , here x is a sequence of tokens and m is the sequence length, we sample a random scaling vector β ∈ R m with independent random coordinates where each coordinate is identically distributed as η. The input data after drop data becomes</p><formula xml:id="formula_9">x dd = (1 + β) x.</formula><p>Similar to feature dropout, we can obtain that data dropout implicitly optimizes the regularized loss as follows:</p><formula xml:id="formula_10">L(F (h(x))) − p · x T ∇xL(x) + p · m j=1 D 2 x j ,x j L(x) · x 2</formula><p>j , and structure dropout implicitly optimizes the regularized loss:</p><formula xml:id="formula_11">L(F (h(x))) − p · h(x) T ∇ h L(x) + p · m i,j=1 D 2 h i ,h j L(x) · hi(x)hj(x), where D 2 h i ,h j L(x) is the (i, j)-th element in Hes- sian matrix D 2</formula><p>h L. Interpretation From the above analysis, we can conclude that feature dropout, structure dropout and data dropout regularize different terms of the model, and they can not be replaced by each other.</p><p>(1) Because the hidden output will be normalized by layer normalization, the term h(x) T ∇ h L(x) equals to zero according to Lemma 2.4 in <ref type="bibr" target="#b1">Arora et al. (2019)</ref>. Therefore, structure dropout implicitly regularizes the term m i,j=1 D 2 h i ,h j L(x). Hence, structure dropout can regularize the whole elements of Hessian of the model with respect to hidden output, while feature dropout only regularizes the diagonal elements of the Hessian. Thus, integrating structure dropout and feature dropout can regularize every component of Hessian with emphasizing the diagonal elements of the Hessian.</p><p>(2) Since x is also normalized, the term x T ∇ x L(x) equals to zero according to Lemma 2.4 in <ref type="bibr" target="#b1">Arora et al. (2019)</ref>. Different from feature dropout and structure dropout, data dropout regularizes Hessian of loss with respect to input data.</p><p>Regularizing Hessian matrix with respect to both input and hidden output can improve model robustness and hence the generalization ability. We put more details in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">UniDrop Integration</head><p>From the above theoretical analysis, the three dropout techniques are performed in different ways to regularize the training of Transformer, each with unique property to improve the model generalization. Therefore, we introduce UniDrop to take the most of each dropout into Transformer. The overview of UniDrop is presented in <ref type="figure" target="#fig_2">Figure 2</ref>.</p><p>To better view each dropout in a model forward pass, we only show a three layers of architecture in <ref type="figure" target="#fig_2">Figure 2</ref>, and each layer with one specific dropout technique. The data dropout is applied in the input layer by dropping out some word embeddings (e.g., embedding of word t i is dropped). In the middle layer, the feature dropout randomly drops several neurons in each word representations (e.g., the third neurons of word t i−1 is dropped). The last layer is directly dropped out through layer dropout 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct experiments on both sequence generation and classification tasks, specifically, neural machine translation and text classification, to validate the effectiveness of UniDrop for Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Neural Machine Translation</head><p>In this section, we introduce the detailed settings for the neural machine translation tasks and report the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets</head><p>We adopt the widely acknowledged IWSLT14 datasets 3 with multiple language pairs, including English↔German (En↔De), English↔Romanian (En↔Ro), English↔Dutch (En↔Nl), and English↔Portuguese-Brazil (En↔Pt-br), a total number of 8 translation tasks. Each dataset contains about 170k∼190k translation data pairs. The datasets are processed by Moses toolkit 4 and byte-pair-encoding (BPE) <ref type="bibr" target="#b33">(Sennrich et al., 2016b)</ref> is applied to obtain subword units. The detailed statistics of datasets are shown in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Model</head><p>We use the transformer_iwslt_de_en configuration 5 for all Transformer models. Specifically, the encoder and decoder both consist of 6 blocks. The source and target word embeddings are shared for each language pair. The dimensions of embedding and feed-forward sub-layer are respectively set to 512 and 1024, the number of attention heads is 4. The default dropout (not our four feature dropout) rate is 0.3 and weight decay is 0.0001. All models are optimized with Adam (Kingma and <ref type="bibr" target="#b20">Ba, 2015)</ref> and the learning rate schedule is same as in <ref type="bibr" target="#b37">Vaswani et al. (2017)</ref>. The weight of label smoothing <ref type="bibr" target="#b31">(Pereyra et al., 2017</ref>) is set to 0.1.</p><p>For the Transformer models with our UniDrop, we set all feature dropout rates to 0.1. The structure dropout LayerDrop is only applied to the decoder with rate 0.1. For the data dropout, the sequence 2 Except the data dropout is only applied in the input layer, feature/structure dropout can be applied in each layer.</p><p>3 https://wit3.fbk.eu/mt.php?release= 2014-01 4 https://github.com/moses-smt/ mosesdecoder/tree/master/scripts 5 https://github.com/pytorch/fairseq keep rate p k and token dropout rate p are respectively 0.5 and 0.2. The other settings are the same as the configuration of the baseline Transformer. To evaluate the model performance, we use beam search  algorithm to generate the translation results. The beam width is 5 and the length penalty is 1.0. The evaluation metric is the tokenized BLEU <ref type="bibr" target="#b30">(Papineni et al., 2002)</ref> score with multi-bleu.perl script 6 . We repeat each experiment three times with different seeds and report the average BLEU. <ref type="table">Table 1</ref> shows the BLEU results of the Transformer baselines and models with different dropouts. Compared with baselines, we can see that the dropouts FD, SD, or DD all bring some improvements 7 . This observation verifies the existence of overfitting in the Transformer. In contrast, our model Transformer+UniDrop achieves the most improvements across all translation tasks, which demonstrates the effectiveness of UniDrop for the Transformer architecture. To further explore the effects of the three different grained dropouts in UniDrop, we conduct ablation studies and respectively remove the FD, SD, and DD from Transformer+UniDrop. The results in <ref type="table">Table 1</ref> show that three ablated models obtain lower BLEU scores compared to the full model. This observation validates the necessity of them for UniDrop. Among all ablation versions, the Transformer-UniDrop w/o FD obtains the least improvements. It is reasonable because FD actually contains four feature dropouts on different positions, which can effectively prevent Transformer from overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Results</head><p>To show the superiority of UniDrop, we also compare the Transformer+UniDrop with several existing works on the widely acknowledged benchmark IWSLT14 De→En translation. These works improve machine translation from different aspects, such as the training algorithm design <ref type="bibr" target="#b41">(Wang et al., 2019b)</ref>, model architecture design <ref type="bibr" target="#b26">(Lu et al., 2019;</ref> and data augmentation <ref type="bibr" target="#b10">(Gao et al., 2019)</ref>  <ref type="table">Table 1</ref>: Machine translation results of the standard Transformer and our models on various IWSLT14 translation datasets. The "+FD", "+SD", "+DD", and "+UniDrop" denotes applying the feature dropout, structure dropout, data dropout, or UniDrop to the standard Transformer. The "w/o FD", "w/o SD" and "w/o DD" respectively indicate the removal of the feature dropout, structure dropout, or data dropout from the model Transformer+UniDrop.</p><p>Avg. and denote the average results of the 8 translation tasks and improvements compared with the standard Transformer. Best results are in bold.</p><p>Approaches BLEU Adversarial MLE <ref type="bibr" target="#b41">(Wang et al., 2019b)</ref> 35.18 DynamicConv  35.20 Macaron <ref type="bibr" target="#b26">(Lu et al., 2019)</ref> 35.40 IOT <ref type="bibr" target="#b50">(Zhu et al., 2021)</ref> 35.62 Soft Contextual Data Aug <ref type="bibr" target="#b10">(Gao et al., 2019)</ref> 35.78 BERT-fused NMT <ref type="bibr" target="#b51">(Zhu et al., 2020)</ref> 36.11 MAT <ref type="bibr" target="#b8">(Fan et al., 2020b)</ref> 36.22 MixReps+co-teaching  36.41 Transformer 34.84 +UniDrop 36.88  BLEU score. Especially, it surpasses the BERTfused NMT model <ref type="bibr" target="#b51">(Zhu et al., 2020)</ref>, which incorporates the pre-trained language model BERT, by a non-trivial margin. We also show some comparisons on IWSLT14 En→De, Ro→En, and Nl→En translations, the results are shown in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>According to the above results, UniDrop successfully unites the FD, SD, and DD, and finally improves the performance of Transformer on neural machine translation tasks, without any additional computation costs and resource requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Text Classification</head><p>We also conduct experiments on text classification tasks to further demonstrate the effectiveness of UniDrop for the Transformer models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Datasets</head><p>We evaluate different methods on the text classification task based on 8 widely-studied datasets, which can be divided into two groups. The first group is from GLUE tasks <ref type="bibr" target="#b40">(Wang et al., 2019a)</ref>, and they are usually used to evaluate the performance of the large-scale pre-trained language models after fine-tuning. The second group is some typical text classification datasets that are widely used in previous works <ref type="bibr" target="#b39">(Voorhees and Tice, 1999;</ref><ref type="bibr" target="#b27">Maas et al., 2011;</ref><ref type="bibr" target="#b48">Zhang et al., 2015)</ref>. The statistics of all datasets are shown in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Model</head><p>We employ RoBERTa BASE  as the strong baseline and fine-tune it on the text classification datasets. Different from BERT BASE <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref>, RoBERTa BASE is pre-trained with dynamic masking, full-sentences without NSP loss and a larger mini-batches. It has 12 blocks, and the dimensions of embedding and FFN are 768 and 3072, the number of attention heads is 12. When fine-tuning, we set the batch size to 32 and the max epoch to 30. Adam is applied to optimize the models with a learning rate of 1e-5 and a warm-up step ratio of 0.1. We employ the polynomial decay strategy to adjust the learning rate. The default dropout and weight decay are both set to 0.1.</p><p>When adding UniDrop to RoBERTa BASE , we empirically set feature dropout rate and LayerDrop rate to 0.1. For data dropout, the sequence keep rate p k and token dropout rate p are respectively 0.5 and 0.1. The other settings are the same as in the baseline RoBERTa BASE . We use the standard accuracy to evaluate different methods on text classification tasks.    <ref type="bibr" target="#b48">Zhang et al. (2015)</ref> and <ref type="bibr" target="#b3">Conneau et al. (2017)</ref>, DPCNN and ULMFiT are from <ref type="bibr" target="#b19">Johnson and Zhang (2017)</ref> and <ref type="bibr" target="#b15">Howard and Ruder (2018)</ref>. Best results are in bold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>In this section, we use IWSLT14 De→En translation as the analysis task to investigate the capability of UniDrop to avoid overfitting, as well as the effects of different dropout components and dropout rates on UniDrop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overfitting</head><p>To show the superiority of UniDrop to prevent Transformer from overfitting, we compare the dev loss during training of Transformer, Transformer with each dropout technique, Transformer+UniDrop, and ablated models of Transformer+UniDrop. <ref type="figure" target="#fig_3">Figure 3</ref> shows loss curves of different models.</p><p>We can observe that the standard Transformer is quickly overfitted during training, though it is equipped with a default dropout. In contrast, the feature dropout, structure dropout, and data dropout, as well as the combinations of any two dropouts (i.e., ablated models), greatly reduce the risk of overfitting to some extent. Among all compared models, our Transformer+UniDrop achieves the lowest dev loss and shows great advantage to prevent Transformer from overfitting. Besides, we also find that the dev loss of Transformer+UniDrop continuously falls until the end of the training. We stop it to keep training epochs of all models same for a fair comparison.</p><p>In Appendix A.4, we also plot the curves of training loss for the above models, together with the dev loss, to make a better understanding of the regularization effects from these dropout techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>In <ref type="table">Table 1</ref>, we have presented some important ablation studies by removing FD, SD, or DD from UniDrop. The consistent decline of BLEU scores demonstrates their effectiveness. Besides, we further investigate the effects of the two existing feature dropouts FD-1, FD-2, two new feature dropouts FD-3, FD-4, and our proposed two-stage  data dropout strategy on Transformer models. The experimental results are shown in <ref type="table" target="#tab_8">Table 6</ref>. From <ref type="table" target="#tab_8">Table 6</ref>, we can see the four ablation models removing FDs underperform the full model Transformer+UniDrop, which means they can work together to prevent Transformer from overfitting. In multi-head attention module, FD-3 brings more BLUE improvement than FD-1. This comparison shows the insufficiency of only applying FD-1 for the Transformer architecture. The Transformer+UniDrop w/o 2-stage DD means we directly apply conventional data dropout to the sequence instead of our proposed 2-stage strategy. Compared with the full model, its performance also decreases. This shows the necessity of keeping the original sequence for data dropout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effects of Different Dropout Rates</head><p>To investigate the effects of FD, SD, and DD dropout rates on the UniDrop, we respectively vary them based on the setting (FD=0.1, SD=0.1, DD=0.2). When varying one dropout component, we keep other dropout rates unchanged. <ref type="figure">Figure 4</ref> shows the corresponding results.</p><p>We can observe that the performance of each dropout for Transformer+UniDrop first increases then decreases when varying the dropout rates from small to large. Especially, varying the rate for FD dropout makes a more significant impact on the model performance since FD contains four feature dropout positions. In contrast, the DD is least sensitive to the dropout rate change, but it still plays a role in the model regularization.</p><p>6 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Dropout</head><p>Dropout is a popular regularization method for neural networks by randomly dropping some neurons during training <ref type="bibr" target="#b34">(Srivastava et al., 2014)</ref>. Following the idea, there are abundant subsequent works designing specific dropout for specific architecture, such as StochasticDepth <ref type="bibr" target="#b16">(Huang et al., 2016)</ref>, DropPath <ref type="bibr" target="#b24">(Larsson et al., 2017)</ref>, Drop-Block <ref type="bibr" target="#b11">(Ghiasi et al., 2018)</ref> for convolutional neural networks, Variational Dropout <ref type="bibr" target="#b9">(Gal and Ghahramani, 2016)</ref>, ZoneOut <ref type="bibr" target="#b23">(Krueger et al., 2017)</ref>, and Word Embedding Dropout <ref type="bibr" target="#b9">(Gal and Ghahramani, 2016)</ref> for recurrent neural networks. Recently, the Transformer architecture achieves great success in a variety of tasks. To improve generalization of Transformer, some recent works propose LayerDrop <ref type="bibr" target="#b7">(Fan et al., 2020a)</ref>, DropHead  and HeadMask <ref type="bibr" target="#b35">(Sun et al., 2020)</ref> as structured regularizations, and obtain better performance than standard Transformer. Instead of designing a specific dropout for Transformer, in this work, we focus on integrating the existing dropouts into one UniDrop to further improve generalization of Transformer without any additional cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Data Augmentation</head><p>Data augmentation aims at creating realisticlooking training data by applying a transformation to a sample, without changing its label . In NLP tasks, data augmentation often refers to back-translation <ref type="bibr" target="#b32">(Sennrich et al., 2016a)</ref>, word replacing/inserting/swapping/dropout <ref type="bibr" target="#b43">(Wei and Zou, 2019;</ref>, etc. In this work, we adopt simple but effective word dropout as data level dropout in our UniDrop. We, additionally, design a two-stage data dropout strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we present an integrated dropout approach, UniDrop, to specifically regularize the Transformer architecture. The proposed UniDrop unites three different level dropout techniques from fine-grain to coarse-grain, feature dropout, structure dropout, and data dropout respectively. We provide a theoretical justification that the three dropouts play different roles in regularizing Transformer. Extensive results on neural machine translation and text classification datasets show that our Transformer+UniDrop outperforms the standard Transformer and various ablation versions. Further analysis also validates the effectiveness of different dropout components and our two-stage data dropout strategy. In conclusion, the UniDrop improves the performance and generalization of the Transformer without additional computational cost and resource requirement.</p><p>In this section, we explain why regularizing Hessian matrix with respect to input or hidden output can improve model robustness and generalization.</p><p>We use D α f L to denote the α-order derivatives of loss L with respect to f . If the hidden output is perturbed by , i.e.,h = h + , the k-th output F k shifts to</p><formula xml:id="formula_12">F k (h + ) =F k (h) + T J F k ,h + 1 2 T (D 2 h F k (h)) + o( 2 ),<label>(5)</label></formula><p>where J F k ,h (x) is the Jacobian between hidden output h and final output F k . Structure dropout regularizes all elements in Hessian matrix D 2 h L. For Hessian matrix of loss function, we have D 2</p><formula xml:id="formula_13">h L = J T F,h (D 2 F L)J F,h + k (D F k L)(D 2 h F k (h))</formula><p>. Thus, regularizing all elements in D 2 h L means regularizing both J F,h and D 2 h F k (h). As shown in Eq.5, regularizing this two terms can make |F k (h+ )−F k (h)| smaller. Therefore, the robustness of the model is improved and the generalization ability of the model can also be improved <ref type="bibr" target="#b14">(Hoffman et al., 2019;</ref><ref type="bibr" target="#b18">Jakubovitz and Giryes, 2018)</ref>.</p><p>Feature dropout regularizes diagonal element of D 2 h L. Using the approximation D 2 h L ≈ J T F,h (D 2 F L)J F,h , regularizing diagonal elements D 2 h L equals to regularizing norm of Jacobian, i.e., ||J F,h || 2 if D 2 F L is roughly a diagonal matrix. For cross-entropy loss, D 2 F L = diag(z) − zz T , where z is the probability vector predicted by the model encoding the distribution over output class labels, the matrix D 2 F L can be approximated by a diagonal matrix. Thus, feature dropout mainly regularizes the first-order coefficient J F k ,h in Taylor expansion in Eq.5, which is different from structure dropout. Since Jacobian is an essential quantity for the generalization <ref type="bibr" target="#b14">Hoffman et al., 2019)</ref>, emphasising this term is necessary for generalization although structure dropout can also regularize it.</p><p>Similar analysis can be applied to data dropout and we only need to replace hidden output h to the input x.   Text classification experiments are conducted in GLUE tasks <ref type="bibr" target="#b40">(Wang et al., 2019a)</ref> and typical text classification benchmarks datasets <ref type="bibr" target="#b39">(Voorhees and Tice, 1999;</ref><ref type="bibr" target="#b27">Maas et al., 2011;</ref><ref type="bibr" target="#b48">Zhang et al., 2015)</ref>. For GLUE tasks, we adopt the four datasets MNLI, QNLI, SST-2 and MRPC. They are used to evaluate the ability of models on language inference, sentiment classification and paraphrase detection. In typical text classification datasets, IMDB is binary film review classification task <ref type="bibr" target="#b27">(Maas et al., 2011)</ref>. Yelp and AG's News datasets are built by <ref type="bibr" target="#b48">(Zhang et al., 2015)</ref>, respectively for sentiment classification and topic classification. TREC is a question classification dataset consisting of 6 question types <ref type="bibr" target="#b39">(Voorhees and Tice, 1999)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Statistics of Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Dropout Attempts</head><p>Besides the different dropout methods introduced in Section 3, we also tried some other dropouts. We first introduce their settings. The 'QKV_proj' applies dropout to query, key, and value after linear projection. In contrast, FD-3 is to add dropout to query, key, and value before projection. Similarly, 'LogitsDrop' means that we use dropout after obtaining output logits from output projection layer. Compared to LogitsDrop, FD-4 directly applies dropout before the output projection layer.  'EncoderDrop' means that we randomly drop the whole information of Transformer encoder with a probability and only use previous outputs to generate the next token during training. Obviously, it is a language modeling task when dropping the encoder. 'Encoder LayerDrop' is that we apply LayerDrop only on the Transformer encoder. <ref type="table" target="#tab_12">Table 9</ref> shows the BLEU scores of different models on IWSLT-2014 De→En translation task. All dropout rates are tuned within [0.1, 0.2, 0.3, 0.4] according to the performance of the dev set.</p><p>FD-1 and FD-2 are two existing feature dropouts for Transformer. We first use them and achieve better BLUE scores than the standard Transformer, which demonstrates the existence of serious overfitting in Transformer model. On this basis, we try to add further feature dropout to prevent Transformer from overfitting. However, we can see that QKK_proj achieves fewer improvements compared with FD-3. Similarly, LogitsDrop also underperforms FD-4. Therefore, we finally use FD-3 and FD-4 as our feature dropout components together with FD-1 and FD-2.</p><p>Among all structure dropout models, decoder LayerDrop outperforms all compared methods. In contrast, EncoderDrop only brings small improvements. Surprisingly, we can see that here the encoder LayerDrop actually has a negative effect on Transformer. Thus we integrate the promising decoder LayerDrop as structured dropout component into UniDrop.  A.4 Loss Curves <ref type="figure" target="#fig_5">Figure 5</ref> shows the loss curves of different models during training. Overall, we can see that our Transfomer+UniDrop obtains the minimal gap of training loss and dev loss compared with other dropout models and the standard Transformer. This observation shows the better capability of UniDrop to prevent Transformer from overfitting. Benefitting from the advantage, Transfomer+UniDrop achieves the best generalization and dev loss on IWSLT14 De→En translation task. <ref type="table" target="#tab_14">Table 10</ref> show the accuracy of standard RoBERTa BASE and RoBERTa LARGE , the models with UniDrop and corresponding ablated models on GLUE tasks. Compared the base models RoBERTa BASE and RoBERTa LARGE , we can observe that UniDrop further improves their performance on text classification tasks. After removing FD, SD, or DD from UniDrop, the corresponding accuracy has decreased more or less. The consistent declines again demonstrate the necessity of the feature dropout, structure dropout and data dropout for UniDrop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Ablation Study on Text Classification</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Structure and overview of feature dropout.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Transformer structure and feature dropout applied in different Transformer components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Different dropout components in UniDrop. The gray positions denote applying the corresponding dropout.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The dev loss of different models on IWSLT14 De→En translation task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>The training and dev loss of different models on IWSLT14 De→En translation task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. The detailed results are shown in Table 2. We can see that the Transformer model with our UniDrop outperforms all previous works and achieve state-of-the-art performance, with 36.88En→De De→En En→Ro Ro→En En→Nl Nl→En Nn→Pt-br Pt-br→En Avg.</figDesc><table><row><cell>Transformer</cell><cell>28.67</cell><cell>34.84</cell><cell>24.74</cell><cell>32.14</cell><cell>29.64</cell><cell>33.28</cell><cell>39.08</cell><cell>43.63</cell><cell>33.25</cell><cell>-</cell></row><row><cell>+FD</cell><cell>29.61</cell><cell>36.08</cell><cell>25.45</cell><cell>33.12</cell><cell>30.37</cell><cell>34.50</cell><cell>40.10</cell><cell>44.74</cell><cell cols="2">34.24 +0.99</cell></row><row><cell>+SD</cell><cell>29.03</cell><cell>35.09</cell><cell>25.03</cell><cell>32.69</cell><cell>29.97</cell><cell>33.94</cell><cell>39.78</cell><cell>44.02</cell><cell cols="2">33.69 +0.44</cell></row><row><cell>+DD</cell><cell>28.83</cell><cell>35.26</cell><cell>24.98</cell><cell>32.76</cell><cell>29.72</cell><cell>34.00</cell><cell>39.50</cell><cell>43.71</cell><cell cols="2">33.59 +0.34</cell></row><row><cell>+UniDrop</cell><cell>29.99</cell><cell>36.88</cell><cell>25.77</cell><cell>33.49</cell><cell>31.01</cell><cell>34.80</cell><cell>40.62</cell><cell>45.62</cell><cell cols="2">34.77 +1.52</cell></row><row><cell>w/o FD</cell><cell>29.24</cell><cell>35.68</cell><cell>25.18</cell><cell>33.17</cell><cell>30.16</cell><cell>33.90</cell><cell>39.97</cell><cell>44.81</cell><cell cols="2">34.01 +0.76</cell></row><row><cell>w/o SD</cell><cell>29.92</cell><cell>36.70</cell><cell>25.59</cell><cell>33.26</cell><cell>30.55</cell><cell>34.75</cell><cell>40.45</cell><cell>45.60</cell><cell cols="2">34.60 +1.35</cell></row><row><cell>w/o DD</cell><cell>29.76</cell><cell>36.38</cell><cell>25.44</cell><cell>33.26</cell><cell>30.86</cell><cell>34.55</cell><cell>40.37</cell><cell>45.27</cell><cell cols="2">34.49 +1.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison with existing works on IWSLT-2014 De→En translation task.</figDesc><table><row><cell>Approaches</cell><cell cols="3">En→De Ro→En Nl→En</cell></row><row><cell>MAT (Fan et al., 2020b)</cell><cell>29.90</cell><cell>-</cell><cell>-</cell></row><row><cell>MixReps+co-teaching (Wu et al., 2020)</cell><cell>29.93</cell><cell>33.12</cell><cell>34.45</cell></row><row><cell>Transformer</cell><cell>28.67</cell><cell>32.14</cell><cell>33.38</cell></row><row><cell>+UniDrop</cell><cell>29.99</cell><cell>33.49</cell><cell>34.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Comparison with existing works on IWSLT- 2014 En→De, Ro→En, and Nl→En translation tasks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Accuracy on GLUE tasks (dev set). The models BiLSTM+Attn, CoVe and BiLSTM+Attn, ELMo are from<ref type="bibr" target="#b40">Wang et al. (2019a)</ref>. Best results are in bold.</figDesc><table><row><cell></cell><cell>IMDB</cell><cell>Yelp</cell><cell>AG</cell><cell>TREC</cell></row><row><cell>Char-level CNN</cell><cell>-</cell><cell cols="2">62.05 90.49</cell><cell>-</cell></row><row><cell>VDCNN</cell><cell>-</cell><cell cols="2">64.72 91.33</cell><cell>-</cell></row><row><cell>DPCNN</cell><cell>-</cell><cell cols="2">69.42 93.13</cell><cell>-</cell></row><row><cell>ULMFiT</cell><cell>95.40</cell><cell>-</cell><cell cols="2">94.99 96.40</cell></row><row><cell>BERT BASE</cell><cell>94.60</cell><cell cols="3">69.94 94.75 97.20</cell></row><row><cell>RoBERTa BASE</cell><cell>95.7</cell><cell>70.9</cell><cell>95.1</cell><cell>97.6</cell></row><row><cell>+UniDrop</cell><cell>96.0</cell><cell>71.4</cell><cell>95.5</cell><cell>98.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Accuracy on the typical text classification datasets. Char-level CNN and VDCNN are from</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 and</head><label>4</label><figDesc>Table 5respectively show the accuracy of different models on GLUE tasks and typical text classification datasets.Compared with the conventional BiLSTM and CNN based models, we can observe the pre-trained models, including ULMFiT, BERT, RoBERTa, achieve obvious improvements on most datasets. Benefiting from better training strategy, RoBERTa BASE outperforms BERT BASE and even BERT LARGE on GLUE tasks.We can see our proposed UniDrop further improve the performance RoBERTa BASE on both small-scale and large-scale datasets. Specifically, UniDrop brings about 0.4 improvements of accuracy on the typical text classification datasets fromTable 5. In contrast, RoBERTa BASE +UniDrop achieves more improvements on GLUE tasks. The experimental results on the 8 text classification benchmark datasets consistently demonstrate the facilitation of UniDrop for Transformer. We show more results and ablation study on text classification task in Appendix A.5.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>The BLEU scores of Transformer+UniDrop on IWSLT14 De→En translation dev set and test test, with varying the rates of FD, SD and DD respectively.</figDesc><table><row><cell></cell><cell cols="2">36.38</cell><cell>36.74</cell><cell>36.88</cell><cell>36.75</cell><cell>36.55</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>38.5</cell><cell></cell><cell></cell><cell></cell><cell>38.5</cell><cell></cell><cell></cell><cell></cell><cell>38.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BLEU score</cell><cell>35.5 36.5 37.5</cell><cell></cell><cell></cell><cell>BLEU score</cell><cell>35.5 36.5 37.5</cell><cell></cell><cell></cell><cell>BLEU score</cell><cell>35.5 36.5 37.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Dev</cell><cell>Test</cell><cell></cell><cell></cell><cell></cell><cell>Dev</cell><cell>Test</cell><cell></cell><cell></cell><cell>Dev</cell><cell>Test</cell><cell></cell></row><row><cell></cell><cell>34.5</cell><cell></cell><cell></cell><cell></cell><cell>34.5</cell><cell></cell><cell></cell><cell></cell><cell>34.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell></row><row><cell></cell><cell cols="3">(a). Varying FD rate.</cell><cell></cell><cell></cell><cell cols="2">(b). Varying SD rate.</cell><cell></cell><cell></cell><cell cols="3">(c). Varying DD rate.</cell></row><row><cell cols="6">Figure 4: De→En En→De Ro→En</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Transformer</cell><cell>34.84</cell><cell cols="2">28.67</cell><cell>32.14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">+UniDrop</cell><cell>36.88</cell><cell cols="2">29.99</cell><cell>33.49</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">w/o FD-1</cell><cell>36.72</cell><cell cols="2">29.84</cell><cell>33.33</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">w/o FD-2</cell><cell>36.57</cell><cell cols="2">29.76</cell><cell>33.28</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">w/o FD-3</cell><cell>36.59</cell><cell cols="2">29.83</cell><cell>33.31</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">w/o FD-4</cell><cell>36.65</cell><cell cols="2">29.59</cell><cell>33.24</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">w/o 2-stage DD</cell><cell>36.61</cell><cell cols="2">29.78</cell><cell>33.12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: Ablation study of data dropout and different</cell></row><row><cell>feature dropouts on IWSLT14 De→En, En→De, and</cell></row><row><cell>Ro→En translation tasks.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 and</head><label>7</label><figDesc>Table 8respectively show the statistics of machine translation and text classification bench-</figDesc><table><row><cell cols="4">Datasets Train Dev Test</cell></row><row><cell>En↔De</cell><cell>160k</cell><cell>7k</cell><cell>7k</cell></row><row><cell>En↔Ro</cell><cell cols="3">180k 4.7k 1.1k</cell></row><row><cell>En↔Nl</cell><cell cols="3">170k 4.5k 1.1k</cell></row><row><cell cols="4">En↔Pt-br 175k 4.5k 1.2k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Statistics for machine translation datasets.</figDesc><table><row><cell>Datasets</cell><cell cols="3">Classes Train Dev</cell></row><row><cell>MNLI</cell><cell>3</cell><cell cols="2">393k 20k</cell></row><row><cell>QNLI</cell><cell>2</cell><cell cols="2">105k 5.5k</cell></row><row><cell>SST-2</cell><cell>2</cell><cell cols="2">67k 0.9k</cell></row><row><cell>MRPC</cell><cell>2</cell><cell cols="2">3.7k 0.4k</cell></row><row><cell>Datasets</cell><cell cols="3">Classes Train Test</cell></row><row><cell>IMDB</cell><cell>2</cell><cell>25k</cell><cell>25k</cell></row><row><cell>Yelp</cell><cell>5</cell><cell cols="2">650k 50k</cell></row><row><cell>AG's News</cell><cell>4</cell><cell cols="2">120k 76k</cell></row><row><cell>TREC</cell><cell>6</cell><cell cols="2">5.4k 0.5k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Statistics for text classification datasets. mark datasets we used to evaluate the UniDrop for Transformer.For machine translation tasks, the four language pairs all contain around 170k∼190k training pairs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>The results of different dropouts on IWSLT14 De→En translation task.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Ablation Study on GLUE tasks (dev set). The "w/o FD", "w/o SD", "w/o DD" indicate respectively removing feature dropout, structure dropout, and data dropout from RoBERTa BASE +UniDrop or RoBERTa LARGE +UniDrop.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We also explored other positions for feature dropout, but their performances are not so good (see Appendix A.3).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ generic/multi-bleu.perl7  The dropout rates of model Transformer+FD, Trans-former+SD, Transformer+DD are tuned with IWSLT14 De→En dev set and respectively set to 0.2, 0.2, 0.3.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the anonymous reviewers for their valuable comments. Xinyu Dai and Lijun Wu are the corresponding authors. This work was partially supported by the NSFC (No. 61976114,61936012) and National Key R&amp;D Program of China (No. 2018YFB1005102).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Supplementary materials for theoretical analysis</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Weighted transformer network for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02132</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Theoretical analysis of auto rate-tuning by batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint/>
	</monogr>
	<note type="report_type">ton. 2016. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1070</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-09" />
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jaime</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1285</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03819</idno>
		<title level="m">Universal transformers</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reducing transformer depth on demand with structured dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020, Addis Ababa</title>
		<meeting><address><addrLine>Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multi-branch attentive transformer. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shufang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang-Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Soft contextual data augmentation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1555</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5539" to="5544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dropblock: A regularization method for convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing</title>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
			<biblScope unit="page" from="10750" to="10760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Guided transformer: Leveraging multiple external sources for representation learning in conversational search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helia</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Bruce</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401061</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1131" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sho</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yaida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02729</idno>
		<title level="m">Robust learning with jacobian regularization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1031</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018-07-15" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46493-0_39</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016-10-11" />
			<biblScope unit="volume">9908</biblScope>
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<idno type="DOI">10.3115/v1/p15-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing</title>
		<meeting>the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1681" to="1691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving dnn robustness to adversarial attacks using jacobian regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jakubovitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01258-8_32</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="514" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep pyramid convolutional neural networks for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1052</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A simple weight decay can improve generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Hertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="950" to="957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Zoneout: Regularizing rnns by randomly preserving hidden activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">János</forename><surname>Kramár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Pezeshki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fractalnet: Ultra-deep neural networks without residuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Understanding and improving transformer from a multi-particle dynamic system point of view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02762</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference</title>
		<meeting><address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06-24" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
		<respStmt>
			<orgName>The Association for Computer Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Are sixteen heads really better than one?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14" />
			<biblScope unit="page" from="14014" to="14024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2002-07-06" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Workshop Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p16-1009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
		<respStmt>
			<orgName>Long Papers. The Association for Computer Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Alleviating the inequality of attention heads for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/2009.09672</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fedor</forename><surname>Moiseev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1580</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5797" to="5808" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The TREC-8 question answering track evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><forename type="middle">M</forename><surname>Tice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Eighth Text REtrieval Conference, TREC 1999</title>
		<meeting>The Eighth Text REtrieval Conference, TREC 1999<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="1999-11-17" />
			<biblScope unit="volume">500</biblScope>
		</imprint>
		<respStmt>
			<orgName>NIST Special Publication. National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improving neural language modeling via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="6555" to="6565" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12915</idno>
		<title level="m">The implicit and explicit regularization effects of dropout</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">EDA: easy data augmentation techniques for boosting performance on text classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1670</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="6381" to="6387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sequence generation with mixed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shufang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Tied transformers: Neural machine translation with shared encoder and decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33015466</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5466" to="5473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><forename type="middle">Jake</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scheduled drophead: A regularization method for transformer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangchunshu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.178</idno>
	</analytic>
	<monogr>
		<title level="m">Findings, EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020-11-20" />
			<biblScope unit="page" from="1971" to="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">{IOT}: Instance-wise layer reordering for transformer structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shufang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Incorporating BERT into neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
