<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ELECTRAMED: A NEW PRE-TRAINED LANGUAGE REPRESENTATION MODEL FOR BIOMEDICAL NLP</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-19">19 Apr 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giacomo</forename><surname>Miolo</surname></persName>
							<email>giacomo.miolo@mip.polimi.it</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Management</orgName>
								<orgName type="department" key="dep2">Economics and Industrial Engineering</orgName>
								<orgName type="institution">Politecnico di Milano</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Mantoan</surname></persName>
							<email>giulio.mantoan@mip.polimi.it</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Management</orgName>
								<orgName type="department" key="dep2">Economics and Industrial Engineering</orgName>
								<orgName type="institution">Politecnico di Milano</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlotta</forename><surname>Orsenigo</surname></persName>
							<email>orsenigo@mip.polimi.it</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Management</orgName>
								<orgName type="department" key="dep2">Economics and Industrial Engineering</orgName>
								<orgName type="institution">Politecnico di Milano</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ELECTRAMED: A NEW PRE-TRAINED LANGUAGE REPRESENTATION MODEL FOR BIOMEDICAL NLP</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-19">19 Apr 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Pre-trained language models · ELECTRA · Biomedical NLP</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The overwhelming amount of biomedical scientific texts calls for the development of effective language models able to tackle a wide range of biomedical natural language processing (NLP) tasks. The most recent dominant approaches are domain-specific models, initialized with general-domain textual data and then trained on a variety of scientific corpora. However, it has been observed that for specialized domains in which large corpora exist, training a model from scratch with just in-domain knowledge may yield better results. Moreover, the increasing focus on the compute costs for pre-training recently led to the design of more efficient architectures, such as ELECTRA. In this paper, we propose a pre-trained domain-specific language model, called ELECTRAMed, suited for the biomedical field. The novel approach inherits the learning framework of the general-domain ELECTRA architecture, as well as its computational advantages. Experiments performed on benchmark datasets for several biomedical NLP tasks support the usefulness of ELECTRAMed, which sets the novel state-of-the-art result on the BC5CDR corpus for named entity recognition, and provides the best outcome in 2 over the 5 runs of the 7th BioASQ-factoid Challange for the question answering task.</p><p>Keywords Pre-trained language models · ELECTRA · Biomedical NLP * Corresponding authors.</p><p>While ELMo and BERT architectures pre-trained on general-domain corpora are well-established top performers for general NLP tasks, they might yield poor results in case of scientific or specific domains, since the corpora used for pre-training, such as news articles and Wikipedia [3], might not include the same terminology adopted in the indomain tasks. For specialized contexts past studies showed that general-domain language models can largely benefit from the use of in-domain textual data <ref type="bibr" target="#b3">[4]</ref>. As a consequence, recent models for biomedical NLP relied on adapted versions of general-domain approaches. Among these, two of the most noteworthy and successful examples are represented by BioBERT [5]  and BlueBERT [4], which are domain-specific language models initialized with the general-domain BERT, and then pre-trained on a wide range of biomedical and scientific corpora. In principle, these last methods rely on the assumption that initializing the pre-training from general-domain models might improve the overall performance for domain-specific purposes. However, it has been recently observed that for domains in which large corpora exist, like the biomedical field, pre-training language models from scratch yields better results than feeding the pre-training phase with general-domain knowledge <ref type="bibr" target="#b5">[6]</ref>.</p><p>To obtain contextualized word embeddings, BERT pre-training is based on masked language modelling (MLM) which aims at predicting a small random subset of masked input tokens, considering only the token context. This approach allows the model to learn bidirectional representations. A different input corruption procedure has been recently proposed, in which instead of masking, and therefore losing, some of the input tokens, these are replaced with plausible alternatives produced by a small generator network. By learning from all the input tokens this novel approach, called ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately), is computationally much more efficient than BERT, and has been shown to outperform the latter in several tasks <ref type="bibr" target="#b6">[7]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The immense body of biomedical scientific texts, which steadily grows at an exponential rate, makes it imperative to develop effective machine learning methods able to automatically extract the rich knowledge therein contained, that can be used to address several biomedical natural language processing (NLP) tasks.</p><p>Prominent NLP advancements in recent years have been mostly driven by the use of deep neural models, which require large corpora of annotated training data. Compared to the general domain, however, the collection of such data in the biomedical field is difficult and expensive, since it necessarily involves domain experts for accurate data labelling. For this reason, semi-supervised pre-trained language models, such as ELMo <ref type="bibr" target="#b0">[1]</ref> and BERT <ref type="bibr" target="#b1">[2]</ref>, were developed and successfully applied in a wide range of NLP tasks.</p><p>ELMo and BERT leverage contextualized word embeddings for which the representation of a word depends on the context where it is used and, therefore, is a function of the entire input sequence. ELMo exploits a deep bidirectional language model pre-trained on a large corpus of texts. It was proposed to address both the syntactic and semantic complexities and ambiguities of words and has been proven to achieve notable results in a variety of NLP problems <ref type="bibr" target="#b0">[1]</ref>. BERT <ref type="bibr" target="#b1">[2]</ref> resorts to the transformer architecture to pre-train bidirectional language representations, instead of relying on recurrent neural networks. By embracing attention mechanisms, BERT showed to distinguish the sense of words at a very fine level and to grasp many of their syntactic and semantic properties. This ability made BERT the state-of-the-art contextualized word representation model for the most challenging natural language understanding problems.</p><p>Inspired by previous research achievements which showed the effectiveness of language models built on domainspecific knowledge, and recognizing the importance of resorting to efficiently pre-trained methods while preserving downstream performances, in this study we describe a new ELECTRA-based model, called ELECTRAMed, suited for the biomedical domain.</p><p>In particular, the main contributions of the present work are as follows:</p><p>• We propose a novel ELECTRA-based language representation model (ELECTRAMed) pre-trained on biomedical corpora. To the best of our knowledge, this is the first ELECTRA-based model specifically developed for the biomedical domain, and the present study is the first which applies, to this domain, a transformer architecture different from BERT.</p><p>• We tested ELECTRAMed on several biomedical benchmark NLP tasks. The results achieved empirically show the effectiveness of the proposed approach, which performed at par, and sometimes better, than state-ofthe-art models while leveraging the reduced computational effort required by ELECTRA-based architectures.</p><p>• We make publicly available the pre-processed datasets used in our study as well as the pre-trained weights of ELECTRAMed and the source code for fine-tuning the model 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials and methods</head><p>As it is generally the case with pre-trained methods for language representation, the development of the new model encompassed two main phases represented, respectively, by pre-training and fine-tuning, as illustrated in the following sections. Since the proposed approach shares the same architecture of ELECTRA, a brief description of the latter is also required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">ELECTRA</head><p>ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately) is a pre-trained language model recently proposed to overcome the computational drawback of approaches based on masked language modelling (MLM), like BERT, where the input is first corrupted by masking some tokens in the sentence and a network is then trained to recover the original identity of the corrupted tokens. Since the network learns from a small percentage of masked-out tokens (typically 15%), these techniques usually require a huge amount of computations to be effective <ref type="bibr" target="#b6">[7]</ref>.</p><p>To address this inefficiency, ELECTRA implements an alternative pre-training procedure called Replaced Token Detection (RTD), in which a network is trained to distinguish real input tokens from synthetic but plausible replacements. Specifically, the model consists of two neural networks which are pre-trained jointly. The first is a generator that performs MLM by providing tokens substitutes, and that then learns to predict the original token from the masked form.</p><p>The second is a discriminator which is trained to detect the synthetic tokens, i.e. to distinguish real tokens from those replaced by the generator. After pre-training the generator is dropped and the discriminator is fine-tuned on labeled data for downstream tasks.</p><p>Compared to MLM, the innovative pre-training procedure embedded in ELECTRA is more computationally efficient since the discriminator is required to predict the class, real vs. replaced, of each of the input tokens, thereby learning from the entire input sequence instead of a small portion of it. This feature lets ELECTRA compete with state-of-thearts models in several NLP problems while reducing dramatically the computational effort needed for pre-training, as described in the seminal paper <ref type="bibr" target="#b6">[7]</ref>. Moreover, by resorting to plausible tokens alternatives for masking, RTD helps in mitigating a disadvantage of the traditional MLM approach, which introduces a potential mismatch in the learning framework between pre-training and fine-tuning due to the use, for input corruption, of the same conventional [MASK] token which does not appear in the fine-tuning phase. Its computational efficiency, together with the promising results achieved for different NLP tasks, motivated the adoption of ELECTRA in the present study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">ELECTRAMed pre-training</head><p>Differently from other prominenent pre-trained models, such as BioBERT, ELECTRAMed wasn't initialized with the weights from ELECTRA. Indeed, pre-training was performed entirely on a biomedical domain corpus. The corpus at hand was published by <ref type="bibr" target="#b3">[4]</ref> and consists of 28,714,373 PubMed abstracts (uncompressed, approximately 26GB), representing the whole amount of abstracts published on the free digital repository until September 2018. In more detail, the corpus contains ∼181M sentences and ∼4B words and was subject to the following common NLP preprocessing steps applied by the proponents of the dataset:</p><p>• Text lowercasing;</p><p>• Special characters (\x00-\x7F) removal;</p><p>• Text tokenization using NLTK Treebank tokenizer 2 .</p><p>In any NLP task, vocabularies are needed to encode tokens with numbers, and are generally built so to contain the most frequent words or subword units. In the present study we made use of SciVocab, a WordPiece vocabulary proposed by <ref type="bibr" target="#b7">[8]</ref> and built on a scientific text corpus by means of the SentencePiece library <ref type="bibr" target="#b2">3</ref> . Compared to the more commonly used vocabulary released with BERT, SciVocab is characterized by almost the same size (∼30k) and 42% of tokens overlap, thereby showing a substantial difference in the words used frequently in scientific texts with respect to the general domain case. Notice that, the use of a more scientific-oriented vocabulary should reduce the incidence of outof-vocabulary (OOV) tokens and, therefore, the loss of information in the text encoding phase. The hyperparameters applied for pre-training ELECTRAMed on the corpus mentioned above are reported in <ref type="table" target="#tab_0">Table 1</ref>. These correspond to the same set of parameters used for the ELECTRA-base model, as described in <ref type="bibr" target="#b6">[7]</ref>. For tokenization we instead resorted to the WordPiece scheme adopted by BERT.</p><p>It is well known that generating models which adhere to the pre-training and fine-tuning paradigm is a highly resourceintensive process. In the case of BERT, for example, the computational complexity of the self-attention layer increases quadratically with the length of the sequences after tokenization. As a result, limiting the maximum length of the input sequence to 128, at least for the first part of the training, and then increasing it to 512 at a later stage, is a commonly adopted strategy to gain efficiency. This is done despite the risk of reducing the ability of the model to capture longdistance dependencies and, therefore, to negatively affect its performance. Differently from other approaches, by leveraging the computational advantages provided by the ELECTRA framework, we were able to investigate the use of a maximum sequence length of 512, instead of 128, for the whole pre-training phase, while keeping the training time on par with other methods. Indeed, pre-training ELECTRAMed took ∼10 days by using one TPU v3 with 8 cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">ELECTRAMed fine-tuning</head><p>After pre-training, ELECTRAMed was fine-tuned and tested on three biomedical NLP tasks, represented by named entity recognition (NER), relationship extraction (RE) and question answering (QA).</p><p>Named entity recognition (NER) is aimed at automatically finding and tagging in a text meaningful terms, called named entities. In a general domain these typically refer to sequences of words corresponding to specific entities in the real world, such as locations, persons, organizations. In the biomedical field named entities can represent the name of genes, diseases, chemical compounds, and drugs, to name a few. The goal of NER is tagging each token in a sentence with one taken from a list of possible named entities, or with the "no entity" label. NER is often applied as a preliminary step for many applications, such as relationship extraction and knowledge base completion. It is worthwhile to notice that, compared to the general domain, NER in the biomedical field is considered to be more complex, since biomedical entities constantly grow in number with the scientific progress, may contain special characters and can be referred to using a wide variety of synonyms and abbreviations.</p><p>Among the available annotation schemes used to label multi-token named entities, in the present work we resorted to BIO tagging <ref type="bibr" target="#b8">[9]</ref>, for which three binary classifiers are trained to label each token in the text as B (the token is the beginning of a named entity), I (the token is inside a named entity but is not the beginning), and O (otherwise).</p><p>For fine-tuning and testing ELECTRAMed on biomedical NER, three publicly available corpora were used. The first, denoted here as NCBI-disease, is the disease corpus of the National Center for Biotechnology Information (NCBI) <ref type="bibr" target="#b9">[10]</ref>, which is a collection of 793 PubMed abstracts annotated at the mention and concept level. In detail, it contains 6892 disease mentions mapped into 790 unique disease concepts. The second dataset (BC5CDR) was used for the BioCreative V Chemical Disease Relation task and is composed by 1500 PubMed articles with 4409 annotated chemicals, 5818 diseases and 3116 chemical-disease interactions <ref type="bibr" target="#b10">[11]</ref>. From the entire collection we extracted the subset with chemical and disease entities, along the lines of previous studies. The third dataset (JNLPBA) was released for an open challenge and derives from the GENIA corpus, which is a collection of abstracts retrieved by controlled search on MEDLINE. For the shared task, the authors simplified the original 36 classes into five super-classes represented, respectively, by proteins, DNA, RNA, cell types and cell lines <ref type="bibr" target="#b11">[12]</ref>. These correspond to the biological named entities of interest.</p><p>Relationship extraction (RE) generally follows NER and is aimed at finding semantic relationships which may occur in a text between two or more entities. In the biomedical domain relationships are extracted between biological entities and, for example, can take the form of different kinds of relations among genes and diseases or of proteinprotein interactions. RE is cast as a text classification problem in which, given a pair of entities, the goal is to assign the correct type of relationship, whether this exists. To validate ELECTRAMed on RE problems two datasets were analyzed. The first is CHEMPROT <ref type="bibr" target="#b12">[13]</ref>, which is a corpus containing annotated relationship types between chemicals and proteins. From the initial group of 10 different kinds of relationships, five of these (CPR:3, CPR:4, CPR:5, CPR:6 and CPR:9) were used for evaluation. The second corpus (DDI-2013) contains 792 texts extracted from the DrugBank database and other 233 MEDLINE abstracts describing drug-drug interactions (DDIs). The corpus was annotated by considering four different types of interactions (i.e. effect, mechanism, advice, and int), where the last was used when a DDI appeared in the text without providing any specific additional information <ref type="bibr" target="#b13">[14]</ref>.  Question answering (QA) can be considered an extension of information retrieval and has the purpose of delivering direct and precise responses to questions asked in natural language. Focusing on the biomedical domain, and in particular on inquiries about the treatment of a disease, a QA tool would provide, as answers, specific drugs which are effective against that disease or short text passages containing the response. Indeed, there are three categories of questions which are typically asked to a QA system: confirmation questions, that can be dealt with by yes-or-no statements, list questions, which expect a list of entities or facts, and factoid questions, which require a single short phrase or sentence as response. In our work, we focused on this last type of questions, for which the goal is finding the correct answer inside a passage. This is achieved by training a model capable of predicting the starting and the ending tokens of the text segment containing the full expected response. For biomedical semantic QA we selected as benchmark the pre-processed version of the BioASQ Task 7b-factoid dataset <ref type="bibr" target="#b14">[15]</ref>, which contains both biomedical questions and gold standard answers, in the form of relevant concepts, articles, snippets, exact answers, summaries and the corresponding full PubMed abstracts as passages.</p><p>As for NER, question answering in the biological domain poses a major challenge in contrast to the open or other restricted domains, for the presence of a highly specialized terminology and for a potential larger gap in technicality between the questions made by non-expert users and the target documents. Another critical issue is the scarcity of labelled datasets. To tackle this problem, prior to addressing the biomedical QA task, we fine-tuned ELECTRAMed on the Stanford Question Answering Dataset (SQuAD v1.1), a large-scale general-domain reading comprehension dataset published by <ref type="bibr" target="#b15">[16]</ref> and including 87,599 and 10,570 examples for training and testing, respectively. This additional fine-tuning was performed in line with previous studies, which showed its effectiveness in improving the performance on domain-specific activities <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b14">[15]</ref>.</p><p>The description of the corpora used for the three NLP tasks in terms of number of training, testing and validation examples, and number of observations for each class, are provided in Tables 2, 3 and 4. The hyperparameters applied for fine-tuning ELECTRAMed on all the tasks are instead indicated in <ref type="table" target="#tab_4">Table 5</ref>. Notice that, the datasets at hand are gold standards for the evaluation of biomedical NLP tools. To enable a fair comparison with previous studies, therefore, we adopted the same partition into training, test and validation sets as originally proposed by the authors    <ref type="table" target="#tab_5">Table 6</ref> shows the detected named entities and the answers provided by ELECTRAMed for samples extracted from the corpora used for NER and QA tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>For named entity recognition the performance of ELECTRAMed was evaluated by means of the F 1 -score (F), defined as the harmonic mean between precision (P) and recall (R). Accordingly to <ref type="bibr" target="#b8">[9]</ref>, precision was computed as the percent- State-of-the-art (SOTA) performance. For NCBI-disease, SOTA1 is BioBERT, <ref type="bibr" target="#b4">[5]</ref>, SOTA2 is Spark NLP, <ref type="bibr" target="#b17">[18]</ref>, SOTA3 is BioFLAIR, <ref type="bibr" target="#b18">[19]</ref>. For BC5CDR, SOTA1 is RL+DS+PA, <ref type="bibr" target="#b19">[20]</ref>, SOTA2 is Spark NLP and SOTA3 is BioFLAIR. For JNLPBA, SOTA1 is Spark NLP, SOTA2 is BioBERT and SOTA3 is BioFLAIR.</p><p>age of named entities found by the model that were correct (i.e. were an exact match of the corresponding entity in the corpus), whereas recall was set as the percentage of entities included in the corpus and detected by the model. For the purpose of relationship extraction, the F 1 -score was applied as well. In this case, the predicted triplets entity-relationentity were deemed correct if the relation and the two entities were the same as the ground truth. Finally, for question answering we resorted to three quality measures, commonly used for assessing tools which generate a list of possible responses to a given inquiry. The first is the mean reciprocal rank (MRR), given by the average of the reciprocal ranks of the results for a sample of queries. The other two are the strict accuracy (SACC) and the lenient accuracy (LACC), for which a question is correctly answered if the gold response is the first element of the list returned by the model, or it is included in the list, respectively.</p><p>For each of the NLP problems ELECTRAMed was compared with the current best state-of-the-art (SOTA) models, including those developed by the participants of the 7th BioASQ Challenge runs (QA activity). The models comprised within each SOTA group are specified in the captions of <ref type="table" target="#tab_6">Tables 7, 8</ref> and 9, for every task and corpus. These tables also contain missing values denoted as "?", to indicate the unavailability of the corresponding measure for the given pair model-dataset. Missing outcomes are associated to precision and recall for both NER and RE tasks. For the sake of completeness, we deemed relevant to provide the results for ELECTRAMed also in terms of these two quality measures. It is also worthwhile to observe that, all the values referred to ELECTRAMed in the tables are obtained by averaging the respective metrics over five runs with different seeds. This is in line with the experimental settings adopted for SciBERT <ref type="bibr" target="#b7">[8]</ref>, for which the outcomes were computed as average over multiple runs. For the other SOTA models, instead, we were unable to establish whether the results reported by the authors were averaged or corresponded to the best outcomes achieved.</p><p>The results obtained by ELECTRAMed for the task of named entity recognition are shown in <ref type="table" target="#tab_6">Table 7</ref>. The proposed model reached the highest F 1 -score (90.03) on the BC5CDR dataset. By performing better than the current SOTA approaches, ELECTRAMed sets a novel state-of-the-art performance on this corpus for NER purposes in terms of F 1 -score. For NCBI-disease and JNLPBA datasets, ELECTRAMed was not among the top three SOTA models, but still reached comparable results on the first corpus.</p><p>The outcomes for the task of relationship extraction are indicated in <ref type="table" target="#tab_7">Table 8</ref>. These results clearly demonstrate the superior effectiveness of SciBERT over the other models, but also show the promising performance of ELECTRAMed. Indeed, on the DDI-2013 dataset the proposed approach provided results which are close to the second-best model (SOTA2) and ranked at the third position by reaching a fairly higher F 1 -score compared to the current SOTA3 (79.13 vs. 72.90).</p><p>Finally, the results achieved for the question answering task are depicted in <ref type="table" target="#tab_8">Table 9</ref>. ELECTRAMed was able to outperform all the competitors in two (batches 1 and 4) out of the five runs, and to provide comparable results with the best approach (KU-DMIS-5) for the second batch. For the remaining runs (batches 3 and 5), ELECTRAMed ranked at the seventh and sixth position, respectively, among all the participants. To investigate the ability of ELECTRAMed of providing high-quality responses across the whole challenge, besides the BioASQ baseline method we selected the models that competed in all the runs and, for each of them, we computed a score given by the ratio between their MRR and the highest MRR reached in a given batch. For each model, the sum of the scores over all the runs can be seen as a measure of its ability of providing responses which are close, if not equal, to the best ones in terms of MRR. The results of this analysis are reported in <ref type="table" target="#tab_0">Table 10</ref> and support the effectiveness of ELECTRAMed for the QA State-of-the-art (SOTA) performance. For CHEMPROT, SOTA1 is SciBERT, <ref type="bibr" target="#b7">[8]</ref>, SOTA2 is BioBERT, SOTA3 is BlueBERT, <ref type="bibr" target="#b3">[4]</ref>. For DDI-2013, SOTA1 is DESC+MOL+SciBERT, <ref type="bibr" target="#b20">[21]</ref>, SOTA2 is BlueBERT, SOTA3 is Hierarchy Bi-LSTMs +Att.+SDP, <ref type="bibr" target="#b21">[22]</ref>.</p><p>task at hand. The proposed model, indeed, achieved the highest total score being associated with ratios more densely distributed around 1. In particular, ELECTRAMed performed better than systems based on the BioBERT approach (KU-DMIS-5 model). Note: The number in round brackets beside each model indicates the ranking in the challenge run. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions &amp; Future Developments</head><p>The recent literature on biomedical NLP has been heavily influenced by BERT-based architectures and by the usage of domain-specific corpora for pre-training. Meanwhile, in the general-domain NLP literature, a plethora of transformerbased architectures have flourished, bringing significant improvements to the first wide-spread implementation. In this study we presented ELECTRAMed, a new pre-trained language model for biomedical NLP. ELECTRAMed was pre-trained from scratch on a biomedical corpus using a domain-specific vocabulary, and was shown to obtain valuable results for some of the most commonly addressed NLP tasks arising in the biomedical field. For ELECTRAMed pretraining we leveraged the efficiency of the ELECTRA architecture, and we were able to perform at par of the current state-of-the-art models while keeping the computational effort low, in terms of both time and cost.</p><p>The results achieved in the present work encourage future studies that can be undertaken along different directions. From one side, it would be worthwhile to investigate the performance of ELECTRAMed when the maximum sequence length of the input is reduced from 512 to 128 for the entire, or at least, the most part of the training phase, with the aim of further reducing the computational requirements. From the other, it would be useful to explore the impact on the model performance of using a vocabulary built upon the selected pre-training corpus. With this study we hope to spark a new wave of transformer-based architectures in the biomedical domain. Consequently, as a future research line it would be also interesting to investigate potential improvements for tasks related to biomedical information extraction by combining existing biomedical domain knowledge resources (e.g. knowledge bases) into novel transformer-based learning frameworks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Hyperparameters used for ELECTRAMed pre-training</figDesc><table><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>Number of layers</cell><cell>12</cell></row><row><cell>Hidden size</cell><cell>768</cell></row><row><cell>FFN inner hidden size</cell><cell>3072</cell></row><row><cell>Attention heads</cell><cell>12</cell></row><row><cell>Attention head size</cell><cell>64</cell></row><row><cell>Embedding Size</cell><cell>768</cell></row><row><cell>Generator Size</cell><cell>1/3</cell></row><row><cell>Mask percent</cell><cell>15</cell></row><row><cell>Learning Rate Decay</cell><cell>Linear</cell></row><row><cell>Warmup steps</cell><cell>10000</cell></row><row><cell>Learning Rate</cell><cell>2e-4</cell></row><row><cell>Adam ǫ</cell><cell>1e-6</cell></row><row><cell>Adam β 1</cell><cell>0.9</cell></row><row><cell>Adam β 2</cell><cell>0.999</cell></row><row><cell>Attention Dropout</cell><cell>0.1</cell></row><row><cell>Dropout</cell><cell>0.1</cell></row><row><cell>Weight Decay</cell><cell>0.01</cell></row><row><cell>Batch size</cell><cell>256</cell></row><row><cell>Train epochs</cell><cell>1M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Description of the corpora used as benchmarks for biomedical NER</figDesc><table><row><cell>Dataset</cell><cell>Entity Type</cell><cell>N. entities</cell><cell>N. train</cell><cell>N. val</cell><cell>N. test</cell></row><row><cell>NCBI-disease</cell><cell>Disease</cell><cell>6,892 dis.</cell><cell>5,429</cell><cell>923</cell><cell>941</cell></row><row><cell>BC5CDR</cell><cell>Disease and</cell><cell>10,227 TOT</cell><cell>3,951</cell><cell>3,957</cell><cell>4,145</cell></row><row><cell></cell><cell>Chemical</cell><cell>4,409 chemicals</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>5,818 diseases</cell><cell></cell><cell></cell><cell></cell></row><row><cell>JNLPBA</cell><cell>Gene and</cell><cell>59,963 TOT</cell><cell>16,845</cell><cell>1,743</cell><cell>3,869</cell></row><row><cell></cell><cell>Protein</cell><cell>35,336 proteins</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>10,589 DNA</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1,069 RNA</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>8,639 cell types</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>4,330 cell lines</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Description of the corpora used as benchmarks for biomedical RE</figDesc><table><row><cell>Dataset</cell><cell>Relationship</cell><cell>N. relations</cell><cell>N. train</cell><cell>N. val</cell><cell>N. test</cell></row><row><cell>CHEMPROT</cell><cell>Chemical-</cell><cell>10,028 TOT</cell><cell>19,460</cell><cell>11,820</cell><cell>16,943</cell></row><row><cell></cell><cell>Protein</cell><cell>1,983 CPR:3</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>5,006 CPR:4</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>484 CPR:5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>727 CPR:6</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1,828 CPR:9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DDI-2013</cell><cell>Drug-Drug</cell><cell>2,937 TOT</cell><cell>18,779</cell><cell>7,244</cell><cell>5,761</cell></row><row><cell></cell><cell></cell><cell>1,212 effect</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>946 mechanism</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>633 advice</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>146 int</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Description of the corpus (BioASQ 7b-factoid) used as benchmark for biomedical QA</figDesc><table><row><cell>Batch</cell><cell>N. questions</cell><cell>N. question-context pairs</cell></row><row><cell>Train</cell><cell>556</cell><cell>5537</cell></row><row><cell>Test batch 1</cell><cell>39</cell><cell>98</cell></row><row><cell>Test batch 2</cell><cell>25</cell><cell>56</cell></row><row><cell>Test batch 3</cell><cell>29</cell><cell>84</cell></row><row><cell>Test batch 4</cell><cell>34</cell><cell>90</cell></row><row><cell>Test batch 5</cell><cell>35</cell><cell>79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameters used for ELECTRAMed fine-tuning</figDesc><table><row><cell>Hyperparameter</cell><cell>NER</cell><cell>RE</cell><cell cols="2">QA_SQuAD QA_BioASQ</cell></row><row><cell>Learning Rate</cell><cell>5e-5</cell><cell>5e-5</cell><cell>3e-5</cell><cell>5e-6</cell></row><row><cell>Adam ǫ</cell><cell>1e-6</cell><cell>1e-6</cell><cell>1e-6</cell><cell>1e-6</cell></row><row><cell>Adam β 1</cell><cell>0.9</cell><cell>0.9</cell><cell>0.9</cell><cell>0.9</cell></row><row><cell>Adam β 2</cell><cell>0.999</cell><cell>0.999</cell><cell>0.999</cell><cell>0.999</cell></row><row><cell>Layerwise LR Decay</cell><cell>0.8</cell><cell>0.8</cell><cell>0.8</cell><cell>0.8</cell></row><row><cell>Learning Rate Decay</cell><cell>Linear</cell><cell>Linear</cell><cell>Linear</cell><cell>Linear</cell></row><row><cell>Attention Dropout</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Dropout</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Weight Decay</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Batch size</cell><cell>32</cell><cell>32</cell><cell>16</cell><cell>16</cell></row><row><cell>Max Sequence Length</cell><cell>128</cell><cell>128</cell><cell>384</cell><cell>384</cell></row><row><cell>Document Stride</cell><cell>NA</cell><cell>NA</cell><cell>128</cell><cell>128</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Detected named entities and answers provided by ELECTRAMed (in bold) for samples taken from NER and QA corpora indomethacin by inhibition of prostaglandin synthesis may diminish the blood pressure maintaining effect of the stimulated effect of the stimulated renin-angiotensin system in sodium and volume depletion. Erenumab, a human monoclonal antibody that inhibits the calcitonin gene-related peptide receptor, is being evaluated for migraine prevention ... of the corpora. Finally, by way of example,</figDesc><table><row><cell>Task</cell><cell>Dataset</cell><cell>Sample</cell></row><row><cell>NER</cell><cell>BC5CDR</cell><cell>Thus, -NHL cell</cell></row><row><cell></cell><cell></cell><cell>line..</cell></row><row><cell>QA</cell><cell>BioASQ-7b</cell><cell>Q: What is the cause of a STAG3 truncating variant?</cell></row><row><cell></cell><cell></cell><cell>A: STAG3 truncating variant as the cause of primary ovarian insufficiency.</cell></row><row><cell></cell><cell></cell><cell>Primary ovarian insufficiency (POI) is a distressing cause of infertility in</cell></row><row><cell></cell><cell></cell><cell>young women ...</cell></row><row><cell>QA</cell><cell>BioASQ-7b</cell><cell>Q: Which receptor is targeted by Erenumab?</cell></row><row><cell></cell><cell></cell><cell>A: ...</cell></row></table><note>NER NCBI-disease Occasional missense mutations in ATM were also found in tumour DNA from patients with B-cell non-Hodgkins lymphomas (B-NHL) and a B</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Precision (P), recall (R) and F 1 -score (F) for the NER task</figDesc><table><row><cell>Benchmark</cell><cell>Metrics</cell><cell>SOTA1</cell><cell>SOTA2</cell><cell>SOTA3</cell><cell>ELECTRAMed</cell></row><row><cell>NCBI-disease</cell><cell>P</cell><cell>88.22</cell><cell>?</cell><cell>?</cell><cell>85.87</cell></row><row><cell></cell><cell>R</cell><cell>91.25</cell><cell>?</cell><cell>?</cell><cell>89.29</cell></row><row><cell></cell><cell>F</cell><cell>89.71</cell><cell>89.13</cell><cell>88.85</cell><cell>87.54</cell></row><row><cell>BC5CDR</cell><cell>P</cell><cell>92.05</cell><cell>?</cell><cell>?</cell><cell>88.76</cell></row><row><cell></cell><cell>R</cell><cell>87.91</cell><cell>?</cell><cell>?</cell><cell>91.34</cell></row><row><cell></cell><cell>F</cell><cell>89.93</cell><cell>89.73</cell><cell>89.42</cell><cell>90.03</cell></row><row><cell>JNLPBA</cell><cell>P</cell><cell>?</cell><cell>72.24</cell><cell>?</cell><cell>69.33</cell></row><row><cell></cell><cell>R</cell><cell>?</cell><cell>83.26</cell><cell>?</cell><cell>78.56</cell></row><row><cell></cell><cell>F</cell><cell>81.29</cell><cell>77.59</cell><cell>77.03</cell><cell>73.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Precision (P), recall (R) and F 1 -score (F) for the RE task</figDesc><table><row><cell>Benchmark</cell><cell cols="2">Metrics SOTA1</cell><cell>SOTA2</cell><cell>SOTA3</cell><cell>ELECTRAMed</cell></row><row><cell>CHEMPROT</cell><cell>P</cell><cell>?</cell><cell>77.02</cell><cell>?</cell><cell>75.47</cell></row><row><cell></cell><cell>R</cell><cell>?</cell><cell>75.90</cell><cell>?</cell><cell>70.67</cell></row><row><cell></cell><cell>F</cell><cell>83.64</cell><cell>76.46</cell><cell>74.40</cell><cell>72.94</cell></row><row><cell>DDI-2013</cell><cell>P</cell><cell>?</cell><cell>?</cell><cell>74.10</cell><cell>80.07</cell></row><row><cell></cell><cell>R</cell><cell>?</cell><cell>?</cell><cell>71.80</cell><cell>78.24</cell></row><row><cell></cell><cell>F</cell><cell>84.08</cell><cell>79.90</cell><cell>72.90</cell><cell>79.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Strict accuracy (SACC), lenient accuracy (LACC) and mean reciprocal rank (MRR) for the QA task</figDesc><table><row><cell cols="2">Batch Competitor</cell><cell>SACC</cell><cell>LACC</cell><cell>MRR</cell></row><row><cell>1</cell><cell>(1) ELECTRAMed</cell><cell>44.62</cell><cell>51.28</cell><cell>47.95</cell></row><row><cell></cell><cell>(2) KU-DMIS-1</cell><cell>41.03</cell><cell>53.85</cell><cell>46.37</cell></row><row><cell></cell><cell>(3) BJUTNLPGroup</cell><cell>30.77</cell><cell>41.03</cell><cell>34.83</cell></row><row><cell></cell><cell>(4) auth-qa-1</cell><cell>25.64</cell><cell>30.77</cell><cell>27.78</cell></row><row><cell>2</cell><cell>(1) KU-DMIS-5</cell><cell>52.00</cell><cell>64.00</cell><cell>56.67</cell></row><row><cell></cell><cell>(2) ELECTRAMed</cell><cell>46.40</cell><cell>62.40</cell><cell>53.16</cell></row><row><cell></cell><cell>(3) QA1</cell><cell>36.00</cell><cell>48.00</cell><cell>40.33</cell></row><row><cell></cell><cell>(4) transfer-learning</cell><cell>24.00</cell><cell>44.00</cell><cell>32.67</cell></row><row><cell>3</cell><cell>(1) QA1</cell><cell>44.83</cell><cell>58.62</cell><cell>51.15</cell></row><row><cell></cell><cell>(2) UNCC_QA_1</cell><cell>44.83</cell><cell>58.62</cell><cell>51.15</cell></row><row><cell></cell><cell>(3)</cell><cell>41.38</cell><cell>65.52</cell><cell>50.23</cell></row><row><cell></cell><cell>google-gold-input</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(7) ELECTRAMed</cell><cell>37.93</cell><cell>58.62</cell><cell>46.62</cell></row><row><cell>4</cell><cell>(1) ELECTRAMed</cell><cell>61.18</cell><cell>82.35</cell><cell>69.55</cell></row><row><cell></cell><cell>(2) KU-DMIS-1</cell><cell>58.82</cell><cell>82.35</cell><cell>69.12</cell></row><row><cell></cell><cell>(3) FACTOIDS</cell><cell>52.94</cell><cell>73.53</cell><cell>61.03</cell></row><row><cell></cell><cell>(4) UNCC_QA3</cell><cell>52.94</cell><cell>73.53</cell><cell>61.03</cell></row><row><cell>5</cell><cell>(1) KU-DMIS-5</cell><cell>28.57</cell><cell>51.43</cell><cell>36.38</cell></row><row><cell></cell><cell>(2) BJUTNLPGroup</cell><cell>28.57</cell><cell>40.00</cell><cell>33.81</cell></row><row><cell></cell><cell>(3) UNCC_QA_1</cell><cell>28.57</cell><cell>42.86</cell><cell>33.05</cell></row><row><cell></cell><cell>(6) ELECTRAMed</cell><cell>24.57</cell><cell>44.00</cell><cell>31.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Scores over the five runs of the 7h BioASQ-factoid Challange</figDesc><table><row><cell>Competitor</cell><cell cols="6">Batch1 Batch2 Batch3 Batch4 Batch5 Total</cell></row><row><cell>BioASQ Baseline</cell><cell>0.323</cell><cell>0.241</cell><cell>0.258</cell><cell>0.364</cell><cell>0.238</cell><cell>1.424</cell></row><row><cell>auth-qa-1</cell><cell>0.579</cell><cell>0.541</cell><cell>0.669</cell><cell>0.536</cell><cell>0.412</cell><cell>2.737</cell></row><row><cell>KU-DMIS-1</cell><cell>0.967</cell><cell>0.771</cell><cell>0.924</cell><cell>0.994</cell><cell>0.886</cell><cell>4.542</cell></row><row><cell>LabZhu,FDU</cell><cell>0.120</cell><cell>0.441</cell><cell>0.775</cell><cell>0.699</cell><cell>0.615</cell><cell>2.650</cell></row><row><cell>ELECTRAMed</cell><cell>1.000</cell><cell>0.938</cell><cell>0.911</cell><cell>1.000</cell><cell>0.864</cell><cell>4.713</cell></row></table><note>Note: Three models by Lab Zhu at Fudan University were proposed. The table includes the one that performed better across the runs.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/gmpoli/electramed</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://www.nltk.org/ 3 https://github.com/google/sentencepiece</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Dr. Sandra Coecke from the Joint Research Center at European Commission and Dr. Anna Beronius from Karolinska Institute for their valuable and fruitful discussions that fostered a positive and encouraging environment which greatly contributed to the development of our work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Analysis methods in neural language processing: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="49" to="72" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Transfer learning in biomedical natural language processing: An evaluation of BERT and ELMo on ten benchmarking datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th BioNLP Workshop and Shared Task</title>
		<meeting>the 18th BioNLP Workshop and Shared Task<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Electra</surname></persName>
		</author>
		<title level="m">Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<title level="m">SciBERT: Pretrained contextualized embeddings for scientific text</title>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Introduction to the CONLL-2000 shared task: Chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">F T K</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buchholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth conference on computational natural language learning and of the second learning language in logic workshop (CONLL/LLL 2000)</title>
		<meeting>the fourth conference on computational natural language learning and of the second learning language in logic workshop (CONLL/LLL 2000)<address><addrLine>Lissabon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="127" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">NCBI disease corpus: a resource for disease name recognition and concept normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BioCreative V CDR task corpus: a resource for chemical disease relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database: The Journal of Biological Databases and Curation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Introduction to the bio-entity recognition task at JNLPBA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tateisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP)</title>
		<meeting>the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP)<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="73" to="78" />
		</imprint>
		<respStmt>
			<orgName>COLING</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Overview of the BioCreative VI chemical-protein interaction track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rabal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Akhondi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Santamaría</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Intxaurrondo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Nandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">V</forename><surname>Buel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laegreid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Doornenbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oyarzábal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lourenço</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valencia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth BioCreative challenge evaluation workshop</title>
		<meeting>the sixth BioCreative challenge evaluation workshop</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="141" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The DDI corpus: An annotated corpus with pharmacological substances and drug-drug interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Herrero-Zazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Segura-Bedmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Declerck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="914" to="920" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pre-trained language model for biomedical question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="727" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno>arXiv preprint: 1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neves</surname></persName>
		</author>
		<idno>1706.08568</idno>
		<title level="m">Neural question answering at BioASQ 5B</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kocaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Talby</surname></persName>
		</author>
		<title level="m">Biomedical named entity recognition at scale</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">BioFLAIR: Pretrained pooled contextualized embeddings for biomedical sequence labeling tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Daniel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1908" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reinforcement-based denoising of distantly supervised NER with partial annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nooralahzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Lønning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Øvrelid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP</title>
		<meeting>the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="225" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using drug descriptions and molecular structures for drug-drug interaction extraction from literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Asada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="page">907</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Drug-drug interaction extraction via hierarchical RNNs on sequence and shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dumontier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="828" to="835" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
