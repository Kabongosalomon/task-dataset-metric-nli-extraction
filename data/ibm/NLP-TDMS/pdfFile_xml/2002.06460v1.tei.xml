<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HighRes-net: Recursive Fusion for Multi-Frame Super-Resolution of Satellite Imagery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Deudon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfredo</forename><surname>Kalaitzis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Israel</forename><surname>Goytom</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><forename type="middle">Rifat</forename><surname>Arefin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Sankaran</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Cornebise</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
						</author>
						<title level="a" type="main">HighRes-net: Recursive Fusion for Multi-Frame Super-Resolution of Satellite Imagery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative deep learning has sparked a new wave of Super-Resolution (SR) algorithms that enhance single images with impressive aesthetic results, albeit with imaginary details. Multi-frame Super-Resolution (MFSR) offers a more grounded approach to the ill-posed problem, by conditioning on multiple low-resolution views. This is important for satellite monitoring of human impact on the planet -from deforestation, to human rights violations -that depend on reliable imagery. To this end, we present HighRes-net, the first deep learning approach to MFSR that learns its sub-tasks in an end-to-end fashion: (i) co-registration, (ii) fusion, (iii) up-sampling, and (iv) registration-atthe-loss. Co-registration of low-resolution views is learned implicitly through a reference-frame channel, with no explicit registration mechanism. We learn a global fusion operator that is applied recursively on an arbitrary number of low-resolution pairs. We introduce a registered loss, by learning to align the SR output to a ground-truth through ShiftNet. We show that by learning deep representations of multiple views, we can super-resolve low-resolution signals and enhance Earth Observation data at scale. Our approach recently topped the European Space Agency's MFSR competition on real-world satellite imagery.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multiple low-resolution images of the same scene contain collectively more information than any individual lowresolution image, due to minor geometric displacements, e.g. shifts, rotations, atmospheric turbulence, and instrument noise. Multi-Frame Super-Resolution (MFSR)  aims to reconstruct hidden high-resolution details <ref type="figure" target="#fig_3">Figure 1</ref>. HighRes-net combines many low-resolution images (300 meters/pixel) into one image of superior resolution. The same site shot in high-resolution (100m/pix) is also shown for reference. Source of low-res and high-res: imgset1087 and imgset0285 of PROBA-V dataset, see section 5. cause undue changes to even a perfect reconstruction.</p><p>Co-registration of multiple images is required for longitudinal studies of land change and environmental degradation. The fusion of multiple images is key to exploiting cheap, high-revisit-frequency satellite imagery, but of lowresolution, moving away from the analysis of infrequent and expensive high-resolution images. Finally, beyond fusion itself, super-resolved generation is required throughout the technical stack: both for labeling, but also for human oversight <ref type="bibr" target="#b13">(Drexler, 2019)</ref> demanded by legal context <ref type="bibr" target="#b17">(Harris et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Contributions</head><p>HighRes-net We propose a deep architecture that learns to fuse an arbitrary number of low-resolution frames with implicit co-registration through a reference-frame channel.</p><p>ShiftNet Inspired by HomographyNet <ref type="bibr" target="#b9">(DeTone et al., 2016)</ref>, we define a model that learns to register and align the super-resolved output of HighRes-net, using ground-truth high-resolution frames as supervision. This registration-atthe-loss mechanism enables more accurate feedback from the loss function into the fusion model, when comparing a super-resolved output to a ground truth high resolution image. Otherwise, a MFSR model would naturally yield blurry outputs to compensate for the lack of registration, to correct for sub-pixel shifts and account for misalignments in the loss.</p><p>End-to-end fusion + registration By combining the two components above, we propose the first architecture to learn the tasks of fusion and registration in an end-to-end fashion.</p><p>We test and compare our approach to several baselines on real-world imagery from the PROBA-V satellite of ESA. Our performance has topped the Kelvins competition on MFSR, organized by the Advanced Concepts Team of ESA <ref type="bibr" target="#b31">(Märtens et al., 2019</ref>) (see section 5).</p><p>The rest of the paper is divided as follows: in Section 2, we discuss related work on SISR and MFSR; Section 3 outlines HighRes-net and section 4 presents ShiftNet, a differentiable registration component that drives our registered loss mechanism during end-to-end training. We present our results in section 5, and in Section 6 we discuss some opportunities for and limitations and risks of super-resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>How much detail can we resolve in the digital sample of some natural phenomenon? <ref type="bibr" target="#b35">(Nyquist, 1928)</ref> observed that it depends on the instrument's sampling rate and the oscillation frequency of the underlying natural signal. <ref type="bibr" target="#b44">(Shannon, 1949</ref>) built a sampling theory that explained Nyquist's observations when the sampling rate is constant (uniform sampling) and determined the conditions of aliasing in a sample. <ref type="figure">Figure 2</ref> illustrates this phenomenon.</p><p>Sampling at high-resolution (left) maintains the frequency of the chirp signal <ref type="bibr">(top)</ref>. Sampling at a lower resolution (right), this apparent chirped frequency is lost due to aliasing, which means that the lower-resolution sample has a fundamentally smaller capacity for resolving the information of the natural signal, and a higher sampling rate can resolve more information.</p><p>Shannon's sampling theory has since been generalized for multiple interleaved sampling frames <ref type="bibr" target="#b36">(Papoulis, 1977;</ref><ref type="bibr" target="#b30">Marks, 2012)</ref>. One result of the generalized sampling theory is that we can go beyond the Nyquist limit of any individual uniform sample by interleaving several uniform samples taken concurrently. When an image is down-sampled to a lower resolution, its high-frequency details are lost permanently and cannot be recovered from any image in isolation. However, by combining multiple low-resolution images, it is possible to recover the original scene at a higher resolution. DFT magnitude F f LR <ref type="figure">Figure 2</ref>. An example of aliasing (shown with red at its most extreme). Top: A chirp harmonic oscillator sin (2πω(t)t), with instantaneous frequency ω(t). Left: The shape of the high-resolution sample resembles the underlying chirp signal. Right: Close to t = 1, the apparent frequency of the low-resolution sample does not match that of the chirp. It happens when the sampling rate falls below the Nyquist rate, 2 s, where s is the highest non-zero frequency of the signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multi-frame / multi-image super-resolution</head><p>Different low-resolution samples may be sampled at different phase shifts, such that the same high-resolution frequency information will be packed with various phase shifts. Consequently, when multiple low-resolution samples are available, the fundamental problem of MFSR is one of fusion by de-aliasing -that is, to disentangle the highfrequency components packed in low-resolution imagery.</p><p>Such was the first work on MSFR by , who framed the reconstruction of a high-resolution image by fusion of low-resolution images in the Fourier domain, assuming that their phase shifts are known. However, in practice the shifts are never known, therefore the fusion problem must be tackled in conjunction with the registration problem <ref type="bibr" target="#b21">(Irani &amp; Peleg, 1991;</ref><ref type="bibr" target="#b15">Fitzpatrick et al., 2000;</ref><ref type="bibr" target="#b2">Capel &amp; Zisserman, 2001)</ref>. Done the right way, a composite super-resolved image can reveal some of the original highfrequency detail that would have been unrecoverable from a single low-resolution image.</p><p>Until now, these tasks have been learned and / or performed separately. So any incompatible inductive biases that are not reconciled through co-adaptation, would limit the applicability of that particular fuser-register combination. To that end, we introduce HighRes-Net, the first fully end-toend deep architecture for MFSR settings, that jointly learns and co-adapts the fusion and (co-)registration tasks to one another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">VIDEO AND STEREO SUPER-RESOLUTION</head><p>The setting of MFSR is related to Video SR and Stereo SR, although MFSR is less constrained in a few important ways: In MFSR, a model must fuse and super-resolve from sets, not sequences, of low-resolution inputs. The training input to our model is an unordered set of low-resolution views, with unknown timestamps. The target output is a single high-resolution image -not another high-resolution video or sequence. When taken at different times, the lowresolution views are also referred to as multi-temporal (see e.g. .</p><p>In Video SR, the training input is a temporal sequence of frames that have been synthetically downscaled. An autoregressive model (one that predicts at time t = T based on past predictions at t &lt; T ), benefits from this additional structure by estimating the motion or optical flow in the sequence of frames <ref type="bibr" target="#b47">(Tao et al., 2017;</ref><ref type="bibr" target="#b42">Sajjadi et al., 2018;</ref><ref type="bibr" target="#b57">Yan et al., 2019;</ref><ref type="bibr" target="#b52">Wang et al., 2019b)</ref>.</p><p>In Stereo SR, the training input is a pair of low-resolution images shot simultaneously with a stereoscopic camera, and the target is the same pair in the original high-resolution. The problem is given additional structure in the prior knowledge that the pair differ mainly by a parallax effect <ref type="bibr" target="#b54">(Wang et al., 2019d;</ref><ref type="bibr">a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Generative perspective</head><p>In addition to aliasing, MFSR deals with random processes like noise, blur, geometric distortions -all contributing to random low-resolution images. Traditionally, MFSR methods have assumed prior knowledge of the motion model, blur kernel, noise and degradation process that generate the data; see for example, <ref type="bibr" target="#b38">(Pickup et al., 2006)</ref>. Given multiple low-resolution images, the challenge of MFSR is to reconstruct a plausible image of higher-resolution that could have generated the observed low-resolution images. Optimization methods aim to improve an initial guess by minimizing an error between simulated and observed low-resolution images. These methods traditionally model the additive noise and prior knowledge about natural images explicitly, to constrain the parameter search space and derive objective functions, using e.g. Total Variation <ref type="bibr" target="#b4">(Chan &amp; Wong, 1998;</ref><ref type="bibr" target="#b14">Farsiu et al., 2004)</ref>, Tikhonov regularization <ref type="bibr" target="#b34">(Nguyen et al., 2001)</ref> or Huber potential <ref type="bibr" target="#b38">(Pickup et al., 2006)</ref> to define appropriate constraints on images.</p><p>In some situations, the image degradation process is complex or not available, motivating the development of nonparametric strategies. Patch-based methods learn to form high-resolution images directly from low-resolution patches, e.g. with k-nearest neighbor search <ref type="bibr" target="#b16">(Freeman et al., 2002;</ref><ref type="bibr" target="#b5">Chang et al., 2004)</ref>, sparse coding and sparse dictionary methods <ref type="bibr" target="#b58">(Yang et al., 2010;</ref><ref type="bibr" target="#b59">Zeyde et al., 2010;</ref><ref type="bibr" target="#b27">Kim &amp; Kwon, 2010)</ref>). The latter represents images in an over-complete basis and allows for sharing a prior across multiple sites.</p><p>In this work, we are particularly interested in superresolving satellite imagery. Much of the recent work in  . Each LR view-reference pair is encoded into a view-specific latent representation. The LR encodings are fused recursively into a single global encoding. In the Decode stage, the global representation is upsampled by a certain zoom factor (×3 in this work). Finally, the super-resolved image is reconstructed by combining all channels of the upsampled global encoding. (b) Registered loss: Generally, the reconstructed SR will be shifted with respect to the ground-truth HR. ShiftNet learns to estimate the (∆x, ∆y) shift that improves the loss. Lanczos resampling: (∆x, ∆y) define two 1D shifting Lanczos kernels that translate the SR by a separable convolution.</p><p>Super-Resolution has focused on SISR for natural images. For instance, <ref type="bibr">(Dong et al., 2014)</ref> showed that training a CNN for super-resolution is equivalent to sparse coding and dictionary based approaches. <ref type="bibr" target="#b26">(Kim et al., 2016)</ref> proposed an approach to SISR using recursion to increase the receptive field of a model while maintaining capacity by sharing weights. Many more networks and learning strategies have recently been introduced for SISR and image deblurring. Benchmarks for SISR <ref type="bibr" target="#b48">(Timofte et al., 2018)</ref>, differ mainly in their upscaling method, network design, learning strategies, etc. We refer the reader to <ref type="bibr" target="#b54">(Wang et al., 2019d)</ref> for a more comprehensive review.</p><p>Few deep-learning approaches have considered the more general MFSR setting and attempted to address it in an endto-end learning framework. Recently, <ref type="bibr" target="#b25">(Kawulok et al., 2019)</ref> proposed a shift-and-add method and suggested "including image registration" in the learning process as future work.</p><p>In the following sections, we describe our approach to solving both aspects of the registration problem -co-registration and registration-at-the-loss -in a memory-efficient manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">HighRes-net: MFSR by recursive fusion</head><p>In this section, we present HighRes-net, a neural network for multi-frame super-resolution inside a single spectral band (greyscale images), using joint co-registration and fusion of multiple low-resolution views in an end-to-end learning framework. From a high-level, HighRes-net consists of an encoder-decoder architecture and can be trained by stochastic gradient descent using high-resolution ground truth as supervision, as shown in <ref type="figure" target="#fig_0">Figure 3</ref>.</p><p>Notation We denote by θ the parameters of HighRes-net trained for a given upscaling factor γ. LR v,i ∈ R C×W ×H is one of a set of K low-resolution views from the same site v, where C, W and H are the number of input channels, width and height of LR v,i , respectively. We denote by SR θ v = F γ θ (LR v,1 , . . . , LR v,K ), the output of HighRes-net and by HR v ∈ R C×γW ×γH a ground truth high-resolution image. We denote by [T 1 , T 2 ] the concatenation of two images channel-wise. In the following we supress the index v over sites for clarity.</p><p>HighRes-Net consists of three main steps: (1) encoding, which learns relevant features associated with each lowresolution view, (2) fusion, which merges relevant information from views within the same scene, and (3) decoding, which proposes a high-resolution reconstruction from the fused summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Encode, Fuse, Decode</head><p>Embed, Encode The core assumption of MFSR is that the low-resolution image set contains collectively more information than any single low-resolution image alone, due to differences in photometric or spatial coverage for instance. However, the redundant low frequency information in multiple views can hinder the training and test performance of a MFSR model. We thus compute a reference image ref as a shared representation for multiple low-resolution views</p><formula xml:id="formula_0">(LR i ) K i=1</formula><p>and embed each image jointly with ref. This highlights differences across the multiple views <ref type="bibr" target="#b43">(Sanchez et al., 2019)</ref>, and potentially allows HighRes-net to focus on difficult high-frequency features such as crop boundaries and rivers during super-resolution. The shared representation or reference image intuitively serves as an anchor for implicitly aligning and denoising multiple views in deeper layers. We refer to this mechanism as implicit co-registration.</p><p>HighRes-net's embedding layer emb θ consists of a convolutional layer and two residual blocks with PReLu activations <ref type="bibr" target="#b18">(He et al., 2015)</ref> and is shared across all views. The embedded hidden states s 0 i are computed in parallel as follows:</p><formula xml:id="formula_1">ref (c,i,j) = median (LR 1 (c,i,j), . . . , LR K (c,i,j)) , (1) s 0 i = emb θ ([LR i , ref ]) ∈ R C h ×W ×H ,<label>(2)</label></formula><p>where ref ∈ R C×W ×H , and C h denotes the channels of the hidden state.</p><p>The imageset is padded if the number of low-resolution views K is not a power of 2: we pad the set with dummy zero-valued views, such that the new size of the imageset K is the next power of 2. See Algorithm 1, line 1.</p><p>Fuse The embedded hidden states s 0 i are then fused recursively, halving by two the number of low-resolution states at each fusion step t, as shown in <ref type="figure" target="#fig_1">Figure 4</ref>. Given a pair of hidden states s t i , s t j , HighRes-net computes a new representation:</p><formula xml:id="formula_2">s t i ,s t j = s t i , s t j + g θ s t i , s t j ∈ R 2C h ×W ×H (3) s t+1 i = s t i + α j f θ s t i ,s t j ∈ R C h ×W ×H ,<label>(4)</label></formula><p>wheres t i ,s t j are intermediate representations; g θ is a sharedrepresentation within an inner residual block (equation 3); f θ is a fusion block, and α j is 0 if the j-th low-resolution view is part of the padding, and 1 otherwise. f θ squashes 2C h input channels into C h channels and consists of a (conv2d+PreLu). Intuitively, g θ aligns the two representations and it consists of two (conv2d + PreLU) layers. The blocks (f θ , g θ ) are shared across all pairs and depths, giving it the flexibility to deal with variable size inputs and significantly reduce the number of parameters to learn.</p><p>Upscale and Decode After T = log 2 K fusion layers, the final low-resolution encoded state s T i contains information from all K input views. Any information of a spatial location that was initially missing from LR i , is now encoded implicitly in s T i . T is called the depth of HighRes-net. Only then, s T i is upsampled with a deconvolutional layer <ref type="bibr" target="#b56">(Xu et al., 2014)</ref> to a higher-resolution space s T HR ∈ R C h ×γW ×γH . The hidden high-resolution encoded state s T HR is eventually convolved with a 1×1 2D kernel to produce a final super-resolved image SR θ ∈ R C×γW ×γH .</p><p>The overall architecture of HighRes-net is summarized in <ref type="figure" target="#fig_0">Figure 3</ref>(a) and the pseudo-code for the forward pass is given in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 HighRes-net forward pass</head><p>Input: low-resolution views LR 1 . . . LR K # pad inputs to next power of 2</p><formula xml:id="formula_3">(LR 1 . . . LR K , α 1 . . . α K ) = pad (LR 1 . . . LR K ) s 0 i = encode (LR i ) // parallelized across K views T = log 2 K k = K for t = 1 . . . T do for i = 1 . . . k/2 do s t i = fuse s t−1 i , s t−1 k−i , α k−i # fuse encodings end for k = k/2 end for SR = decode s T i # output super-resolved view</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Registration matters</head><p>Co-registration matters for fusion. HighRes-net learns to implicitly co-register multiple low-resolution views LR i and fuse them into a single super resolved image SR θ . We note that since the recursive fusion stage accepts only the encoded low-resolution / reference pairs. So no aspect of our low-res co-registration scheme comes with the builtin assumption that the difference in low-resolution images must be explained only by translational motion.</p><p>A more explicit registration-at-the-loss can also be used for measuring similarity metrics and distances between SR θ and HR. Indeed, training HighRes-Net alone, by minimizing a reconstruction error such as the mean-squared error between SR θ and HR, leads to blurry outputs, since the neural network has to compensate for pixel and sub-pixel misalignments between its output SR θ and HR.</p><p>Here, we present ShiftNet-Lanczos, a neural network that can be paired with HighRes-net to account for pixel and sub-pixel shifts in the loss, as depicted in <ref type="figure" target="#fig_0">Figure 3(b)</ref>. Our ablation study A.2 and qualitative visual analysis suggest that this strategy helps HighRes-net learn to super-resolve and leads to clearly improved results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ShiftNet-Lanczos</head><p>ShiftNet learns to align a pair of images with sub-pixel translations. ShiftNet registers pairs of images by predicting two parameters defining a global translation. Once a subpixel translation is found for a given pair of images, it is applied through a Lanczos shift kernel to align the images. Lanczos kernel for shift / interpolation To shift and align an image by a sub-pixel amount, it must be convolved with a filter that shifts for the integer parts and interpolates for the fractional parts of the translation. Standard options for interpolation include the nearest-neighbor, sinc, bilinear, bicubic, and Lanczos filters <ref type="bibr" target="#b50">(Turkowski, 1990)</ref>. The sinc filter has an infinite support as opposed to any digital signal, so in practice it produces ringing or ripple artifacts -an example of the Gibbs phenomenon. The nearest-neighbor and bilinear filters do not induce ringing, but strongly attenuate the higher-frequency components (over-smoothing), and can even alias the image. The Lanczos filter reduces the ringing significantly by using only a finite part of the sinc filter (up to a few lobes from the origin). Experimentally, we found the Lanczos filter to perform the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Objective function</head><p>In our end-to-end setting, registration improves superresolution as HighRes-net receives more informative gradient signals when its output is aligned with the ground truth high-resolution image. Conversely, super-resolution benefits registration, since good features are key to align images <ref type="bibr" target="#b7">(Clement et al., 2018)</ref>. We thus trained HighRes-Net and ShiftNet-Lanczos in a cooperative setting, where both neural networks work together to minimize an objective function, as opposed to an adversarial setting where a generator tries to fool a discriminator. HighRes-net infers a latent super-resolved variable and ShiftNet maximizes its similarity to a ground truth high-resolution image with sub-pixel shifts.</p><p>By predicting and applying sub-pixel translations in a differentiable way, our approach for registration and superresolution can be combined in an end-to-end learning framework. Shift-Net predicts a sub-pixel shift ∆ from a pair of high-resolution images. The predicted transformation is applied with Lanczos interpolation to align the two images at a pixel level. ShiftNet and HighRes-Net are trained end-to-end, to minimize a joint loss function. Our objective function is composed of a registered reconstruction loss, described in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Sub-pixel registered loss</head><p>Input: super-resolution SR θ , ground-truth high-res HR Leaderboard score &amp; loss We iterated our design based on our performance on the leaderboard of the ESA competition. The leaderboard is ranked by the cPSNR (clear Peak Signal-to-Noise Ratio) score. It is similar to the meansquared error, but also corrects for brightness bias and clouds in satellite images <ref type="bibr" target="#b31">(Märtens et al., 2019)</ref>, but the proposed architecture is decoupled from the choice of loss.</p><p>See Algorithm 2 for computing the registered loss θ,∆ . We further regularize the L2 norm of ShiftNet's output with a hyperparameter λ and our final joint objective is given by:</p><p>L θ,∆ (SR θ , HR) = θ,∆ + λ||∆|| 2 (5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>The PROBA-V satellite carries two different cameras for capturing a high-resolution / low-resolution pair. This makes the PROBA-V dataset one of the first publicly available datasets for MFSR with naturally occurring low-resolution and high-resolution pairs of satellite imagery.</p><p>Perils of synthetic data This is in contrast to most of the work in SR (Video, Stereo, Single-Image, Multi-Frame), where it is common practice to train with low-resolution images that have been artificially generated through simple bilinear down-sampling 1 <ref type="bibr" target="#b1">(Bulat et al., 2018;</ref><ref type="bibr" target="#b53">Wang et al., 2019c;</ref><ref type="bibr" target="#b33">Nah et al., 2019)</ref>. Methods trained on artificially 1 See also, image restoration track at CVPR19. The vast majority of challenges were performed on synthetically downscaled / degraded images: REDS4: "NTIRE 2019 challenge on video deblurring: Methods and results", NTIRE Workshop at CVPR2019. downscaled datasets are biased, in the sense that they learn to undo the action of a simplistic downscaling operator. On the downside, they also tune to its inductive biases. For example, standard downsampling kernels (e.g. bilinear, bicubic) are simple low-pass filters, hence the eventual model implicitly learns that all input images come from the same band-limited distribution. In reality, natural complex images are only approximately band-limited <ref type="bibr" target="#b40">(Ruderman, 1994)</ref>. Methods that are trained on artificially downscaled datasets fail to generalize to real-world low-resolution, low quality images <ref type="bibr" target="#b45">(Shocher et al., 2018)</ref>. For this reason, we experiment only on PROBA-V, a dataset that does not suffer from biases induced by artificial down-sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Proba-V Kelvin dataset</head><p>The performance of our method is illustrated with satellite imagery from the Kelvin competition, organized by ESA's Advanced Concept Team (ACT).</p><p>The Proba-V Kelvin dataset <ref type="bibr" target="#b31">(Märtens et al., 2019)</ref> contains 1450 scenes (RED and NIR spectral bands) from 74 handselected Earth regions around the globe at different points in time. The scenes are split into 1160 scenes for training and 290 scenes for testing. Each data-point consists of exactly one 100m resolution image as 384 × 384 greyscale pixel images (HR) and several 300m resolution images from the same scene as 128 × 128 greyscale pixel images (LR), spaced days apart. We refer the reader to the Proba-V manual <ref type="bibr" target="#b55">(Wolters et al., 2014)</ref> for further details on image acquisition.</p><p>Each scene comes with at least 9 low-resolution views, and an average of 19. Each view comes with a noisy quality map. The quality map is a binary map, that indicates concealed pixels due to volatile features, such as clouds, cloud shadows, ice, water and snow. The sum of clear pixels (1s in the binary mask) is defined as the clearance of a lowresolution view. These incidental and noisy features can change fundamental aspects of the image, such as the contrast, brightness, illumination and landscape features. We use the clearance scores to randomly sample from the imageset of low-resolution views, such that views with higher clearance are more likely to be selected. This strategy helps to prevent overfitting. See <ref type="figure">Supplementary Material ??</ref> for more details.</p><p>Working with missing &amp; noisy values A quality map can be used as a binary mask to indicate noisy or occluded pixels, due to clouds, snow, or other volatile objects. Such a mask can be fed as an additional input channel in the respective low-resolution view, in the same fashion as the reference frame. When missing value masks are available, neural networks can learn which parts of the input are anomalous, noisy, or missing, when provided with such binary masks (see e.g. <ref type="bibr" target="#b6">(Che et al., 2018)</ref>). In satellite applications where clouds masks are not available, other segmentation methods would be in order to infer such masks as a preprocessing step (e.g. <ref type="bibr" target="#b29">(Long et al., 2015)</ref>). In the case of the PROBA-V dataset, we get improved results when we make no use of the masks provided. Instead we use the masks only to inform the sampling scheme within the low-resolution imageset to prevent overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparisons</head><p>All experiments use the same hyperparameters, see Supplementary Material ??. By default, each imageset is padded to 32 views for training and testing, unless specified otherwise. Our PyTorch implementation takes less than 9h to train on a single NVIDIA V100 GPU. At test time, it takes less than 0.2 seconds to super-resolve (×3 upscaling) a scene with 32 low-resolution 128 × 128 views. Our implementation is available on GitHub 2 .</p><p>We evaluated different models on ESA's Kelvin competition. Our best model, HighRes-Net trained jointly with ShiftNet-Lanczos, scored consistently at the top of the public and final leaderboard, see <ref type="table" target="#tab_3">Table 1</ref>.</p><p>We compared HighRes-net to several other approaches:</p><p>ESA baseline -upsamples and averages the subset of lowresolution views with the highest clearance in the set.</p><p>SRResNet -SISR approach by <ref type="bibr" target="#b28">Ledig et al. (2017)</ref>.</p><p>SRResNet+ShiftNet -trained jointly with ShiftNet.</p><p>SRResNet-6 + ShiftNet -at test time, it independently upsamples 6 low-resolution views, then co-registers/aligns them with ShiftNet, and averages them.</p><p>ACT (Advanced Concepts Team, Märtens et al., 2019) -CNN with 5 channels for the 5 clearest low-resolution views.</p><p>DeepSUM ) -like SRResNet-6+Shift-Net, it upsamples independently a fixed number of lowresolution views, co-registers and fuses them. Here, the co-registration task is learned separately. One caveat with upsampling-first approaches is that the memory cost and training cycle grows quadratically with the upscaling factor. On the ESA dataset, this means that DeepSUM must train with 3 × 3 times the volume of intermediate representations, which can take from several days to a week to train on a NVIDIA V100 GPU.</p><p>HighRes-net (trained jointly with ShiftNet, see sections 3 and 4) -in contrast to DeepSUM, our approach upsamples after fusion, conserving memory, and takes up to 9 hours to train on a NVIDIA V100 GPU.</p><p>HighRes-net+ -averages the outputs of two pre-trained HighRes-net models at test time, one with K bounded to 16 input views and the other to 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">ESA Kelvin leaderboard</head><p>The Kelvin competition used the corrected clear PSNR (cPSNR) quality metric as the standardized measure of performance. The cPSNR is a variant of the Peak Signal to Noise Ratio (PSNR) used to compensate for pixel-shifts and brightness bias. We refer the reader to <ref type="bibr" target="#b31">(Märtens et al., 2019)</ref> for the motivation and derivation of this quality metric. The cPSNR metric is normalized by the score of the ESA baseline algorithm so that a score smaller than 1 means "better than the ESA baseline". We also use it as our training objective with sub-pixel registration (see also section 3(b) on ShiftNet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">ABLATION STUDY</head><p>We ran an ablation study on the labeled data (1450 image sets), split in 90% / 10% for training and testing. Our results suggest that more low-resolution views improve the reconstruction, plateauing after 16 views, see Supplementary Material ??. Another finding is that registration matters for MFSR, both in co-registering low-resolution views, and the registered loss, see Supplementary Material ??. Finally, selecting the k clearest views for fusion leads to overfitting. A workaround is to randomly sample views with a bias for clearance, see ??.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>On the importance of grounded detail Scientific and investigative application warrant a firm grounding of any prediction on real, not synthetic or hallucinated, imagery. The PROBA-V satellite  was launched by ESA to monitor Earth's vegetation growth, water resources and agriculture. As a form of data fusion and enrichment, multi-frame super-resolution could enhance the vision of such satellites for scientific and monitoring applications <ref type="bibr" target="#b3">(Carlson &amp; Ripley, 1997;</ref><ref type="bibr" target="#b37">Pettorelli et al., 2005)</ref>. More broadly, satellite imagery can help NGOs and non-profits monitor the environment and human rights <ref type="bibr" target="#b8">(Cornebise et al., 2018;</ref><ref type="bibr" target="#b20">Helber et al., 2018;</ref><ref type="bibr" target="#b41">Rudner et al., 2019;</ref><ref type="bibr" target="#b39">Rolnick et al., 2019)</ref> at scale, from space, ultimately contributing to the UN sustainable development goals. Low-resolution imagery is cheap or sometimes free, and it is frequently updated. However, with the addition of fake or imaginary details, such enhancement would be of little value as scientific, legal, or forensic evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Future work</head><p>Registration matters for the fusion and for the loss. The former is not explicit in our model, and its mechanism deserves closer inspection. Also, learning to fuse selectively with attention, would allow HighRes-net reuse all useful parts of a corrupted image. It is hard to ensure the authenticity of detail. It will be important to quantify the epistemic uncertainty of super-resolution for real world applications. In the same vein, a meaningful super-resolution metric depends on the downstream prediction task. More generally, good similarity metrics remain an open question for many computer visions tasks <ref type="bibr" target="#b0">(Bruna et al., 2015;</ref><ref type="bibr" target="#b24">Johnson et al., 2016;</ref><ref type="bibr" target="#b22">Isola et al., 2017;</ref><ref type="bibr" target="#b28">Ledig et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Conclusion</head><p>We presented HighRes-net -the first deep learning approach to MFSR that learns the typical sub-tasks of MFSR in an end-to-end fashion: (i) co-registration, (ii) fusion, (iii) upsampling, and (iv) registration-at-the-loss.</p><p>It recursively fuses a variable number of low-resolution views by learning a global fusion operator. The fusion also aligns all low-resolution views with an implicit coregistration mechanism through the reference channel. We also introduced ShiftNet-Lanczos, a network that learns to register and align the super-resolved output of HighRes-net with a high-resolution ground-truth.</p><p>Registration is vital, to align many low-resolution inputs (co-registration) and to compute similarity metrics between shifted signals. Our experiments suggest that an end-toend cooperative setting (HighRes-net + ShiftNet-Lanczos) improves training and test performance. By design, our approach is fast to train and to test, with a low memoryfootprint by doing the bulk of the compute (co-registration + fusion) while maintaining the low-resolution image height &amp; width.</p><p>There is an abundance of low-resolution yet high-revisit low-cost satellite imagery, but they often lack the detailed information of expensive high-resolution imagery. We believe MFSR can uplift its potential to NGOs and non-profits that contribute to the UN Sustainable Development Goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Experimental details</head><p>We trained our models on low-resolution patches of size 64 × 64. HighRes-net's architecture is described in Table 2. We denote by Conv2d(in, out, k, s, p) a conv2D layer with in and out input/output channels, kernels of size k × k, stride s and padding p. We used the ADAM optimizer <ref type="bibr" target="#b60">(Kingma &amp; Ba, 2014)</ref> with default hyperparameters and trained our models on batches of size 32, for 400 epochs, using 90% of the data for training and 10% for validation. Our learning rate is initialized to 0.0007, decayed by a factor of 0.97 if the validation loss plateaus for more than 2 epochs. To regularize ShiftNet, we set λ = 10 −6 .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Registration matters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.1.">REGISTERED LOSS</head><p>The only explicit registration that we perform is at the loss stage, to allow the model partial credit for a solution. This solution can be enhanced but otherwise mis-registered with respect to the ground truth. We trained our base model HighRes-net without ShiftNet-Lanczos and observed a drop in performance as shown in <ref type="table">Table 4</ref>. Registration matters and aligning outputs with targets helps HighRes-net generate sharper outputs and achieve competitive results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.2.">IMPLICIT CO-REGISTRATION</head><p>The traditional practice in MFSR is to explicitly co-register the LR views prior to super-resolution . The knowledge of sub-pixel miss-alignments tells an algorithm what pieces of information to fuse from each LR image for any pixel in the SR output. Contrary to the conventional practice in MFSR, we propose implicit co-registeration by pairing LR views with a reference frame, also known as an anchor. In this sense, we never explicitly compute the relative shifts between any LR pair. Instead, we simply stack each view with a chosen reference frame as an additional channel to the input. We call this strategy implicit co-registration. We found this strategy to be effective in the arXiv:2002.06460v1 [cs.CV] 15 Feb 2020  following ablation study which addresses the impact of the choice of a reference frame aka anchor.</p><p>We observe the median reference is the most effective in terms of train and test score. We suspect the median performs better than the mean because the median is more robust to outliers and can help denoise the LR views. Interestingly, training and testing without a shared reference performed worse than the ESA baseline. This shows that co-registration (implicit or explicit) matters. This can be due to the fact that the model lacks information to align and fuse the multiple views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.3.">SHIFTNET ARCHITECTURE</head><p>ShitNet has 8 layers of (conv2D + BatchNorm2d + ReLU) modules. Layer 2, 4 and 6 are followed by MaxPool2D. The final output is flattened to a vector x of size 32, 768. Then, we compute a vector of size 1, 024, x = ReLU(fc1(dropout(x))). The final shift prediction is fc2(x) of size 2. The bulk of the parameters come from fc1, with 32, 768 × 1, 024 weights. These alone, account for 99% of ShiftNet's parameters. Adding a MaxPool2D on top of layer 3, 5, 7 or 8 halves the parameters of ShiftNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Towards permutation invariance</head><p>A desirable property of a fusion model acting on an unordered set of images, is permutation-invariance: the output of the model should be invariant to the order in which the LR views are fused. An easy approach to encourage permutation invariant neural networks is to randomly shuffle the inputs at training time before feeding them to a model <ref type="bibr" target="#b63">(Vinyals et al., 2015)</ref>.</p><p>In addition to randomization, we still want to give more importance to clear LR views (with high clearance score), which can be done by sorting them by clearance. A good trade-off between uniform sampling and deterministic sorting by clearance, is to sample k LR views without replace- ment and with a bias towards higher clearance:</p><formula xml:id="formula_4">p(i | C 1 , . . . , C k ) = e βCi k j=1 e βCj ,<label>(1)</label></formula><p>where k is the total number of LR views, C i is the clearance score of LR i and β regulates the bias towards higher clearance scores, When β = 0, this sampling strategy corresponds to uniform sampling and when β = +inf , this corresponds to picking the k-clearest views in a deterministic way. Our default model was trained with β = 50 and our experiments are reported in <ref type="table" target="#tab_8">Table 6</ref>.</p><p>From <ref type="table" target="#tab_8">Table 6</ref>, β = ∞ reaches best training score and worst testing score. For β = 50 and β = 0, the train/test gap is much more reduced. This suggests that the deterministic strategy is overfitting and randomness prevents overfitting (diversity matters). On the other hand, β = 50 performs significantly better than β = 0 suggesting that biasing a model towards higher clearances could be beneficial i.e., clouds matter too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">On the parallax effect</head><p>The parallax p is a measure of space that is inversely proportional to the distance d from the object, see e.g. <ref type="bibr" target="#b64">(Zeilik &amp; Gregory, 1998)</ref>:</p><formula xml:id="formula_5">p ∝ 1/d</formula><p>If 32 low-resolution satellite views are acquired during a single fly-over, the successive geolocations would indeed be significantly different and the parallax effect would be magnified. This is indeed the case in aerial photography, for instance, because the imagery is captured from much closer to the ground.</p><p>With satellite imagery, one might be interested in detecting vegetation growth (PROBA-V), road networks (infrastructure), farms / ranches (agriculture), deforestation in the Amazon, or human presence and buildings. In all these monitoring applications, the objects of interests are no more than 50m tall, e.g. trees.</p><p>The parallax effect between low-res images does not inhibit the super-resolution of the such objects. The lowest of Low Earth Orbit (LEO) altitudes for a satellite is 300 km (PROBA-V is about 800km), so the relative depth variation is at most 50m / 300,000m = 0.0033%. In other words, the parallax effect is imperceptible for 50m tall objects.</p><p>Here is a calculation to support this argument: Given a point A at height 50m (distance d A = 300, 000m − 50m), and a point B at height 0 (distance d B = 300, 000m), their relative change in motion is: p A /p B = d B /d A = 30/29.995. This means that if point A moves 30m, then point B moves 5 millimeters less than 30m due to parallax. In the case of a fast LEO satellite like PROBA-V, its geolocation is accurate enough such that the translational shifts are mostly within a sub-pixel accuracy, and they almost never exceed 2 pixels. On the ground, 2 pixels amount to a baseline length of (2 px) * (300 m/px) = 600m. So between two images where point A (50m altitude) moved 600m, and point B (0m altitude) has moved 0.005 * 20 = 0.1 m. Hence, the parallax effect is imperceptible for 50m tall objects. Even less so for the objects that we have underlined above, and our experimental state-of-the-art results support this claim.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Schematic of the full processing pipeline, trained end-to-end. At test time, only HighRes-net is used. (a) HighRes-net: In the Encode stage, an arbitrary number of LR views are paired with the reference low-resolution image (the median low-resolution in this work)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>HighRes-net's global fusion operator consists of a coregistration g θ and a fusion f θ block which aligns and combines two representations into a single representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc>∆x, ∆y) = ShiftNet (SR θ , HR) # register SR to HR # 1D Lanczos kernels for x and y sub-pixel shifts (κ ∆x , κ ∆y ) = LanczosShiftKernel (∆x, ∆y) # 2D sub-pixel shift by separable 1D convolutions SR θ,∆ = SR θ * κ ∆x * κ ∆y # sub-pixel registered loss θ,∆ = loss (SR θ,∆ , HR) # θ,∆ loss (SR θ , HR)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 .</head><label>1</label><figDesc>Public leaderboard scores vs. nviews for HighRes-net + ShiftNet. Lower is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Public &amp; final leaderboard cPSNR scores in ESA's Kelvin competition. Lower is better.</figDesc><table><row><cell>METHOD</cell><cell>PUBLIC</cell><cell>FINAL</cell></row><row><cell cols="3">SRRESNET ESA BASELINE SRRESNET + SHIFTNET ACT SRRESNET-6 + SHIFTNET 0.9808 0.9794 1.0095 1.0084 1.0000 1.0000 1.0002 0.9995 0.9874 0.9879 HIGHRES-NET (OURS) 0.9496 0.9488 HIGHRES-NET+ (OURS) 0.9474 0.9477 DEEPSUM 0.9488 0.9474</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>ResidualBlock(h) architecture</figDesc><table><row><cell></cell><cell></cell><cell>1.00</cell><cell></cell><cell></cell><cell></cell><cell>test_score train_score</cell></row><row><cell></cell><cell></cell><cell>0.98</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Scores</cell><cell>0.94 0.96</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.92</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.90</cell><cell>1</cell><cell>2</cell><cell>4 No. of views 8</cell><cell>16</cell><cell>32</cell></row><row><cell>LAYER0</cell><cell>Conv2d(in=h, out=h, k3, s1, p1)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LAYER1</cell><cell>PReLU</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LAYER2</cell><cell>Conv2d(in=h, out=h, k3, s1, p1)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LAYER3</cell><cell>PReLU</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Thanks to weight sharing, HighRes-net super-resolves scenes with 32 views in 5 recursive steps, while requir-ing less than 600K parameters. ShiftNet has more than 34M parameters (34,187,648) but is dropped during test time. We report GPU memory requirements in table 3 for reproducibility purposes.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">1.1. How many frames do you need?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">We trained and tested HighRes-net with ShiftNet using 1 to 32 frames. With a single image, our approach performs worse than the ESA baseline. Doubling the number of frames significantly improves both our training and valida-tion scores. After 16 frames, our model's performance stops increasing as show in Figure 1.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Equal contribution 1 Element AI, London, UK 2 Mila, Montreal, Canada 3 Université de Montréal, Montreal, Canada 4 McGill Uni-versity, Montreal, Canada. Correspondence to: Alfredo Kalaitzis</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">&lt;freddie@element.ai&gt;.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>HRNet architecture</figDesc><table><row><cell>STEP</cell><cell>LAYERS</cell><cell>NUMBER OF PARAMETERS</cell></row><row><cell>ENCODE</cell><cell>Conv2d(in=2, out=64, k3, s1, p1) PReLU ResidualBlock(64) ResidualBlock(64) Conv2d(in=64, out=64, k3, s1, p1)</cell><cell>1216 1 73,858 73,858 36,928</cell></row><row><cell>FUSE</cell><cell>ResidualBlock(128) Conv2d(in=128, out=64, k3, s1, p1) PReLU</cell><cell>295,170 73,792 1</cell></row><row><cell>DECODE</cell><cell>ConvTranspose2d(in=64, out=64, k3, s1) PreLU Conv2d(in=64, out=1, k1, s1)</cell><cell>36,928 1 65</cell></row><row><cell cols="2">RESIDUAL (OPTIONAL) Upsample(scale factor=3.0, mode='bicubic')</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell>591,818 (TOTAL)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>GPU memory requirements to train HighRes-net + Shift-Net on patches of size 64 × 64 with batches of size 32, and a variable number of low-resolution frames. Registration matters: Train and test scores for HighResnet trained with and without ShiftNet-Lanczos. Lower is better.</figDesc><table><row><cell cols="2"># VIEWS 32 16 4 GPU MEMORY (GB) 27 15 6</cell></row><row><cell>HIGHRES-NET</cell><cell>SCORE TRAIN TEST</cell></row><row><cell>UNREGISTERED LOSS REGISTERED LOSS</cell><cell>0.9616 0.9671 0.9501 0.9532</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Scores for HighRes-net + ShiftNet-Lanczos trained and tested with different references as input. Lower is better.</figDesc><table><row><cell>REFERENCE</cell><cell>SCORE TRAIN TEST</cell></row><row><cell cols="2">NO CO-REGISTRATION MEAN OF 9 LRS MEDIAN OR 9 LRS (BASE) 0.9501 0.9532 1.0131 1.0088 0.9636 0.9690</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>scores per sampling strategy for HighRes-net + ShiftNet. Lower is better.</figDesc><table><row><cell>SAMPLING STRATEGY</cell><cell>SCORE TRAIN TEST</cell></row><row><cell cols="2">β = ∞ (K-CLEAREST) 0.9386 0.9687 β = 0 (UNIFORM-K) 0.9638 0.9675 β = 50 (BASE) 0.9501 0.9532</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/ElementAI/HighRes-net</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Catherine Lefebvre, Laure Delisle, Alex Kuefler, Buffy Price, David Duvenaud, Anna Jung-Bluth, Carl Shneider, Xavier Gitiaux, Shane Maloney for their helpful comments on our manuscript. We are extremely grateful to the Advanced Concepts Team of the ESA for organizing the Kelvin competition, and the participating teams for working hard to pushing multi-frame super-resolution to its limits. Finally, this version has been improved considerably thanks to the feedback that we received during the peer-review process of ICLR 2020.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Super-resolution with deep convolutional sufficient statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05666</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">To learn image super-resolution, use a gan to learn how to do image degradation first</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="185" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Super-resolution from multiple views using learnt image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Capel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR</title>
		<meeting>the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On the relation between ndvi, fractional vegetation cover, and leaf area index. Remote sensing of Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ripley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="241" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Total variation blind deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="370" to="375" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<editor>I-I. IEEE</editor>
		<meeting>the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for multivariate time series with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6085</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Image registration and super resolution from first principles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bierbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Sethna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.05583</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Witnessing atrocities: Quantifying villages destruction in darfur with crowdsourcing and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farfour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AI for Social Good NIPS2018 Workshop</title>
		<meeting><address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03798</idno>
		<title level="m">Deep image homography estimation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Proba-v mission for global vegetation monitoring: standard products and image quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dierckx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sterckx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Benhadj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Livens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Duhoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van Achteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Francois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mellab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saint</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2589" to="2614" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Reframing superintelligence: Comprehensive ai services as general intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Drexler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast and robust multiframe super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on image processing</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1327" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Image registration. Handbook of medical imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Fitzpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Maurer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="447" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Examplebased super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geospatial evidence in international human rights litigation: Technical and legal considerations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Drake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wyndham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Wolfinbarger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Lott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lerner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAS Scientific Responsibility</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Human Rights and Law Program</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.123</idno>
		<ptr target="http://dx.doi.org/10.1109/ICCV.2015.123" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV &apos;15</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV &apos;15<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Mapping informal settlements in developing countries with multi-resolution, multi-spectral data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Helber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gram-Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Varatharajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Azam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coca-Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kopackova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bilinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00812</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Improving resolution by image registration. CVGIP: Graphical models and image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="231" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image-toimage translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The promise and peril of a digital ecosystem for the planet, September 2019</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Campbell</surname></persName>
		</author>
		<ptr target="https://medium.com/@davidedjensen99356/building-a-digital-ecosystem-for-the-planet-557c41225dc2" />
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning for fast super-resolution reconstruction from multiple images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kawulok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Benecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hrynczenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kostrzewa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Piechaczek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nalepa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Smolka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">109960B. International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10996</biblScope>
		</imprint>
	</monogr>
	<note>Real-Time Image Processing and Deep Learning</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Single-image super-resolution using sparse regression and natural image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1127" to="1133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Introduction to Shannon sampling and interpolation theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Marks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Superresolution of proba-v images using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Märtens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Izzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krzic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Astrodynamics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="387" to="402" />
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deepsum: Deep neural network for super-resolution of unregistered multitemporal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Molini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Valsesia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fracastoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Magli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ntire 2019 challenge on video deblurring: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A computationally efficient superresolution image reconstruction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="573" to="583" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Certain topics in telegraph transmission theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nyquist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the American Institute of Electrical Engineers</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="617" to="644" />
			<date type="published" when="1928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generalized sampling expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papoulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on circuits and systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="652" to="654" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Using the satellitederived ndvi to assess ecological responses to environmental change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pettorelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename><surname>Vik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mysterud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Gaillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Stenseth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in ecology &amp; evolution</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="503" to="510" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Optimizing and learning for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Pickup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="4" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Tackling climate change with machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rolnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Donti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Kaack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kochanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Milojevic-Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waldman-Brown</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05433</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The statistics of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Ruderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network: computation in neural systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="517" to="548" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi3net: Segmenting flooded buildings via fusion of multiresolution, multisensor, and multitemporal satellite imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Rudner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rußwurm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bischke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kopačková</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Biliński</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="702" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Framerecurrent video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6626" to="6634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning disentangled representations of satellite image time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Serrurier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ortner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.08863</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Communication in the presence of noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IRE</title>
		<meeting>the IRE</meeting>
		<imprint>
			<date type="published" when="1949" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">zero-shot superresolution using deep internal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3118" to="3126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Highway networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Detailrevealing deep video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4472" to="4480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Ntire 2018 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multiple frame image restoration and registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Computer Vision and Image Processing</title>
		<imprint>
			<date type="published" when="1984" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Filters for common resampling tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Turkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphics gems</title>
		<imprint>
			<publisher>Academic Press Professional, Inc</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="147" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning parallax attention for stereo image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12250" to="12259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Edvr: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Flickr1024: A large-scale dataset for stereo image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Deep learning for image super-resolution: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06068</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Probav products user manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wolters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dierckx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Swinnen</surname></persName>
		</author>
		<ptr target="http://proba-v.vgt.vito.be/sites/default/files/ProductUserManual.pdf" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1790" to="1798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Frame and feature-context video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13057</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Image superresolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on curves and surfaces</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deepsum: Deep neural network for super-resolution of unregistered multitemporal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Molini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Valsesia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fracastoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Magli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Multiple frame image restoration and registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Computer Vision and Image Processing</title>
		<imprint>
			<date type="published" when="1984" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06391</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Introductory astronomy and astrophysics. 4th. Fort Worth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeilik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gregory</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Saunders</publisher>
			<pubPlace>TX</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
