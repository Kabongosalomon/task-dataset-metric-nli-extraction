<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ScopeFlow: Dynamic Scene Scoping for Optical Flow</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviram</forename><surname>Bar-Haim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ScopeFlow: Dynamic Scene Scoping for Optical Flow</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose to modify the common training protocols of optical flow, leading to sizable accuracy improvements without adding to the computational complexity of the training process. The improvement is based on observing the bias in sampling challenging data that exists in the current training protocol, and improving the sampling process. In addition, we find that both regularization and augmentation should decrease during the training protocol.</p><p>Using an existing low parameters architecture, the method is ranked first on the MPI Sintel benchmark among all other methods, improving the best two frames method accuracy by more than 10%. The method also surpasses all similar architecture variants by more than 12% and 19.7% on the KITTI benchmarks, achieving the lowest Average End-Point Error on KITTI2012 among two-frame methods, without using extra datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The field of optical flow estimation has benefited from the availability of acceptable benchmarks. In the last few years, with the adoption of new CNN <ref type="bibr" target="#b14">[15]</ref> architectures, a greater emphasis has been placed on the training protocol.</p><p>A conventional training protocol now consists of two stages: (i) pretraining on larger and simpler data and (ii) finetuning on more complex datasets. In both stages, a training step includes the following: (i) sampling batch frames and flow maps, (ii) applying photometric augmentations to the frames, (iii) applying affine (global and relative) transformations to the frames and flow maps, (iv) cropping a fixed size random crop from both input and flow maps, (v) feeding the cropped frames into a CNN architecture, and (vi) backpropagating the loss of the flow estimation.</p><p>While photometric augmentations include variations of the input image values, affine transformations are used to augment the variety of input flow fields. Due to the limited motion patterns represented by today's optical flow datasets, these regularization techniques are required for the data driven training. We chose the word scoping, to define <ref type="bibr" target="#b4">5</ref>   <ref type="figure">Figure 1</ref>. Model size and accuracy trade-off. Average-end-pointerror of the leading methods on the MPI Sintel benchmark vs. the number of trainable parameters. PWC-Net based models are marked in blue. Our model is in the bottom left corner, achieving the best performance with low number of parameters during training. <ref type="bibr" target="#b0">1</ref> Methods that use more than two frames. <ref type="bibr" target="#b1">2</ref> SelFlow uses half of the parameters in test time. <ref type="bibr" target="#b2">3</ref> Trained with additional datasets. the process of affine transformation followed by cropping, as this process sets the scope of the input frames.</p><p>To improve optical flow training, we ask the following questions: Q1. How do fixed size crops affect this training? Q2. What defines a good set of scopes for optical flow? Q3. Should regularization be relaxed after pretraining?</p><p>Our experiments employ the smallest PWC-Net <ref type="bibr" target="#b26">[27]</ref> variant of Hur &amp; Roth <ref type="bibr" target="#b10">[11]</ref>, with only 6.3M trainable parameters, in order to support low memory, real time processing. We demonstrate that by answering these questions and contributing to the training procedure, it is possible to train a dual frame, monocular and small sized model to outperform all other models on the MPI Sintel benchmark. The trained model improves the accuracy of the baseline model, which uses the same architecture, by 12%. See <ref type="figure">Fig. 1</ref> for a comparison to other networks.</p><p>Moreover, despite using the smallest PWC-Net variant, our model outperformed all other PWC-Net variants on both KITTI 2012 and KITTI 2015 benchmarks, improving the baseline model results by 12.2% and 19.7% on the public test set, and demonstrating once more the power of using the improved training protocol.</p><p>Lastly, albeit no public benchmark is available for occlusion estimation, we compared our occlusion results to other published results on the MPI Sintel dataset, demonstrating more than 5% improvement of the best published F1 score.</p><p>Our main contributions are: (i) showing, for the first time, as far as we can ascertain, that CNN training for optical flow and occlusion estimation can benefit from cropping randomly sized scene scopes, (ii) exposing the powerful effect of regularization and data augmentation on CNN training for optical flow and (iii) presenting an updated generally applicable training scheme and testing it across benchmarks, on the widely used PWC-Net network architecture.</p><p>Our code is available online and our models are openly shared, in order to encourage follow-up work, to support reproducibility, and to provide an improved performance to off the shelf real-time models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The deep learning revolution in optical flow started with deep descriptors <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b1">2]</ref> and densification methods <ref type="bibr" target="#b33">[34]</ref>. Dosovitskiy et al. <ref type="bibr" target="#b3">[4]</ref> presented FlowNet, the first deep end-to-end network for optical flow dense matching, later improved by Ilg et al. <ref type="bibr" target="#b11">[12]</ref>, incorporating classic approaches, like residual image warping. Ranjan &amp; Black <ref type="bibr" target="#b23">[24]</ref> showed that deep model size can be much smaller with a coarse to fine pyramidal structure. Hui et al. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> suggested a lighter version for FlowNet, adding features matching, pyramidal processing and features driven local convolution. Xu et al. <ref type="bibr" target="#b30">[31]</ref> adapted semi-global matching <ref type="bibr" target="#b7">[8]</ref> to directly process a reshaped 4D cost volume of features learned by CNN, inspired by common practices in stereo matching. Yang &amp; Ramanan <ref type="bibr" target="#b31">[32]</ref> suggested a method for directly learning to process the 4D cost volume, with a separable 4D convolution. Sun et al. <ref type="bibr" target="#b26">[27]</ref> proposed PWC-Net, which includes pyramidal processing of warped features, and a direct processing of a partial layerwise cost volume, demonstrated strong performance on optical flow benchmarks. Many extensions were suggested to the PWC-Net architecture, among them multi-frame processing, occlusion estimation, iterative warping and weight sharing <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b10">11]</ref>. Pretraining optical flow models Today's leading optical flow learning protocols, include pretraining on large scale data. The common practice is to pretrain on the Fly-ingChairs <ref type="bibr" target="#b3">[4]</ref> and then on FlyingThings3D <ref type="bibr" target="#b19">[20]</ref> (FChairs and FThings). As shown by recent works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b11">12]</ref>, the multistage pretraining ordering is critical. The FChairs dataset includes 21,818 pairs of frames, generated by CAD models <ref type="bibr" target="#b0">[1]</ref>, with flicker images background. FThings is a natural extension of the FChairs dataset, having 22,872 larger 3D scenes with more complex motion patterns. Hur &amp; Roth <ref type="bibr" target="#b10">[11]</ref> created a version of FChairs with ground truth occlusions, called FlyingChairsOcc (denoted FChairsOcc), to allow supervised pretraining on occlusion labels. Datasets and benchmarks The establishment of larger complex benchmarks, such as MPI Sintel <ref type="bibr" target="#b2">[3]</ref> and KITTI <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22]</ref>, boosted the evolution of optical flow models. The MPI Sintel dataset was created from the Sintel movie, composed of 25, relatively long, annotated scenes, with 1064 training frames in total. The final pass category of Sintel is a challenging one, having many realistic effects to mimic natural scenes. The KITTI2012 dataset comprises 194 training pairs with annotated flow maps, while KITTI2015 has 200 dynamic color training pairs. Furthermore, some methods are using more datasets during the finetune process, such as HD1K <ref type="bibr" target="#b13">[14]</ref>, Driving and Monkaa <ref type="bibr" target="#b19">[20]</ref>. Motion categories MPI Sintel provides a stratified view of the error magnitude of challenging motion patterns. The ratio of the best mean error for the small motion category (slower than 10 pixels per frame) to the large motion category (faster than 40 pixels per frame) is approximately x44. In Sec. 3, we present one possible theoretical explanation for the poor performance of state of the art methods in large motions, and suggest an approach to improve the accuracy of this pixels category.</p><p>Another example is the category of unmatched pixels. This category includes pixels belonging to regions that are visible only in one of two adjacent frames (occluded pixels). As expected, these pixels share much higher end-point-error than match-able pixels: the ratio of the best match-able EPE to the best non match-able is approximately 9.5.</p><p>Different deep learning approaches were suggested to tackle the problems of fast objects and occlusion estimation. Among the different solutions suggested were: occlusion based loss <ref type="bibr" target="#b27">[28]</ref> and model <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> separation, and multiframe support for long-range, potentially occluded, spatiotemporal matches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b22">23]</ref>. We suggest a new approach for applying multiple strategies online. Our findings imply that the training can be improved by applying scene scope variations, while taking into account the probability of sampling valid examples from different flow categories. Training procedure and data augmentation Fleet &amp; Weiss <ref type="bibr" target="#b4">[5]</ref> showed the importance of photometric variations, boundary detection and scale invariance to the success of optical flow methods. In recent years, the importance of the training choices attracted more attention <ref type="bibr" target="#b16">[17]</ref>. Sun et al. <ref type="bibr" target="#b25">[26]</ref> used training updates to improve the accuracy of the initial PWC-Net model by more than 10%, showing they could improve the reported accuracy of FlowNetC (a sub network of FlowNet) by more than 50%, surpassing FlowNet2 [10] performance, with their updated training protocol. Mayer et al. <ref type="bibr" target="#b18">[19]</ref> suggests that no single best general-purpose training protocol exists for optical flow, and different datasets require different care. These conclusions are in line with our findings on the importance of proper training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">PWC-Net architectures</head><p>PWC-Net <ref type="bibr" target="#b26">[27]</ref> is the most popular architecture for optical flow estimation to date, and many variants for this architecture were suggested <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref>. PWC-Net architecture was built over traditional design patterns for estimating optical flow, given two temporally consecutive frames, such as: pyramidal coarse-to-fine estimation, cost volume processing, iterative layerwise feature warping and others. Features warping In PWC-Net, a CNN encoder creates feature maps for the different network layers (scales). The features of the second image are backward warped, using the upsampled flow of the previous layer processing, for every layer l, except the last layer l T op , by:</p><formula xml:id="formula_0">c l w (x) = c l 2 (x + up ×2 (f l+1 (x)) (1) where x is the pixel location, c l w (x)</formula><p>is the backward warped feature map of the second image, f l+1 (x) is the output flow of the coarser layer, and up ×2 is the ×2 up-sampling module, followed by a bi-linear interpolation. Cost volume decoding A correlation operation applied on the first and backward warped second image features, in order to construct a cost volume:</p><formula xml:id="formula_1">cost l (x 1 , x 2 ) = 1 N (c l 1 (x 1 )) T c l w (x 2 )<label>(2)</label></formula><p>where c l n (x) ∈ R N is a feature vector of image n. The cost volume is then processed by a CNN decoder, in order to estimate the optical flow directly. In some variants of PWC-Net <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b10">11]</ref> there is an extra decoder with similar architecture for occlusion estimation. Iterative residual processing Our experiments employ the Iterative Residual Refinement proposed by Hur &amp; Roth <ref type="bibr" target="#b10">[11]</ref>. The reasons we chose to test our changes for the PWC-Net architecture on the IRR variant are: (i) IRR has the lowest number of trainable parameters among all PWC-Net variants, making a state of the art result obtained with proper training more significant, (ii) it uses shared weights that could be challenged with scope and scale changes, and if successful, it would demonstrate the power of a rich, scope invariant feature representations, (iii) this variant is using only two frames -demonstrating the power of dynamic scoping without long temporal connections, and (iv) the occlusion map allows the direct evaluation of our training procedure on occlusion estimation. Therefore, any success with this variant directly translates to real-time relatively low complexity optical flow estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Scene scoping analysis</head><p>Due to the limited number of labeled examples available for optical flow supervised learning, most of the leading methods, in both supervised and unsupervised learning, are using cropping of a fixed sized randomly located patches. We are interested in understanding the chances of a pixel to be sampled, within a randomly located fixed size crop, as a function of its location in the image. 1D image random cropping statistics Consider a 1D image with a width W , a crop size w and a pixel location x. Let ∆x denote the distance of the pixel from the closest border, and ∆w denote the difference between the image width W and the crop size w. Let Crops large be the set of crop sizes with w larger than half of the width, W 2 &lt; w ≤ W . Let Crops small be the complement set of crop sizes smaller or equal to half of the width, 0 &lt; w ≤ W 2 . Two instances of this setup are depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>Using the notations above, pixels are separated into three different categories, described in the following lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1.</head><p>For an image size W and a randomly chosen crop of size 0 &lt; w ≤ W the probability of a pixel, with coordinate x and distance to the closest border ∆x to be sampled, is as follows:</p><formula xml:id="formula_2">P (x|W, w) =      1 if ∆w &lt; ∆x w ∆w+1 if w ≤ ∆x ∆x ∆w+1 otherwise (3)</formula><p>where ∆w + 1 is the number of valid crops.</p><p>Proof. For illustration purposes, the three cases are color coded, respectively, as green, orange, and red, in <ref type="figure" target="#fig_0">Fig.2</ref>. We handle each case separately. (i) Green: Every valid placement must leave out up to ∆w pixels from the left or the right. Since ∆x is larger than ∆w, the pixel x must be covered. (ii) Orange: In this case, there are w possible locations for pixel x within the patch, all of which are achievable, since ∆x is large enough. Therefore, w patch locations out of all possible patches contain pixel x. (iii) Red: In this case, the patch can start at any displacement from the edge that is nearer to x, that is not larger than ∆x, and still cover pixel x. Therefore, there are exactly ∆x locations out of the ∆w + 1 possible locations.</p><p>2D image random cropping statistics Since the common practice in current state-of-the-art optical flow training protocols is to crop a fixed sized crop, in the range W 2 &lt; w ≤ W (w ∈ Crops large ), we will focus in the reminder of this section on the green and red categories, which are the relevant categories for crop sizes with each dimension [h,w] larger than half of the corresponding image dimension (i.e. in Crops large ), and represent a cropping of more than a quarter of the image.</p><p>From the symmetry of our 1D random cropping setup, in both x and y axes, we can use Eq. 3 in order to calculate the probability of sampling pixels in a 2D image of size [H, W ], with a randomly located crop of a fixed size [h, w]. The probability of sampling a central (green) pixel remains 1, while the probability of sampling a marginal (red) pixel (x, y) in 2D, is given by:</p><formula xml:id="formula_3">P red (x, y|H, h, W, w) = min (∆x, ∆w) min (∆y, ∆h) (∆w + 1)(∆h + 1) (4) Where ∆h = H − h, ∆w = W − w</formula><p>the difference between the image and the crop width and height, and ∆x, ∆y represent the distance from the closest border, as before. Eq. 4 represents the ratio between the number of crop locations where a (marginal) pixel with ∆x, ∆y is sampled to the number of all unique valid crop locations. An illustration of this sampling probability is demonstrated in <ref type="figure" target="#fig_1">Fig. 3</ref> for varying ratios of crop size axes and image axes. Fixed crop size sampling bias As in the 1D cropping setup, given an image of size [H, W ] and a crop size [h, w], we can define a central area (equivalent to the green pixels in 1D), which will always be sampled. Respectively, we can define a marginal area (equivalent to the red pixels in 1D), where Eq. 4 holds.</p><p>Analyzing Eq. 4 we can infer the following: (i) in the marginal area, for a fixed crop size [h, w], the probability of being sampled decreases quadratically along the image diagonal, when ∆x and ∆y both decrease together, and (ii) in the marginal area, for a fixed pixel, the probability of being sampled decreases quadratically when the crop size decreases (when ∆w and ∆h both decrease together).</p><p>Therefore, when using a fixed sized crop to augment a dataset with a random localization approach, there will be a dramatic sampling bias towards pixels in the center of the image, preserved by the symmetric range of random affine parameters. For example, with the current common cropping approach for the MPI Sintel data-set, the probability of the upper left corner pixel to be sampled in a crop equals 1 (1024−768+1)(436−384+1) = 0.000073%, while the pixels in the central [332, 512] crop will be sampled in any randomized crop location.</p><p>This sampling bias could have a sizable influence on the training procedure. <ref type="figure" target="#fig_1">Fig. 3</ref> illustrates the distribution of fast pixels in both MPI Sintel and KITTI datasets. Noticeably, pixels of fast moving objects (with speed larger than 40 pixels per frame) are often located at the marginal area, while slower pixels are more centered in the scene. This should not be a surprise, since (i) lower pixels belong to nearer scene objects and thus have a larger scale and velocity, and (ii) fast moving objects usually cross the image borders.</p><p>Moreover, many occluded pixels are also located close to the image borders. Therefore, increasing a crop size could also help to observe a more representative set of occlusions during training. Therefore, we hypothesized that larger crops can also improve the ability to infer occluded pixels motion from the context. <ref type="figure" target="#fig_1">Fig. 3</ref> shows the crop size effect on the probability to sample different motion categories. Clearly, the category of fast pixels suffers the most from reduction of the crop sizes. We tested four different strategies for cropping the scene dynamically (per mini batch) during training: (S1) fixed partial crop size (the common approach), (S2) cropping the largest valid crop size, (S3) randomizing crop size ratios from a pre-defined set with:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Scene scoping approaches</head><formula xml:id="formula_4">R f ixed = {(0.73, 0.69), (0.84, 0.86), (1, 1)} (5a) (r h , r w ) = randchoice(R f ixed ), (5b) where (r h , r w )</formula><p>are one of the three crop ratios, and strategy (S4) is a range-based crop size randomization: s = randint(round(r min · S), round(r max · S)), (6) where s is the crop axis size (h or w), S is the full image axis size (H or W), and [r min , r max ] is the range of crop size ratios s S for sampling. We also employ different affine transformations, and dynamically change the zooming range along the training, to enlarge the set of possible scene scopes, and improve the robustness of features to different scales. In Sec. 5.2 we describe the experiments done in order to find an appropriate approach for feeding the network with a diversity of scene scopes and reducing the inherent sampling bias explained in this section, caused by the current cropping mechanisms.</p><p>In addition to testing the scope modifications based on our analysis, we were also interested in testing different parameters of the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training, regularization and augmentation</head><p>Learning rate and training schedules The common LR schedules, proposed by Ilg et al. <ref type="bibr" target="#b11">[12]</ref>, used to train deep optical flow networks, are S long or S short for the pretraining stage, and S f t for the finetune phases. We used the shorter schedule, suggested by <ref type="bibr" target="#b10">[11]</ref>, of using S short for pretraining, half of S short for FThings finetuning, and S f t for Sintel and KITTI datasets. We also examine the effect of retraining and over-training specific stages of the multi-phase training.  Data augmentation The common practice in the current training protocol employs two types of data augmentation techniques: photometric and geometric. The details of these augmentations did not change much since FlowNet <ref type="bibr" target="#b3">[4]</ref>. The photometric transformations include input image perturbation, such as color and gamma corrections. The geometric augmentations include a global or relative affine transformation, followed by random horizontal and vertical flipping. Due to the spatial symmetric nature of the translation, rotation and flipping parameters, we decided to focus on the effect of zooming changes, followed by our new cropping approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regularization</head><p>The common protocol also includes weight decay and adding random Gaussian noise to the augmented image. In our experiments, we tested the effect of eliminating these sources of regularization at different stages of the multi-phase training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we describe the experiments and results for our research questions. Specifically, we tested (i) how can we change the current training pipeline in order to improve the final accuracy, and (ii) the effect of feeding the network with different scopes of the input during training, using different cropping and zooming changes.</p><p>All our experiments on KITTI used both KITTI2012 and KITTI2015, and for Sintel both the clean and final pass, for training and validation sets. We denote the combined datasets of Sintel and KITTI as Sintel combined and KITTI combined. We also tested the approach, suggested by Sun et al. <ref type="bibr" target="#b26">[27]</ref>, to first train on a combined Sintel dataset, followed by another finetune on the final pass.</p><p>All of our experiments employ the common End Point Error metric for flow evaluation, and F1 for occlusion evaluation. KITTI experiments also present outlier percentage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Finetuning a pretrained model</head><p>Since the cost of pretraining is approximately ×7 than of the final finetune, we first present experiments done on the finetuning phase, in which we employ models pretrained on FChairs and FThings, published by the authors of IRR-PWC. These experiments are conducted on the Sintel dataset, since it has similar statistics of displacements to the FChairs dataset <ref type="bibr" target="#b18">[19]</ref> used for pretraining. We tested different training protocol changes, and found that substantial gains could be achieved using the following changes: 1. Cropping strategies. During the initial finetune, we tested the cropping approaches specified in Sec. 3.1 on Sintel. The results specified in Tab. 1 show that the range-based crop size randomization approach (Eq. 6) was comparable to taking the maximal valid crop (although much more efficient computationally), and both improved Sintel validation error of models trained with smaller fixed crop sizes. --0.48 CNN SelFlow <ref type="bibr" target="#b16">[17]</ref> 0.59 0.52 0.555 fwd-bwd FlowDispOccBoundary <ref type="bibr" target="#b12">[13]</ref> 0.703 0.654 0.678 CNN IRR-PWC <ref type="bibr" target="#b10">[11]</ref> 0.712 0.669 0.690 CNN ScopeFlow (Ours) 0.740 0.711 0.725 CNN <ref type="table">Table 7</ref>. Occlusion estimation comparison on Sintel. Results were calculated with F1 score (higher is better).</p><p>3% of improvement during Sintel finetune, demonstrating the benefit of reducing augmentation in advanced stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Applying changes to the full training procedure</head><p>We then tested the changes from Sec. 5.1, along with all four cropping approaches described in Sec. 3.1, on the different stages of the common curriculum learning pipeline.</p><p>Since we wanted to test our training changes and compare our results to other variants of the baseline architecture, we decided not to use any other dataset, other than the common pretraining or benchmarking datasets. For FChairs and FThings, all trained models were evaluated on Sintel training images, as an external test set. FChairs pretraining For pretraining, we downloaded the newly released version of FChairs <ref type="bibr" target="#b10">[11]</ref>, which includes occlusions ground truth annotations. We trained two versions of the IRR-PWC model on FChairsOCC, for 108 epochs on 4 GPUs: (i) C1: removing weight decay and random noise (ii) C2: same as (i) with reduced zoom in. We then evaluated both models and the original model, trained by the authors with weight decay and original zoom in of 1.5, denoted by C*. Results are depicted in Tab. <ref type="bibr" target="#b1">2</ref> that regularization is important in early stages, since removing either weight decay and random noise, or reducing the zoom-in hurt the performance. FThings finetune We then trained three versions of IRR-PWC on FThings, for 50 epochs: (i) T2: resuming C* training with batch size of 2, with the original crop size of [384, 768], (ii) T3: resuming C1 with the maximal crop size, and (iii) T4: resuming T3 without weight decay and random noise. We can infer from the results in Tab. 3: (i) increasing the scope during FChairs training leads to better accuracy on the Sintel test set, and (ii) over-training without weight decay and random noise did not improve the results on the external test set (but did on the validation). KITTI finetune We trained two different versions, both with the same protocol, for 550 epochs on KITTI combined: (i) resuming T2 and (ii) resuming T3. Although T3 got better performance in the evaluation, after finetuning, both results were similar on KITTI validation, as shown in Tab. 4. Sintel finetune Two different versions were trained with the same protocol, for 290 epochs on Sintel combined, both from T3: (i) with weight decay and random noise and (ii) without weight decay and random noise. The results, presented in Tab. 5, show that reducing regularization in Sintel finetune produced an extra gain. Dynamic scene scoping Since the scoping approaches were already tested on Sintel during the initial finetune, we further tested the four different approaches for dynamic scene scoping, detailed in Sec. 3.1, on the combined KITTI dataset. The results are depicted in Tab. 6. For KITTI, cropping the maximal valid crop per batch shows noticeable improvement from using a fixed sized crop. However, for KITTI datasets, strategy S4 (Eq. 6) shows much better performance than using the maximal valid crop size. In order to find the optimal range of crop size ratios (Eq. 6), we trained different models with different ranges of crop size to image ratios [r min , r max ]. All models used an upper crop size ratio limit r max equal to 1 (i.e. the maximal valid crop for the batch), and different lower limit r min , ranging from 0.5 to 1 and representing random crop sizes with different aspect ratios, which are larger than a quarter of the image. <ref type="figure" target="#fig_2">Fig. 4</ref> shows the training and validation accuracy as a function of the lower ratio of the range of randomized crop sizes. Specifically, the best results obtained with r min equals the 0.95, as also demonstrated in <ref type="figure" target="#fig_2">Fig. 4</ref>. The validation accuracy improves consistently when increasing r min from 0.5 to 0.95 and then starting to deteriorate until r min is reaching the maximal valid crop size. As can be seen, when enlarging the crop size expectation, we also reduce the regularization provided by the larger number of scopes (as analyzed in Eq. 4). This observation can be considered as additional evidence of a regularization-accuracy tradeoff in the training process. It also emphasizes the power of Eq. 6, in improving the training outcome, while keeping the regularization provided by partial cropping. Re-finetune with dynamic scene scoping In order to further understand the effect of this regularization-accuracy trade-off, we re-trained three models with the best random approach ([r min , r max ] = [0.95, 1]) on the KITTI combined set, using the same finetuning protocol. We took three different models, finetuned with r min ∈ {0.9, 0.95, 1}, as the checkpoint for this second finetune.</p><p>As described in the lower part of Tab. 6, finetuning again on the KITTI dataset improved the validation accuracy for all starting points (compared to their accuracy after <ref type="figure">Figure 5</ref>. Improving estimation for fast moving pixels. A qualitative comparison with the other two leading methods on the Sintel benchmark. Images were downloaded from MPI Sintel website, evaluated online on a test image, for the category of fast pixels (40+). Left to right: averaged first and second image and flow visualization for each method. EP E40+ is the end point error calculated on fast pixels. the first finetune). Surprisingly, in the second finetune, repeating the best approach (of randomizing using Eq. 6 with ([r min , r max ] = [0.95, 1])) did not provide the best result. The best approach was to finetune for the second time from a model with a larger range ([r min , r max ] = [0.9, 1]), thus stronger regularization, but lower EPE in the first finetune. We propose to consider this as additional evidence for the notion that gradually reducing regularization in optical-flow training, helps to achieve a better final local minima. Full training insights Concluding Sec. 5.2 experiments, we suggest the following: (i) larger scopes can improve optical flow training as long as the regularization provided by small crops is not needed, (ii) range based crop size randomization (Eq. 6) is a good strategy when regularization is needed, (iii) strong regularization is required on early stages, and should be relaxed when possible, and (iv) gains on early stages do not always improve the final accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Occlusion estimation</head><p>We evaluated the occlusion estimation of our trained models, using the F1 score, during all stages of the full training. As demonstrated in Tab. 5, it appears that gains in optical flow estimation are highly correlated with improvements in occlusion estimation. This might be due to the need for a network to identify non-matchable pixels and to infer the flow from the context. Tab. 7 shows a comparison of our F1 score to other reported results, on the MPI Sintel dataset. Our updated training protocol improves the best reported occlusion result by more than 5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Official Results</head><p>Evaluating our best models on the MPI Sintel and KITTI benchmarks shows a clear improvement over the IRR-PWC baseline, and an advantage over all other PWC-Net variants. MPI Sintel We uploaded results for three different versions: (i) with reduced regularization, (ii) with improved zooming schedule and (iii) with the best dynamic scoping approach. As Tab. 8 shows, there is a consistent improvements on the test set. This is congruent with the results in Tab. 1, obtained on the validation set.</p><p>At the time of submission, our method ranks first place on the MPI Sintel benchmark, improving two-frame methods by more than 10%, surpassing other competitive methods trained on multiple datasets, with multiple frames and all other PWCNet variants, using an equal or larger size of trainable parameters. On the clean pass, we improve the IRR-PWC result by 20 ranks and 7%. Interestingly, analyzing Sintel categories in Tab. 8, our model is leading in the categories of fast pixels (S40 + ) and non-occluded pixels, while also producing the best estimation for occluded pixels among two frame methods. This is consistent with our insights on these challenging categories from Sec. 3. <ref type="figure">Fig. 5</ref> shows a comparison of our method in the category of fast pixels, with the other two leading methods on Sintel. KITTI On KITTI 2012 and KITTI 2015, we saw a consistent improvement from the baseline model results, of more than 19.7% and 12% respectively, surpassing all other published methods of the popular PWC-Net architecture, and achieving state-of-the art EPE results among two frame methods. Since our training protocol can be readily applied to other methods, we plan, as future work, to test it on other leading architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>While a lot of effort is dedicated for finding effective network architectures, much less attention is provided to the training protocol. However, when performing complex multi-phase training, there are many choices to be made, and it is important to understand, for example, the proper way to schedule the multiple forms of network regularization. In addition, the method used for sampling as part of the augmentation process can bias the training protocol toward specific types of samples.</p><p>In this work, we show that the conventional scope sampling method leads to the neglect of many challenging samples, which hurts performance. We advocate for the use of larger scopes (crops and zoom-out) when possible, and a careful crops positioning when needed. We further show how regularization and augmentation should be relaxed as training progresses. The new protocol developed has a dramatic effect on the performance of our trained models and leads to state of the art results in a very competitive domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Introduction</head><p>With this appendix, we would like to provide more details on our training pipeline and framework, as well as more visualizations of the improved flow and occlusion estimation.</p><p>The ScopeFlow approach provides an improved training pipeline for optical flow models, which reduces the bias in sampling different regions of the input images while keeping the power of the regularization provided by fixed-size partial crops. Due to the sizable impact on performance that can be achieved by the improved training pipeline, we created a generic, easy to configure, training package, in order to encourage others to train state of the art models with our improved pipeline, as described in Sec. C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dynamic scoping</head><p>The common pipeline of batch sampling and augmentation in optical flow training includes four stages: (i) sampling images, (ii) applying random photometric changes, (iii) applying a random affine transformation, and (iv) cropping a fixed-size randomly located patch. We propose changes for stages (iii) and (iv), by choosing the zooming parameters more carefully along with the training, and incorporating a new randomized cropping scheme, presented and extensively tested in our paper. <ref type="figure" target="#fig_3">Fig. 6</ref> provides a demonstration of the ScopeFlow pipeline, which enlarges the variety of scopes presented during the data-driven process while reducing the bias towards specific categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. ScopeFlow software package</head><p>In order to simplify the applicability of our approach, we created a small and easy to use package, which supports YAML configurations of a multi-stage optical flow model training and evaluation. We found this approach very helpful when running experiments for finding the best scoping augmentation approach.</p><p>Our code and models are available online. Furthermore, we provide easy visualization of our online augmentation pipeline, as described in the README of our package.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison to the IRR baseline</head><p>In our experiments, we use the IRR <ref type="bibr" target="#b10">[11]</ref> variant, of the popular PWC-Net architecture, to evaluate our method. This variant has shown to provide excellent results, while keeping a low number of parameters. To emphasize the improvements, we give here a thorough comparison, of all the public results obtained in the main three benchmarks, for our method and the IRR baseline. Training with ScopeFlow online-processing approach leads to the learning of richer features and reduces the error in challenging motion categories, such as fast speed and occluded pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. MPI Sintel</head><p>Other than leading the MPI Sintel <ref type="bibr" target="#b2">[3]</ref> table, as can be seen in Tab. 11 and Tab. 12 in Sec. G, we improve the baseline IRR models by a large margin in all metrics, and in particular the challenging metrics of occlusions (14.7%) and fast pixels (18.4%). The only metric that did not improve is the metrics of low-speed pixels (&lt; 40), which should not be a surprise, since our method reduces the bias between the fast and slow pixels, as shown in our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. KITTI 2012</head><p>We uploaded our results to the KITTI 2012 <ref type="bibr" target="#b6">[7]</ref> benchmark. As can be seen in Tab. 9 and Tab. 10, training the IRR model with ScopeFlow pipeline improves the mean EPE by more than 20%. Moreover, the improvement is achieved for all thresholds of outliers and for all metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IRR on KITTI 2012:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error</head><p>Out-Noc Out-All Avg-Noc Avg-All In addition, <ref type="figure">Fig. 7</ref> provides a qualitative comparison to the baseline IRR model on the KITTI 2012 benchmark. As can be seen, most of the improvement provided by the ScopeFlow pipeline is in the challenging occluded and marginal pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. KITTI 2015</head><p>We uploaded our results to the KITTI 2015 <ref type="bibr" target="#b21">[22]</ref> benchmark. As can be seen in Tab. 11 and Tab. 12, training the IRR model with ScopeFlow pipeline improves the mean EPE by more than 12%, in the default category of 3 pixels. Moreover, the improvement is achieved for all thresholds of outliers and for all metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IRR on KITTI 2015:</head><p>Error In addition, <ref type="figure">Fig. 8</ref> provides a qualitative comparison to the leading VCN model on the KITTI 2015 benchmark, showing a clear improvement for handling non-background challenging objects. Our results are leading the category of non-background pixels, which belong to faster foreground objects. <ref type="figure" target="#fig_4">Fig. 9</ref> provides a demonstration of the contribution of different training changes, composing the ScopeFlow pipeline presented in our paper, to the improvement of the final flow. As expected, most of the improvements are in the marginal image area. Our method improves, in particular, the moving objects, which have many occluded and fast-moving pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Occlusions comparison</head><p>In order to provide a qualitative demonstration of our improved occlusion estimation, we compared our results to the methods with the highest reported occlusion estimation. We provide a layered view of false positive, false negative and true positive predictions. All occlusion estimations created using the pre-trained models, published by the authors, and sampled from the Sintel final pass dataset. <ref type="figure" target="#fig_5">Fig. 10</ref> shows that the model trained with the ScopeFlow pipeline improves occlusion estimation in the marginal image area and mainly for foreground objects. We used the F1 metric, with an average approach of 'micro' (the same trend presented by all averaging approaches).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Public tables</head><p>We uploaded our results to the two main optical flow benchmarks: MPI Sintel and KITTI <ref type="bibr">(2012 &amp; 2015)</ref>. In the subsections below, we provide the screenshots that capture the sizable improvements achieved by using our pipeline for training an optical flow model, with an off-the-shelf, low parameters model. Since our method can support almost any architecture, we plan, as future work, to apply it to other architectures as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1. MPI Sintel</head><p>We add here two screenshots of the public table: (i) the table on the day of upload <ref type="bibr">(14.10.19)</ref>, and (ii) the table after the official submission deadline for CVPR 2020. As shown in <ref type="figure">Fig. 11</ref>, our method ranks first on MPI Sintel since 14.10.19, surpassing all other methods, and leading the categories of: (i) matchable pixels, (ii) pixels far more than 10px from the nearest occlusion boundary, and (iii) fastmoving pixels (&gt; 40 pixels per frame). We also provide a screenshot taken after the official CVPR paper submission deadline, in <ref type="figure" target="#fig_0">Fig. 12</ref>, showing our method still leading the Sintel benchmark. We changed our method's name after the initial upload (on 14.10.19) from OFBoost to ScopeFlow. <ref type="figure">Figure 7</ref>. Qualitative comparison to the IRR baseline on KITTI 2012 benchmark. Our improved training pipeline got the lowest AEPE on KITTI 2012 among all other two-frame methods, using a low parameters off-the-shelf model architecture, which has an inferior performance on the KITTI benchmarks. Occluded regions are marked in red, erroneous regions with a higher intensity. Most of the improvement provided by ScopeFlow is in these challenging marginal pixels. <ref type="figure">Figure 8</ref>. Qualitative comparison to the VCN <ref type="bibr" target="#b31">[32]</ref> method KITTI 2015 benchmark. Although the VCN architecture gets the best outlier percentage among all pixels, we have a better handling for non-background objects among all other two-frame methods. <ref type="figure" target="#fig_1">Fig. 13</ref> shows a screenshot of the KITTI 2012 flow table, with the lowest outlier threshold (of 2%), taken on the CVPR paper submission deadline. Our method provides the lowest average EPE among all published two-frame methods, lower by 23% from the IRR-PWC baseline results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2. KITTI 2012</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3. KITTI 2015</head><p>Fig. 14 shows a screenshot of the KITTI 2015 flow table, taken on the CVPR paper submission deadline. Our method provides the lowest percentage of outliers, averaged over foreground regions, among all published two-frame methods. Moreover, the percentage of outliers, averaged over all ground truth pixels, is lower by more than 12% from the IRR-PWC baseline results.   <ref type="bibr" target="#b12">[13]</ref>, (b) IRR-PWC <ref type="bibr" target="#b10">[11]</ref> baseline, and (c) ScopeFlow (ours). First frame on the left column and ground truth flow on the right column. For each occlusion map: false positive are in blue, false negative in red, and true positive in white. All occlusion maps estimated using Sintel Final samples and the original models published by the authors. Our improvements are mainly for foreground objects on the image margins. <ref type="figure">Figure 11</ref>. Public Sintel table on the day of upload (taken on 14.10.19). Our method is leading the challenging final pass of the MPI Sintel benchmark. We renamed our method for clarity from OFBoost to ScopeFlow. ScopeFlowR is our method with regularization changes, ScopeFlowZ is our version with zooming changes. ScopeFlow is our final version with dynamic scoping. <ref type="figure" target="#fig_0">Figure 12</ref>. Public Sintel table after CVPR papers submission deadline (taken on <ref type="bibr">18.11.19)</ref>. Our method is still leading the main Sintel table after the addition of many new methods. <ref type="figure" target="#fig_1">Figure 13</ref>. Public KITTI 2012 flow table (with the lowest outlier threshold of 2%) on the CVPR paper submission deadline (taken on <ref type="bibr">15.11.19)</ref>. Our method is with the lowest AEPE among all published two-frame methods, lower by 23% from the IRR-PWC baseline. <ref type="figure" target="#fig_2">Figure 14</ref>. Public KITTI 2015 flow table on the CVPR paper submission deadline (taken on <ref type="bibr">15.11.19)</ref>. Our method is with the lowest percentage of foreground (objects) outliers among all published two-frame methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of Lemma 1. The probability for a pixel to be sampled within a valid random crop location, depends on the image width W , the crop width w and the distance to the closest border ∆x. For both samples W = 8. Top: w = 3 (w ∈ Crops small ). Bottom: w = 5 (w ∈ Crops large ). Each pixel is labeled with ∆x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Sampling bias caused by fixed size random crops. (a),(b),(c): Pixel sampling probability maps for a fixed sized crop, with ratios of 50%, 70% and 90% respectively, for each axis. The probability to sample a marginal pixel shrinks drastically with the crop size. (d),(e): areas with strong prevalence for motion categories. High velocities tend to start from lower corners, while small ones tend to occur in the middle and upper part of the scene. (f),(g): graphs of the changing ratio of sampling probabilities between fast (&gt; 40) and slow (&lt; 10) pixels, for different crop and image axes ratios. clearly, fast pixels benefit more when increasing the crop size than slow pixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Accuracy of models trained with different ranges of random crop sizes, on the combined KITTI dataset. The maximal crop size is the full image. Validation AEPEs improve when increasing the minimal crop size ratio (rmin in Eq. 6) up to 95% of the full image axes. AEPE for a fixed sized crop based training: * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Randomized scoping within [rmin, rmax] = [0.5, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 .</head><label>9</label><figDesc>Ablation visualization on the MPI Sintel training set. (a) First image, (b) IRR-PWC baseline, (c) ScopeFlowR (reduced regularization), (d) ScopeFlowZ (zooming schedule), (e) ScopeFlow (final model), (f) Ground Truth flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 .</head><label>10</label><figDesc>Occlusion comparison over Sintel final pass. Comparison of occlusion estimations created by: (a) FlowNet-CSSR-ft-sd</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 3 .Table 5 .Table 6 .</head><label>1356</label><figDesc>.312 1.728 1.651 0.057 2.623 3.224 1.654 1.644 0.056 2.453 3.108 1.580 1.649 0.056 2.428 3.053 1.182 1.594 0.059 2.537 3.081 1.070 1.402 0.051 2.320 2.987 1.225 1.607 0.059 2.349 2.971 1.094 1.434 0.051 Finetuning experiments. Results were calculated with AEPE, except outlier percentage for KITTI validation. RD + RN is for removing random noise and weight decay. Zoom changes include an increased zoom out and a gradual reduction of the zoom in. Max crop is for using the maximal valid crop size for a batch. Random crop is for using Eq. 6 when sampling the crop size. The negative effect of over-training and reducing regularization on early stages. T3 and T4 were trained with larger scopes. The positive effect of reducing regularization in finetune. All models trained on Sintel combined, from T3. Random cropping experiments. Ranges are specified in [], sets in {}, and fixed sizes in ().</figDesc><table><row><cell>RD+ RN</cell><cell>Zoom changes</cell><cell>Max crop</cell><cell cols="2">Random crop</cell><cell cols="2">Sintel train</cell><cell>Sintel val</cell><cell>KITTI train</cell><cell>KITTI val</cell><cell>KITTI val Out%</cell><cell>Method</cell><cell>MEPE</cell><cell>Outlier %(EPE &gt;3 px)</cell></row><row><cell cols="11">x 2.660 3Model Max zoom WD+RN VAL MEPE Sintel MEPE x x x</cell><cell cols="2">#1: FP (320,896) #2: FF (370,1224) #4: RR [0.75,0.9] #3: FR {(0.73,0.69),(0.84,0.86),(1,1)} 1.466 1.651 1.594 1.472 #4: RR [0.9,1] 1.435 #4: RR [0.95,1] 1.402 Re-finetune #2: FF (370,1224) -&gt;#4: RR [0.95,1] 1.421 #4: RR [0.95,1] -&gt;#4: RR [0.95,1] 1.393 #4: RR [0.9,1] -&gt;#4: RR [0.95,1] 1.377 FP is fixed partial, FF is fixed 0.057 0.059 0.052 0.053 0.053 0.051 0.052 0.051 0.051 full, RR is random range (Eq. 6), FR is fixed range (Eq. 5a). Up:</cell></row><row><cell>C*</cell><cell></cell><cell>1.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">-</cell><cell cols="2">3.138</cell><cell cols="2">best results from each method described in Sec. 3.1 (the method number is on the left). Bottom: retraining experiments show that</cell></row><row><cell>C1</cell><cell></cell><cell>1.5</cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell cols="2">1.622</cell><cell cols="2">3.264</cell><cell cols="2">it is better to train a more regularized model in the first KITTI</cell></row><row><cell>C2</cell><cell></cell><cell>1.3</cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell cols="2">1.597</cell><cell cols="2">3.321</cell><cell cols="2">finetune, although it gets a lower MEPE on the first finetune.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Method</cell><cell>Clean Final Mean Type</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FlowNet2 [12]</cell><cell>0.377 0.348 0.362 fwd-bwd</cell></row><row><cell cols="2">Things Model</cell><cell cols="2">WD+ RN</cell><cell cols="2">Start From</cell><cell cols="2">Epochs</cell><cell cols="2">Val MEPE</cell><cell>Sintel MEPE</cell><cell>MirrorFlow S2DFlow ContinualFlow [23]</cell><cell>0.39 0.348 0.369 CNN 0.47 0.403 0.436 CNN</cell></row><row><cell cols="2">T2</cell><cell></cell><cell></cell><cell>C*</cell><cell></cell><cell cols="4">109-159 1.843</cell><cell>2.613</cell><cell></cell></row><row><cell cols="2">T3</cell><cell></cell><cell></cell><cell>C1</cell><cell></cell><cell cols="4">109-159 1.829</cell><cell>2.544</cell><cell></cell></row><row><cell cols="2">T4</cell><cell>x</cell><cell></cell><cell>T3</cell><cell></cell><cell cols="4">159-165 1.817</cell><cell>2.545</cell><cell></cell></row><row><cell cols="11">KITTI model Start from Val MEPE Outliers</cell><cell></cell></row><row><cell cols="2">T2 K</cell><cell></cell><cell></cell><cell cols="2">T2</cell><cell></cell><cell cols="2">1.474</cell><cell cols="2">0.054</cell><cell></cell></row><row><cell cols="2">T3 K</cell><cell></cell><cell></cell><cell cols="2">T3</cell><cell></cell><cell cols="2">1.475</cell><cell cols="2">0.053</cell><cell></cell></row><row><cell cols="11">Sintel model WD+RN Val MEPE Val OCC F1</cell><cell></cell></row><row><cell cols="2">T3 SC1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">2.119</cell><cell cols="2">0.700</cell><cell></cell></row><row><cell cols="2">T3 SC2</cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell cols="2">2.108</cell><cell cols="2">0.703</cell><cell></cell></row><row><cell cols="11">2. Zooming strategies. We found that applying a new</cell><cell></cell></row><row><cell cols="11">random zooming range of [0.8, 1.5] alone, which increases</cell><cell></cell></row><row><cell cols="11">the zoom out, and gradually reducing the zoom in to 1.3,</cell><cell></cell></row><row><cell cols="11">achieved considerable gains for Sintel in all evaluation</cell><cell></cell></row><row><cell cols="11">parameters, with and without cropping strategy changes.</cell><cell></cell></row><row><cell cols="11">Interestingly, increasing the zoom out range without any</cell><cell></cell></row><row><cell cols="11">change to the crop size provided 50% of this gain. We sug-</cell><cell></cell></row></table><note>Table 2. Removing regularization in pretraining. Models trained 108 epochs, from initialized weights, without weight decay and random noise, for two maximal zoom values, on FChairsOcc.Table 4. Higher gains in early stages do not always translate to fine-tune gains. All models trained on KITTI combined.gest that this is additional evidence for the existing bias in small crop sizes, as explained in Sec. 3. 3. Removing artificial regularization. Removing the ran- dom noise and weight decay helped us to achieve extra 2%-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 8 .</head><label>8</label><figDesc>Public benchmarks results. Models with comparable architecture (PWC-Net) are marked with *. Models using extra data in finetune are marked with +. Our baseline model is marked withˆ. We get the best EPE results in both Sintel and KITTI2012 benchmarks, surpassing all other comparable variants of our baseline model on KITTI2015, with a considerable improvement to our baseline.</figDesc><table><row><cell>, showing</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 12 .</head><label>12</label><figDesc>ScopeFlow results on KITTI 2015</figDesc><table><row><cell></cell><cell>Fl-bg</cell><cell>Fl-fg</cell><cell>Fl-all</cell></row><row><cell cols="4">All / All 7.68 % 7.52 % 7.65 %</cell></row><row><cell cols="4">Noc / All 4.92 % 4.62 % 4.86 %</cell></row><row><cell cols="4">Table 11. IRR results on KITTI 2015</cell></row><row><cell cols="2">ScopeFlow on KITTI 2015:</cell><cell></cell><cell></cell></row><row><cell>Error</cell><cell>Fl-bg</cell><cell>Fl-fg</cell><cell>Fl-all</cell></row><row><cell cols="4">All / All 6.72 % 7.36 % 6.82 %</cell></row><row><cell cols="4">Noc / All 4.44 % 4.49 % 4.45 %</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant ERC CoG 725974).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Seeing 3d chairs: Exemplar partbased 2d-3d alignment using a large dataset of cad models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cnnbased patch matching for optical flow with thresholded hinge embedding loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="2710" to="2719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision (ECCV), Part IV</title>
		<editor>A. Fitzgibbon et al.</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2012-10" />
			<biblScope unit="volume">7577</biblScope>
			<biblScope unit="page" from="611" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Husser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Mathematical Models in Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Patchbatch: a batch augmented loss for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gadot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="328" to="341" />
			<date type="published" when="2008-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Liteflownet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.07414</idno>
		<title level="m">A lightweight optical flow cnn -revisiting data fidelity and regularization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Iterative residual refinement for joint optical flow and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Occlusions, motion and depth boundaries with a generic network for disparity, optical flow or scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The hci benchmark suite: Stereo and flow ground truth with uncertainties for urban autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kondermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Honauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Krispin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Andrulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burkhard</forename><surname>Gussefeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Rahimimoghaddam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claus</forename><surname>Brenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DDFlow: Learning Optical Flow with Unlabeled Data Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Selflow: Self-supervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Proflow: Learning to predict optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrés</forename><surname>Bruhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What makes good synthetic training data for learning disparity and optical flow estimation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">942960</biblScope>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DGC-Net: Dense geometric correspondence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iaroslav</forename><surname>Melekhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Tiulpin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Continual occlusions and optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Neoral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Ochman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A fusion approach for multi-frame optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhile</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Models matter, so does training: An empirical study of cnns for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Occlusion aware unsupervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4884" to="4893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Optical flow in mostly rigid scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Accurate optical flow via direct cost volume processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Volumetric correspondence networks for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hierarchical discrete distribution decomposition for match density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Interponet, a brain inspired neural network for optical flow dense interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
