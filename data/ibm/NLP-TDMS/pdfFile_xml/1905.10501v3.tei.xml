<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Reason in Large Theories without Imitation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitij</forename><surname>Bansal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IST</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IST</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
							<email>szegedy@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IST</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IST</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><forename type="middle">N</forename><surname>Rabe</surname></persName>
							<email>mrabe@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IST</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IST</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Loos</surname></persName>
							<email>smloos@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IST</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IST</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Toman</surname></persName>
							<email>vtoman@ist.ac.at</email>
							<affiliation key="aff0">
								<orgName type="institution">IST</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Reason in Large Theories without Imitation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we demonstrate how to do automated theorem proving in the presence of a large knowledge base of potential premises without learning from human proofs. We suggest an exploration mechanism that mixes in additional premises selected by a tf-idf (term frequency-inverse document frequency) based lookup in a deep reinforcement learning scenario. This helps with exploring and learning which premises are relevant for proving a new theorem. Our experiments show that the theorem prover trained with this exploration mechanism outperforms provers that are trained only on human proofs. It approaches the performance of a prover trained by a combination of imitation and reinforcement learning. We perform multiple experiments to understand the importance of the underlying assumptions that make our exploration approach work, thus explaining our design choices.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Theorem proving is a challenging benchmark for automated reasoning, and is an important milestone on the road to demonstrating that machine learning can produce a deep understanding of abstract concepts. In the long run, automated mathematical reasoning may become an important tool in engineering and scientific discovery. Due to their success in many other areas, neural networks have recently been considered as a way to guide theorem proving <ref type="bibr" target="#b0">[Alemi et al., 2016</ref><ref type="bibr" target="#b1">, Gauthier et al., 2017</ref><ref type="bibr" target="#b2">, Loos et al., 2017</ref><ref type="bibr" target="#b4">, Bansal et al., 2019</ref><ref type="bibr" target="#b5">, Paliwal et al., 2020</ref> and demonstrate approximate mathematical reasoning abilities in latent space .</p><p>While there is only a relatively small number of fundamental proof rules (or proof tactics) applicable at any point in a proof, there is a very large number of premises (i.e., previously proven theorems and lemmas) that could be invoked. The main problem of reasoning in large theories thus is to identify the premises relevant in the current context and thereby reduce the branching factor of the proof search to a manageable size. This problem will become even more pronounced over time, as the theorem provers become more powerful, growing the number of available premises.</p><p>Previous works have relied on human proofs to either directly provide or learn <ref type="bibr" target="#b4">[Bansal et al., 2019</ref><ref type="bibr" target="#b5">, Paliwal et al., 2020</ref> which premises are relevant to the current proof. However, any open-ended system for mathematical reasoning needs to be able to learn which premises are relevant without human guidance. In this work, we thus consider the problem of training a theorem prover without access to human proofs.</p><p>We demonstrate that training the theorem prover without human data succeeds when we use reinforcement learning and help exploration by selecting a portion of the premises with a tf-idf <ref type="bibr" target="#b7">[Manning et al., 2008]</ref> metric. The theorem prover thereby learns to prove more theorems than the prover trained on human proofs alone and almost as many as with the combination of both approaches. Hence, we solve one of the road blocks on the way to open-ended learning of mathematical reasoning in large theories.</p><p>2 Related work Reinforcement learning. Reinforcement learning without imitation learning has been successful for computer games (cf. <ref type="bibr" target="#b8">Mnih et al. [2013]</ref>) and it was demonstrated later in <ref type="bibr" target="#b9">Silver et al. [2017]</ref> that imitation learning is not necessary for complex games like Chess and Go. For more complex games with much larger action spaces, learning methods still rely on human imitation due to the exploration problem (cf. <ref type="bibr" target="#b10">Vinyals et al. [2019]</ref>). The question of exploration is well studied in reinforcement learning <ref type="bibr" target="#b11">[Houthooft et al., 2016</ref><ref type="bibr" target="#b12">, Burda et al., 2019</ref>, but existing approaches such as -greedy do not work for premise selection because of very large (practically infinite) action space.</p><p>Learning premise selection. Premise selection has been an active research topic in the domain of automated theorem proving <ref type="bibr" target="#b13">[Alama et al., 2014</ref><ref type="bibr" target="#b14">, Kaliszyk and Urban, 2015</ref><ref type="bibr" target="#b15">, Blanchette et al., 2016</ref><ref type="bibr" target="#b16">, Wang et al., 2017</ref>. Neural networks were first applied to premise selection for automated theorem proving in <ref type="bibr" target="#b0">Alemi et al. [2016]</ref>. Other works have focused on tactic selection  and tactic generation <ref type="bibr" target="#b17">[Yang and Deng, 2019]</ref>. All of these works rely on human proof data. <ref type="bibr" target="#b18">Kaliszyk et al. [2018]</ref>, <ref type="bibr" target="#b19">Zombori et al. [2019]</ref> and <ref type="bibr" target="#b20">Zombori et al. [2020]</ref> are based on reinforcement learning, however they do not address the problem of premise selection, which is the hard part of exploration due to the unbounded repository of premises.</p><p>We use the HOList environment <ref type="bibr" target="#b4">[Bansal et al., 2019]</ref> in our experiments. Similar to HOList for HOL Light <ref type="bibr" target="#b21">[Harrison, 1996]</ref>, GamePad , CoqGym <ref type="bibr" target="#b17">[Yang and Deng, 2019]</ref>, TacticToe <ref type="bibr" target="#b1">[Gauthier et al., 2017]</ref>, and Proverbot9001 <ref type="bibr" target="#b22">[Sanchez-Stern et al., 2019]</ref> are machine learning environments for proof assistants Coq <ref type="bibr">[Coq]</ref> and HOL4 <ref type="bibr" target="#b24">[Slind and Norrish, 2008]</ref>. We chose to use HOList because of the breadth of topics of mathematics in the dataset. Additionally, HOList is already integrated into a reinforcement learning setup, which our approach relies on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>Theorem proving. In this work we build on proof assistants, which have been built to enable humans to write and then automatically check proofs. In contrast to mathematical textbooks and papers, which are written mostly in natural language, we call mathematics formalized in proof assistants to be formal mathematics. In this work we focus on the proof assistant HOL Light <ref type="bibr" target="#b21">[Harrison, 1996]</ref>, in which a wide range of mathematical theories have been formalized, and which has been famously used for the formalization of the Kepler conjecture <ref type="bibr" target="#b25">[Hales et al., 2017]</ref>. Other proof assistants include Coq <ref type="bibr">[Coq]</ref>, HOL4 <ref type="bibr" target="#b24">[Slind and Norrish, 2008]</ref>, Isabelle <ref type="bibr" target="#b26">[Wenzel et al., 2008]</ref>, and Lean <ref type="bibr" target="#b27">[de Moura et al., 2015]</ref>. HOL Light, as in many other proof assistants, relies mostly on "backward" proof steps. In contrast to "forward" proof steps, in which we only manipulate already proven statements, backward proofs start with a proof goal (the statement of the theorem to be proven) and apply proof tactics until all goals are proven.</p><p>In <ref type="figure">Figure 1</ref>, we give an example of a backward proof. The goal here is to prove x + 0 = x, for all x ∈ N, and we apply the tactic MATCH_MP_TAC to the goal. Like many tactics, this tactic takes a  <ref type="bibr" target="#b5">Paliwal et al. [2020]</ref>. Note that this work is largely agnostic to the model architectures.</p><p>premise (i.e. a previously proven theorem or lemma) as a parameter. In this example, we use the induction theorem NAT_INDUCTION as a premise. This tactic application splits the first goal into two subgoals, corresponding to the base case and the induction step. The semantics of an application of a proof tactic is that, if all subgoals are proven, then also the goal to which the tactic has been applied is proven. In our case, we can prove both of the subgoals by simple arithmetic reasoning, provided by the tactic ARITH_TAC. This tactic here does not require additional premises and returns an empty list of subgoals (for both of the subgoals we apply it to), meaning that they are proven, and hence the original goal is proven.</p><p>Learning proof guidance for interactive theorem proving. It is a long-standing goal in artificial intelligence to automate the theorem proving process described above, in particular to relieve the human experts from selecting the tactics and premises in each proof step. Historically, most works focused on designing advanced search algorithms, leading to entire fields such as SAT and SMT solving and first-order theorem proving. Recently, learning proof search strategies from data has become an area of active research <ref type="bibr" target="#b0">[Alemi et al., 2016</ref><ref type="bibr" target="#b1">, Gauthier et al., 2017</ref><ref type="bibr" target="#b2">, Loos et al., 2017</ref><ref type="bibr" target="#b18">, Kaliszyk et al., 2018</ref><ref type="bibr" target="#b4">, Bansal et al., 2019</ref><ref type="bibr" target="#b28">, Lederman et al., 2020</ref>.</p><p>In this work, we follow the approach by <ref type="bibr" target="#b4">Bansal et al. [2019]</ref>, which has shown the unique ability to find relevant premises, which has been a big challenge for the classical techniques. <ref type="figure">Figure 2</ref> illustrates the tactic and premise selection architecture introduced by <ref type="bibr" target="#b4">Bansal et al. [2019]</ref> and improved by <ref type="bibr" target="#b5">Paliwal et al. [2020]</ref>. For each proof step, this architecture scores the tactics and it also produces a relevance score for each potential premise. While there are only few proof tactics, there are on average around 10,000 potential premises for each proof step. The number of potential premises will likely further grow in the future if this approach is applied to even larger mathematical theories. Then, for each tactic, the top-k premises are given as arguments (unless the tactic does not require premise arguments). This results in a list of candidate tactic applications, which can be used as the actions in any search approach. We adopted the same search strategy as <ref type="bibr" target="#b4">Bansal et al. [2019]</ref> and <ref type="bibr" target="#b5">Paliwal et al. [2020]</ref>, which is a simple breadth-first search with a parameter of how many of the candidate tactic applications should be expanded per proof goal.</p><p>The tactic and premise selection architecture is trained on successful proofs. For imitation learning, tuples of goal, tactic, and used premises are extracted from human proofs formalized in HOL Light. The focus of this work, however, is to not learn from human proofs, and instead learn in a reinforcement learning setup from the proofs that earlier versions of the policy have found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning without Imitation</head><p>In this section, we explain the setup for learning to prove in the absence of human proofs, and the considerations that informed our final design.</p><p>Much of mathematics that has been formalized by humans is in pursuit of formalizing certain theorems such as the four color theorem <ref type="bibr" target="#b29">[Gonthier, 2008]</ref> and the Kepler conjecture <ref type="bibr" target="#b25">[Hales et al., 2017]</ref>. Since formalization is a challenging and tedious process requiring experts, a very small fraction of mathematics is formalized after decades of human effort. This work paves the way for a critical piece of a system wherein its knowledge base of formally proven theorems grows continuously. We separate two key aspects of such a system. First, proposing potentially true statements, also referred to in the literature as conjecturing. Second, given a new statement, proving it in absence of existing proofs to learn from. We would like to tackle the latter question in this work directly.  The key information the human proofs provide is the overall direction of the proof via selection of the relevant premises at each proof step. In case human proofs are available, one can first train a machine learning model to imitate them (as described in Section 3), as has been done in several previous works (Section 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reinforcement learning loop.</head><p>In the absence of human proofs, we need a mechanism to incrementally improve a proof guidance model, which motivates the reinforcement learning setup we use. <ref type="figure" target="#fig_2">Figure 3a</ref> shows the components of the reinforcement learning loop from <ref type="bibr" target="#b4">Bansal et al. [2019]</ref>, which we build upon. A proof guidance model is trained with continuously expanding training data. In order to generate the continuously expanding training data, several theorem provers run in lockstep with training the policy and premise selection network. The provers try to prove the statements in the training set using the model for proof guidance as it is training. If it manages to prove a statement, the proof is used to generate additional training data. Intuitively, we would like to reward the choices of tactics and premises that were "useful" at each step in the proof. A subtle but crucial aspect is that the proofs are pruned before generating the training data. In this step, premises that are not necessary for the proof are removed. This interplay between over-approximation and pruning is a major contributing factor to the efficiency of our exploration method and is studied in Subsection 4.2. <ref type="figure" target="#fig_2">Figure 3b</ref> shows how the list of premises are picked, including our proposed change to aid exploration of the premises. Given a goal, the currently learnt model is used to pick the top-k 1 highest scored premises from the knowledge base of premises available: {P 1 , P 2 , . . . , P k1 }. Simultaneously, we propose generating another list of premises {Q 1 , Q 2 , . . . Q k2 } to explore, picked according to a metric discussed shortly (Section 4.1). The final set of premises is obtained by interleaving the two premise lists. k 1 and k 2 are hyperparameters which can be varied to control exploitation of the learnt model (higher k 1 ) vs exploration (higher k 2 ). The REFERENCE setup is one without our modification (i.e., k 2 = 0). On the other hand, the EXPLORE setup includes additional premises as proposed here.</p><p>There are three aspects to this that we wish to highlight, and that have informed the design: the strategy to generate the list of premises to explore, the effect of using irrelevant premises in an action (over-approximation of premise lists), and pruning. We discuss each of these in detail, designing experiments to inform different choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Information retrieval for premise selection</head><p>One of the key failure modes of our reinforcement learning setup trained without human proofs is not being able to prove new theorems. With no new training data, the learning process would stall. Thus, it is crucial that we continuously expand the horizon of theorems that we are able to prove. Since we generate new training data only when we manage to prove a theorem, we wish to pick premises most likely relevant to prove the current goal.</p><p>In information retrieval literature, notions such as tf-idf <ref type="bibr" target="#b7">[Manning et al., 2008]</ref> have been used to retrieve relevant documents corresponding to a query. We view premise selection as a retrieval problem, thinking of the current goal we are trying to prove as the query, and the knowledge base of previously proven theorems (premises) as the documents from which we would like to retrieve.</p><p>Given a goal G, we use pre-engineered similarity scoring s(G, P ) to rank a potential premise P for its usefulness in the next action (tactic application) with the target of proving the goal. In our setup, we restrict our attention to functions of the form s(G, P ) = r(G)/ r(G) , r(P )/ r(P ) , where r is some simple vector representation of the expression and ·, · the dot product. This is also sometimes referred to as the cosine simarity. We consider r of the form r(P ) i = tf(P, i)idf(i), where the i-component corresponds to the tokens occurring in the formulas, idf is the "inverse document frequency" function which is precomputed by idf(i) = log(N/n i ), where N is the total number of theorems and n i is the number of theorem containing the i-th token. For the term frequency function tf(P, i), we have tested three possible choices: boolean weighting: 1 if f i (P ) &gt; 0 and 0 otherwise, logarithm weighting: 1 + log(f i (P )) and natural: tf(P, i) = f i (P ), where f i (P ) is the number of occurrences of the i-th token in expression P .</p><p>Running a full reinforcement learning loop uses over 25 years of CPU resources (see supplement, Appendix B). Since it is prohibitive to run a lot of experiments with the full system, it suggests that we should first evaluate the quality of similarity metrics in an offline manner. That is, we measure how well those metrics perform on existing proof traces and we only apply the similarity scores that performed best in a separate evaluation.</p><p>In order to measure the quality of a similarity score, we have adopted the metrics from <ref type="bibr" target="#b0">Alemi et al. [2016]</ref>. We have measured the average relative maximum rank and the top-k recall numbers for a small set of relevant k values (for k = 8, 16, 32 and 64) on a random selection of proofs comprising of 20% of the training set of the "complex" corpus of HOList. The relative maximum rank of a true positive document (premise) is the absolute maximum rank of the true documents divided by the size of all possible premises (from which we make the selection), the average is then taken over all retrieval tasks. The results are summarized in <ref type="table" target="#tab_0">Table 1</ref>. Note that low average maximum relative rank and high recall values indicate better premise selection performance of the similarity measure.</p><p>To summarize, we pick the boolean term-frequency weighting scheme. In addition, we speculate it could be helpful to add more variation of premises picked for exploration for a given goal, as over the course of a loop same goals are attempted multiple times. To add this variation, we add a dropout probability hyperparameter p to components of the representation: r(G) i is zeroed with probability p when computing the representation for G. p is set to 0.1 unless specified otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Over-approximation of premises</head><p>Our exploration is based on the assumption that adding a few irrelevant premises does not influence the outcome of tactic applications significantly. This allows us to accelerate the search by trying to over-approximate the set of premises used in tactic applications. We study the behavior of the most frequently occurring tactics that take premises as an argument: MESON and REWRITE.</p><p>MESON is based on a first-order logic solver. We study how many extra premises we can add before MESON starts to fail to prove a goal. To do so, we first sample at random proof steps from the human proofs (which are always successful). For each proof step we add as tactic arguments random irrelevant premises from the knowledge base of premises available at that proof step. We report the ratio of successful proof attempts after adding a set of tactic parameters with varying cardinality. For REWRITE operations, we study the number of extra premises we can add and expect not to change the outcome of the rewrite operation. For both experiments, we sampled random proofs and one random proof step with the desired type of tactic (MESON or REWRITE), until we sampled 250 steps successfully. Then for each l ∈ {1, 2, 4, 8, 16, 32}, we sampled five different random parameter lists with length l. In our experiment, we append those parameters to our parameter list and execute the same tactic with the extended parameter list. <ref type="table" target="#tab_1">Table 2</ref> shows the ratio of application with the outcome being identical with that of the tactic application without the extra parameters. We can see that even adding 32 extra random parameters does not change the outcome of the rewrite tactics over 70% of the time. However, MESON tends to time out with more than 32 extra premises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pruning</head><p>As discussed in Section 4.2, if a tactic application succeeds, not all premises provided might have been used. In fact, we are using this fact to accelerate our exploration. However, we do not wish to learn from these irrelevant premises. Thus, for generating training data, for each proof step in our proof we greedily try to remove the premises: given a proof step with premises {P i } n i=1 , we rerun the proof step without P n . If the result of the proof step remains unchanged, we drop P n . Then, we continue with trying to drop P n−1 , and so on. We use the dropped premises as hard negatives, such that premises which are ranked highly by the model but are not useful are demoted. Demoting pruned premises allows other premises that had a high score but did not make it into the top-k to get a chance in the future. Pruning also ensures that any extra premises added for exploration that are in fact unnecessary are not learnt upon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>Environment and benchmark. We evaluate our approach in the HOList environment <ref type="bibr" target="#b4">[Bansal et al., 2019]</ref> based on the HOL Light <ref type="bibr" target="#b21">[Harrison, 1996]</ref> theorem prover. We conduct our experiments on the "complex" corpus of the HOList benchmark derived from theorems in HOL Light's mathematical library from various areas of mathematics such as topology, multivariate calculus, real and complex analysis, geometric algebra, and measure theory. It includes well-known theorems such as Abel's theorem for power series, the fundamental theorem of calculus, and that the roots of the characteristic polynomial of a complex matrix are its eigenvalues. <ref type="table" target="#tab_2">Table 3</ref> gives two more concrete examples of theorems in the benchmark.</p><p>The task is to attempt to prove a theorem in the benchmark, with theorems and definitions appearing before it in the benchmark available as premises. The evaluation criterion is the fraction of theorems proven with this constraint. The benchmark comes with theorems divided into training, validation, and test sets. <ref type="table" target="#tab_3">Table 4</ref> gives the statistics of theorems in the "complex" corpus of the benchmark, which we attempt to prove. Definitions (totaling 637) and "core" corpus of theorems (totaling 2320), containing basic mathematics which the "complex" corpus builds upon, are available as premises, but are not attempted to be proven.   <ref type="figure">Figure 4</ref>: Results of our main experiment. We report the percentage of validation theorems proven on the HOList benchmark. The numbers in bold are the state-of-the-art in their respective categories (including results from this work). The main takeaway is that the best RL loop trained without human proof data outperforms the model trained purely on human data, and approaches the performance of the best RL loop trained with human data.</p><p>Training and evaluation. During training, we generate data for training by trying to prove statements in the training set of 10,214 theorems. We train for 8 million steps. Details of our hardware setup and hyperparameters are provided in the supplementary materials. For evaluation of all our experiments trained in the reinforcement learning setup, we focus on the number of statements proven in the held-out validation set of 3,225 theorems. We run a continuous evaluation on samples of the validation set as well as a final evaluation on the full validation set. The precise meaning of the metrics in our plots and tables are given below.</p><p>• Continuous validation performance (represented by dots in the plots) runs every 80,000 training steps on a random sample of validation theorems, and reports the fraction of proven theorems from that sample. Since not all validation theorems are attempted and the sample changes each evaluation, the metric is slightly noisy, but it allows us to monitor overfitting during training. • Final validation performance (reported in the tables and plots) is the fraction of all validation theorems proven by the final checkpoint at 8 million steps. This metric also allows for comparison with models trained purely by imitation learning. • Cumulative validation performance -reported in the tables and plots -is the fraction of all validation theorems proven by any continuous-validation run up until that point in the loop. The table reports the cumulative performance of the whole loop (i.e., after 8 million steps).</p><p>Main experiment. In the main experiment we are interested in understanding the ability of the EXPLORE approach proposed in Section 4, in particular, its ability to learn to prove without human proof data available. This experiment is referred to as ZERO EXPLORE. As we see in the plot on the right in <ref type="figure">Figure 4</ref>, the loop continuously expands the horizon of the theorems it is able to prove.</p><p>To put the performance in context, we categorize the results on this benchmark into three categories. First, pure human imitation, wherein the model is trained only on human proof data and no theorem prover is used during training. Second, human RL, wherein the the model is trained on human proof data as well data generated by running the prover in a reinforcement learning (RL) loop. Finally, zero RL, wherein no human data is available, and all data is generated by running a prover in an RL loop. The results are summarized in the table in <ref type="figure">Figure 4</ref>.  loops indicate a ceiling unrelated to availability of proof data: such as the proof search algorithm, or model architecture.</p><p>Finally, we compare against the RL setup from <ref type="bibr" target="#b4">Bansal et al. [2019]</ref>, and run it without human proof data (ZERO REFERENCE). We run into the failure mode of not being able to prove new statements and thus stalling, discussed in Section 4.1.</p><p>6 Ablation studies Bootstrapping. In Section 5, we observe that the ZERO REFERENCE loop stalls very quickly.</p><p>Here, we try to understand to what extent is the failure a bootstrapping issue. We attempt to prove all statements in the training set using the best hand-engineered metric in Section 4.1 for premise selection and random tactic selection. This proves around 20% percent of the statements. We start a zero RL loop as in ZERO REFERENCE, but providing these additional proofs to train upon, calling it ZERO SEEDED. We see in <ref type="figure" target="#fig_3">Figure 5a</ref> that it does not stall like the reference loop. At the same time, it does not reach the same level of performance as the ZERO EXPLORE, which explores throughout.</p><p>Premise selection ablation. Through this ablation, we wish to understand the capability of the hand-engineered metrics in Section 4.1 to prove theorems. To keep the focus on premise selection, we still learn the tactic selection, and use similar amounts of resources trying to prove statements as in one RL loop. Technically, we do this by setting k 1 (number of premises chosen by the learnt network) to 0, and rest of the setup as in the ZERO EXPLORE loop. In <ref type="figure" target="#fig_3">Figure 5</ref>, under premise-selection ablation we see the results: it manages to prove 43% of the statements cumulatively, compared with 64% when learning with a combination of exploration and exploitation in the RL loop.</p><p>Dropout. In Section 4.1 we suggest a 10% token dropout probability when deciding premises to explore to introduce more diversity of premises in the loop overall at the cost of picking slightly less relevant premises at a specific point. We evaluate this experimentally (dropout ablation in <ref type="figure" target="#fig_3">Figure 5</ref>), but do not see a major difference: we can observe a very slight gain requiring further verification.</p><p>Human loop ablation. We run a human loop where we do not add premises for exploration (human ablation, <ref type="figure" target="#fig_3">Figure 5</ref>). We do not see a significant difference in the performance. It is not surprising, as the human loop has proofs for all statements and is thus not reliant on premise exploration to find relevant premises, unlike the zero RL loops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we demonstrate that it is possible to learn a premise selection model for theorem proving in the absence of human proofs. We show that, on our benchmark, we exceed the performance of a network trained purely on human proofs, and approach the performance of the system that combines reinforcement learning with imitation learning.</p><p>k 1 (number of premises to be picked by the network) and k 2 (number of premises to be added for exploration) are decided as follows. In the table above, for experiments where k 2 is defined k 2 is picked as shown. For experiments where k 2 is defined, k 2 is derived it as max( k/2 , k 2 ). k 1 is picked as k − k 2 .</p><p>p is the token dropout probability, defined in Section 4.1.</p><p>For technical reasons, for the training to start some non-empty training data is needed. For human RL loops, since human proof data is available, the generated seed data is not strictly necessary. For zero RL loops, some data needs to be provided. We generate this data by trying to prove all theorems in the training set on a randomly initialized model (i.e. the 0-th checkpoint of a model).</p><p>The hyperparameters used for the provers to generate the seed data is as follows:</p><p>• prover tree search strategy: BFS • timeout: 1000 seconds • maximum number of actions considered (per goal): 20 • maximum successful actions (per goal): 5 • maximum number of premises: k = k 1 = 24, k 2 = 0 (reference), k = k 2 = 16, k 1 = 0, p = 0.0 (exploration). • number of premise samples per tactic: 1 • tactic timeout: 5000 milliseconds • maximum goals explored: 1000000 (practically, no limit)</p><p>Validation provers:</p><p>• prover tree search strategy: BFS • timeout: 1000 seconds • maximum number of actions considered (per goal): 20 • maximum successful actions (per goal): 14 • maximum number of premises: 20 • number of premise samples per tactic: 4 • tactic timeout: 500 milliseconds • maximum goals explored: 1000000 (practically, no limit) Following only apply to continuous validation, not final validation which is run at final checkpoint, and on the full set.</p><p>• continuous validation interval: 20 rounds (80,000 training steps) • probability to prove a validation theorem: 0.1 (Explanation: the 2000 provers running in the RL loop for training, each independently decides to re-purpose itself with this probability to help with continuous evaluation. With this probability, this leads to over 2000 proof attempts per validation interval on average.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Hardware setup</head><p>We used eight NVIDIA Tesla V100 GPUs for distributed training, an additional GPU was used purely for evaluation, and we maintained a separate parameter server on a CPU machine. The provers generating training data and running validation run distributed, using 2000 CPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Reinforcement learning loop resources</head><p>Since the improvement of the premise guidance is heavily reliant on generation of data, we run up to 2000 theorem provers distributing the statements each prover is attempting to prove. Computing predictions takes a few milliseconds but actions in the proof assistant can take up to half a second. We use 8 GPUs for training the policy network and the experience collection uses CPUs only. Combined with proof search, to have a reasonable chance of proving a statement, we run the theorem prover with a timeout of 5 minutes for a statement in the training set. Training over 5 days, a single reinforcement learning loop takes over 25 years of CPU resources and 40 days of GPU resources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>∈ N : ((x + 0 = x) ⇒ (x + 1 + 0 = x + 1)) ARITH_TAC PROVEN ARITH_TAC PROVEN Formally proving ∀x ∈ N : x + 0 = x.Score each available premise, then select top-Architecture for tactic and premise selection by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) RL loop (b) Change to the theorem prover to add exploration of premises.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The figure on the left gives a high-level overview of the components in the reinforcement learning (RL) loop. The figure on the right shows how the model being trained is used for premise selection. We propose a modification to the premise selection process to aid exploration in the RL loop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Ablation experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Retrieval performance on human proof logs Term Freq. av rel max rank recall@16 recall@32 recall@64 recall@128</figDesc><table><row><cell>boolean</cell><cell>0.24</cell><cell>0.15</cell><cell>0.19</cell><cell>0.25</cell><cell>0.31</cell></row><row><cell>logarithm</cell><cell>0.35</cell><cell>0.1</cell><cell>0.13</cell><cell>0.17</cell><cell>0.21</cell></row><row><cell>natural</cell><cell>0.46</cell><cell>0.06</cell><cell>0.08</cell><cell>0.09</cell><cell>0.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Tactic success rates with extra random parameters, 1 second timeout.</figDesc><table><row><cell>Number of extra premises</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell>MESON success rate</cell><cell cols="5">0.995 0.986 0.97 0.873 0.53</cell><cell>0.06</cell></row><row><cell>REWRITE unchanged rate</cell><cell cols="6">0.99 0.979 0.954 0.93 0.858 0.731</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Examples of theorems in the benchmark (compressed for brevity)Alternative characterization of orthogonal matrices.</figDesc><table /><note>∀A.orth(A) ⇐⇒ (∀i. |A i | = 1 ∧ ∀i! = j.A i ⊥ A j ) Property about absolute neighborhood retract (ANR). ∀S ⊆ R n . ANR(frontier(S)) =⇒ ANR(closure(S))</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">: Benchmark statistics</cell></row><row><cell>Split</cell><cell># of Theorems</cell></row><row><cell>Training</cell><cell>10214</cell></row><row><cell>Validation</cell><cell>3225</cell></row><row><cell>Testing</cell><cell>3184</cell></row><row><cell>Total</cell><cell>16623</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc><ref type="bibr" target="#b5">Paliwal et al. [2020]</ref> based on graph neural networks (GNN) is the state-of-the-art on pure human imitation on this benchmark, and we use the network architecture in our experiments as well. Compared to pure human imitation, the ZERO EXPLORE RL loop using no human proofs is able to prove more theorems on a single checkpoint: 49.95% (pure imitation) vs 56.3% (zero RL).Next, we compare to the best human RL loop (HUMAN EXPLORE), one with everything identical as in ZERO EXPLORE, except we let the model learn from additional human proof data. We see that the ZERO EXPLORE loop comes very close to the performance of the corresponding human RL loop: 59.9% vs 56.3% on a single checkpoint, and 69.1% vs 64.1% cumulatively. ZERO EXPLORE is able to reach over 90% of the human RL loop's performance. This is an important indicator as HUMAN</figDesc><table><row><cell></cell><cell>50 60</cell><cell cols="2">(a) Bootstrapping</cell><cell></cell><cell>56.31 [64.2] 53.09 [60.6]</cell><cell>50 60</cell><cell></cell><cell cols="2">(b) Premise selection ablation</cell><cell>56.31 [64.2]</cell></row><row><cell>proven %</cell><cell>30 40</cell><cell></cell><cell></cell><cell>Zero Explore Zero Seeded</cell><cell>proven %</cell><cell>30 40</cell><cell></cell><cell></cell><cell>Zero Explore Only hand-engineered</cell><cell>[43.8]</cell></row><row><cell></cell><cell>10 20</cell><cell>continuous continuous</cell><cell>final final</cell><cell>cumulative cumulative</cell><cell></cell><cell>10 20</cell><cell></cell><cell>continuous continuous</cell><cell>final final</cell><cell>cumulative cumulative</cell><cell>22.54</cell></row><row><cell></cell><cell>0</cell><cell cols="3">1M 2M 3M 4M 5M 6M 7M 8M Training step</cell><cell></cell><cell></cell><cell>0</cell><cell cols="2">1M 2M 3M 4M 5M 6M 7M 8M Training step</cell></row><row><cell></cell><cell>50 60</cell><cell cols="3">(c) Dropout ablation (for Zero Explore)</cell><cell>56.31 [64.2] 54.2 [63.4]</cell><cell>50 60 70</cell><cell></cell><cell cols="2">(d) Human ablation</cell><cell>59.47 59.91 [68.2] [69.1]</cell></row><row><cell>proven %</cell><cell>30 40</cell><cell></cell><cell></cell><cell>10% dropout 0% drouput</cell><cell>proven %</cell><cell>30 40</cell><cell></cell><cell></cell><cell>Human Explore Human Reference</cell></row><row><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell>continuous continuous</cell><cell>final final</cell><cell>cumulative cumulative</cell><cell></cell><cell>10</cell><cell></cell><cell>continuous continuous</cell><cell>final final</cell><cell>cumulative cumulative</cell></row><row><cell></cell><cell>0</cell><cell cols="3">1M 2M 3M 4M 5M 6M 7M 8M Training step</cell><cell></cell><cell>0</cell><cell>0</cell><cell cols="2">Training step 1M 2M 3M 4M 5M 6M 7M 8M</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deepmath -deep sequence models for premise selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Eén</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6280-deepmath-deep-sequence-models-for-premise-selection" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<editor>Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="2235" to="2243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to reason with HOL4 tactics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><forename type="middle">Urban</forename><surname>Tactictoe</surname></persName>
		</author>
		<ptr target="https://easychair.org/publications/volume/LPAR-21" />
	</analytic>
	<monogr>
		<title level="m">LPAR-21, 21st International Conference on Logic for Programming, Artificial Intelligence and Reasoning</title>
		<editor>Thomas Eiter and David Sands</editor>
		<meeting><address><addrLine>Maun, Botswana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="125" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep network guided proof search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<ptr target="https://easychair.org/publications/paper/ND13" />
	</analytic>
	<monogr>
		<title level="m">LPAR-21, 21st International Conference on Logic for Programming, Artificial Intelligence and Reasoning</title>
		<editor>Thomas Eiter and David Sands</editor>
		<meeting><address><addrLine>Maun, Botswana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="85" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">GamePad: A learning environment for theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1xwKoR9Y7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">HOList: An environment for machine learning of higher-order theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitij</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><forename type="middle">N</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stewart</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilcox</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v97/bansal19a/bansal19a.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="454" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph representations for higher-order logic and theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitij</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>978-1-57735-823-7</idno>
		<ptr target="https://www.aaai.org/Papers/AAAI/2020GB/AAAI-PaliwalA.9149.pdf" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mathematical reasoning in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><forename type="middle">N</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitij</forename><surname>Bansal</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Ske31kBtPr" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno>978-0-521-86571-5</idno>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning. CoRR, abs/1312</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1312.5602" />
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5602</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mastering the game of go without human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bolton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="issue">7676</biblScope>
			<biblScope unit="page">354</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Grandmaster level in StarCraft II using multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dudzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petko</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyuk</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Kroiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Agapiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Leblond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dalibard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Sulsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrina</forename><surname>Wünsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckinney</surname></persName>
		</author>
		<idno>0028-0836</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">575</biblScope>
			<biblScope unit="issue">7782</biblScope>
			<biblScope unit="page" from="350" to="354" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">VIME: variational information maximizing exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><forename type="middle">De</forename><surname>Turck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6591-vime-variational-information-maximizing-exploration" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<editor>Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="1109" to="1117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale study of curiosity-driven learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJNwDjAqYX" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Premise selection for mathematics by corpus analysis and kernel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Alama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Heskes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kühlwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeni</forename><surname>Tsivtsivadze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10817-013-9286-5</idno>
		<ptr target="https://doi.org/10.1007/s10817-013-9286-5" />
	</analytic>
	<monogr>
		<title level="j">J. Autom. Reasoning</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="213" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning-assisted theorem proving with millions of lemmas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jsc.2014.09.032</idno>
		<ptr target="https://doi.org/10.1016/j.jsc.2014.09.032" />
	</analytic>
	<monogr>
		<title level="j">J. Symb. Comput</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="109" to="128" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A learning-based fact selector for Isabelle/HOL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasmin</forename><forename type="middle">Christian</forename><surname>Blanchette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Greenaway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kühlwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10817-016-9362-8</idno>
		<ptr target="https://doi.org/10.1007/s10817-016-9362-8" />
	</analytic>
	<monogr>
		<title level="j">Journal of Automated Reasoning</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="219" to="244" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Premise selection for theorem proving by deep graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihe</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6871-premise-selection-for-theorem-proving-by-deep-graph-embedding" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="2786" to="2796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to prove theorems via interacting with proof assistants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v97/yang19a/yang19a.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6984" to="6994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reinforcement learning of theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miroslav</forename><surname>Olsák</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/8098-reinforcement-learning-of-theorem-proving" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<editor>Nicolò Cesa-Bianchi, and Roman Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
			<biblScope unit="page" from="8836" to="8847" />
		</imprint>
	</monogr>
	<note>Samy Bengio,</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Towards finding longer proofs. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Zombori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrián</forename><surname>Csiszárik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905" />
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Prolog technology reinforcement learning prover. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Zombori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chad</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2004.06997" />
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A tutorial introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Light</surname></persName>
		</author>
		<idno type="DOI">10.1007/BFb0031795</idno>
		<ptr target="https://doi.org/10.1007/BFb0031795" />
	</analytic>
	<monogr>
		<title level="m">Formal Methods in Computer-Aided Design, First International Conference, FMCAD &apos;96</title>
		<editor>Mandayam K. Srivas and Albert John Camilleri</editor>
		<meeting><address><addrLine>Palo Alto, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">1166</biblScope>
			<biblScope unit="page" from="265" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Generating correctness proofs with neural networks. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sanchez-Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yousef</forename><surname>Alhessi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorin</forename><surname>Lerner</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.07794" />
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The Coq Proof Assistant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coq</surname></persName>
		</author>
		<ptr target="http://coq.inria.fr" />
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A brief overview of HOL4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Slind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Norrish</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-71067-7_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-71067-7_6" />
	</analytic>
	<monogr>
		<title level="m">Theorem Proving in Higher Order Logics, 21st International Conference</title>
		<editor>Otmane Aït Mohamed, César A. Muñoz, and Sofiène Tahar</editor>
		<meeting><address><addrLine>TPHOLs; Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008-08-18" />
			<biblScope unit="volume">5170</biblScope>
			<biblScope unit="page" from="28" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A formal proof of the kepler conjecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gertrud</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat Dat</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Hoang Le Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Magron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat</forename><surname>Mclaughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thang Nguyen</surname></persName>
		</author>
		<ptr target="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/78FBD5E1A3D1BCCB8E0D5B0C463C9FBC/S2050508617000014a.pdf/formal_proof_of_the_kepler_conjecture.pdf" />
	</analytic>
	<monogr>
		<title level="j">In Forum of Mathematics, Pi</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2017" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The isabelle framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarius</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">C</forename><surname>Paulson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Nipkow</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-71067-7_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-71067-7_7" />
	</analytic>
	<monogr>
		<title level="m">Theorem Proving in Higher Order Logics, 21st International Conference</title>
		<editor>Otmane Aït Mohamed, César A. Muñoz, and Sofiène Tahar</editor>
		<meeting><address><addrLine>TPHOLs; Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008-08-18" />
			<biblScope unit="volume">5170</biblScope>
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The Lean theorem prover (system description)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>De Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soonho</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Avigad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Floris</forename><surname>Van Doorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Von Raumer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-21401-6</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-21401-6" />
	</analytic>
	<monogr>
		<title level="m">Automated Deduction -CADE-25 -25th International Conference on Automated Deduction</title>
		<editor>Amy P. Felty and Aart Middeldorp</editor>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">9195</biblScope>
			<biblScope unit="page" from="378" to="388" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning heuristics for quantified boolean formulas through reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Lederman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><forename type="middle">N</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjit</forename><surname>Seshia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJluxREKDB" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Formal proof-the four-color theorem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georges</forename><surname>Gonthier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Notices of the AMS</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1382" to="1393" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A Hyperparameters and hardware setup A.1 Policy network training parameters • batch size: 16 goals, 256 premises • number of workers: 8 • optimizer: Adam • Adam epsilon: 1e-3 • initial learning rate: 1e-4 • learning rate decay: exponential, 0.98/100000 steps • embedding size</title>
		<idno>128 • non-linearity: ReLU • hidden layer dropout: 0.5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">• pre-combiner embedding size: 4096 • number of combiner layers: 3 • ratio of human training data: 0.7 (for human loops), 0.0 (for zero loops) • ratio of historical training data: 0.2 (for human loops), 0.5 (for zero loops) • ratio of fresh training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Gnn</surname></persName>
		</author>
		<idno>data: 0.1</idno>
		<imprint/>
	</monogr>
	<note>for human loops), 0.5 (for zero loops</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Training provers: • total number of provers: 2000 (see continuous validation note) • training round interval (each time a model checkpoint is written out): 4000 training steps • prover tree search strategy: BFS • timeout: 300 seconds • maximum number of actions considered (per goal): [10, 30] • maximum successful actions</title>
		<imprint/>
	</monogr>
	<note>These are picked uniformly at random from a given interval, are indicated below as. per goal. 10, 18] • maximum number of premises (k): [2, 32</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">• number of premise samples per tactic: 4 • tactic timeout: 500 milliseconds • maximum goals explored: 1000000 (practically</title>
		<imprint/>
	</monogr>
	<note>no limit</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
