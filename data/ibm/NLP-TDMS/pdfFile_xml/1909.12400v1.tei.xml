<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Markov Decision Process for Video Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladyslav</forename><surname>Yushchenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">iNTENCE automotive electronics GmbH</orgName>
								<address>
									<postCode>2 TU</postCode>
									<settlement>Darmstadt</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Araslanov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">iNTENCE automotive electronics GmbH</orgName>
								<address>
									<postCode>2 TU</postCode>
									<settlement>Darmstadt</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">iNTENCE automotive electronics GmbH</orgName>
								<address>
									<postCode>2 TU</postCode>
									<settlement>Darmstadt</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Markov Decision Process for Video Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We identify two pathological cases of temporal inconsistencies in video generation: video freezing and video looping. To better quantify the temporal diversity, we propose a class of complementary metrics that are effective, easy to implement, data agnostic, and interpretable. Further, we observe that current state-of-the-art models are trained on video samples of fixed length thereby inhibiting long-term modeling. To address this, we reformulate the problem of video generation as a Markov Decision Process (MDP). The underlying idea is to represent motion as a stochastic process with an infinite forecast horizon to overcome the fixed length limitation and to mitigate the presence of temporal artifacts. We show that our formulation is easy to integrate into the state-of-the-art MoCoGAN framework. Our experiments on the Human Actions and UCF-101 datasets demonstrate that our MDP-based model is more memory efficient and improves the video quality both in terms of the new and established metrics. * This work was done while VY was at TU Darmstadt.</p><p>1 Note that the model is still conditioned on the particular training data distribution, hence not truly "unconditional". Still, we adhere to the common terminology used in the literature.</p><p>subsequence samples for training</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video synthesis is a very challenging problem <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref>, arguably even more challenging than the already difficult image generation task <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20]</ref>. The temporal dimension of the data introduces an additional mode of variation, since feasible motions are dependent on the object category and the scene appearance. Consequently, the evaluation of video synthesis methods should account not only for the quality of individual frames but also for their temporal coherence, motion realism, and diversity.</p><p>In this work, we take a closer look at the temporal quality of unconditional video generators, represented by the stateof-the-art MoCoGAN approach <ref type="bibr" target="#b28">[29]</ref>. Note that this subcategory of video generation is different from future frame prediction <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16]</ref>, which takes a number of initial frames as input. We only rely on the training data as input instead. 1 <ref type="figure">Figure 1</ref>. Problem illustration on a Tai Chi sequence. Every 6 th frame is shown. Top row: The ground truth video is a nonrepetitive action sequence. Second row: Even when trained only on one video, MoCoGAN <ref type="bibr" target="#b28">[29]</ref> can only reproduce the sequence until the training length, marked by the red boundary, and the motion freezes thereafter. Third row: Increasing the training length comes at increased memory costs and only delays the freezing. Last row: Our MDP approach uses shorter training sequences yet extends the movement duration, indicated by the blue boundary.</p><p>We find that the common training strategy of sampling a fixed-length video subsequence at training time often leads to degenerate solutions. As illustrated in <ref type="figure">Fig. 1</ref>, the MoCo-GAN model exhibits temporal artifacts as soon as the video sequence length at inference time exceeds the length of the temporal window at training time. We establish two common types of such artifacts. If the model continues to predict the last frame without change, we refer to that as freezing. On the other hand, looping occurs when the exact subsequence of frames is continually repeated.</p><p>To address these limitations, we make two main contributions. First, to tackle the detrimental effect of fixedlength video training, we reformulate video generation as a Markov Decision Process (MDP). This reformulation allows approximating an infinite forecast horizon in order to optimize every generated frame w.r.t. to its long-term effect on future frames. One benefit of our MDP formulation is that it is model-agnostic. We evaluate it by applying it to the state-of-the-art MoCoGAN <ref type="bibr" target="#b28">[29]</ref>, which requires only a minor modification of the original design and does not signif-To appear in Proceedings of the 2019 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), Seoul, Korea, 2019. c 2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. icantly increase the model capacity. Second, we propose a family of evaluation metrics to detect and measure the temporal artifacts. Our new metrics are model-free, simple to implement, and offer an easy interpretation. In contrast to the Inception Score (IS) <ref type="bibr" target="#b19">[20]</ref> or the recent Fr√©chet Video Distance (FVD) <ref type="bibr" target="#b29">[30]</ref>, the proposed metrics do not require model pre-training and, hence, do not build upon a datasensitive prior. Our experiments show that our MDP-based formulation leads to a consistent improvement of the video quality, both in terms of the artifact mitigation as well as on the more common metrics, the IS and FVD scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video generation models can be divided into two main categories: conditional and unconditional. Exemplified by the task of future frame prediction, conditional models historically preceded the latter and some of their features lend themselves to unconditional prediction. Therefore, we first give a brief overview of conditional approaches.</p><p>Conditional video generation. One of the first networkbased models for motion dynamics used a temporal extension of Restricted Boltzmann Machines (RBMs) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27]</ref> with a focus on resolving the intractable inference <ref type="bibr" target="#b24">[25]</ref>. The increasing volume of video data for deep learning shifted the attention to learning suitable representations and enabling some control over the generated frames <ref type="bibr" target="#b5">[6]</ref>. Srivastava et al. <ref type="bibr" target="#b22">[23]</ref> show that unsupervised sequence-tosequence pre-training with LSTMs <ref type="bibr" target="#b7">[8]</ref> enhances the performance on the supervised frame prediction task. Patchbased quantization of the output space <ref type="bibr" target="#b17">[18]</ref> or predicting pixel motion <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref> can improve the frame appearance at larger resolutions. In contrast, Kalchbrenner et al. <ref type="bibr" target="#b8">[9]</ref> predict pixel-wise intensities and extend the context model of PixelCNNs <ref type="bibr" target="#b30">[31]</ref> to the temporal domain. A coarse-to-fine strategy allows to decouple the structure from the appearance <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>, or dedicate individual stages of a pipeline to multiple scales <ref type="bibr" target="#b15">[16]</ref>.</p><p>The frames of a distant future cannot be extrapolated deterministically due to the stochastic nature of the problem <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b35">36]</ref> (i.e. there are multiple feasible futures for a given initial frame). In practice, this manifests itself in frame blurring -a gradual loss of details in the frame. To alleviate this effect, Mathieu et al. <ref type="bibr" target="#b15">[16]</ref> used an adversarial loss <ref type="bibr" target="#b4">[5]</ref>. Liang et al. <ref type="bibr" target="#b13">[14]</ref> further show that adversarial learning of the pixel flows leads to better generalisation.</p><p>Unconditional video generation. These more recent methods are based on the GAN framework <ref type="bibr" target="#b4">[5]</ref> and incorporate some of the insights from their conditional counterparts. For example, Vondrick et al. <ref type="bibr" target="#b33">[34]</ref> decouple the active foreground from a static background by using an architecture with two parallel generator streams. Saito et al. <ref type="bibr" target="#b18">[19]</ref> use two generators to disentangle the video representation into distinct temporal and spatial domains. Following <ref type="bibr" target="#b31">[32]</ref>, the state-of-the-art MoCoGAN of Tulyakov et al. <ref type="bibr" target="#b28">[29]</ref> decomposes the latent representation into content and motion parts for finer control over the generated scene. In addition, the discriminator in the MoCoGAN model is separated into image and video modules. While the image module targets the visual quality of individual frames, the focus of the video discriminator is the temporal coherence.</p><p>Evaluating unconditional video generators. Borrowed from the image generation literature <ref type="bibr" target="#b19">[20]</ref>, the Inception Score (IS) has become one of the established metrics for quality assessment in videos <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref>. IS incorporates the entropy of the class distributions obtained from a separately trained classifier. Therefore, it is only meaningful if the training data distribution of the classifier matches the one on which it will be evaluated later. Following <ref type="bibr" target="#b6">[7]</ref>, Unterthiner et al. <ref type="bibr" target="#b29">[30]</ref> recently proposed the Fr√©chet Video Distance (FVD) that compares the distributions of feature embeddings of real and generated data.</p><p>However, these metrics provide only a holistic measure of the video quality and do not allow for a detailed assessment of its individual properties. One of the desirable qualitative traits of video generators is their ability to produce realistic videos of arbitrary length. Yet, the established experimental protocol evaluates only on video sequences of a fixed length. Indeed, some previous work <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34]</ref> is even tailored to a pre-defined video length, both at training and at inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MDP for Video Generation</head><p>To motivate MDP for video generation, we first review MoCoGAN <ref type="bibr" target="#b28">[29]</ref> and discuss its limitations. After a short presentation of the MDP formalism (c.f . <ref type="bibr" target="#b25">[26]</ref> for a comprehensive introduction), we then integrate MDP into MoCo-GAN to incorporate knowledge of the infinite-time horizon into the generative process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>MoCoGAN. <ref type="figure" target="#fig_1">Figure 2a</ref> illustrates the main components of MoCoGAN: the generator, the image discriminator, and the video discriminator. At every timestep, the stochastic generator G emits one frame x t and maintains a recurrent state h t perturbed by random noise. The image discriminator D I provides feedback for a single image; the video discriminator D V evaluates a contiguous subsequence of frames x t of a pre-defined length |x t | = K. The training objective is specified by the familiar max-min game  We find that MoCoGAN's samples exhibit looping and freezing patterns (see Sec. 5.4 for results and analyses). The intuitive reason comes from the specifics of training: to save memory, the training samples contain only subsequences of the complete video. As a result, the gradient signal from the video discriminator is unaware of the frames following the subsequence. The predefined length of the subsequence ultimately determines the maximum length of a sample with a non-repeating pattern. <ref type="bibr" target="#b1">2</ref> MDP. In an MDP defined by the tuple (S, A, T, œÄ, r), the agent interacts with the environment by performing actions, a t ‚àà A, based on the current state, s t ‚àà S. The environment specifies the outcome of the action by returning a reward, r(s t , a t ), and the next state, s t+1 = T (s t , a t ). The goal of the agent is to find the optimal policy œÄ * : S ‚Üí A, maximizing the discounted cumulative reward</p><formula xml:id="formula_0">œÄ * = arg max œÄ ‚àû t=0 Œ≥ t r(s t , a t ), a t ‚àº œÄ(s t ),<label>(2)</label></formula><p>where Œ≥ ‚àà (0, 1) is the discount factor to ensure the convergence of the sum. In the context of an MDP the generator G plays the role of the agent's policy. The frames predicted by G are the actions. The hidden recurrent state h t becomes the agent's state s t . The additive noise at every timestep determines the transition function T . A frame incurs a reward r t as the score provided by the discriminators. Due to the deterministic mapping s t ‚Üí a t , the MoCoGAN's G corresponds to a deterministic policy <ref type="bibr" target="#b20">[21]</ref> (i.e. the sampling in Eq. (2) becomes an equality). The optimization task for the agent is a <ref type="bibr" target="#b1">2</ref> To verify this, we also trained the MoCoGAN model on longer subsequences and found the breaking point to occur at a correspondingly later timestep. search for the optimal policy œÄ * :</p><formula xml:id="formula_1">max œÄ r(s t , a t ) + E a=œÄ(st) ‚àû i=t+1 Œ≥ i‚àít r(s i , a) . (3)</formula><p>Observe that the MoCoGAN objective for D V is equivalent to only the first term of Eq. (3), the immediate reward, since the D V computes only a single score for a given video sample. In contrast, we also consider the future rewards, i.e. the second term of Eq. (3). To this end, we decompose the score of the video generator into immediate rewards associated with individual frames. We then learn a utility Q-function approximating the expected cumulative reward, E t Œ≥ t r t . Its definition is also known as Bellman's optimality principle:</p><formula xml:id="formula_2">Q(s t , a t ) = r(s t , a t ) + max a=œÄ(st) Q(s t+1 , a).<label>(4)</label></formula><p>By training the generator to maximize the Q-function instead of just the immediate reward, we arrive at an approximate solution of Eq. (3). In the next section, we detail how MoCoGAN can be extended to this setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Integrating MDP into MoCoGAN</head><p>We need the model implementing the MDP to comply with two requirements: (a) The Markov property needs to be fulfilled, i.e. the next state s t+1 given the previous state s t is conditionally independent from the past history s i&lt;t . (b) By causality, the immediate reward r t is a function of the current state s t and the action a t and incorporates no knowledge about future actions.</p><p>The MoCoGAN generator already satisfies the Markov property using a parametrized RNN mapping from the cur-rent state to the next. However, the video discriminator has to be modified to satisfy the second requirement. This modification is straightforward to implement and leads to a variant of the Temporal Convolutional Network (TCN) <ref type="bibr" target="#b1">[2]</ref>. <ref type="figure" target="#fig_1">Figure 2b</ref> gives an overview of the proposed MDPextension for the video discriminator. The key property of this design is that the t th output -a scalar -corresponds to a temporal receptive field of the frames up to the t th timestep. In this way the immediate reward will capture only the relevant motion history. Fortunately, adapting the MoCoGAN video discriminator to this architecture is straightforward (c.f . supplemental material for more details).</p><p>To implement Eq. (4), alongside r t we also predict another time-dependent scalar, the Q-value. As discussed in Sec. 3.1, the purpose of the Q-value is to approximate the expected cumulative reward, E ‚àû t=0 Œ≥ t r(s t , a t ) . We use the squared difference loss, defined for each timestep by</p><formula xml:id="formula_3">L Q,t = 1 K ‚àí t + 1 K i=t Œ≥ i‚àít r i ‚àí Q t 2 2 , 1 ‚â§ t ‚â§ K, (5)</formula><p>where Œ≥ ‚àà (0, 1) is the discounting factor specifying the lookahead span: larger values encourage the Q-value to account for the future outcome far ahead; low values focus the Q-value on the immediate effect of the current frames.</p><p>Our TCN-based D V ensures that the parameters for predicting Q t are now shared for all t. As a result, Eq. (5) forces even the last Q K to incorporate knowledge of rewards beyond the temporal window of size K. Hence, by maximizing Q K , the generator will implicitly maximize the rewards for t &gt; K. Contrast this to the original D V producing a single score for the complete K-frame sequence: due to lack of causality, the generator is "unaware" that at inference time the requested video length may exceed K.</p><p>Note that the definition in Eq. (5) is confined to a limited time window of length K to ensure that the memory consumption remains manageable. Now, our task is to train the generator by maximizing the Q-value incorporating the long-term effects of individual predictions. However, since we keep K fixed, each consecutive Q t in Eq. (5) will be optimized w.r.t. to the sum containing one term fewer. That is, Q 1 will approximate a sum of K immediate rewards, Q 2 a sum of K ‚àí1 terms, etc. As a result, Q 1 incorporates the effect of the 1 st frame on K ‚àí 1 future frames, whereas Q K‚àí1 will only observe the influence of the (K ‚àí 1) th frame on the last prediction. It is therefore evident that the Q-values are not equally informative for modeling the long-term dependencies as supervision to the generator.</p><p>To reflect this observation in our training, we introduce an additional discounting factor Œ≤ ‚àà [0, 1] that shifts the weight of the long-term supervision to the first frames, but offsets the reliance on the Q-value for the last predictions.</p><p>Concretely, the new term in the generator loss is</p><formula xml:id="formula_4">L T = K t=1 Œ≤ t Q t .<label>(6)</label></formula><p>To summarize, extending the original MoCoGAN training objective (Eq. 1) into our MDP-based GAN yields</p><formula xml:id="formula_5">min D I ,D V E xt,xt L I (x real t , x fake t ) + L V (x real t , x fake t ) + 1 K K t=1 L Q,t (x real t ) + L Q,t (x fake t ) , (7a) max G E xt,xt L I (x fake t ) + L V (x fake t ) + L T .<label>(7b)</label></formula><p>Here, we split the original objective in Eq. (1) into the discriminator-and generator-specific losses for illustrative purposes although the joint nature of the max-min optimization problem remains. Following standard practice <ref type="bibr" target="#b4">[5]</ref>, we optimize the new objective by alternately updating the discriminators using Eq. (7a) and the generator using Eq. (7b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Quantifying Temporal Diversity</head><p>Motivated by our observation of the looping and freezing artifacts (see <ref type="figure">Fig. 1</ref>), we propose an interpretable way to quantify the temporal diversity of the video. Here, our assumption is that realistic videos comprise a predominantly unique sequence of frames. The idea then is to compare the predicted frame to the preceding ones: if there is a match, this indicates a re-occurring pattern in the sequence.</p><p>Let X = (x t ) t=1..N be a sequence of frames predicted by the model. Our diversity measure relies on a distance function of choice between arbitrary frames d(x i , x j ) as</p><formula xml:id="formula_6">t-d = 1 N N i=2 min j&lt;i d(x i , x j ),<label>(8)</label></formula><p>where we use prefix "t-" for disambiguation. Eq. (8) essentially finds the most similar preceding frame and averages the distance over all such pairs in the sequence. The obvious dual of this metric is to replace the distance function d(¬∑, ¬∑) in Eq. (8) with a similarity measure s(¬∑, ¬∑) and substitute the min for the max operation. In this work, we use two instantiations of Eq. (8): the t-DSSIM employs the structural similarity (SSIM) <ref type="bibr" target="#b34">[35]</ref> in the distance function DSSIM = 1 2 (1‚àíSSIM); t-PSNR utilizes the peak signal-tonoise ratio (PSNR) as a similarity measure. Hence, higher t-DSSIM and lower t-PSNR indicate higher diversity of frames within a sequence. We show next that despite its apparent simplicity, our proposed metric effectively captures deficiencies in frame diversity.  <ref type="table">Table 1</ref>. Comparison of IS, FVD, t-DSSIM, and t-PSNR metrics for groundtruth videos and videos with purposely crafted artifacts. The Gaussian noise is drawn from N (¬µ = 0, œÉ 2 = 0.03). </p><formula xml:id="formula_7">Configuration IS ‚Üë FVD ‚Üì t-DSSIM ‚Üë t-PSNR ‚Üì</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>Following the established evaluation protocol from previous studies <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29]</ref>, we use the following benchmarks:</p><p>(1) Human actions <ref type="bibr" target="#b2">[3]</ref>: The dataset contains 81 videos of 9 people performing 9 actions, e.g. walking, jumping, etc. All videos are extracted with 25 fps and downscaled to 64 √ó 64 pixels. We also add a flipped copy of each video sequence to the training set. Following Tulyakov et al. <ref type="bibr" target="#b28">[29]</ref> we used only 4 action classes, which amounts to 72 videos for training in total. (2) UCF-101 <ref type="bibr" target="#b21">[22]</ref>: This dataset consists of 13 220 videos with 101 classes of human actions grouped into 5 categories: human-object and human-human interaction, body motion, playing musical instruments, and sports. This dataset is challenging due to a high diversity of scenes, motion dynamics, and viewpoint changes. (3) Tai-Chi: The dataset contains 72 Tai Chi videos taken from the UCF-101 dataset. <ref type="bibr" target="#b2">3</ref> All videos are centered on the performer and downscaled to 64√ó64 pixels. We use this dataset for our ablation studies as it has moderate complexity, yet represents real-world motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Overview</head><p>We first verify that t-PSNR and t-DSSIM effectively quantify the temporal artifacts. We then employ these metrics to analyze the MoCoGAN model <ref type="bibr" target="#b28">[29]</ref> w.r.t. these artifacts. Next, we study the effect of the time-horizon hyperparameters, Œ≥ and Œ≤, of our MDP approach. Finally, we validate our approach on the Human Actions dataset and on the more challenging UCF-101 dataset. We compare our model to TGAN <ref type="bibr" target="#b18">[19]</ref> and MoCoGAN, where we find a consistent improvement of the temporal diversity over the baseline.</p><p>We compute the IS following Saito et al. <ref type="bibr" target="#b18">[19]</ref>, who trained the C3D network <ref type="bibr" target="#b27">[28]</ref> on the Sports-1M dataset <ref type="bibr" target="#b9">[10]</ref> and then further finetuned on UCF-101 <ref type="bibr" target="#b21">[22]</ref>. For FVD we use the original implementation by Unterthiner et al. <ref type="bibr" target="#b29">[30]</ref>. To manage computational time, we calculate the FVD for the first 16 frames, sampled from 256 videos, and derive the FVD mean and variance from 4 trials, similar to IS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Metric evaluation</head><p>We design a set of proof-of-concept experiments to study the properties of the newly introduced t-PSNR and t-DSSIM. Concretely, we synthesize the looping and freezing patterns in the ground-truth videos from UCF-101 and Tai Chi. We construct 16 frames by sampling 8 frames directly from the dataset and completing the sequence with an artifact counterpart. Looping-FWD contains a repeating subsequence from the original video (Original), whereas Looping-BWD reverses the frame order. The size of the re-occurring subsequence in Freezing is one. To put the results in context, we also compare to the mainstream IS as well as the recent FVD scores and study the robustness of all metrics to additive Gaussian noise ‚àº N (¬µ, œÉ 2 ). The results are summarized in <ref type="table">Table 1</ref>.</p><p>We observe that t-PSNR and t-DSSIM correlate well with the more sophisticated IS and FVD. Recall that both IS and FVD require training a network on videos of fixed length, hence (i) can be computed only for short-length videos, due to GPU constraints; (ii) may be misleading (e.g. Tai Chi results in <ref type="table">Table 1</ref>) when the training data for the inception network is different from the evaluated data. By contrast, t-PSNR and t-DSSIM prove to be faithful in quantifying the artifacts we study, as they are data-agnostic and accommodate videos of arbitrary length. However, our metrics are permutation invariant, do not assess the quality of the frames themselves, and are not robust to random noise. Hence we stress their complementary role to IS and FVD as a measure of the overall video quality.   <ref type="table">Table 2</ref>. Results of the ablation study of the MDP approach on the Tai Chi dataset. Our MDP configurations assume a selection of hyperparameters Œ≤ and Œ≥. For comparison, we include the results from the MoCoGAN baseline. By leveraging the long-term rewards, our MDP model improves the temporal diversity (t-PSNR and t-DSSIM) and FVD scores at the cost of a slight drop in IS. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">MoCoGAN: a case study</head><p>Here, we study the temporal diversity of the MoCoGAN model <ref type="bibr" target="#b28">[29]</ref> using our t-PSNR and t-DSSIM scores.</p><p>We train MoCoGAN 4 on the UCF-101 dataset with temporal windows of size K = 16, and apply our temporal metrics to the samples from the generator. To enable a more detailed view of the temporal dynamics, we inspect the video samples as a function of time in <ref type="figure" target="#fig_2">Fig. 3</ref> by plotting the values of the summands in Eq. (8) for each timestep. To rule out the possibility of any degenerate phenomena in the original data, we also plot the corresponding curves of the ground-truth sequences alongside. This clearly shows that MoCoGAN exhibits a vanishing diversity of video framesa pattern that is not found in the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">MDP approach: an ablation study</head><p>Here, we perform an ablation study of our MDP approach by varying the time-horizon hyperparameters, Œ≥ and Œ≤, introduced in Sec. 3.2. Recall that Œ≥ controls the timespan of the future predictions modeled by the Q-value: lower values imply a shorter time horizon, whereas higher values encourage the model to learn long-term dependencies. Parameter Œ≤, on the other hand, specifies how accounting for the long-term effect is distributed over the timesteps. High values specify equal distribution; lower values force the model to encode the long-term effects more in the earlier than in the later timesteps. As a boundary case, we also consider Œ≤ = 0 and Œ≥ = 0 to gage the effect of the architecture change in the video discriminator (TCN), which is needed to implement reward causality (c.f . Sec. 3.2). As quantitative measures, we use the Inception Score (IS) <ref type="bibr" target="#b19">[20]</ref>, the Fr√©chet Video Distance (FVD) <ref type="bibr" target="#b29">[30]</ref>, as well our temporal metrics, t-DSSIM and t-PSNR, introduced in Sec. 4.</p><p>The results in <ref type="table">Table 2</ref> show that by leveraging the increasing values of the time-horizon hyperparameters, our model clearly improves the temporal diversity in terms of t-PSNR and t-DSSIM. Moreover, we also observe that the TCN baseline (Œ≥ = 0, Œ≤ = 0) performs worse than the original MoCoGAN in terms of temporal diversity. This is easily understood when considering that the TCN alone does not have any lookahead into the future (c.f . <ref type="figure" target="#fig_1">Fig. 2b</ref>). However, once we enable taking the future rewards into account by virtue of our MDP formulation, we not only reach but actually surpass the temporal diversity of the baseline MoCoGAN, as expected.</p><p>The somewhat inferior IS and FVD scores might be due to their sensitivity to the data prior, as discussed in Sec. 5.3. This hypothesis is also supported by a qualitative comparison between MoCoGAN and our MDP model. <ref type="figure" target="#fig_4">Figure 4</ref> gives one such example; more results can be found in the supplemental material. While we observe no notable difference in per-frame quality, the motion between consecutive frames from our MDP model is more apparent than the samples from MoCoGAN (e.g., the torso of the performer).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Human Actions and UCF-101</head><p>We perform further experiments on the Human Actions and the more challenging UCF-101 datasets. <ref type="bibr" target="#b4">5</ref> We select Œ≥ = 0.9, Œ≤ = 0.7 for our MDP model, which provide a good trade-off between the improved t-PSNR, t-DSSIM, FVD and only a slight drop of IS on Tai Chi (c.f . Sec. 5.2). For reference, we train the TCN baseline, MDP-0, by setting Œ≥ = 0 and Œ≤ = 0 to decouple the influence of modeling Model K Human Actions UCF-101  the long-term effects from the changes in the MoCoGAN architecture to comply with reward causality. We also train our MDP model and MoCoGAN on an extended temporal window K = 24. Recall that higher K require more GPU memory, but give the model an advantage, since it observes longer sequences at training time. Therefore, we aim to mitigate the artifacts while keeping K constant.</p><formula xml:id="formula_8">IS ‚Üë FVD ‚Üì t-DSSIM ‚Üë t-PSNR ‚Üì IS ‚Üë FVD ‚Üì t-DSSIM ‚Üë t-PSNR ‚Üì Raw</formula><p>The quantitative results are summarized in <ref type="table" target="#tab_2">Table 3</ref>. For both the Human Actions and UCF-101 datasets, we observe a consistent improvement of our MDP model in terms of temporal diversity measured by t-PSNR and t-DSSIM. Moreover, our model also outperforms MoCoGAN in terms of IS on both datasets, as well as FVD on the UCF-101 dataset. This can be explained by the more varied nature of motion on these datasets compared to the Tai Chi dataset, which makes taking into account future frames more important. On the Human Actions dataset, the FVD score for our model is inferior to MoCoGAN. Recall from Sec. 5.2, that for IS and FVD metrics we did not fine-tune the inception classifiers on the Human Actions dataset, which impedes the interpretability of the scores on this dataset. A visual inspection of the per-frame quality (c.f . <ref type="figure" target="#fig_5">Fig. 5</ref> for examples) reveals no perceptual loss compared to the baseline model. In contrast, disabling MDP modeling (MDP-0) leads to a clear deterioration in video quality.</p><p>On both datasets, our model with K = 16 is also superior to MoCoGAN with K = 24 in terms of IS and FVD, and reaches on par performance in terms of t-PSNR and t-DSSIM. Yet, our MDP-based formulation is significantly more memory efficient, since extending the temporal window at training incurs addition memory costs. Concretely, at training time the MDP model with K = 16 consumes roughly 20% more memory than MoCoGAN, whereas setting K = 24 for the original MoCoGAN incurs a 50% higher memory footprint. Note that simply increasing the number of parameters of D V in MoCoGAN is less effective than our proposed MDP approach (see MoCoGAN-D + V in Tab. 5.6). Also, our MDP model with K = 24 improves further over K = 16 on UCF-101 and regarding the temporal metrics on Human Actions. A visual inspection of the samples from Human Actions did not reveal any perceptible difference to MoCoGAN or our MDP with K = 16, despite the inferior IS and FVD scores; we believe this to be an artifact of the evaluation specifics. The IS score of our MDP model is slightly inferior only to TGAN <ref type="bibr" target="#b18">[19]</ref>. However, TGAN can produce video sequences of only fixed length, whereas our MDP model can generate videos of arbitrary length, owing to the recurrent generator.</p><p>The qualitative results in <ref type="figure" target="#fig_6">Fig. 6</ref> show that our model can generate complex scenes from UCF-101 that are visually comparable to the MoCoGAN samples. Similar to our observation on Human Actions, MDP-0 produces poorer samples, which asserts the efficacy of the underlying MDP. Since the interpretation of the UCF-101 results is difficult, we examine a visualization of a pairwise L 1 -distance between two frames in the video, shown in <ref type="figure">Fig. 7</ref>. The distance matrix can be represented as a lower triangular twodimensional heatmap, owing to the symmetry of L 1 . We observe that while MoCoGAN exhibits a looping pattern, our MDP approach tends to preserve the temporal qualities of the ground-truth datasets. Note that some samples in Human Actions can be naturally periodic (e.g. hand-waving), hence, we do not expect our model to dispense with the looping pattern completely. The overall results suggest that modeling long-term dependencies with an MDP consistently leads to more diverse motion dynamics, which becomes more apparent in increasingly complex scenes.  <ref type="figure">Figure 7</ref>. Heatmap comparison between ground truth, MoCo-GAN, and our MDP models trained on the Human Actions dataset (left) and UCF-101 (right) (different scales). (a) ground truth, (b) MoCoGAN, (c) MDP (Œ≥ = 0.9, Œ≤ = 0.7). Our MDP model alleviates the looping artifact on Human Actions, where it can still appear natural. On the more complex UCF-101, our MDP is able to approximate the temporal quality of the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head><p>We revealed two pathological cases in the videos synthesized by the state-of-the-art MoCoGAN model, namely freezing and looping. To quantify the temporal diversity, we proposed an interpretable class of metrics. We showed that the SSIM-and PSNR-based metrics, t-PSNR and t-DSSIM, effectively complement IS and FVD to quantify temporal artifacts. Next, we traced the artifacts to the limited training length, which inhibits long-term modeling of the video sequences. As a remedy, we reformulated video generation as an MDP and incorporated it into MoCoGAN. We showed the efficacy of our MDP model on the challenging UCF-101 dataset both in terms of our temporal metrics, as well as in IS and FVD scores. Maintaining the recurrent state between the training iterations or imposing a tractable prior on the state suggest promising extensions of this work toward generating long-sequence videos.</p><p>Vladyslav Yushchenko 1 * Nikita Araslanov 2 Stefan Roth 2 1 iNTENCE automotive electronics GmbH, 2 TU Darmstadt</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>We elaborate on the evaluation protocol used in our study, as well as provide additional qualitative examples both from our approach and MoCoGAN <ref type="bibr" target="#b28">[29]</ref>. To enable reproducibility of our approach, we detail the architecture of our MDP-based video discriminator and the training specifics of our MDP approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. A Note on Reproducibility</head><p>In the main text, we indicated a discrepancy between the Inception Score (IS) we attained on UCF-101 <ref type="bibr" target="#b21">[22]</ref> and the IS reported in the original work <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29]</ref>. Recall that we compute the IS following Saito et al. <ref type="bibr" target="#b18">[19]</ref>, who trained the C3D network <ref type="bibr" target="#b27">[28]</ref> on the Sports-1M dataset <ref type="bibr" target="#b9">[10]</ref> and then further finetuned it on UCF-101 <ref type="bibr" target="#b21">[22]</ref>. We calculate the IS by sampling the first 16 frames from 10K videos and determining the mean and variance over 4 trials. As in previous work <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29]</ref>, we use the first training split of the UCF-101 dataset. <ref type="bibr" target="#b0">1</ref> We use original authors' implementation, for both MoCoGAN 2 and TGAN 3 and train the respective models for 100K iterations. We did not observe further improvements of the IS for longer training schedules.</p><p>To facilitate transparency and reproducibility of our experiments, we highlight two contributing factors that we carefully considered in our evaluation: IS implementation and model selection.</p><p>IS implementation. We found the IS to be sensitive to subtle differences in implementation. Recall that we use the original TGAN <ref type="bibr" target="#b18">[19]</ref> evaluation code in our experiments. Algorithm-A in <ref type="figure">Fig. 8a</ref> shows the main steps of this evaluation for a single sample of video. While C3D <ref type="bibr" target="#b27">[28]</ref> is trained for a resolution of 112 √ó 112, video generators produce sequences of resolution 64√ó64. Additionally, C3D requires the input image sequence to be normalized with the * This work was done while VY was at TU Darmstadt. <ref type="bibr" target="#b0">1</ref> More details on the splits of the UCF-101 dataset are available at http://crcv.ucf.edu/data/UCF101.php. <ref type="bibr" target="#b1">2</ref> MoCoGAN repository provided at https://github.com/ sergeytulyakov/mocogan. <ref type="bibr" target="#b2">3</ref> Code repository by <ref type="bibr" target="#b18">[19]</ref>, provided at https://github.com/ pfnet-research/tgan. <ref type="figure">Figure 8</ref>. Two options for IS computation. While the C3D network requires inputs of size 112 √ó 112, the models for video generation compute sequences at resolution 64√ó64. To adapt this output, we can either (a) normalize the input at resolution 128 √ó 128 and crop a centered window 112 √ó 112, or (b) normalize the video directly at resolution 112√ó112. We show that despite a rather subtle difference, these two reasonable approaches lead to a notable deviation in the Inception Score. mean and standard deviation used for the network training. The TGAN evaluation (c.f . <ref type="figure">Fig. 8a</ref>) upsamples the videos to 128 √ó 128, normalizes them, and feeds the center crop of 112 √ó 112 into the C3D network. However, an alternative evaluation, shown by Algorithm-B in <ref type="figure">Figure 8b</ref>, is to upsample the video directly to 112 √ó 112, applying the normalization at that scale, and feeding the result into C3D without cropping. This subtle change in the evaluation leads to a tangible difference in the Inception Score. We summarize the results in <ref type="table">Table 4</ref> and compare these two versions of evaluation to the reported scores in the original works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29]</ref>.</p><formula xml:id="formula_9">Data: Generated Video X64√ó64 ‚àà R K√ó64√ó64 Result: Inception Score S ‚àà R Bicubic interpolationX128√ó128 ‚Üê scale(X64√ó64) Normalize X128√ó128 ‚Üê norm(X128√ó128) Center crop X112√ó112 ‚Üê crop(X128√ó128) Forward pass S ‚Üê C3D(X112√ó112) (a) Algorithm-A Data: Generated Video X64√ó64 ‚àà R K√ó64√ó64 Result: Inception Score S ‚àà R Bicubic interpolationX112√ó112 ‚Üê scale(X64√ó64) Normalize X112√ó112 ‚Üê norm(X112√ó112) Forward pass S ‚Üê C3D(X112√ó112) (b) Algorithm-B</formula><p>Leaving out cropping for IS computation with Algorithm-B leads to higher values of the IS in comparison to Algorithm-A. Note that the IS for MoCoGAN produced by Algorithm-B is closer to the reported values: it scores 12.03 ¬± 0.07, which approaches the reported score of 12.42 ¬± 0.03. However, the opposite is i Method Inception Score ‚Üë</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reported</head><p>Reproduced Algorithm-A</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproduced</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm-B</head><p>MoCoGAN <ref type="bibr" target="#b28">[29]</ref> 12.42 ¬± 0.03 11.15 ¬± 0.10 12.03 ¬± 0.07 TGAN-Normal <ref type="bibr" target="#b18">[19]</ref> 9.18 ¬± 0.11</p><p>8.11 ¬± 0.07 9.90 ¬± 0.06 TGAN-SVC <ref type="bibr" target="#b18">[19]</ref> 11.85 ¬± 0.07 11.91 ¬± 0.21 14.04 ¬± 0.08 MDP (ours) 11.86 ¬± 0.11 11.86 ¬± 0.11 13.00 ¬± 0.07 <ref type="table">Table 4</ref>. Reproducibility of the Inception Score (IS) on the UCF-101 dataset. Despite using the original implementation provided by the authors <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29]</ref> we observe a discrepancy between the reproduced IS and the scores reported in the original work. We identify two factors affecting the reproducibility: model selection and IS implementation. For the IS implementation, we consider Algorithm-A (c.f . <ref type="figure">Figure 8a)</ref> and Algorithm-B (c.f . <ref type="figure">Figure 8b)</ref>. While TGAN-SVC with Algorithm-A roughly corresponds to the reported values, the opposite is the case for MoCoGAN. Since mixing the results from the two evaluation algorithms changes the ranking, it is essential that the methods are compared based on the same IS implementation and we ensure this in our experiments (c.f . Appendix B for a more detailed discussion). <ref type="figure">Figure 9</ref>. Changes in the Inception Score over the course of training. Since Inception Score is not the training objective of the GAN models, it is instructive to look at fluctuations in this score over the course of training. We find that longer training does not necessarily improve the IS. Yet, computing the IS is computationally expensive and since there is no train-test split in the conventional sense, an intermediate IS evaluation is equivalent to "peeking" into the test performance of the final model. To improve reproducibility, we therefore evaluate the models trained for a fixed number of iterations as indicated by the blue line.</p><p>the case for TGAN-SVC as expected, since Algorithm-A is the unaltered version of the evaluation provided by the TGAN authors <ref type="bibr" target="#b18">[19]</ref> (we attribute the discrepancy for TGAN-Normal to model selection, which we will discuss shortly). Importantly, regardless of the evaluation protocol our MDP model always outperforms the MoCoGAN baseline and achieves 13.00 ¬± 0.07 with Algorithm-B.</p><p>Although the choice of Algorithm-B over Algorithm-A does not change the ranking of the methods, we emphasize that mixing the results produced by the two algorithms does and can essentially invalidate the experimental conclusions. Therefore, we stick to Algorithm-A for all methods in the experiments that we presented in the main text.</p><p>We conclude that the specifics of IS implementation lead to tangibly different results and stress the importance of using the same evaluation strategy for all methods in the experiments. Moreover, we additionally report the Fr√©chet Video Distance (FVD) as well as our two new metrics, t-PSNR and t-DSSIM, in the main paper to provide a complementary view to the IS.</p><p>Model selection. Recall that the IS for TGAN-Normal using Algorithm-A, 8.11 ¬± 0.07, is still inferior to the reported score of 9.18 ¬± 0.11 (c.f . <ref type="table">Table 4</ref>). To investigate this discrepancy, we observe that the standard training objective <ref type="bibr" target="#b4">[5]</ref> used to train video generation models <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29]</ref> serves only as a proxy criterion for the Inception Score. As a result, lower training loss, or even convergence of training, does not necessarily imply an improvement in the Inception Score. <ref type="figure">Figure 9</ref> illustrates this observation: the Inception Score fluctuates over the course of training, even after a considerable number of training iterations. Indeed, the IS achieved by TGAN-Normal at around 30K iterations is the highest, 9.48 ¬± 0.13, which is closer to the 9.18 ¬± 0.11 reported by the TGAN authors <ref type="bibr" target="#b18">[19]</ref> (in fact better). Although it is disputable which strategy of selecting the final model for evaluation is more meaningful, we argue against searching for the best IS across training iterations. One reason is that there is no conventional train-test split for video generation, hence computing the IS for intermediate models amounts to "peeking" into the test-time performance of the model. Selecting the best IS also inhibits reproducibility. Since IS fluctuations are rather random, there can be no fixed training schedule defined a-priori to reproduce the result. Additionally, computing the IS is computationally expensive, as it requires thousands of forward passes with a pre-trained classification network (C3D).</p><p>We believe that more transparency both at the stage of model selection and IS implementation can improve reproducibility of the Inception Score. We adhere to this principle and ensure a pre-defined training schedule and exactly same evaluation methodology to enable a fair and reproducible experiments.</p><p>ii Model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv3D</head><p>BatchNorm <ref type="bibr" target="#b38">[39]</ref>   <ref type="table">Table 5</ref>. Original MoCoGAN video discriminator DV and its TCN architecture adaptation. The 3D convolution is optionally followed by BatchNorm <ref type="bibr" target="#b38">[39]</ref> and LeakyReLU <ref type="bibr" target="#b41">[42]</ref> activations, as indicated by the checkmarks. The three parameters for the kernel, stride, padding, and dilation correspond to the temporal and two spatial dimensions (height and width), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Qualitative Examples</head><p>We make additional qualitative results available at https://sites.google.com/view/mdp-for-video-generation. The examples provide a visual comparison of MoCoGAN and our MDP-based model on the Tai Chi, Human Actions, and UCF-101 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Architecture</head><p>Recall that our MDP approach is based on an extension of the video discriminator from the original MoCoGAN model to a TCN-like model implementing reward causality. As <ref type="table">Table 5</ref> details, we only modify the temporal domain and replace the standard convolutions with their dilated variants. The hyperparameters of the TCN are the number of the dilated layers (blocks) and the convolution kernel size. Following Bai et al. <ref type="bibr" target="#b1">[2]</ref>, we set both hyperparameters to 3, since in such configuration the receptive field of the last TCN output covers the entire input sequence of 16 frames. Note that the TCN version does not increase the number of parameters of the original D V , but even reduces it due to a smaller kernel size in the temporal domain. It is only the addition of the Q-value approximation that slightly increases the model capacity of D V . The architecture of the image discriminator and the generator remain in their original form <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Training details</head><p>We follow the training protocol of MoCoGAN <ref type="bibr" target="#b28">[29]</ref> and use the ADAM optimizer <ref type="bibr" target="#b40">[41]</ref> for training all networks with a learning rate of 2 √ó 10 ‚àí4 and moment hyperparameters Œ≤ 1 = 0.5, Œ≤ 2 = 0.999. As regularization, we only use weight decay of 10 ‚àí5 . We leave the settings for the motion and the content subspaces of MoCoGAN to their default parameters with d c = 50 and the motion dimension set to d m = 10. We also keep the additive noise ‚àº N (0 , 0.1) for images fed to the discriminators.</p><p>In order to be compatible with the MoCoGAN evaluation (c.f . Appendix B), all networks are trained for 100K iterations with a mini-batch size of 32, while the seed is kept constant (0) to ensure equivalent parameter initialization across the experiments. For our ablation study on the Tai-Chi dataset, we used a batch size of 64 and trained for 50K iterations. The training length for the models is set to K = 16 frames, unless explicitly stated otherwise. We sample the original data with a fixed sampling stride of 2, as in MoCoGAN training. <ref type="bibr" target="#b3">4</ref> This means that in order to acquire 16 frames for training, we extract a 32 frame sequence with a random starting point from the ground truth dataset and take every second frame. This procedure increases the amount of motion by reducing the fps of the original video sequence, since e.g. on the UCF-101 dataset the originally sampled 16 frames provide imperceptible changes to scene appearance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The original MoCoGAN architecture (a) and our proposed modification of DV for modeling the MDP (b). Our MDP reformulation follows the TCN design<ref type="bibr" target="#b1">[2]</ref>: a sequence of 3D-convolutional layers with layer-specific dilations and strides. The input to the next convolutional layer is the output of the previous one. The last layer produces the immediate rewards {rt} 1‚â§t‚â§K and the {Qt} 1‚â§t‚â§K , i.e. the Q-values are produced by the same network, DV .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>t-PSNR and t-DSSIM decomposed as functions of time. In contrast to the ground truth, the diversity of the MoCoGAN samples vanishes with time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Tai Chi comparison between MoCoGAN (top row) exhibiting the freezing artifact, and our MDP model (bottom row) generating perceivable motion (e.g. torso).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Random samples on Human Actions. (a) MoCoGAN, (b) MDP-0, (c) MDP. Disabling MDP leads to poorer video quality in (b), while modelling long-term rewards leads to comparable per-frame quality of the samples from our MDP model (c) w.r.t. MoCoGAN baseline (a), also reflected by IS, yet tangibly higher temporal diversity measured by t-PSNR and t-DSSIM. From the video sequence of 64 frames, every 8 th frame is shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Random samples of the MoCoGAN baseline and MDP models on UCF-101. (a) MoCoGAN with looping artifact. (b) Our MDP-0 without modeling future rewards exhibits a freezing pattern. (c) Our MDP model. In (c), while the first sample has some looping, the second does not have temporal artifacts. From the video sequence of 64 frames, every 8 th frame is shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Looping-FWD 2.03 ¬± 0.03 336.7 ¬± 13.5 0.0062 ‚àû Looping-BWD 1.69 ¬± 0.03 541.7 ¬± 19.4 0.0062 ‚àû Freezing 1.55 ¬± 0.05 254.4 ¬± 15.5 0.0062 ‚àû</figDesc><table><row><cell>Tai Chi</cell><cell>Original</cell><cell>1.63 ¬± 0.05</cell><cell>115.3 ¬± 6.9</cell><cell>0.013</cell><cell>36.50</cell></row><row><cell></cell><cell>Original</cell><cell cols="2">40.74 ¬± 0.20 472.8 ¬± 18.5</cell><cell>0.073</cell><cell>27.10</cell></row><row><cell>UCF-101</cell><cell cols="3">Original + Looping-FWD 38.59 ¬± 0.22 597.2 ¬± 13.5 36.69 ¬± 0.23 444.8 ¬± 17.2 Looping-BWD 35.16 ¬± 0.78 737.7 ¬± 40.0</cell><cell>0.107 0.034 0.034</cell><cell>25.44 ‚àû ‚àû</cell></row><row><cell></cell><cell>Freezing</cell><cell cols="2">32.45 ¬± 0.22 667.3 ¬± 17.8</cell><cell>0.034</cell><cell>‚àû</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>IS ‚Üë 1.63 ¬± 0.05 4.49 ¬± 0.04 4.52 ¬± 0.05 4.15 ¬± 0.04 4.24 ¬± 0.06 3.92 ¬± 0.07 3.99 ¬± 0.04</figDesc><table><row><cell>FVD ‚Üì</cell><cell>118 ¬± 5</cell><cell>828 ¬± 38</cell><cell>1108 ¬± 50</cell><cell>787 ¬± 10</cell><cell>782 ¬± 40</cell><cell>744 ¬± 40</cell><cell>809 ¬± 22</cell></row><row><cell>t-DSSIM ‚Üë</cell><cell>0.0135</cell><cell>0.0031</cell><cell>0.0031</cell><cell>0.0024</cell><cell>0.0037</cell><cell>0.0035</cell><cell>0.0035</cell></row><row><cell>t-PSNR ‚Üì</cell><cell>36.48</cell><cell>45.37</cell><cell>57.34</cell><cell>50.16</cell><cell>44.87</cell><cell>44.39</cell><cell>45.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of our two MDP models to the state of the art. Temporal metrics are calculated for 64 frames. Our MDP model consistently improves the temporal video quality in terms of t-PSNR, t-DSSIM, and IS. Moreover, it is more memory efficient as it is comparable to MoCoGAN K = 24 and can produce videos of arbitrary length in contrast to TGAN. Note that since TGAN<ref type="bibr" target="#b18">[19]</ref> can only generate videos of 16 frames, we do not compute t-PSNR and t-DSSIM for this model here.</figDesc><table><row><cell>dataset</cell><cell>-</cell><cell>3.39 ¬± 0.08</cell><cell>49 ¬± 2</cell><cell>0.0815</cell><cell>23.35</cell><cell>40.80 ¬± 0.26</cell><cell>452 ¬± 49</cell><cell>0.0723</cell><cell>28.34</cell></row><row><cell cols="3">TGAN (Normal) 16 2.90 ¬± 0.04</cell><cell>977 ¬± 31</cell><cell>-</cell><cell>-</cell><cell>8.11 ¬± 0.07</cell><cell>1686 ¬± 24</cell><cell>-</cell><cell>-</cell></row><row><cell>TGAN (SVC)</cell><cell cols="2">16 3.65 ¬± 0.10</cell><cell>227 ¬± 10</cell><cell>-</cell><cell>-</cell><cell cols="2">11.91 ¬± 0.21 1324 ¬± 23</cell><cell>-</cell><cell>-</cell></row><row><cell>MoCoGAN</cell><cell cols="2">16 3.53 ¬± 0.02</cell><cell>300 ¬± 8</cell><cell>0.0259</cell><cell>33.76</cell><cell cols="2">11.15 ¬± 0.10 1351 ¬± 49</cell><cell>0.0337</cell><cell>33.29</cell></row><row><cell>MoCoGAN-D + V</cell><cell cols="2">16 3.51 ¬± 0.02</cell><cell>245 ¬± 6</cell><cell>0.0243</cell><cell>34.79</cell><cell cols="2">11.48 ¬± 0.15 1314 ¬± 45</cell><cell>0.0358</cell><cell>33.61</cell></row><row><cell>MoCoGAN</cell><cell cols="2">24 3.47 ¬± 0.02</cell><cell>318 ¬± 9</cell><cell>0.0254</cell><cell>35.72</cell><cell cols="2">10.49 ¬± 0.09 1352 ¬± 49</cell><cell>0.0387</cell><cell>32.63</cell></row><row><cell>MDP-0 (ours)</cell><cell cols="3">16 3.55 ¬± 0.03 1413 ¬± 15</cell><cell>0.0559</cell><cell>33.31</cell><cell>6.16 ¬± 0.08</cell><cell>2147 ¬± 87</cell><cell>0.0160</cell><cell>47.36</cell></row><row><cell>MDP (ours)</cell><cell cols="2">16 3.55 ¬± 0.02</cell><cell>641 ¬± 8</cell><cell>0.0604</cell><cell>30.12</cell><cell cols="2">11.86 ¬± 0.11 1277 ¬± 56</cell><cell>0.0370</cell><cell>32.77</cell></row><row><cell>MDP (ours)</cell><cell cols="2">24 3.49 ¬± 0.03</cell><cell>686 ¬± 12</cell><cell>0.0661</cell><cell>29.39</cell><cell cols="2">12.14 ¬± 0.18 1293 ¬± 58</cell><cell>0.0454</cell><cell>31.05</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t , x fake t ) + L V (x real t , x fake t ) ,(1)where x real t and x real t are samples from the training data, the generator provides x fake t and x fake t , and L I and L V are defined by the scalar scores of D I and D V<ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that the Tai Chi subset used in the evaluation of MoCoGAN<ref type="bibr" target="#b28">[29]</ref> is not publicly available and could not be obtained due to licensing restrictions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use the publicly available code provided by the MoCoGAN authors at https://github.com/sergeytulyakov/mocogan.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">To ensure a fair comparison, we use the same inception network for IS and FVD and train other methods<ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29]</ref> using the authors' implementation (c.f . supplemental material for details).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The documentation for training MoCoGAN is provided at https://github.com/sergeytulyakov/mocogan/wiki/ Training-MoCoGAN.iii</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. The authors thank Sergey Tulyakov and Masaki Saito for helpful clarifications.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stochastic variational video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271[cs.CV]</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Actions as space-time shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lena</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*2016</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Probabilistic video generation using holistic attribute control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">M</forename><surname>Lehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*2017</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J√ºrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Oriol Vinyals, Alex Graves, and Koray Kavukcuoglu. Video pixel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A√§ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving video generation for multi-functional applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Kratzwald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danda</forename><surname>Pani Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Acharya</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11453</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Stochastic adversarial video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01523[cs.CV]</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual motion GAN for future-flow embedded video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha√´l</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical video generation from orthogonal information: Optical flow and texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsunori</forename><surname>Ohnishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Video (language) modeling: a baseline for generative models of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha√´l</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chopra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6604</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Temporal generative adversarial nets with singular value clipping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichi</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunta</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*2016. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deterministic policy gradient algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402[cs.CV],2012.5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning multilevel distributed representations for high-dimensional sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The recurrent temporal restricted Boltzmann machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*2009</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Introduction to Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Modeling human motion using binary latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*2017</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">MoCoGAN: Decomposing motion and content for video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Towards accurate generative models of video: A new metric and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rapha√´l</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marinier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01717[cs.CV]</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Conditional image generation with PixelCNN decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A√§ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*2016</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Decomposing motion and content for natural video sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to generate longterm future via hierarchical prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungryull</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*2016</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><forename type="middle">L</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*2016</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271[cs.CV]</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Temporal generative adversarial nets with singular value clipping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichi</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunta</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402[cs.CV],2012.5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">MoCoGAN: Decomposing motion and content for video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
