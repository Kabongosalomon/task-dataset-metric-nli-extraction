<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SLK-NER: Exploiting Second-order Lexicon Knowledge for Chinese NER</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dou</forename><surname>Hu</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingwei</forename><surname>Wei</surname></persName>
							<email>weilingwei@iie.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National Computer System Engineering Research Institute of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SLK-NER: Exploiting Second-order Lexicon Knowledge for Chinese NER</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.18293/SEKE2020-153</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-lexicon knowledge</term>
					<term>attention mechanism</term>
					<term>Chi- nese named entity recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although character-based models using lexicon have achieved promising results for Chinese named entity recognition (NER) task, some lexical words would introduce erroneous information due to wrongly matched words. Existing researches proposed many strategies to integrate lexicon knowledge. However, they performed with simple first-order lexicon knowledge, which provided insufficient word information and still faced the challenge of matched word boundary conflicts; or explored the lexicon knowledge with graph where higher-order information introducing negative words may disturb the identification.</p><p>To alleviate the above limitations, we present new insight into second-order lexicon knowledge (SLK) of each character in the sentence to provide more lexical word information including semantic and word boundary features. Based on these, we propose a SLK-based model with a novel strategy to integrate the above lexicon knowledge. The proposed model can exploit more discernible lexical words information with the help of global context. Experimental results on three public datasets demonstrate the validity of SLK. The proposed model achieves more excellent performance than the state-of-the-art comparison methods.</p><p>Index Terms-lexicon knowledge, attention mechanism, Chinese named entity recognition</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Named Entity Recognition (NER) aims to locate and classify named entities into predefined entity categories in the corpus, which is a fundamental task for various downstream applications such as information retrieval <ref type="bibr" target="#b0">[1]</ref>, question answering <ref type="bibr" target="#b1">[2]</ref>, machine translation <ref type="bibr" target="#b2">[3]</ref>, etc. Word boundaries in Chinese are ambiguities and word segmentation errors have a negative impact on identifying Name Entity (NE) <ref type="bibr" target="#b3">[4]</ref>, which would make Chinese NER more difficult to identify. Explicit discussions have approved that character-based taggers can outperform word-based counterparts <ref type="bibr" target="#b4">[5]</ref>.</p><p>Because entity boundaries usually coincide with some word boundaries, integrating external lexicon knowledge into character-based models has attracted research attention <ref type="bibr" target="#b4">[5]</ref>. Although lexicon can be useful, in practice the lexical words may introduce erroneous information and suffer from word boundary conflicts, which easily lead to wrongly matched entities and limit system the performance <ref type="bibr" target="#b5">[6]</ref>. To address the above issues, many sequence-based efforts have been devoted to incorporated lexicon knowledge into sentences <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. DOI reference number: 10.18293/SEKE2020-153.  <ref type="figure">Fig. 1</ref>. An example of a word character lattice. The top is that predicting the label uses from left to right sequence and the bottom is using from right to left sequence. The green arrow line represents that the character (start) can match with the lexical word (end). The blue arrow line represents the lexical word (start) information would be integrated into the character (end).</p><p>However, these strategies explore simple first-order lexicon knowledge(FLK) of each character as shown in the green arrow line in <ref type="figure">Fig.1</ref>. FLK only contains the lexical features of the characters itself, which cannot offer adequate word information. For example, the character "京(Jing)" only introduces "南京(Nanjing)" based on FLK. The wrongly matched word information would misidentify as "南 京(Nanjing)" instead of "南 京 市(Nanjing City)". As a result, they continue to suffer from boundary conflicts between potential words being incorporating in the lexicon. The conflict caused by this deficiency mainly comes from the middle of the named entity, such as "大(Big)" and "江(River)" in "长 江 大 桥(Yangtze River Bridge)".</p><p>Recently, some models attempted to aggregate rich higherorder lexicon knowledge, such as graph structure <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. This higher-order information probably introduces irrelevant words with the character, limiting the performance to some extent. In addition, the existence of shortcut paths may cause the model degeneration into a partially word-based model, which would suffer from segmentation errors.</p><p>To address the above issue, we introduce the secondorder lexicon knowledge (SLK) to each character in the input sentence, that is the neighbor's lexicon knowledge of the character, as elaborated in <ref type="figure">Fig.1</ref> with the blue arrow lines. The SLK of "京(Jing)" contains both "南京市(Nanjing City)" and "南京(Nanjing)" from its left neighbor "南(Nan)", and "南京市(Nanjing City)" from its right neighbor "市(City)". With regard to global semantics of the sentence, "南 京 市(Nanjing City)" is more likely to be the named entity than "南 京(Nanjing)" due to higher semantic similarity of "南 京市(Nanjing City)". Similarly, the SLK of "江(River)" is the potential words "长江大桥(Yangtze River Bridge)" and "长江(Yangtze River)", and the SLK of "大(Big)" is "长江 大 桥(Yangtze River Bridge)" and "大 桥(Big Bridge)". By synthesizing global considerations, these lexicon knowledge guides the character subsequence "长江大桥(Yangtze River Bridge)" to be recognized as the named entity.</p><p>To take advantage of this insight, we proposed a SLKbased model with a novel strategy named SLK-NER, to integrate more informative lexicon words into the character-based model. Specifically, we assign SLK to each character and ensure no shortcut paths between characters. Furthermore, we utilize global contextual information to fuse the lexicon knowledge via attention mechanism. The model enables capture more useful lexical word features automatically and relieves the word boundary conflicts problem for better Chinese NER performance.</p><p>The main contributions can be summarized as follows:</p><p>• Insight. We present a new insight about second-order lexicon knowledge (SLK) of the character. SLK can provide sufficient lexicon knowledge into characters in sentences and is capable of relieving the challenge of word boundary conflicts. • Method. To properly leverage SLK, we propose a Chinese NER model named SLK-NER with a novel strategy to integrate lexicon knowledge into the character-based model. SLK-NER can enable to capture more beneficial word features with the help of global context information via attention mechanism. • Evaluation. Experimental results demonstrate the efficiency of SLK and our model significantly outperforms pervious methods, achieving state-of-the-art over three public Chinese NER datasets. The source code and dataset are available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Early character-based methods for NER considered few word information in character sequence <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. To tackle this limitation, many works generally use lexicon as extra word information for Chinese NER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Sequence-based Methods</head><p>Zhang et al. <ref type="bibr" target="#b4">[5]</ref> introduced a lattice LSTM to model all potential words matching a sentence to exploit explicit word information and achieved state-of-the-art results. Lattice LSTM enlightened various approaches for the useage of lexicon knowledge. Chain-structured LSTM <ref type="bibr" target="#b7">[8]</ref> integrated word boundary features into input character vector via four strategies. Gui et al. <ref type="bibr" target="#b6">[7]</ref> extended rethinking mechanism to relieve word boundary conflicts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graph-based Methods</head><p>With the development of graph, there are some studies improved by graph neural networks. For instance, Gui et al. <ref type="bibr" target="#b8">[9]</ref> proposed a GNN-based method to explore multiple graph-based interactions among characters, potential words, and the whole-sentence semantics and effectively alleviated the word ambiguity. Sui et al. <ref type="bibr" target="#b9">[10]</ref> proposed a collaborative graph network to assign both self-matched and the nearest contextual lexical words. Ding et al. <ref type="bibr" target="#b10">[11]</ref> proposed a multidigraph structure to learn the contextual information of the characters and the lexicon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>The overall architecture of our proposed model is illustrated in <ref type="figure" target="#fig_0">Fig.2</ref>. First, we encode character-based sentences to explicitly capture the contextual features of the sentence via character encoding layer. Second, to integrate more lexicon knowledge, we construct the second-order lexicon knowledge (SLK) for each character. Third, a fusion layer with the global attention information is used for fusing different SLK to alleviate the impact of word boundary conflicts. Finally, a standard CRF model <ref type="bibr" target="#b14">[15]</ref> is employed for decoding labels.</p><p>Formally, we denote an input sentence as s = {c 1 , c 2 ..., c n }, where c i means the ith character. The lexicon D is the same as <ref type="bibr" target="#b4">[5]</ref>, which is built by using automatically segmented large raw text. For ith character, we use − − → FW i to denote a set of words obtained by matching all possible forward subsequences in lexicon D <ref type="bibr" target="#b7">[8]</ref>. Similarly, we use ← − − FW i to denote the words for ith character in backward process. The knowledge involved in these sets represents the FLK corresponding to the ith character, i.e., FW i = − − → FW i ∪ ← − − FW i . Based on FLK, SLK of ith character can be defined as:</p><formula xml:id="formula_0">SW i = − − → FW i−1 ∪ ← − − FW i+1 , i ∈ [1, n].<label>(1)</label></formula><p>As the example shows in <ref type="figure" target="#fig_0">Fig.2</ref>, SLK of the character "京(Jing)" is the word set including "南京(Nanjing)" and "南 京市(Nanjing City)". SLK can mitigate the negative impact of word boundary conflicts. Therefore, we utilize SLK in our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Character Encoding Layer</head><p>Given the sentence s, a pre-trained model BERT <ref type="bibr" target="#b11">[12]</ref> encodes each character c i in the sentence to a vector.</p><formula xml:id="formula_1">x c i = BERT (c i ).<label>(2)</label></formula><p>To capture more contextual information, we apply bidirectional Gate Recurrent Unit:</p><formula xml:id="formula_2">h c i = GRU (x c i ), i ∈ [1, n].<label>(3)</label></formula><p>The hidden state of last character contains the global features of the input sentence, i.e., g = h c n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Lexicon Knowledge Encoding Layer</head><p>To represent the semantic information of SLK of ith character, we embed jth lexical word sw ij in SW i to distributional space as a semantic vector:</p><formula xml:id="formula_3">x sw ij = e w (sw ij ),<label>(4)</label></formula><p>where e w is a pre-trained word embedding lookup table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Contextual Lexicon Knowledge Fusion</head><p>Not all lexical words contribute equally to the representation of the character meaning. Hence, we introduce a global contextual information to extract such SLK that are important to the meaning of the character and aggregate them to refine a character vector. Specifically, for the jth word in the matching set SW i of the ith character, we can obtain a hidden representation u ij for word embedding x sw ij :</p><formula xml:id="formula_4">u ij = W u x sw ij + b u ,<label>(5)</label></formula><p>where W u and b u are update parameters. We measure the importance of lexical word as the similarity and get a normalized importance weight α ij . Then, the SLK of ith character can be computed as a weighted sum of the word information.</p><formula xml:id="formula_5">α ij = exp(u T ij g) j exp(u T ij g) ,<label>(6)</label></formula><formula xml:id="formula_6">h sw i = j α ij x sw ij .<label>(7)</label></formula><p>Finally, the final representation of ith character is denoted as</p><formula xml:id="formula_7">r i = [h sw i ; h c i ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Decoding and Training</head><p>To formulate the dependencies between successive labels, a standard CRF layer is used to make sequence tagging. We define matrix O to be scores calculated based on the final representations R = {r 1 , ..., r n }:</p><formula xml:id="formula_8">O = W o R + b o ,<label>(8)</label></formula><p>where W o and b o are trainable parameters. Then, the probability of tag sequence y = {y 1 , ..., y n } is:</p><formula xml:id="formula_9">p(y|s) = exp( i (O i,yi + T yi−1,yi )) ŷ exp( i O i,ŷi + Tŷ i−1,ŷi )) ,<label>(9)</label></formula><p>where T is a transition score matrix, andŷ denotes all possible tag sequences. While decoding, we apply the Viterbi <ref type="bibr" target="#b15">[16]</ref> algorithm to get label sequence with the highest score.</p><p>Given training examples {(s j , y j )}| N j=1 , we optimize the model by minimizing the negative log-likelihood loss:</p><formula xml:id="formula_10">L = − j log(p(y j |s j )).<label>(10)</label></formula><p>IV. EXPERIMENTS A. Experimental Settings 1) Datasets: As shown in <ref type="table" target="#tab_1">Table I</ref>, we evaluate our model on three datasets, OntoNotes4, Weibo and Resume. OntoNotes4 is a multilingual corpus in the news domain that contains four types of named entities. Weibo dataset consists of annotated NER messages drawn from Sina Weibo 2 . The corpus contains PER, ORG, GEP, and LOC for both named entity and nominal mention. Resume dataset is composed of resumes collected from Sina Finance 3 . It is annotated with 8 types of named entities. For OntoNotes4, we use the same training, validing and testing splits as <ref type="bibr" target="#b16">[17]</ref>. Since other datasets have already been split, we don't change them. 2) Comparisons: The methods evaluated are as follows. BiLSTM-CRF <ref type="bibr" target="#b12">[13]</ref> was a sequence labeling model consisting of BiLSTM layer and CRF layer. BERT <ref type="bibr" target="#b11">[12]</ref> was a pretrained model with deep bidirectional transformer. CAN <ref type="bibr" target="#b13">[14]</ref> investigated CNN-based model with attention layers to capture features of the character and its contexts. Lattice-LSTM <ref type="bibr" target="#b4">[5]</ref> encoded characters in a sequence and all potential words that match a lexicon.</p><p>LGN <ref type="bibr" target="#b8">[9]</ref> used lexicon to construct the graph and provide word-level features. The literature <ref type="bibr" target="#b10">[11]</ref> applied a multi-digraph structure to incorporate gazetteer information, and we denote MG-GNN for convenience. WC-LSTM <ref type="bibr" target="#b7">[8]</ref> was used to add word information into the start or the end character of the word. LR-CNN <ref type="bibr" target="#b6">[7]</ref> extended the rethinking mechanism when using lexicon. CGN <ref type="bibr" target="#b9">[10]</ref> investigated collaborative graph network (CGN) to leverage lexical knowledge.</p><p>3) Implementation Details: We use lexicon and word embeddings provided by <ref type="bibr" target="#b17">[18]</ref>, which is pretrained on Chinese Giga-Word using word2vec model. For character embeddings, we apply the bert-base Chinese model 4 (12-layer, 768-hidden, 12-heads). For characters and words that do not appear in the pretrained embeddings, we initialize them with a uniform distribution <ref type="bibr" target="#b4">5</ref> . When training, character embeddings and word embeddings are updated along with other parameters. For hyper-parameter configuration, we set max length of sentences to 250, word embedding size to 50, the dimensionality of Bi-GRU to 512, the number of Bi-GRU layer to 1, the dropout to 0.1, the batch size to 32. We use Adam to optimize all the trainable parameters with learning rate 5e − 5. For evaluation, we use the Precision(P), Recall(R) and F1 score(F1) as metrics in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Results</head><p>Firstly, we compare SLK-NER with three general sequence labeling model for NER. All of them performed without any lexicon knowledge. The results in the first block in <ref type="table" target="#tab_1">Table II,</ref> show that our proposed model achieves best F1 and R, which proves the efficiency of SLK-NER.</p><p>Next, the second block in <ref type="table" target="#tab_1">Table II</ref> shows the performance of graph-based models. SLK-NER gives better F1 and R than LGN, MG-GNN and CGN. Although these baselines explore lexicon knowledge via the graph structure, they performed without the consideration of contextual information. Hence, <ref type="bibr" target="#b4">5</ref> The range is [− we attribute the benefits to the efficiency of global contextaware in SLK-NER.</p><p>Furthermore, the third block in <ref type="table" target="#tab_1">Table II</ref> shows results of state-of-the-art sequence-based models. We can observe that our proposed model achieves a remarkably improvement on F1 over three datasets. The results strongly verify the integrating SLK into character-based model enables to boost the performance. By leveraging the SLK properly, our model is capable of improving NER in various domains, such as social network, news and Chinese resume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Strategies Analysis</head><p>In this part, we explore the effects of strategies about lexicon knowledge.</p><p>1) Lexicon Knowledge Types: We conduct comparative experiments on different kinds of lexicon knowledge. The results are illustrated in <ref type="table" target="#tab_1">Table III</ref>. We can clearly see that the character-based model performs poorly without lexicon knowledge, demonstrating the usefulness of lexicon. Besides, adding FLK makes a small improvement on F1. While adding SLK outperforms significantly on F1 in all datasets. The fact demonstrates the efficiency of SLK, and reveals that leveraging second-order lexicon knowledge can indeed alleviate the word boundary conflicts. Interestingly, when using both FLK and SLK, the F1 declines over three datasets. We conjecture the reason is there may be some negative word conflicts simultaneously for a character which limit the performance.</p><p>2) Lexicon Knowledge Encoding: We analyze the difference between the strategy in our model (Global-Attention) with four strategies proposed by <ref type="bibr" target="#b7">[8]</ref> for encoding word information, including Self-Attention, Shortest Word First, Longest Word First and Average. The results in <ref type="table" target="#tab_1">Table IV</ref> show that global attention in our model achieves best performance on F1 score. This demonstrates that our model can combine more informative features to determine the word boundary and effectively alleviate the negative influence of word boundary conflicts. <ref type="figure" target="#fig_1">Fig.3</ref> shows the F1 score of several baselines and SLK-NER against sentence length on OntoNotes4 dataset. BERT and SLK-NER outperform significantly than other baselines, which indicates the ability to capture long dependencies. However, BERT ignores the word information among the sentence. SLK-NER obtains a higher F1 over different sentence lengths compared to BERT, which proves the SLK and global contextaware can capture more useful contextual information. <ref type="figure" target="#fig_1">Fig. 3</ref>. F1 against sentence length on OntoNotes4 dataset. We split samples into six parts according to the sentence length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Sentence Length Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have investigated a lexicon-based model in Chinese NER task. We present a new insight about secondorder lexicon knowledge to incorporate informative lexicon into character-based model. Based on this insight, SLK-NER is proposed to integrate more contextual word information into each character utilizing the global context. SLK-NER can effectively alleviate the impact of word boundary conflicts and word segmentation errors. Extensive experiments on three public datasets have demonstrated the superior performance of SLK-NER than state-of-the-art models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>1 https://github.com/zerohd4869/SLK-NER 南 The whole architecture of SLK-NER. It is comprised of character encoding layer, lexicon knowledge encoding layer, contextual lexicon knowledge fusion and a CRF decoding layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 dim , + 3 dim</head><label>3</label><figDesc>], where dim denotes the size of embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I THE</head><label>I</label><figDesc>STATISTICS OF THE DATASETS.</figDesc><table><row><cell>Dataset</cell><cell>Training</cell><cell>Validation</cell><cell>Testing</cell></row><row><cell>OntoNotes4</cell><cell>15724</cell><cell>4301</cell><cell>4346</cell></row><row><cell>Weibo</cell><cell>1350</cell><cell>270</cell><cell>270</cell></row><row><cell>Resume</cell><cell>3821</cell><cell>463</cell><cell>477</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II EXPERIMENTAL</head><label>II</label><figDesc>RESULTS(%) ON THREE DATASETS. OF DIFFERENT FUSION STRATEGIES ON THREE DATASETS.</figDesc><table><row><cell>Models</cell><cell></cell><cell>P</cell><cell cols="3">OntoNotes4 R</cell><cell>F1</cell><cell>P</cell><cell>Weibo R</cell><cell>F1</cell><cell>P</cell><cell>Resume R</cell><cell>F1</cell></row><row><cell cols="2">BiLSTM-CRF[13]</cell><cell cols="2">72.0</cell><cell>75.1</cell><cell cols="2">73.5</cell><cell>60.8</cell><cell>52.9</cell><cell>56.6</cell><cell>93.7</cell><cell>93.3</cell><cell>93.5</cell></row><row><cell>BERT[12]</cell><cell></cell><cell cols="2">78.0</cell><cell>80.4</cell><cell cols="2">79.2</cell><cell>61.2</cell><cell>63.9</cell><cell>62.5</cell><cell>94.2</cell><cell>95.8</cell><cell>95.0</cell></row><row><cell>CAN[14]</cell><cell></cell><cell cols="2">75.1</cell><cell>72.3</cell><cell cols="2">73.6</cell><cell>55.4</cell><cell>63.0</cell><cell>59.3.</cell><cell>95.1</cell><cell>94.8</cell><cell>94.9</cell></row><row><cell>LGN[9]</cell><cell></cell><cell cols="2">76.1</cell><cell>73.7</cell><cell cols="2">74.9</cell><cell>-</cell><cell>-</cell><cell>60.2</cell><cell>95.3</cell><cell>95.5</cell><cell>95.4</cell></row><row><cell>MG-GNN[11]</cell><cell></cell><cell cols="2">74.3</cell><cell>76.2</cell><cell cols="2">75.2</cell><cell>63.1</cell><cell>56.3</cell><cell>59.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CGN[10]</cell><cell></cell><cell cols="2">75.1</cell><cell>74.5</cell><cell cols="2">74.8</cell><cell>-</cell><cell>-</cell><cell>63.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">LatticeLSTM[5]</cell><cell cols="2">76.4</cell><cell>71.6</cell><cell cols="2">73.9</cell><cell>53.0</cell><cell>62.3</cell><cell>58.8</cell><cell>94.8</cell><cell>94.1</cell><cell>94.5</cell></row><row><cell>WC-LSTM[8]</cell><cell></cell><cell cols="2">76.1</cell><cell>72.9</cell><cell cols="2">74.4</cell><cell>52.6</cell><cell>67.4</cell><cell>59.8</cell><cell>95.3</cell><cell>95.2</cell><cell>95.2</cell></row><row><cell>LR-CNN[7]</cell><cell></cell><cell cols="2">76.4</cell><cell>72.6</cell><cell cols="2">74.5</cell><cell>-</cell><cell>-</cell><cell>59.9</cell><cell>95.4</cell><cell>94.8</cell><cell>95.1</cell></row><row><cell>SLK-NER</cell><cell></cell><cell cols="2">77.9</cell><cell>82.2</cell><cell cols="2">80.2</cell><cell>61.8</cell><cell>66.3</cell><cell>64.0</cell><cell>95.2</cell><cell>96.4</cell><cell>95.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE III</cell><cell></cell><cell></cell></row><row><cell cols="11">EXPERIMENTAL RESULTS (%) OF DIFFERENT ENCODING STRATEGIES ON THREE DATASETS.</cell></row><row><cell>Encoding Strategy</cell><cell>P</cell><cell cols="3">OntoNotes4 R</cell><cell cols="2">F1</cell><cell>P</cell><cell>Weibo R</cell><cell>F1</cell><cell>P</cell><cell>Resume R</cell><cell>F1</cell></row><row><cell>using SLK</cell><cell>77.9</cell><cell></cell><cell cols="2">82.2</cell><cell cols="2">80.2</cell><cell>61.8</cell><cell>66.3</cell><cell>64.0</cell><cell>95.2</cell><cell>96.4</cell><cell>95.8</cell></row><row><cell>using FLK</cell><cell>76.6</cell><cell></cell><cell cols="2">82.9</cell><cell cols="2">79.8</cell><cell>61.8</cell><cell>64.6</cell><cell>63.2</cell><cell>95.1</cell><cell>96.2</cell><cell>95.6</cell></row><row><cell>using SLK and FLK</cell><cell>76.4</cell><cell></cell><cell cols="2">82.7</cell><cell cols="2">79.6</cell><cell>60.6</cell><cell>63.6</cell><cell>62.1</cell><cell>94.9</cell><cell>96.2</cell><cell>95.5</cell></row><row><cell>no lexicon</cell><cell>77.7</cell><cell></cell><cell cols="2">81.3</cell><cell cols="2">79.6</cell><cell>56.7</cell><cell>66.5</cell><cell>61.2</cell><cell>94.2</cell><cell>96.1</cell><cell>95.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE IV</cell><cell></cell><cell></cell></row><row><cell cols="5">OntoNotes4 EXPERIMENTAL RESULTS (%) Fusion Strategy P R</cell><cell>F1</cell><cell></cell><cell>P</cell><cell>Weibo R</cell><cell>F1</cell><cell>P</cell><cell>Resume R</cell><cell>F1</cell></row><row><cell>Global-Attention</cell><cell>77.9</cell><cell></cell><cell>82.2</cell><cell></cell><cell cols="2">80.2</cell><cell>61.8</cell><cell>66.3</cell><cell>64.0</cell><cell>95.2</cell><cell>96.4</cell><cell>95.8</cell></row><row><cell>Self-Attention</cell><cell>77.2</cell><cell></cell><cell>81.2</cell><cell></cell><cell cols="2">79.1</cell><cell>55.9</cell><cell>60.1</cell><cell>57.9</cell><cell>94.2</cell><cell>96.3</cell><cell>95.2</cell></row><row><cell>Shortest Word First</cell><cell>77.1</cell><cell></cell><cell>81.5</cell><cell></cell><cell cols="2">79.2</cell><cell>55.8</cell><cell>57.7</cell><cell>56.7</cell><cell>93.9</cell><cell>96.1</cell><cell>95.0</cell></row><row><cell>Longest Word First</cell><cell>77.1</cell><cell></cell><cell>81.6</cell><cell></cell><cell cols="2">79.3</cell><cell>57.6</cell><cell>56.9</cell><cell>57.3</cell><cell>94.7</cell><cell>96.1</cell><cell>95.4</cell></row><row><cell>Average</cell><cell>78.6</cell><cell></cell><cell>80.8</cell><cell></cell><cell cols="2">79.7</cell><cell>56.4</cell><cell>58.4</cell><cell>57.3</cell><cell>94.3</cell><cell>96.3</cell><cell>95.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.weibo.com 3 https://finance.sina.com.cn/stock/ 4 https://github.com/google-research/bert</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Named entity recognition in query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="267" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Core techniques of question answering systems over knowledge bases: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Diefenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Maret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information systems</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="529" to="569" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved named entity recognition using machine translation-based crosslingual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandipan</forename><surname>Dandapat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computación y Sistemas</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="495" to="504" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Named entity recognition for chinese social media with jointly trained embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="548" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Chinese NER using lattice LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1554" to="1564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Named entity recognition with bidirectional lstm-cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
		</imprint>
		<respStmt>
			<orgName>TACL</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cnn-based chinese ner with lexicon rethinking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruotian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lujun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th IJCAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4982" to="4988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An encoding strategy based word-character lstm for chinese ner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongge</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueran</forename><surname>Zu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2379" to="2389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A lexiconbased graph neural network for chinese ner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicheng</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1039" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Leverage lexical knowledge for chinese named entity recognition via collaborative graph network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianbo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3821" to="3831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A neural multi-digraph model for chinese ner with gazetteers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixue</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1462" to="1467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<meeting><address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Can-ner: Convolutional attention network for chinese named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuying</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3384" to="3393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Error bounds for convolutional codes and an asymptotically optimum decoding algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Viterbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="269" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Named entity recognition with bilingual constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="52" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Analogical reasoning on Chinese morphological and semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renfen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wensi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="138" to="143" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
