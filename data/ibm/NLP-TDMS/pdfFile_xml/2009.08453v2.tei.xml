<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code: https://github.com/szq0214/MEAL-V2</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a simple yet effective distillation framework that is able to boost the vanilla ResNet-50 to 80%+ Top-1 accuracy on ImageNet without tricks. We construct such a framework through analyzing the problems in existing classification system and simplify the base method ensemble knowledge distillation via discriminators [42] by:</p><p>(1) adopting the similarity loss and discriminator only on the final outputs and (2) using the average of softmax probabilities from all teacher ensembles as the stronger supervision. Intriguingly, three novel perspectives are presented for distillation: (1) weight decay can be weakened or even completely removed since the soft label also has a regularization effect; (2) using a good initialization for students is critical; and (3) one-hot/hard label is not necessary in the distillation process if the weights are well initialized. We show that such a straight-forward framework can achieve state-of-the-art results without involving any commonlyused techniques, such as architecture modification; outside training data beyond ImageNet; autoaug/randaug; cosine learning rate; mixup/cutmix training; label smoothing; etc. Our method obtains 80.67% top-1 accuracy on ImageNet using a single crop-size of 224×224 with vanilla ResNet-50, outperforming the previous state-of-the-arts by a significant margin under the same network structure. Our result can be regarded as a strong baseline using knowledge distillation, and to our best knowledge, this is also the first method that is able to boost vanilla ResNet-50 to surpass 80% on Ima-geNet without architecture modification or additional training data. On smaller ResNet-18, our distillation framework consistently improves from 69.76% to 73.19%, which shows tremendous practical values in real-world applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b29">[28]</ref> have been proven useful in many visual tasks, such as image classification <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b16">15]</ref>, object detection <ref type="bibr" target="#b12">[11,</ref><ref type="bibr" target="#b41">39]</ref>, semantic seg-   <ref type="bibr" target="#b19">[18]</ref>, MobileNet V3-Small 1.0 <ref type="bibr" target="#b19">[18]</ref>, MobileNet V3-Large 1.0 <ref type="bibr" target="#b19">[18]</ref>, EfficientNet-B0 <ref type="bibr" target="#b50">[47]</ref> and ResNet-50 <ref type="bibr" target="#b16">[15]</ref>. mentation <ref type="bibr" target="#b35">[33]</ref>, as well as some particular scenarios, like transferring feature representation <ref type="bibr" target="#b57">[54]</ref>, learning detectors from scratch <ref type="bibr" target="#b45">[43]</ref>, etc. In order to achieve highest possible accuracy, many training techniques and data augmentation methods have been proposed, such as mixup <ref type="bibr" target="#b59">[56]</ref>, cutmix <ref type="bibr" target="#b58">[55]</ref>, autoaug <ref type="bibr" target="#b4">[3]</ref>, randaug <ref type="bibr" target="#b5">[4]</ref>, fix resolution discrepancy <ref type="bibr" target="#b52">[49]</ref>, etc. Some works also focus on modifying the network structures, e.g., SE module <ref type="bibr" target="#b21">[20]</ref>, Stem block <ref type="bibr" target="#b45">[43]</ref>, Split-attention <ref type="bibr" target="#b60">[57]</ref>. This paper is to similarly obtain the best possible performance of a network, but our proposed method is orthogonal to the above techniques. In general, our method only relies on a teacher-student paradigm with a powerful ensemble of teachers and a good initialization of the student. It is simple, straight-forward, but effective and can achieve state-of-the-art performance on large-scale dataset. The advantages of our method are: 1) no architecture modification; 2) no outside training data beyond Ima-geNet; 3) no complex learning rate scheduler like cosine lr; 4) no extra data augmentation like mixup, autoaug, etc.</p><p>The objective of this paper is to give a better understanding on knowledge distillation and promote the capability and robustness of the classification networks through distillation. We first analyze and introduce several critical factors and limitations that will degrade the performance in the existing classification systems. We find the main drawback in the conventional training strategy of a network, i.e. with the one-hot label, is the inferior ability to distinguish the semantically similar categories, as shown in <ref type="figure">Fig. 2</ref>  <ref type="bibr" target="#b2">(1)</ref>. Networks trained with one-hot label are incapable of handling the semantically similar instances. We observe knowledge distillation <ref type="bibr" target="#b18">[17]</ref> is surprisingly effective in dealing with this circumstance as the supervision from teacher networks is smoothed and much lower than one-hot value. Therefore, the distilled students will encourage representations of examples to lie in tight equally separated clusters and enforce similar instances more distinguishable in feature space, similar to label smoothing <ref type="bibr" target="#b38">[36,</ref><ref type="bibr" target="#b46">44]</ref>. We show multiple promising improvements on various network architectures using distillation in <ref type="figure" target="#fig_1">Fig. 1</ref>. The potential improvement of our method can be larger if replacing with stronger teachers.</p><p>We also have a few interesting discoveries in our training process, for example, among them we would like to emphasize that the one-hot/hard label is not necessary if the weights are already well initialized and could not be used in the distillation process <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b44">42]</ref>. Some discussions about this perspective are provided in our Appendix. Also, weight decay can be weakened or even removed since the soft label also has a regularization effect, and using a good initialization is critical for distillation. While some previous studies deem that structure might be more crucial than pre-trained parameters on some downstream tasks like object detection <ref type="bibr" target="#b45">[43]</ref>, segmentation <ref type="bibr" target="#b25">[24]</ref>, etc., we still believe that boosting the performance of standard and classical network structures is interesting and useful, especially if the networks are already small and compact, like MobileNet V3, EfficientNet-B0, since the proposed method can be effortlessly generalized to other elaborated or searched architectures. Our method can be considered as a post-process to distill small and compact models for further boosting their performance, while no modification is required.</p><p>Our contributions in this paper are as follows:</p><p>• We provide empirical analysis and insights through individual class's accuracy to expose the mechanism of how knowledge distillation helps classification. It is not trivial to understand the principles behind it. • We present a simple yet effective and practical framework that can boost the performance of existing tiny models by a significant marginal. We show evidence and provide a demonstration, and also give detailed guidance on how to establish such a strong framework. • We further visualize our model to explore where the superior performance is from. Moreover, we transfer trained parameters to other datasets like fine-grained recognition to show the transferability of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image Classification. Image classification is a fundamental task in computer vision. AlexNet <ref type="bibr" target="#b27">[26]</ref> is considered as the seminal design that is proven feasible for deep neural networks on the large-scale datasets. After that, many innovative network structures have been proposed. Szegedy et al. <ref type="bibr" target="#b47">[45]</ref> proposes an "Inception" design that concatenates features maps produced by various sizes of kernels. He et al. <ref type="bibr" target="#b16">[15]</ref> creatively proposed residual blocks with skip connections, which is firstly enable to train extremely deep networks more than 100 layers. Huang et al. <ref type="bibr" target="#b22">[21]</ref> further proposed densely layer-wise connections for building DenseNet. Besides, some architectures are also targeting at mobile device scenario, such as MobileNet series <ref type="bibr" target="#b20">[19,</ref><ref type="bibr" target="#b43">41,</ref><ref type="bibr" target="#b19">18]</ref>, ShuffleNet <ref type="bibr" target="#b61">[58,</ref><ref type="bibr" target="#b36">34]</ref>, etc. With the development of these modern neural network designs and automatic architecture search <ref type="bibr" target="#b64">[61,</ref><ref type="bibr" target="#b50">47]</ref>, this task has been one of the fastest moving areas and achieved surprising results which even surpasses human-level performance on largescale datasets like ImageNet <ref type="bibr" target="#b6">[5]</ref> and OpenImage <ref type="bibr" target="#b28">[27]</ref>. Knowledge Distillation. Hinton et al. <ref type="bibr" target="#b18">[17]</ref> pioneered the concept of distilling knowledge from a larger teacher network or ensemble into a smaller compressed student. Mathematically, this paradigm of training the student on softened teacher predictive distribution is using the conventional cross-entropy with predicted labels. The student is encouraged to mimic the teacher output distribution, which helps the student generalize much better on validation set and in certain cases leads to the student performing even better than the teacher itself. These studies argued that the teacher distribution provided much richer information about an image compared to just one-hot labels. Further studies extended this concept by using internal feature representations <ref type="bibr" target="#b42">[40,</ref><ref type="bibr" target="#b44">42]</ref>, adversarial training with discriminators <ref type="bibr" target="#b44">[42]</ref> and transfer flow of solution procedure matrix as the student initialization <ref type="bibr" target="#b56">[53]</ref>. Some works also proposed online distillation <ref type="bibr" target="#b54">[51,</ref><ref type="bibr" target="#b63">60]</ref> that do not rely on a pre-trained teacher, so teacher and student can be learned simultaneously. Network Compression. Knowledge distillation is a natural way to produce the compressed student network through imitating the teacher's softened prediction. Besides it, other methods like weight quantization <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b62">59,</ref><ref type="bibr" target="#b24">23]</ref> and binarization <ref type="bibr" target="#b3">[2,</ref><ref type="bibr" target="#b40">38]</ref>, weight pruning <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b14">13]</ref> and channel pruning [29, <ref type="bibr" target="#b34">32,</ref><ref type="bibr" target="#b17">16]</ref> can also achieve the compression purpose. Knowledge distillation differs from them as the compressed network is designed before training so there is no additional operation required, such as reconstruction, retraining, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Analysis: Problems in Existing Classification System</head><p>Consider a classification task that distinguishes various breeds of dogs, e.g. toy poodle, miniature poodle, etc., the output predictive distribution of a higher capability teacher always provides the student model with the extra information of how alike one breed of dogs looks to the other. This helps the student learn more generalized features of each dog breed compared of providing just one hot-label which fails to supply any comparative knowledge. Also, in the process of trying to mimic the distribution of a much deeper teacher model the student tries to find a compact solution of transformation. This inherently enforces the student to explore more informative knowledge and generalize better. Impressively, in certain cases the student manages to outperform its teacher due to this superior generalization. Training with one-hot labels accompanying with crossentropy loss is a "balanced" learning system, which means the objective will enforce each class to be equidistant to all remaining class's distance, so the learned model is not sensitive to the semantically similar classes (e.g., different dog breeds) or dissimilar classes (e.g., dog and fish). Some situations it even gives risk to performing with respect to incorrect annotation of classes. By using dynamical soft labels from knowledge distillation of a strong teacher, different examples from the same or different classes can have very different similarities to other classes, thus the student can capture additional subtle information and prevent from overfitting. We illustrate training curves in <ref type="figure">Fig. 3</ref> by using onehot/hard and soft labels. We use exactly the same training hyper-parameters and settings, good initialized parameters, learning rate schedule, etc. We found if the initialization is already well-learned, training with one-hot label is easy to be overfitting (blue curves), while with soft labels (orange curves), the model can continue to learn new knowledge and generalize better on the validation set. Moreover, we select two semantically similar classes (toy poodle and standard poodle) and two dissimilar classes (tench and jay), and test their accuracy on ImageNet val set with the moderate and extremely good ResNet-50 <ref type="bibr" target="#b2">1</ref> . Results are shown in <ref type="table" target="#tab_0">Table 1</ref>, interestingly, it can be observed the overall accuracy gap mainly hinges on the semantically similar classes. Intuitively, these classes are difficult to distinguish in a classification system thus the bottleneck also lies here. In this 1 PyTorch official model (76.15%) and ours (80.67%). <ref type="figure">Figure 3</ref>. Comparison of Top-1 (left) and 5 (right) val accuracy by using hard and soft labels. We use exactly the same training hyperparameters and settings, including the same initialized parameters, learning rate schedule, etc., but the different supervision shapes. paper, we aim to diagnose these problems, we will give insights about how distillation method alleviates them below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Solution: What Can KD Solve?</head><p>In this section, we discuss the following aspects that knowledge distillation can handle in the modern classification system: (1) enlarge the distance of samples between semantically similar classes; (2) overcome multiple objects problem; (3) take advantage of random crop augmentation and avoid its limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Semantically Similar and Dissimilar Classes</head><p>As aforementioned in <ref type="table" target="#tab_0">Table 1</ref> and <ref type="figure" target="#fig_1">Fig. 2 (1)</ref>, the performance of good or moderate models primarily depends on the semantically similar classes. To further diagnose how this happens, we visualize the embedding distributions in <ref type="figure">Fig. 2</ref>  <ref type="bibr" target="#b3">(2)</ref>. As expected, the clusters of toy and standard poodle breeds are mixed up together, and tench and jay are sep-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT: tench</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Crop Augmentation</head><p>Multi-object <ref type="figure">Figure 4</ref>. Illustration of random crop data augmentation strategy on an image. This strategy randomly crops regions crossing a predefined scope, e.g. 8%∼100% of the whole image size on Ima-geNet but it will involve massive noises and incorrect labels. arate from each other. Impressively, the model trained with knowledge distillation can split the similar classes representations to some extend and make the border of two clusters clearer, which greatly facilitates the final classifying. Let us take this one step further to see how knowledge distillation obtains this function, we visualize the supervisions from teacher ensemble in <ref type="figure">Fig. 2 (3)</ref>. This is the main difference in our framework when learning with one-hot or distilled supervisions. we are interested and illustrate the supervisions from the training set since those are the ones used for distillation. We show the averaged probability of all samples in each class. The prediction of major class is only 0.3∼0.4 comparing to 1.0 as one-hot labels. These softened labels make models less confident and generalize much better, especially on those semantically similar classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Multi-Object/Random-Crop Issues</head><p>Multi-object is the scenario where there are at least two categories in which the images can be classified, this is a widely existing situation in ImageNet dataset as shown in <ref type="figure">Fig. 4</ref>. The one-hot label of this image is tench but a human is also contained in it and the area is even larger than the tench. It causes a label mismatch problem if using standard one-hot label for training. This paper argues that distillation can tackle this circumstance as the label is predicted by a well pretrained teacher network and label distribution depends on the content of input image instead of the assigned labels. Also, the soft label can be a multi-peak distribution which can model the mixed information of multiple objects.</p><p>Random crop data augmentation is an indispensable technique heavily utilized in modern network training. While as shown in <ref type="figure">Fig. 4</ref>, this strategy tends to involve a large proportion of noise if cropping on the background area or a small region of objects, which means it will always re-sult in inaccurate labels for the augmented regions. In the case of incorrect region by the random cropping, the global ground-truth label is used for such input. Despite deep neural networks are highly tolerant to noises in labels but the incorrectness will inevitably impair the effectiveness of learning process. Instead, knowledge distillation will predict the true probability distribution for each input independently and reflect what the input patch really is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Framework Components</head><p>In this section, we begin by introducing each component in our proposed framework, including: 1) teacher ensemble; 2) KL-divergence loss; 3) the discriminator. Then, we present the training details and techniques that we used and did not use in our distillation training. Teachers Ensemble is used to generate more accurate predictions for guiding the student training. Different from MEAL <ref type="bibr" target="#b44">[42]</ref> that selected one teacher through a teacher selection module in each training iteration, we adopt the average of softmax probabilities from multiple pre-trained teachers as an ensemble. Let T θ be the teacher network, the output ensemble probabilityp T θ e can be described as:</p><formula xml:id="formula_0">p T θ e (X) = 1 K K t=1 p t T θ (X)<label>(1)</label></formula><p>where p T θ t is the t-th teacher's softmax prediction. X is the inout image and K is the number of total teachers. e denotes the ensembled probability. KL-divergence is a measure metric of how one probability distribution is different from another reference distribution. In our approach, we train the student network S θ by minimizing the KL-divergence between its output p S θ (x i ) and the ensembled soft labelsp T θ (x i ) generated by the teacher ensemble. The loss function of KL-divergence can be formulated as (temperature is used as 1 following <ref type="bibr" target="#b44">[42]</ref>):</p><formula xml:id="formula_1">L KL (S θ ) = − 1 N N i=1p T θ e (x i ) log( p S θ (x i ) p T θ e (x i ) )<label>(2)</label></formula><p>where N is the number of samples. In practice, we can simply minimize the equivalent cross-entropy loss as follows:</p><formula xml:id="formula_2">L CE (S θ ) = − 1 N N i=1p T θ e (x i ) logp S θ (x i )<label>(3)</label></formula><p>Discriminator is a binary classifier to distinguish the input features are from teacher ensemble or student network. It consists of a sigmoid function following the binary crossentropy loss. The loss can be formulated as: </p><formula xml:id="formula_3">L D = − 1 N N i=1 y i · logp D i + (1 − y i ) · log(1 −p D i )<label>(4)</label></formula><formula xml:id="formula_4">! ! ! " 1 % # $ ! Input KL Loss Student (b) Ours</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Teacher Ensembles</head><p>Teacher? Student?</p><p>! Teacher mean output <ref type="figure">Figure 5</ref>. An illustration of the comparison between MEAL <ref type="bibr" target="#b44">[42]</ref> and our method. We use an ensemble of all teacher networks instead of the teacher selection module as adopted in MEAL.  <ref type="bibr" target="#b5">[4]</ref>, etc. warmup <ref type="bibr" target="#b13">[12]</ref> where y i is the binary label for the input features x i , y ∈ {0, 1}, andp D i is the corresponding probability vector. We define a sigmoid function to model the individual teacher or student probability:</p><formula xml:id="formula_5">p D (x; θ) = σ(f θ ({x T , x S }))<label>(5)</label></formula><p>where f θ is a three-fc-layer subnetwork and θ is its parameters, σ(x) = 1/(1 + exp(−x)) is the logistic function. In our model, we use the last output layer before softmax as the representation for the discriminator input. Consider that our teacher supervision is an ensemble of multiple networks, it is not convenient to obtain the intermediate outputs. Also, to make the whole framework neater, we only adopt the similarity loss and discriminator on the final outputs of networks for distillation. We show from our experimental results that supervision from the last layer of teacher ensemble is competent to distill a strong student.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Model Capacity and Weight Decay</head><p>Weight decay is a widely used regularization technique in neural networks while it is not used in our framework. It is worthwhile to discuss the motivation behind this choice. Weight decay is basically delivering the same effects as L 2 regularization. Since L 2 will penalize the large parameters in a network (as shown in <ref type="figure">Fig. 8</ref>), in our perspective, such an operation will impair the capacity of a network. We illustrate a comparison in <ref type="figure" target="#fig_3">Fig. 6</ref> (right) of using weight decay and without it. It is curious to ask why previous models need weigh decay and it also seems helpful. We conjecture most of the previous networks are not yet saturated even those are trained with massive data augmentation and more training epochs, hence weight decay can help to prevent from overfitting and learn more information, the lost capacity is negligible and not so necessary. But for our initialized model, the performance is already high and we guess it is somewhat close to the upper bound of the network itself's capability, so the loss of capacity will be crucial for a network and weight decay may be harmful. Moreover, since our supervision from a strong teacher ensemble is fairly precise, the student should have enough capability to mimic such distribution, weight decay may not be applicable as it will reduce the complexity of a network. Also, the soft label itself in distillation has the regularization effect, so weight decay is not so necessary for distillation framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>Main Dataset. We conduct experiments on ILSVRC 2012 classification dataset <ref type="bibr" target="#b6">[5]</ref> that consists of 1,000 classes, with a number of 1.2 million training images and 50,000 validation images. We adopt the basic data augmentation scheme following <ref type="bibr" target="#b39">[37]</ref>, i.e., RandomResizedCrop and RandomHor-izontalFlip, and apply the single-crop operation at test time. Transfer Learning Datasets. We study the transferability of our learned models on two mainstream tasks: the multiple-object/fine-grained classification and object detection. We conduct experiments on the following datasets: PASCAL VOC 2007 <ref type="bibr" target="#b9">[8]</ref>, CUB200-2011 <ref type="bibr" target="#b53">[50]</ref>, Birdsnap <ref type="bibr" target="#b2">[1]</ref> and CIFAR-10 <ref type="bibr" target="#b26">[25]</ref> for classification, and COCO <ref type="bibr" target="#b32">[31]</ref> with RetinaNet <ref type="bibr" target="#b31">[30]</ref> for detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experimental Settings</head><p>We use a mini-batch size of 512 with 8 GPUs for training our models. SGD optimizer is adopted with a step learning rate decay scheduler. The initial learning rate is set to 0.01. We train with a total number of 180 epochs and the learning rate multiplied by 0.1 at 100 epoch. The weight decay is not used (set to 0) in our training. We apply this strategy to all our experiments regardless of what kind of teacher and student architectures we choose. We use the models in timm 2 . If the input size of a student network is 224×224, we choose senet154 and resnet152 v1s as teachers according to the input size of the pre-trained models. For 380 × 380, we use efficientnet b4 ns and efficientnet b4 as teachers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Main Results</head><p>On ResNet-50. Our results on ResNet-50 are shown in <ref type="table">Table 3</ref>. Under 224 × 224 input size, our method achieves 80.67% Top-1 accuracy, outperforming the previous state-of-the-art method MEAL [42] by 2.46%. Furthermore, our results are even better than ResNeSt-50 <ref type="bibr" target="#b60">[57]</ref> (fast) that requires to modify the network architecture and learned with many training tricks. After enlarging the input size to 380×380, our performance is further improved to 81.72%, outperforming FixRes (*) <ref type="bibr" target="#b52">[49]</ref> by 2.62% with slightly smaller input. On Small Networks. We choose MobileNet V3 Small-0.75/1.0/Large-1.0 and EfficientNet-B0 networks which are already compact models to verify the effectiveness of our proposed method. Our results are shown in <ref type="table">Table 4</ref>, on Mo-bileNet V3-Small 0.75 and 1.0, our method improves the original models by 2.20% and 2.25% accuracy without any architecture modification. Such huge increases are fairly surprising since the models are already compact, more im-portantly, the gains are totally free during inference stage. On MobileNet V3-Large 1.0 and EfficientNet-B0, although the improvement is not as large as Small 0.75 and 1.0, we still obtain 1.72% and 1.49% increase on ImageNet. Note that for EfficientNet-B0, 77.3/93.5 accuracy is from their paper <ref type="bibr" target="#b50">[47]</ref> and 76.8/93.2 is the actual accuracy from their pre-trained models in timm. With More Data Augmentation. We'd like to further explore whether our models have been saturated on the target data by injecting more data augmentation like CutMix in training. The results are shown in <ref type="table">Table 3</ref>, we involve CutMix and keep other settings the same as our basic experiments, we obtain Top-1/5 80.98%/95.35%, which outperform the baseline MEAL V2 by 0.31%/0.26%. While the improvement is not so large, it indicates that our model is not yet over-fitting and still has room to boost. Moreover, our results are 1.18%/0.45% better than FixRes+CutMix (*) under smaller input resolution (224 vs. 320). Intriguingly, the results on ResNet-50 are very close to the teachers we used in distillation (81.38%/95.39% and 80.86%/95.35%), since the scale of our student is much smaller than the teacher architectures, it's surprising that the student can catch up the teachers without additional training data. Criterion for Choosing Teachers. One critical factor for choosing teacher networks is the accuracy. From our experiments, it shows that stronger teachers usually distill better students. Another factor is the training settings on teachers, such as input resolution, image color space, etc. These setting should match the ones used during distillation so that the teachers can provide correct probability of the input as the supervision for students. Data augmentation can be different between the stages of training teachers and distilla- Left is the comparison of using weight decay (1e-4) and without it. We can observe that with weight decay, the training is surprisingly unstable in the first stage.</p><p>tion, like we can use CutMix to train teachers but not use it in distillation, vice versa. Comparison with Other Distillation Methods. We compare our distillation results with other state-of-the-art distillation methods in <ref type="table" target="#tab_3">Table 5</ref> on two aspects: (1) the mimicking ability through comparing the performance gap between teacher and student. Such comparison can directly reflect the true superiority and learning ability, no matter what kind of structures on teacher and student you use. It can be observed that our gaps between teachers and students are smaller than both CRD <ref type="bibr" target="#b51">[48]</ref> and AE-KD <ref type="bibr" target="#b8">[7]</ref> by a significant margin. (2) the absolute accuracy on the same structure of student. If choosing ResNet-18 as the student, our method obtains 73.19% Top-1 accuracy, outperforming CRD <ref type="bibr" target="#b51">[48]</ref> by 2.02%. Although different methods used different architectures as teachers, like AE-KD <ref type="bibr" target="#b8">[7]</ref> and our method applied multiple network ensemble as a teacher, such result still can verify the effectiveness of our method in a certain extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Ablation Study and Analysis</head><p>There are many factors in knowledge distillation that determines the performance of a student. Since we use the same teacher ensemble for all ResNet-50, MobileNet V3 and EfficientNet-B0 under 224×224 input, the results indicate that the student architecture or capacity itself is a crucial factor. If we compare MEAL V1 and V2 we can further derive the conclusion that teacher's performance, i.e. the quality of supervision, is another factor for the student, gen-erally, the stronger teachers can consistently distill stronger students. In the following, we would like to exam the impact of the initialization and discriminator. Effects of initialization. To verify whether the initialization of a student has a big impact, we conduct the ablation study through adopting (1) randomly initialized weights, (2) pytorch moderate weights and (3) timm superior weights. Results are illustrated in <ref type="figure" target="#fig_3">Fig. 6</ref>, we trained with additional 120 epochs with lr = 0.1 if the parameters are randomly initialized. Intriguingly, the convergence with randomly initialized weights is not as good as using pre-trained parameters, especially the second and third stages when lr is smaller. The final accuracy (77.07%) is only slightly better than that with hard label. Adopting the standard one-hot label pre-trained weights can dramatically improve the results to 79.48%, but is still slightly worse than the superior initialization. Models in timm are trained with massive data augmentation techniques so the accuracy is higher. It seems that our framework can inherit the knowledge in such initialization and promotes it to a better status of student. Since the good initialization essentially is learned from hard label, the knowledge learned by hard label is complementary with that learned from soft label. When training from scratch, the hard label is suggested to be involved with a pre-training stage to encode more information even it is not accurate or strong, then remove it in the latter part of distillation and inherit weights with soft-label solely for better guidance.</p><p>We also examine the sensitivity of initialization for the final performance. We chose tf efficientnet b0 (Top-1/5: 76.85%/93.25%) and efficientnet b0 (77.70%/93.53%) as the student initialization in timm, respectively. They have the same architecture but the performance is different due to the discrepant training settings. Interestingly, we got Top-1 78.29% and 78.23% respectively for the two initializations with the same teacher ensembles and training hyperparameters. It indicates that the final performance is not sensitive to the subtle difference of good initializations. With or w/o the discriminator. The discriminator is used to prevent the student from being overfitting on the training data. It can slow down the moving of a student to mimic the  teachers' output, which can be regarded as a regularization effect. In the scenario of MEAL V2, our teacher ensembles are usually powerful and strong, meanwhile, the student architectures are usually smaller and more compact than the teachers, meaning that the capability and learning ability are also much worse than the pretrained teacher networks, even we force the student to produce the same predictions as strong teachers, the outputs between student and teacher ensembles still have inevitable gaps which cannot be eradicated through the KL-divergence loss. That is to say, the discriminator is very easy to distinguish that the feature is from a student or teacher ensemble and the regularization effect will be weakened. The performance comparison of using and without the discriminator is shown in our Appendix. Even so, in MEAL V2 we still see slight improvement on performance by using the discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Visualization</head><p>We provide two visualizations schemes to understand why our trained model can achieve substantially better results. The first is the histogram of weights in particular convolutional layers as shown in <ref type="figure" target="#fig_4">Fig. 7</ref>, we choose the pytorch pre-trained model for a comparison. We can see our weights always have a wider scope of values and fewer elements are close to zero. We argue the behavior of wider scope on weights reflects the larger capacity of a network since weights can have more potential values or status to be. Our second is the evolution of weights percentile during training, as in <ref type="figure">Fig. 8</ref>. The reference is a model trained with one-hot labels and both of them use the same initialization. Consistently, we observe the values of weights tend to diffuse rather than the convergence. We conjecture this is caused by weight decay as we have discussed earlier.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Transfer Learning</head><p>Fine-tuning backbone. On the classification task, we finetune the entire network of ResNet-50 using the parameters of the pretrained model as initialization. Since PAS-CAL VOC classification is a multi-label problem, we apply sigmoid cross-entropy objective for it, and softmax crossentropy for other datasets. On COCO detection with Reti-naNet <ref type="bibr" target="#b31">[30]</ref>, we use exactly the same hyper-parameters in detectron2 <ref type="bibr" target="#b55">[52]</ref> but replacing the initialization with ours. Fixing backbone. We freeze the entire backbone and solely train the last linear layer. This is the linear evaluation to verify the quality of learned representations. For detection task, we freeze the first stage of backbone instead of the entire network. More details on finetuning and linear evaluation will be given in Appendix. Our results are shown in <ref type="table" target="#tab_4">Table 6</ref> and 7. In the most cases and datasets, a consistent improvement is achieved by using our trained parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We have presented a new paradigm of knowledge distillation based on a teacher ensemble and a discriminator. We show that such a simple framework can achieve promising results without tricks on a variety of network structures including the extremely tiny and compact models. On Im-ageNet, our method achieves 80.67% top-1 accuracy using a single crop of 224×224 on the vanilla ResNet-50. Our results show that existing networks' potential has not been fully exploited and there is still room to boost and enhance through our framework. We hope the proposed method can inspire more studies along this direction of boosting tiny and compact models through knowledge distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details of Transferring</head><p>Fine-tuning backbone. On classification task, we fine-tune the entire network of ResNet-50 using the parameters of the pre-trained model as initialization. We train for 200 epochs with a batch size of 128 and an initial learning rate of 0.01. Since PASCAL VOC classification is a multi-label problem, we apply sigmoid cross-entropy objective for it, and softmax cross-entropy for other datasets. We use SGD with a momentum parameter of 0.9 and weight decay of 0.0001. We perform standard random crops with resize and flips as data augmentation during fine-tuning. The training image size is 224×224, at test time, we resize images to 256 pixels and take a 224×224 center crop. On COCO detection with RetinaNet <ref type="bibr" target="#b31">[30]</ref>, we use exactly the same hyper-parameters in detectron2 <ref type="bibr" target="#b55">[52]</ref> but replacing the initialization with ours. Fixing backbone. We freeze the entire backbone and solely train the last linear layer. This is the linear evaluation protocol to verify the quality of learned representations. We adopted the same training setting as fine-tuning but using a larger initial learning rate of 0.1. For detection task, we freeze the first stage of backbone instead of the entire network and follow the experimental settings of detec-tron2 <ref type="bibr" target="#b55">[52]</ref>. Overview of transfer learning datasets. As shown in Table 8, we provide a brief overview of five datasets that we used in our transfer learning experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Comparison</head><p>With or w/o the discriminator. As we have introduced in the main text, the discriminator is used to prevent the student from being overfitting on the training data. It can slow down the moving of a student to mimic the teachers' output, which can be regarded as a regularization effect. In the scenario of MEAL V2, our teacher ensembles are usually large and heavy, while, the student architectures are smaller and more compact than the teachers, meaning that the capability and learning ability are also much worse than the pretrained teacher networks, even we force the student to produce the same predictions as strong teachers, the outputs between student and teacher ensembles still have inevitable gaps which cannot be eradicated through the KL-divergence loss. That is to say, the discriminator is easy to distinguish that the feature is from a student or teacher ensemble and the regularization effect from the discriminator will be weakened. <ref type="figure" target="#fig_6">Fig. 9</ref> illustrates the performance comparison with and without the discriminator. Even so, in MEAL V2 we still see slight improvement by using the discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Discussions</head><p>Why is the hard/one-hot label not necessary in knowledge distillation? The one-hot labels in ImageNet are annotated by humans, thus there are certainly some incorrect or missing annotations into them. Also, a non-negligible proportion of images in ImageNet contain more than one object within a single image, the one-hot label is determined by the annotators among multiple objects which cannot represent the complete content of this image precisely. We argue that if the teacher ensembles are strong enough, which can provide high-quality predictions for the input image, involving such inaccurate hard labels will mislead the student to a wrong optimum and incur inferior performance. Moreover, the distilled soft labels can overcome the noise and mismatching issues caused by the random crop data augmentation strategy adopted in deep model training.</p><p>Training w/ and w/o the good initialization. As we mentioned in the main paper, training without the good initialization obtained inferior performance. However, after involving the hard labels and training the initialization with the hard labels by standard settings, our distillation framework can boost such initialization model by ∼3% and the final performance is competitive. This procedure indicates that hard labels and soft labels are complementary. Also, it is equivalent to our proposed framework since our good initialization is trained with hard labels. Hence, our framework can be regarded as a new procedure: hard label pre-training + soft label finetuning. How about the generalization ability of our method on large students? We tried to use some large models like ResNeXt-101 32×48d for the students as used in teacher networks, which means that the student has similar capability with teachers. As expected, the improvement is not as considerable as those of small students, we still see some increase on performance. Generally, the soft supervision from teacher ensembles is better than the human-annotated hard labels. Especially when the scale and performance gap between teachers and students are enormous, the improvement will be more effective and notable. That is to say, in most of our experimental cases, the stronger teachers can consistently produce and distill stronger students. Is there still room to improve the performance of vanilla ResNet-50? It's definitely Yes. Replacing the teacher ensembles we used with more and stronger networks could be helpful, but the training cost will be increased accordingly. Also, some of the common tricks like cosine decay learning rate might be useful for the performance but it needs more resources to verify and the framework will become not neat. The current choices are the compromise and a trade-off under the considerations of training efficiency, computational resources, etc. Our purpose of this paper is primarily to verify the effectiveness of our proposed perspective, rather than the highest accuracy. Still, it will be very interesting to explore the upper bound performance of a fixed-structure network, such as ResNet-50.</p><p>The relationship to the lottery ticket hypothesis <ref type="bibr" target="#b10">[9]</ref>. Lottery ticket hypothesis assumes that we can find a subnetwork from a trained giant model, and retraining such initialization of subnetwork can scale its accuracy back to the original giant model. We also observe that the initialization of the student is crucial in our framework for the super teachers to play a role in knowledge distillation process. While, the difference from lottery ticket hypothesis is that our student is trained solely, rather than being selected as a sub-network from a large pre-trained network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>e n tN et -B 0 R e sN e t-5 0 M o b ile -V 3 -L 1 .0 M o b ile -V 3 -S 1 .0 M o b ile -V 3-S0 .7 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>An illustration of improvement with our MEAL V2 on ImageNet. The architectures from left to right are MobileNet V3-Small 0.75</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 . ( 1 )</head><label>21</label><figDesc>is the comparison of class-wise accuracy on four semantically similar and dissimilar classes between PyTorch official model and ours. (2) is the feature embedding visualization using t-SNE [35] on these four classes. (3) is the visualization of supervision for one-hot label and soft label in distillation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>"Figure 6 .</head><label>6</label><figDesc># = %. ' "# = %. %' "# = %. %%' "# = %. ' "# = %. %' "# = %. %%' A comparison of training from random initialized parameters, inferior parameters and superior parameters. Left are the training accuracy curves and middle are validation accuracy curves. The results indicate that a good initialization is crucial for the final performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>An illustration of weight distributions in the first, middle (last conv in block 2) and last conv layers in a ResNet-50 model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( 1 )Figure 8 .</head><label>18</label><figDesc>Finetune w/ hard label (2) Ours Evolution of weights percentile. We monitor the middle convolutional layer (last conv in block 2) in ResNet-50 with elements at 10%, 25%, 50%, 75%, 90% during whole training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Comparison of Top-1 accuracy with and without the discriminator on ImageNet validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of class-wise accuracy on four semantically similar and dissimilar classes. "Vanilla" is PyTorch official model.</figDesc><table><row><cell></cell><cell cols="2">Semantically dissimilar</cell><cell cols="2">Semantically similar</cell><cell>Accuracy</cell></row><row><cell cols="2">Model tench (%)</cell><cell>jay (%)</cell><cell cols="3">toy poodle (%) standard poodle (%) Avg (%)</cell></row><row><cell>Vanilla</cell><cell>90</cell><cell>92</cell><cell>58</cell><cell>80</cell><cell>76.15</cell></row><row><cell cols="2">Ours 94 +4%</cell><cell>92 +0%</cell><cell>66 +8%</cell><cell>92 +12%</cell><cell>80.67 +4.52%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Item-by-item comparison of techniques that we use and do not use in our distillation training.</figDesc><table><row><cell>What we do not use</cell><cell>What we use</cell></row><row><cell>architecture modification</cell><cell>an ensemble of giant pre-trained teachers</cell></row><row><cell>outside training data</cell><cell>KL divergence loss</cell></row><row><cell>hard/one-hot labels during distillation</cell><cell>a good initialization for the student</cell></row><row><cell>cosine/linear decay learning rate</cell><cell>step decay/milestone learning rate (0.01-0.001)</cell></row><row><cell>weight-decay</cell><cell></cell></row><row><cell>cutout [6]/mixup [56]/cutmix [55] training</cell><cell></cell></row><row><cell>label smoothing [46]</cell><cell></cell></row><row><cell>autoaug [3]/randaug</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Comparison with other state-of-the-art knowledge distillation methods. We show the gap between teacher and student to demonstrate the mimicking ability of each method. TE denotes the ensembled result of our teachers. † denotes training with CutMix.</figDesc><table><row><cell>Method</cell><cell>Teacher(s)</cell><cell>Student</cell><cell>Gap with Teacher</cell></row><row><cell>CRD (ICLR'20) [48]</cell><cell>73.31/91.42</cell><cell>71.17/90.13</cell><cell>∆2.14/1.39</cell></row><row><cell>CRD+KD (ICLR'20) [48]</cell><cell>73.31/91.42</cell><cell>71.38/90.49</cell><cell>∆1.93/0.93</cell></row><row><cell>AE-KD (NeurIPS'20) [7] (#1)</cell><cell>75.67/92.50</cell><cell>67.81/88.21</cell><cell>∆7.86/4.29</cell></row><row><cell>AE-KD (NeurIPS'20) [7] (#3)</cell><cell>76.85/93.60</cell><cell>68.28/88.21</cell><cell>∆8.57/5.39</cell></row><row><cell>AE-KD (NeurIPS'20) [7] (#5)</cell><cell>77.52/93.85</cell><cell>69.14/88.93</cell><cell>∆8.38/4.92</cell></row><row><cell></cell><cell>T1: 81.38/95.39</cell><cell></cell><cell>∆0.71/0.30</cell></row><row><cell>Ours (MEAL V2)</cell><cell>T2: 80.86/95.35</cell><cell>80.67/95.09</cell><cell>∆0.19/0.26</cell></row><row><cell></cell><cell>TE: 82.67/96.13</cell><cell></cell><cell>∆2.00/1.04</cell></row><row><cell></cell><cell>T1: 81.38/95.39</cell><cell></cell><cell>∆0.40/0.04</cell></row><row><cell>Ours (MEAL V2)  †</cell><cell>T2: 80.86/95.35</cell><cell>80.98/95.35</cell><cell>∆-0.13/0.00</cell></row><row><cell></cell><cell>TE: 82.67/96.13</cell><cell></cell><cell>∆1.69/0.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Transfer accuracy on classification task.</figDesc><table><row><cell cols="5">VOC2007 CUB200-2011 Birdsnap CIFAR-10</cell></row><row><cell>From Scratch</cell><cell>72.20</cell><cell>58.90</cell><cell>66.97</cell><cell>94.71</cell></row><row><cell>Fine-tune:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Base model</cell><cell>94.00</cell><cell>80.70</cell><cell>75.28</cell><cell>97.24</cell></row><row><cell>Ours</cell><cell>95.10</cell><cell>83.70</cell><cell>75.55</cell><cell>97.26</cell></row><row><cell>Freeze backbone:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Base model</cell><cell>87.50</cell><cell>66.27</cell><cell>55.01</cell><cell>81.69</cell></row><row><cell>Ours</cell><cell>90.80</cell><cell>68.29</cell><cell>53.56</cell><cell>84.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Transfer accuracy on COCO detection using RetinaNet.</figDesc><table><row><cell></cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell cols="2">AP Small AP M edium AP Large</cell></row><row><cell cols="2">Fine-tune all layers:</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Base model 37.234 56.436 39.769</cell><cell>22.899</cell><cell>41.167</cell><cell>47.576</cell></row><row><cell>Ours</cell><cell cols="3">37.501 56.829 40.148</cell><cell>21.483</cell><cell>41.264</cell><cell>48.655</cell></row><row><cell cols="3">Freeze first stage of backbone:</cell><cell></cell><cell></cell></row><row><cell cols="4">Base model 37.253 56.636 39.912</cell><cell>22.143</cell><cell>41.567</cell><cell>47.322</cell></row><row><cell>Ours</cell><cell cols="3">37.501 56.850 40.245</cell><cell>22.471</cell><cell>41.410</cell><cell>48.853</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Overview of five datasets used in our experiments for transfer learning.</figDesc><table><row><cell>Dataset</cell><cell>#class</cell><cell>Property</cell><cell cols="2">#Train(+val) set #Testing set</cell></row><row><cell>VOC 2007 [8]</cell><cell>20</cell><cell>Multi-object</cell><cell>5,011</cell><cell>4,952</cell></row><row><cell>CUB200-2011 [50]</cell><cell>200</cell><cell>Fine-grained</cell><cell>5,994</cell><cell>5,794</cell></row><row><cell>Birdsnap [1]</cell><cell>500</cell><cell>Fine-grained</cell><cell>47,386</cell><cell>2,443</cell></row><row><cell>CIFAR-10 [25]</cell><cell>10</cell><cell>Standard</cell><cell>50,000</cell><cell>10,000</cell></row><row><cell>MS COCO [31]</cell><cell>80</cell><cell>Detection</cell><cell>123,287</cell><cell>40,775</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/rwightman/pytorch-image-mod els/.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Comparison of validation accuracy on ImageNet dataset for ResNet-50 architecture under single crop evaluation. (*) indicates that they used horizontal flip, shifted center crop and color jittering for training</title>
		<imprint/>
	</monogr>
	<note>Table 3. Resolution #Params Top-1 (%) Top-5 (%)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparison of validation accuracy on ImageNet for MobileNet V3-Small 0.75/1.0/Large 1.0 and EfficientNet-B0 architectures</title>
	</analytic>
	<monogr>
		<title level="m">Network Resolution #Params Top-1 (%) Top-5 (%)</title>
		<imprint/>
	</monogr>
	<note>Table 4.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Birdsnap: Large-scale fine-grained visual categorization of birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiongxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung</forename><surname>Woo Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><forename type="middle">L</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02830</idno>
		<title level="m">Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Agree to disagree: Adaptive ensemble knowledge distillation in gradient space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03635</idno>
		<title level="m">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dropblock: A regularization method for convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10727" to="10737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1389" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Quantized neural networks: Training neural networks with low precision weights and activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6869" to="6898" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integer-arithmetic-only inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skirmantas</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2704" to="2713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanan</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08710</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Pruning filters for efficient convnets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning efficient convolutional networks through network slimming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoumeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2736" to="2744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Meal: Multi-model ensemble via adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhankui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4886" to="4893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dsod: Learning deeply supervised object detectors from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1919" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Kwang-Ting Cheng, and Marios Savvides. Is label smoothing truly incompatible with knowledge distillation: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitian</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<imprint>
			<pubPlace>Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Contrastive representation distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Online ensemble model compression using knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devesh</forename><surname>Walawalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggyu</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenzhuo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01064</idno>
		<title level="m">Trained ternary quantization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Knowledge distillation by on-the-fly native ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7517" to="7527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
