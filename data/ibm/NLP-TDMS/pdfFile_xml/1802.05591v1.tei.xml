<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards End-to-End Lane Detection: an Instance Segmentation Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Neven</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ESAT-PSI</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country>KU</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>De Brabandere</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ESAT-PSI</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country>KU</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ESAT-PSI</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country>KU</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">Proesmans</forename><surname>Luc</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ESAT-PSI</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country>KU</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Gool</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ESAT-PSI</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country>KU</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards End-to-End Lane Detection: an Instance Segmentation Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern cars are incorporating an increasing number of driver assist features, among which automatic lane keeping. The latter allows the car to properly position itself within the road lanes, which is also crucial for any subsequent lane departure or trajectory planning decision in fully autonomous cars. Traditional lane detection methods rely on a combination of highly-specialized, hand-crafted features and heuristics, usually followed by post-processing techniques, that are computationally expensive and prone to scalability due to road scene variations. More recent approaches leverage deep learning models, trained for pixel-wise lane segmentation, even when no markings are present in the image due to their big receptive field. Despite their advantages, these methods are limited to detecting a pre-defined, fixed number of lanes, e.g. ego-lanes, and can not cope with lane changes. In this paper, we go beyond the aforementioned limitations and propose to cast the lane detection problem as an instance segmentation problem -in which each lane forms its own instance -that can be trained end-to-end. To parametrize the segmented lane instances before fitting the lane, we further propose to apply a learned perspective transformation, conditioned on the image, in contrast to a fixed "bird's-eye view" transformation. By doing so, we ensure a lane fitting which is robust against road plane changes, unlike existing approaches that rely on a fixed, predefined transformation. In summary, we propose a fast lane detection algorithm, running at 50 fps, which can handle a variable number of lanes and cope with lane changes. We verify our method on the tuSimple dataset and achieve competitive results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Fully autonomous cars are the main focus of computer vision and robotics research nowadays, both at an academic and industrial level. The goal in each case is to arrive at a full understanding of the environment around the car through the use of various sensors and control modules. Camera-based lane detection is an important step towards such environmental perception as it allows the car to properly position itself within the road lanes. It is also crucial for any subsequent lane departure or trajectory planning decision. As such, performing accurate camera-based lane detection in real-time is a key enabler of fully autonomous driving.</p><p>Traditional lane detection methods (e.g. <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b34">[35]</ref>) rely on a combination of highly-specialized, handcrafted features and heuristics to identify lane segments. Popular choices of such hand-crafted cues include colorbased features <ref type="bibr" target="#b6">[7]</ref>, the structure tensor <ref type="bibr" target="#b24">[25]</ref>, the bar filter <ref type="bibr" target="#b33">[34]</ref>, ridge features <ref type="bibr" target="#b25">[26]</ref>, etc., which are possibly combined with a hough transform <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b36">[37]</ref> and particle or Kalman filters <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b33">[34]</ref>. After identifying the lane segments, post-processing techniques are employed to filter out misdetections and group segments together to form the final lanes. For a detailed overview of lane detection systems we refer the reader to <ref type="bibr" target="#b2">[3]</ref>. In general, these traditional approaches are prone to robustness issues due to road scene variations that can not be easily modeled by such model-based systems. More recent methods have replaced the hand-crafted feature detectors with deep networks to learn dense predictions, i.e. pixel-wise lane segmentations. Gopalan et al. <ref type="bibr" target="#b10">[11]</ref> use a pixel-hierarchy feature descriptor to model contextual information and a boosting algorithm to select relevant contextual features for detecting lane markings. In a similar vein, Kim and Lee <ref type="bibr" target="#b18">[19]</ref> combine a Convolutional Neural Network (CNN) with the RANSAC algorithm to detect lanes starting from edge images. Note that in their method the CNN is mainly used for image enhancement and only if the road scene is complex, e.g. it includes roadside trees, fences, or intersections. Huval et al. <ref type="bibr" target="#b15">[16]</ref> show how existing CNN  <ref type="bibr" target="#b20">[21]</ref> show how a multi-task network can jointly handle lane and road marking detection and recognition under adverse weather and low illumination conditions. Apart from the ability of the aforementioned networks to segment out lane markings better <ref type="bibr" target="#b15">[16]</ref>, their big receptive field allows them to also estimate lanes even in cases when no markings are present in the image. At a final stage, however, the generated binary lane segmentations still need to be disentangled into the different lane instances.</p><p>To tackle this problem, some approaches have applied post-processing techniques that rely again on heuristics, usually guided by geometric properties, as done in <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b11">[12]</ref> for example. As explained above, these heuristic methods are computationally expensive and prone to robustness issues due to road scene variations. Another line of work <ref type="bibr" target="#b19">[20]</ref> casts the lane detection problem as a multi-class segmentation problem, in which each lane forms its own class. By doing so, the output of the network contains disentangled binary maps for each lane and can be trained in an end-to-end manner. Despite its advantages, this method is limited to detecting only a predefined, fixed number of lanes, i.e. the ego-lanes. Moreover, since each lane has a designated class, it can not cope with lane changes.</p><p>In this paper, we go beyond the aforementioned limitations and propose to cast the lane detection problem as an instance segmentation problem, in which each lane forms its own instance within the lane class. Inspired by the success of dense prediction networks in semantic segmentation <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b5">[6]</ref> and instance segmentation tasks <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b4">[5]</ref>, we design a branched, multi-task network, like <ref type="bibr" target="#b26">[27]</ref> for lane instance segmentation, consisting of a lane segmentation branch and a lane embedding branch that can be trained end-to-end. The lane segmentation branch has two output classes, background or lane, while the lane embedding branch further disentangles the segmented lane pixels into different lane instances. By splitting the lane detection problem into the aforementioned two tasks, we can fully utilize the power of the lane segmentation branch without it having to assign different classes to different lanes. Instead, the lane embedding branch, which is trained using a clustering loss function, assigns a lane id to each pixel from the lane segmentation branch while ignoring the background pixels. By doing so, we alleviate the problem of lane changes and we can handle a variable number of lanes, unlike <ref type="bibr" target="#b19">[20]</ref>.</p><p>Having estimated the lane instances, i.e. which pixels belong to which lane, as a final step we would like to convert each one of them into a parametric description. To this end, curve fitting algorithms have been widely used in the literature. Popular models are cubic polynomials <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b24">[25]</ref>, splines <ref type="bibr" target="#b0">[1]</ref> or clothoids <ref type="bibr" target="#b9">[10]</ref>. To increase the quality of the fit while retaining computational efficiency, it is common to convert the image into a "bird's-eye view" using a perspective transformation <ref type="bibr" target="#b38">[39]</ref> and perform the curve fitting there. Note that the fitted line in the bird's-eye view can be reprojected into the original image via the inverse transformation matrix. Typically, the transformation matrix is calculated on a single image, and kept fixed. However, if the ground-plane changes form (e.g. by sloping uphill), this fixed transformation is no longer valid. As a result, lane points close to the horizon may be projected into infinity, affecting the line fitting in a negative way.</p><p>To remedy this situation we also apply a perspective transformation onto the image before fitting a curve, but in contrast to existing methods that rely on a fixed transformation matrix for doing the perspective transformation, we train a neural network to output the transformation coefficients.</p><p>In particular, the neural network takes as input the image and is optimized with a loss function that is tailored to the lane fitting problem. An inherent advantage of the proposed method is that the lane fitting is robust against road plane changes and is specifically optimized for better fitting the lanes. An overview of our full pipeline can be seen in <ref type="figure" target="#fig_0">Fig. 1</ref>. Our contributions can be summarized to the following: (1) A branched, multi-task architecture to cast the lane detection problem as an instance segmentation task, that handles lane changes and allows the inference of an arbitrary number of lanes. In particular, the lane segmentation branch outputs dense, per-pixel lane segments, while the lane embedding branch further disentangles the segmented lane pixels into different lane instances. (2) A network that given the input image estimates the parameters of a perspective transformation that allows for lane fitting robust against road plane changes, e.g. up/downhill slope.</p><p>The remainder of the paper is organized as follows. Section II describes our pipeline for semantic and instance lane segmentation, followed by our approach for converting the segmented lane instances into parametric lines. Experimental results of the proposed pipeline are presented in Section III. Finally, Section IV concludes our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHOD</head><p>We train a neural network end-to-end for lane detection, in a way that copes with the aforementioned problem of lane switching as well as the limitations on the number of lanes. This is achieved by treating lane detection as an instance segmentation problem. The network, which we will refer to as LaneNet (cf. <ref type="figure">Fig. 2</ref>), combines the benefits of binary lane segmentation with a clustering loss function designed for one-shot instance segmentation. In the output of LaneNet, each lane pixel is assigned the id of their corresponding lane. This is further explained in the Section II-A.</p><p>Since LaneNet outputs a collection of pixels per lane, we still have to fit a curve through these pixels to get the lane parametrization. Typically, the lane pixels are first projected into a "bird's-eye view" representation, using a fixed transformation matrix. However, due to the fact that the transformation parameters are fixed for all images, this raises issues when non-flat ground-planes are encountered, e.g. in slopes. To alleviate this problem, we train a network, referred to as H-Net, that estimates the parameters of an "ideal" perspective transformation, conditioned on the input image. This transformation is not necessarily the typical "bird's eye view". Instead, it is the transformation in which the lane can be optimally fitted with a low-order polynomial. Section II-B describes this procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. LANENET</head><p>LaneNet is trained end-to-end for lane detection, by treating lane detection as an instance segmentation problem. This way, the network is not constrained on the number of lanes it can detect and is able to cope with lane changes. The instance segmentation task consists of two parts, a segmentation and a clustering part, which are explained in more detail in the following sections. To increase performance, both in terms of speed and accuracy <ref type="bibr" target="#b26">[27]</ref>, these two parts are jointly trained in a multi-task network (see <ref type="figure">Fig. 2</ref>).</p><p>binary segmentation The segmentation branch of LaneNet (see <ref type="figure">Fig. 2, bottom branch)</ref> is trained to output a binary segmentation map, indicating which pixels belong to a lane and which not. To construct the ground-truth segmentation map, we connect all ground-truth lane points 1 together, forming a connected line per lane. Note that we draw these ground-truth lanes even through objects like occluding cars, or also in the absence of explicit visual lane segments, like dashed or faded lanes. This way, the network will learn to predict lane location even when they are occluded or in adverse circumstances. The segmentation network is trained with the standard cross-entropy loss function. Since the two classes (lane/background) are highly unbalanced, we apply bounded inverse class weighting, as described in <ref type="bibr" target="#b28">[29]</ref>.</p><p>instance segmentation To disentangle the lane pixels identified by the segmentation branch, we train the second branch of LaneNet for lane instance embedding (see <ref type="figure">fig. 2</ref>, top branch). Most popular detect-and-segment approaches (e.g. <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b37">[38]</ref>) are not ideal for lane instance segmentation, since bounding box detection is more suited for compact objects, which lanes are not. Therefore we use a one-shot method based on distance metric learning, proposed by De Brabandere et al. <ref type="bibr" target="#b4">[5]</ref>, which can easily be integrated with standard feed-forward networks and which is specifically designed for real-time applications.</p><p>By using their clustering loss function, the instance embedding branch is trained to output an embedding for each lane pixel so that the distance between pixel embeddings belonging to the same lane is small, whereas the distance between pixel embeddings belonging to different lanes is maximized. By doing so, the pixel embeddings of the same lane will cluster together, forming unique clusters per lane. This is achieved through the introduction of two terms, a variance term (L var ), that applies a pull force on each embedding towards the mean embedding of a lane, and a distance term (L dist ), that pushes the cluster centers away from each other. Both terms are hinged: the pull force is only active when an embedding is further than δ v from its cluster center, and the push force between centers is only active when they are closer than δ d to each-other. With C denoting the number of clusters (lanes), N c the number of elements in cluster c, x i a pixel embedding, µ c the mean embedding of cluster c, · the L2 distance, and [x] + = max(0, x) the hinge, the total loss L is equal to L var + L dist with: </p><formula xml:id="formula_0">     L var = 1 C C c=1 1 Nc Nc i=1 [ µ c − x i − δ v ] 2 + L dist = 1 C(C−1) C c A =1 C c B =1, c A =c B [δ d − µ c A − µ c B ] 2 + (1)</formula><p>Once the network has converged, the embeddings of lane pixels will be clustered together (see <ref type="figure">fig. 2</ref>), so that each cluster will lay further than δ d from each other and the radius of each cluster is smaller than δ v .</p><p>clustering The clustering is done by an iterative procedure. By setting δ d &gt; 6δ v in the above loss, one can take a random lane embedding and threshold around it with a radius of 2δ v to select all embeddings belonging to the same lane. This is repeated until all lane embeddings are assigned to a lane. To avoid selecting an outlier to threshold around, we first use mean shift to shift closer to the cluster center and then do the thresholding (see <ref type="figure">Fig. 2</ref>).</p><p>network architecture LaneNet's architecture is based on the encoder-decoder network ENet <ref type="bibr" target="#b28">[29]</ref>, which is consequently modified into a two-branched network. Since ENet's encoder contains more parameters than the decoder, completely sharing the full encoder between the two tasks would lead to unsatisfying results <ref type="bibr" target="#b26">[27]</ref>. As such, while the original ENet's encoder consists of three stages (stage 1,2,3), LaneNet only shares the first two stages (1 and 2) between the two branches, leaving stage 3 of the ENet encoder and the full ENet decoder as the backbone of each separate branch. The last layer of the segmentation branch outputs a one channel image (binary segmentation), whereas the last layer of the embedding branch outputs a N-channel image, with N the embedding dimension. This is schematically depicted in <ref type="figure">Fig. 2</ref>. Each branch's loss term is equally weighted and back-propagated through the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CURVE FITTING USING H-NET</head><p>As explained in the previous section, the output of LaneNet is a collection of pixels per lane. Fitting a polynomial through these pixels in the original image space is not ideal, as one has to resort to higher order polynomials to be able to cope with curved lanes. A frequently used solution to this problem is to project the image into a "bird's-eye view" representation, in which lanes are parallel to each other and as such, curved lanes can be fitted with a 2nd to 3rd order polynomial.</p><p>However, in these cases the transformation matrix H is calculated once, and kept fixed for all images. Typically, this leads to errors under ground-plane changes where the vanishing-point, which is projected onto infinity, shifts up or downwards (see <ref type="figure">Fig. 4, second row)</ref>.</p><p>To resolve this issue we train a neural network, H-Net, with a custom loss function: the network is optimized end-toend to predict the parameters of a perspective transformation H, in which the transformed lane points can be optimally fitted with a 2nd or 3rd order polynomial. The prediction is conditioned on the input image, allowing the network to adapt the projection parameters under ground-plane changes, so that the lane fitting will still be correct (see the last row of <ref type="figure">Fig. 4</ref>). In our case, H has 6 degrees of freedom:</p><formula xml:id="formula_1">H =   a b c 0 d e 0 f 1  </formula><p>The zeros are placed to enforce the constraint that horizontal lines remain horizontal under the transformation.</p><p>curve fitting Before fitting a curve through the lane pixels P, the latter are transformed using the transformation matrix outputted by H-Net. Given a lane pixel p i = [x i , y i , 1] T ∈ P, the transformed pixel p i = [x i , y i , 1] T ∈ P is equal to Hp i . Next, the least-squares algorithm is used to fit a n-degree polynomial, f (y ), through the transformed pixels P .</p><p>To get the x-position, x * i of the lane at a given y-position y i , the point p i = [−, y i , 1] T is transformed to p i = Hp i = [−, y i , 1] T and evaluated as: x * i = f (y i ). Note that the xvalue is of no importance and indicated with '-'. By reprojecting this point p * i = [x * i , y i , 1] T into the original image space we get: p * i = H −1 p * i with p * i = [x * i , y i , 1] T . This way, we can evaluate the lane at different y positions. This process is illustrated in <ref type="figure" target="#fig_1">fig. 3</ref>. loss function In order to train H-Net for outputting the transformation matrix that is optimal for fitting a polynomial through lane pixels, we construct the following loss function. Given N ground-truth lane points p i = [x i , y i , 1] T ∈ P, we first transform these points using the output of H-Net:</p><formula xml:id="formula_2">P = HP with p i = [x i , y i , 1] T ∈ P .</formula><p>Through these projected points, we fit a polynomial f (y ) = αy 2 + βy + γ using the least squares closed-form solution:</p><formula xml:id="formula_3">w = (Y T Y) −1 Y T x with w = [α, β, γ] T , x = [x 1 , x 2 , ..., x N ] T and Y =    y 2 1 y 1 1 . . . . . . . . . y 2 N y N 1   </formula><p>for the case of a 2nd order polynomial. The fitted polynomial is evaluated at each y i location, giving us a x * i prediction. Comparison between a fixed homography and a conditional homography (using H-Net) for lane fitting. The green dots can't be fitted correctly using a fixed homography because of groundplane changes, which can be resolved by using a conditional homography using H-Net (last row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type</head><p>Filters Size/ <ref type="table">Stride  Output  Conv+BN+ReLU  16  3x3  128x64  Conv+BN+ReLU  16  3x3  128x64  Maxpool  2x2/2  64x32  Conv+BN+ReLU  32  3x3  64x32  Conv+BN+ReLU  32  3x3  64x32  Maxpool  2x2/2  32x16  Conv+BN+ReLU  64  3x3  32x16  Conv+BN+ReLU  64  3x3  32x16  Maxpool  2x2/2  16x8  Linear+BN+ReLU  1x1  1024  Linear  1x1  6   TABLE I  H-NET NETWORK ARCHITECTURE.</ref> These predictions are projected back:</p><formula xml:id="formula_4">p * i = H −1 p * i with p * i = [x * i , y i , 1] T and p * i = [x * i , y i , 1] T .</formula><p>The loss is:</p><formula xml:id="formula_5">Loss = 1 N i=1,N (x * i − x i ) 2</formula><p>Since the lane fitting is done by using the closed-form solution of the least squares algorithm, the loss is differentiable. We use automatic differentiation to calculate the gradients.</p><p>network architecture The network architecture of H-Net is kept intentionally small and is constructed out of consecutive blocks of 3x3 convolutions, batchnorm and ReLUs. The dimension is decreased using max pooling layers, and in the end 2 fully-connected layers are added. See <ref type="table">Table I</ref>   <ref type="table">TABLE II  LANE DETECTION PERFORMANCE ON THE TUSIMPLE TEST SET.</ref> testing images, under good and medium weather conditions. They are recorded on 2-lane/3-lane/4-lane or more highway roads, at different daytimes. For each image, they also provide the 19 previous frames, which are not annotated. The annotations come in a json format, indicating the xposition of the lanes at a number of discretized y-positions. On each image, the current (ego) lanes and left/right lanes are annotated and this is also expected on the test set. When changing lanes, a 5th lane can be added to avoid confusion.</p><p>The accuracy is calculated as the average correct number of points per image:</p><formula xml:id="formula_6">acc = im C im S im</formula><p>with C im the number of correct points and S im the number of ground-truth points. A point is correct when the difference between a ground-truth and predicted point is less than a certain threshold. Together with the accuracy, they also provide the false positive and false negative scores:</p><formula xml:id="formula_7">F P = F pred N pred F N = M pred N gt</formula><p>with F pred the number of wrongly predicted lanes, N pred the number of predicted lanes, M pred the number of missed ground-truth lanes and N gt the number of all ground-truth lanes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Setup</head><p>LaneNet is trained with an embedding dimension of 4, with δ v = 0.5 and δ d = 3. The images are rescaled to 512x256 and the network is trained using Adam with a batch size of 8 and a learning rate 5e-4 until convergence.</p><p>H-Net is trained for a 3rd-order polynomial fit, with a scaled version of input image with dimension 128x64. The network is trained using Adam with a batch size of 10 and learning rate 5e-5 until convergence.</p><p>Speed Given an input resolution of 512x256, a 4dimensional embedding per pixel and using a 3rd order polynomial fit, our lane detection algorithm can run up to 50 frames per second. A full breakdown of the different components can be found in <ref type="table" target="#tab_3">Table IV</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments</head><p>Interpolation method In <ref type="table">Table III</ref> we calculate the accuracy of lane fitting using no transformation, a fixed transformation and a conditional transformation based on H-Net. We also measure the difference between a 2nd or 3rd order polynomial fit. When directly fitting the curve in the original image space without a transformation, this leads to inferior results; expectedly since curved lanes are difficult to fit using low-order polynomials.</p><p>By using a fixed transformation we already obtain better results, with a 3rd order polynomial performing better than a 2nd order one. However, as already mentioned in Section II-B, not all lane-points can be fitted under a fixed transforma-tion (see also <ref type="figure">Fig. 4</ref>). When the slope of the ground-plane changes, points close to the vanishing-point can not be fitted correctly and are therefore ignored in the MSE-measure, but still counted as missed points.</p><p>Using the transformation matrix generated by H-Net, which is optimized for lane fitting, the results outperform the lane fitting with a fixed transformation. Not only do we get a better MSE-score, but using this method allows us to fit all points, no matter if the slope of the ground-plane changes.</p><p>tuSimple results By using LaneNet combined with a 3rd order polynomial fitting and the transformation matrix from H-Net, we reach 4th place on the tuSimple challenge, with only a 0.5% difference between the first entry. The results can be seen in <ref type="table">Table II</ref>. Note that we have only trained on the training images of the tuSimple dataset, which is unclear for the other entries, as is their speed performance too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this paper we have presented a method for end-toend lane detection at 50 fps. Inspired by recent instance segmentation techniques, our method can detect a variable number of lanes and can cope with lane change maneuvers, in contrast to other related deep learning approaches.</p><p>In order to parametrize the segmented lanes using low order polynomials, we have trained a network to generate the parameters of a perspective transformation, conditioned on the image, in which lane fitting is optimal. This network is trained using a custom loss function for lane fitting. Unlike the popular "bird's-eye view" approach, our method is robust against ground-plane's slope changes, by adapting the parameters for the transformation accordingly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>System overview. Given an input image, LaneNet outputs a lane instance map, by labeling each lane pixel with a lane id. Next, the lane pixels are transformed using the transformation matrix, outputted by H-Net which learns a perspective transformation conditioned on the input image. For each lane a 3rd order polynomial is fitted and the lanes are reprojected onto the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Curve fitting. Left: The lane points are transformed using the matrix H generated by H-Net. Mid: A line is fitted through the transformed points and the curve is evaluated at different heights (red points). Right: The evaluated points are transformed back to the original image space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig. 4. Comparison between a fixed homography and a conditional homography (using H-Net) for lane fitting. The green dots can't be fitted correctly using a fixed homography because of groundplane changes, which can be resolved by using a conditional homography using H-Net (last row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Visual results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Top row: ground-truth lane points. Middle row: LaneNet output. Bottom row: final lane predicts after lane fitting. IN PIXELS) BETWEEN FITTED LANE POINTS AND GT POINTS ON VALIDATION SET USING A 2ND ORDER AND 3RD ORDER POLYNOMIAL UNDER DIFFERENT TRANSFORMATIONS. A POINT WHICH CAN NOT BE FITTED IS NOT ADDED TO THE MSE, BUT IS CONSIDERED A MISS AND CONTRIBUTES TO THE AVERAGE MISS/LANE.</figDesc><table><row><cell cols="2">2th ordr (MSE)</cell><cell cols="2">3rd ordr (MSE) Avg. miss/lane</cell></row><row><cell>no transform</cell><cell>53.91</cell><cell>17.23</cell><cell>0</cell></row><row><cell>fixed transform</cell><cell>48.09</cell><cell>9.42</cell><cell>0.105</cell></row><row><cell>cond. transform</cell><cell>33.82</cell><cell>5.99</cell><cell>0</cell></row><row><cell></cell><cell cols="2">TABLE III</cell><cell></cell></row><row><cell cols="3">MSE (time (ms)</cell><cell>fps</cell></row><row><cell>LaneNet</cell><cell>Forward pass Clustering</cell><cell>12 4.6</cell><cell>62.5</cell></row><row><cell>H-Net</cell><cell>Forward pass Lane Fitting</cell><cell>0.4 2</cell><cell>416.6</cell></row><row><cell></cell><cell>Total</cell><cell>19</cell><cell>52.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV SPEED</head><label>IV</label><figDesc>OF THE DIFFERENT COMPONENTS FOR AN IMAGE SIZE OF 512X256 MEASURED ON A NVIDIA 1080 TI. IN TOTAL, LANE DETECTION CAN RUN AT 52 FPS.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Depending on the dataset, this can be a discretized set of lane points, parts of lane markings, etc.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: The work was supported by Toyota, and was carried out at the TRACE Lab at KU Leuven (Toyota Research on Automated Cars in Europe -Leuven).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Real time Detection of Lane Markers in Urban Streets. Intelligent Vehicles Symposium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep watershed transform for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno>abs/1611.08303</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recent progress in road and lane detection: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bar-Hillel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Raz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="727" to="745" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Novel Lane Detection System With Efficient Ground Truth Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="365" to="374" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno>abs/1708.02551</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Lane detection using color-based segmentation. Intelligent Vehicles Symposium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="706" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Probabilistic Lane Tracking in Difficult Road Scenarios Using Stereovision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Danescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nedevschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="272" to="282" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A random finite set approach to multiple lane detection. ITSC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wiest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reuter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szczot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="270" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Stable road lane model based on clothoids. Advanced Microsystems for Automotive Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gackstatter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heinemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klinker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="133" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Learning Approach Towards Detection and Tracking of Lane Markings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shneier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1088" to="1098" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gurghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koduri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Bailur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Murali</surname></persName>
		</author>
		<title level="m">DeepLanes: End-To-End Lane Position Estimation Using Deep Neural Networks. CVPR Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Accurate and robust lane detection based on Dual-View Convolutional Neutral Network. Intelligent Vehicles Symposium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1041" to="1046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Cnn</forename><surname>Mask</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1703.06870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multi-lane detection in urban driving environments using conditional random fields. Intelligent Vehicles Symposium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-N</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Seo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1297" to="1302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An Empirical Evaluation of Deep Learning on Highway Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kiske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pazhayampallil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Migimatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng-Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mujica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>CoRR abs/1504.01716</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An efficient lane detection algorithm for lane departure detection. Intelligent Vehicles Symposium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="976" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust Lane Detection and Tracking in Challenging Scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="26" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Robust Lane Detection Based On Convolutional Neural Network and Random Sample Consensus. ICONIP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="454" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-To-End Ego Lane Estimation Based on Sequential Transfer Learning for Self-Driving Cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1194" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">VPGNet: Vanishing Point Guided Network for Lane and Road Marking Detection and Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bailo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno>abs/1710.06288</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep Neural Network for Structural Prediction and Lane Detection in Traffic Scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Prokhorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learning Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="690" to="703" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Combining Statistical Hough Transform and Particle Filter for robust lane detection and tracking. Intelligent Vehicles Symposium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wörgötter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Markelic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="993" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Kalman particle filter for lane recognition on rural roads. Intelligent Vehicles Symposium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Loose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust lane markings detection and road geometry computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Canero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lumbreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Automotive Technology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="395" to="407" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast Scene Understanding for Autonomous Driving. Deep Learning for Vehicle Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">workshop at the IEEE Symposium on Intelligent Vehicles</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">ENet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno>abs/1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Recurrent instance segmentation. ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">Convolutional networks for biomedical image segmentation. International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiple cue data fusion with particle filters for road course detection in vision systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schweiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="400" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<title level="m">A novel curve lane detection based on Improved River Flow and RANSA. ITSC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Real-time Lane detection by using multiple cues. Control Automation and Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-J</forename><surname>Kang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2334" to="2337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lane-mark extraction for automobiles under complex conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2756" to="2767" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<title level="m">Monocular object instance segmentation and depth ordering with CNNs. ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">A novel lane detection based on geometrical model and Gabor filter. Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="59" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Real-time lane and obstacle detection on the system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Broggi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">IV</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
