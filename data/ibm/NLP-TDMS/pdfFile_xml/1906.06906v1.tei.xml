<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Interactive Multi-Task Learning Network for End-to-End Aspect-Based Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruidan</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore ‡ SAP Innovation Center</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee</forename><forename type="middle">Sun</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore ‡ SAP Innovation Center</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore ‡ SAP Innovation Center</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore ‡ SAP Innovation Center</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Interactive Multi-Task Learning Network for End-to-End Aspect-Based Sentiment Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Aspect-based sentiment analysis produces a list of aspect terms and their corresponding sentiments for a natural language sentence. This task is usually done in a pipeline manner, with aspect term extraction performed first, followed by sentiment predictions toward the extracted aspect terms. While easier to develop, such an approach does not fully exploit joint information from the two subtasks and does not use all available sources of training information that might be helpful, such as document-level labeled sentiment corpus. In this paper, we propose an interactive multi-task learning network (IMN) which is able to jointly learn multiple related tasks simultaneously at both the token level as well as the document level. Unlike conventional multi-task learning methods that rely on learning common features for the different tasks, IMN introduces a message passing architecture where information is iteratively passed to different tasks through a shared set of latent variables. Experimental results demonstrate superior performance of the proposed method against multiple baselines on three benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Aspect-based sentiment analysis (ABSA) aims to determine people's attitude towards specific aspects in a review. This is done by extracting explicit aspect mentions, referred to as aspect term extraction (AE), and detecting the sentiment orientation towards each extracted aspect term, referred to as aspect-level sentiment classification (AS). For example, in the sentence "Great food but the service is dreadful", the aspect terms are "food" and "service", and the sentiment orientations towards them are positive and negative respectively.</p><p>In previous works, AE and AS are typically treated separately and the overall task is performed in a pipeline manner, which may not fully exploit the joint information between the two tasks.</p><p>Recently, two studies <ref type="bibr" target="#b26">(Wang et al., 2018;</ref><ref type="bibr" target="#b11">Li et al., 2019)</ref> have shown that integrated models can achieve comparable results to pipeline methods. Both works formulate the problem as a single sequence labeling task with a unified tagging scheme 1 . However, in their methods, the two tasks are only linked through unified tags, while the correlation between them is not explicitly modeled. Furthermore, the methods only learn from aspect-level instances, the size of which is usually small, and do not exploit available information from other sources such as related documentlevel labeled sentiment corpora, which contain useful sentiment-related linguistic knowledge and are much easier to obtain in practice.</p><p>In this work, we propose an interactive multitask learning network (IMN), which solves both tasks simultaneously, enabling the interactions between both tasks to be better exploited. Furthermore, IMN allows AE and AS to be trained together with related document-level tasks, exploiting the knowledge from larger document-level corpora. IMN introduces a novel message passing mechanism that allows informative interactions between tasks. Specifically, it sends useful information from different tasks back to a shared latent representation. The information is then combined with the shared latent representation and made available to all tasks for further processing. This operation is performed iteratively, allowing the information to be modified and propagated across multiple links as the number of iterations increases. In contrast to most multi-task learning schemes which share information through learning a common feature representation, IMN not only allows shared features, but also explicitly models the interactions between tasks through the message passing mechanism, allowing different tasks to better influence each other.</p><p>In addition, IMN allows fined-grained tokenlevel classification tasks to be trained together with document-level classification tasks. We incorporated two document-level classification tasks -sentiment classification (DS) and domain classification (DD) -to be jointly trained with AE and AS, allowing the aspect-level tasks to benefit from document-level information. In our experiments, we show that the proposed method is able to outperform multiple pipeline and integrated baselines on three benchmark datasets 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Aspect-Based Sentiment Analysis. Existing approaches typically decompose ABSA into two subtasks, and solve them in a pipeline setting. Both AE <ref type="bibr" target="#b22">(Qiu et al., 2011;</ref><ref type="bibr" target="#b31">Yin et al., 2016;</ref><ref type="bibr" target="#b27">Wang et al., 2016a</ref><ref type="bibr" target="#b13">Li and Lam, 2017;</ref><ref type="bibr" target="#b7">He et al., 2017;</ref><ref type="bibr" target="#b12">Li et al., 2018b;</ref><ref type="bibr" target="#b0">Angelidis and Lapata, 2018)</ref> and AS <ref type="bibr" target="#b5">(Dong et al., 2014;</ref><ref type="bibr">Nguyen and Shirai, 2015;</ref><ref type="bibr" target="#b23">Tang et al., 2016;</ref><ref type="bibr" target="#b29">Wang et al., 2016b;</ref><ref type="bibr" target="#b14">Liu and Zhang, 2017;</ref><ref type="bibr" target="#b2">Chen et al., 2017;</ref><ref type="bibr" target="#b3">Cheng et al., 2017;</ref><ref type="bibr" target="#b24">Tay et al., 2018;</ref><ref type="bibr" target="#b17">Ma et al., 2018;</ref><ref type="bibr">He et al., 2018a,b;</ref><ref type="bibr" target="#b10">Li et al., 2018a)</ref> have been extensively studied in the literature. However, treating each task independently has several disadvantages. In a pipeline setting, errors from the first step tend to be propagated to the second step, leading to a poorer overall performance. In addition, this approach is unable to exploit the commonalities and associations between tasks, which may help reduce the amount of training data required to train both tasks.</p><p>Some previous works have attempted to develop integrated solutions.  proposed to model the problem as a sequence labeling task with a unified tagging scheme. However, their results were discouraging. Recently, two works <ref type="bibr" target="#b26">(Wang et al., 2018;</ref><ref type="bibr" target="#b11">Li et al., 2019)</ref> have shown some promising results in this direction with more sophisticated network structures. However, in their models, the two subtasks are still only linked through a unified tagging scheme, while the interactions between them are not explicitly mod-2 Our source code can be obtained from https:// github.com/ruidan/IMN-E2E-ABSA eled. To address this issue, a better network structure allowing further task interactions is needed.</p><p>Multi-Task Learning. One straightforward approach to perform AE and AS simultaneously is multi-task learning, where one conventional framework is to employ a shared network and two task-specific network to derive a shared feature space and two task-specific feature spaces. Multitask learning frameworks have been employed successfully in various natural language processing (NLP) tasks <ref type="bibr" target="#b4">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b16">Luong et al., 2015;</ref>. By learning semantically related tasks in parallel using a shared representation, multi-task learning could capture the correlations between tasks and improve the model generalization ability in certain cases. For ABSA, <ref type="bibr" target="#b9">He et al. (2018b)</ref> have shown that aspectlevel sentiment classification can be significantly improved through joint training with documentlevel sentiment classification. However, conventional multi-task learning still does not explicitly model the interactions between tasks -the two tasks only interact with each other through error back-propoagation to contribute to the learned features and such implicit interactions are not controllable. Unlike existing methods, our proposed IMN not only allows the representations to be shared, but also explicitly models the interactions between tasks, by using an iterative message passing scheme. The propagated information contributes to both learning and inference to boost the overall performance of ABSA.</p><p>Message Passing Architectures. Networked representations for message passing graphical model inference algorithms have been studied in computer vision <ref type="bibr" target="#b1">(Arnab et al., 2018)</ref> and NLP <ref type="bibr" target="#b6">(Gormley et al., 2015)</ref>. Modeling the execution of these message passing algorithms as a network results in recurrent neural network architectures. We similarly propagate information in a network and learn the update operators, but the architecture is designed for solving multi-task learning problems. Our algorithm can similarly be viewed as a recurrent neural network since each iteration uses the same network to update the shared latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>The IMN architecture is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. It accepts a sequence of tokens {x 1 , . . . , x n } as input into a feature extraction component f θs that is shared among all tasks. This component consists of a word embedding layer followed by a few feature extraction layers. Specifically, we employ m s layers of CNNs after the word embedding layer in f θs .</p><p>The output of f θs is a sequence of latent vectors {h s 1 , h s 2 , ..., h s n } shared among all the tasks. After initialization by f θs , this sequence of latent vectors is later updated by combining information propagated from different task components through message passing. We denote h s(t) i as the value of the shared latent vector corresponding to x i after t rounds of message passing, with h s(0) i denoting the value after initialization.</p><p>The sequence of shared latent vectors 3 {h s 1 , h s 2 , ..., h s n } is used as input to the different task-specific components.</p><p>Each task-specific component has its own sets of latent and output variables. The output variables correspond to a label sequence in a sequence tagging task; in AE, we assign to each token a label indicating whether it belongs to any aspect or opinion 4 term, while in AS, we label each word with its sentiment. In a classification task, the output corresponds to the label of the input instance: the sentiment of the document for the sentiment classification task (DS), and the domain of the document for the domain classification task (DD). At each iteration, appropriate information is passed back to the shared latent vectors to be combined; this could be the values of the output variables or the latent variables, depending on the task. In addition, we also allow messages to be passed between the components in each iteration. Specifically for this problem, we send information from the AE task to the AS task as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. After T iterations of message passing, which allows information to be propagated through multiple hops, we use the values of the output variables as predictions. For this problem, we only use the outputs for AE and AS during inference as these are the end-tasks, while the other tasks are only used for training. We now describe each component and how it is used in learning and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Aspect-Level Tasks</head><p>AE aims to extract all the aspect and opinion terms 5 appearing in a sentence, which is formulated as a sequence tagging problem with the BIO tagging scheme. Specifically, we use five class labels: Y ae = {BA, IA, BP, IP, O}, indicating the beginning of and inside of an aspect term, the beginning of and inside of an opinion term, and other words, respectively. We also formulate AS as a sequence tagging problem with labels Y as = {pos, neg, neu}, indicating the tokenlevel positive, negative, and neutral sentiment orientations. <ref type="table" target="#tab_0">Table 1</ref> shows an example of aspectlevel training instance with gold AE and AS labels. In aspect-level datasets, only aspect terms get sentiment annotated. Thus, when modeling AS as a sequence tagging problem, we label each token that is part of an aspect term with the sentiment label of the corresponding aspect term. For exam-Input The fish is fresh but the variety of fish is nothing out of ordinary . ple, as shown in <ref type="table" target="#tab_0">Table 1</ref>, we label "fish" as pos, and label "variety", "of ", "fish" as neg, based on the gold sentiment labels of the two aspect terms "fish" and "varity of fish" respectively. Since other tokens do not have AS gold labels, we ignore the predictions on them when computing the training loss for AS.</p><formula xml:id="formula_0">AE O BA O BP O O BA IA IA O O O O BP O AS - pos - - - - neg neg neg - - - - - -</formula><p>The AE component f θae is parameterized by θ ae and outputs {ŷ ae 1 , ...,ŷ ae n }. The AS component f θas is parameterized by θ as and outputs {ŷ as 1 , ...,ŷ as n }. The AE and AS encoders consist of m ae and m as layers of CNNs respectively, and they map the shared representations to {h ae 1 , h ae 2 , ..., h ae n } and {h as 1 , h as 2 , ..., h as n } respectively. For the AS encoder, we employ an additional self-attention layer on top of the stacked CNNs. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we makeŷ ae i , the outputs from AE available to AS in the selfattention layer, as the sentiment task could benefit from knowing the predictions of opinion terms. Specifically, the self-attention matrix A ∈ R n×n is computed as follows:</p><formula xml:id="formula_1">score (i =j) ij = (h as i W as (h as j ) T ) · 1 |i − j| · P op j (1) A (i =j) ij = exp(scoreij) n k=1 exp(score ik )<label>(2)</label></formula><p>where the first term in Eq.(1) indicates the semantic relevance between h as i and h as j with parameter matrix W as , the second term is a distancerelevant factor, which decreases with increasing distance between the ith token and the jth token, and the third term P op j denotes the predicted probability that the jth token is part of any opinion term. The probability P op j can be computed by summing the predicted probabilities on opinionrelated labels BP and IP inŷ ae j . In this way, AS is directly influenced by the predictions of AE. We set the diagonal elements in A to zeros, as we only consider context words for inferring the sentiment of the target token. The self-attention layer outputs h as i = n j=1 A ij h as j . In AE, we concatenate the word embedding, the initial shared representation h s(0) i , and the task-specific representation h ae i as the final representation of the ith token. In AS, we concatenate h s(0) i and h as i as the final representation. For each task, we employ a fullyconnected layer with softmax activation as the decoder, which maps the final token representation to probability distributionŷ ae i (ŷ as i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Document-Level Tasks</head><p>To address the issue of insufficient aspect-level training data, IMN is able to exploit knowledge from document-level labeled sentiment corpora, which are more readily available. We introduce two document-level classification tasks to be jointly trained with AE and AS. One is documentlevel sentiment classification (DS), which predicts the sentiment towards an input document. The other is document-level domain classification (DD), which predicts the domain label of an input document. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the task-specific operation f θo consists of m o layers of CNNs that map the shared representations {h s 1 , ..., h s n } to {h o 1 , ..., h o n }, an attention layer att o , and a decoding layer dec o , where o ∈ {ds, dd} is the task symbol. The attention weight is computed as:</p><formula xml:id="formula_2">a o i = exp(h o i W o ) n k=1 exp(h o k W o )<label>(3)</label></formula><p>where W o is a parameter vector. The final document representation is computed as</p><formula xml:id="formula_3">h o = n i=1 a o i h o i .</formula><p>We employ a fully-connected layer with softmax activation as the decoding layer, which maps h o toŷ o .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Message Passing Mechanism</head><p>To exploit interactions between different tasks, the message passing mechanism aggregates predictions of different tasks from the previous iteration, and uses this knowledge to update the shared latent vectors {h s 1 , ..., h s n } at the current iteration. Specifically, the message passing mechanism integrates knowledge fromŷ ae i ,ŷ as i ,ŷ ds , a ds i , and a dd i computed on an input {x 1 , ..., x n }, and the shared hidden vector h s i is updated as follows:</p><formula xml:id="formula_4">h s(t) i =f θ re (h s(t−1) i :ŷ ae(t−1) i :ŷ as(t−1) i : y ds(t−1) : a ds(t−1) i : a dd(t−1) i )<label>(4)</label></formula><p>where t &gt; 0 and [:] denotes the concatenation operation. We employ a fully-connected layer with ReLu activation as the re-encoding function f θre .</p><p>To update the shared representations, we incorporateŷ</p><formula xml:id="formula_5">ae(t−1) i andŷ as(t−1) i</formula><p>, the outputs of AE and AS from the previous iteration, such that these information are available for both tasks in current round of computation. We also incorporate information from DS and DD.ŷ ds indicates the overall sentiment of the input sequence, which could be helpful for AS. The attention weights a ds i and a dd i generated by DS and DD respectively reflect how sentiment-relevant and domain-relevant the ith token is. A token that is more sentiment-relevant or domain-relevant is more likely to be an opinion word or aspect word. This information is useful for the aspect-level tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learning</head><p>Instances for aspect-level problems only have aspect-level labels while instances for documentlevel problems only have document labels. IMN is trained on aspect-level and document-level instances alternately. When trained on aspect-level instances, the loss function is as follows:</p><formula xml:id="formula_6">L a (θ s ,θ ae , θ as , θ ds , θ dd , θ re ) = 1 N a Na i=1 1 n i n i j=1 ( l(y ae i,j ,ŷ ae(T ) i,j ) + l(y as i,j ,ŷ as(T ) i,j ))<label>(5)</label></formula><p>where T denotes the maximum number of iterations in the message passing mechanism, N a denotes the total number of aspect-level training instances, n i denotes the number of tokens contained in the ith training instance, and y ae i,j (y as i,j ) denotes the one-hot encoding of the gold label for AE (AS). l is the cross-entropy loss applied to each token. In aspect-level datasets, only aspect terms have sentiment annotations. We label each token that is part of any aspect term with the sentiment of the corresponding aspect term. During model training, we only consider AS predictions on these aspect term-related tokens for computing the AS loss and ignore the sentiments predicted on other tokens 6 .</p><p>When trained on document-level instances, we 6 Let l(y as i,j ,ŷ </p><formula xml:id="formula_7">as(T ) i,j ) = 0 in Eq.(5) if y ae i,j is not BA or IA Algorithm 1 Pseudocode for training IMN Require: D a = {(x a i , y ae i , y as i ) Na i=1 }, D ds = {(x ds i , y ds i ) N ds i=1 } and D dd = {(x dd i , y dd i ) N dd i=1<label>}</label></formula><formula xml:id="formula_8">L d (θ s , θ ds , θ dd ) = 1 N ds N ds i=1 l(y ds i ,ŷ ds i ) + 1 N dd N dd i=1 l(y dd i ,ŷ dd i )<label>(6)</label></formula><p>where N ds and N dd denote the number of training instances for DS and DD respectively, and y ds i and y dd i denote the one-hot encoding of the gold label. Message passing iterations are not used when training document-level instances.</p><p>For learning, we first pretrain the network on the document-level instances (minimize L d ) for a few epochs, such that DS and DD can make reasonable predictions. Then the network is trained on aspectlevel instances and document-level instances alternately with ratio r, to minimize L a and L d . The overall training process is given in Algorithm 1. D a , D ds , and D dd denote the aspect-level training set and the training sets for DS, DD respectively. D ds and D a are from similar domains. D dd contains review documents from at least two domains with y ds i denoting the domain label, where one of the domains is similar to the domains of D a and D ds . In this way, linguistic knowledge can be transferred from DS and DD to AE and AS, as  Datasets. <ref type="table" target="#tab_3">Table 2</ref> shows the statistics of the aspect-level datasets. We run experiments on three benchmark datasets, taken from Se-mEval2014 <ref type="bibr" target="#b21">(Pontiki et al., 2014)</ref> and SemEval 2015 <ref type="bibr" target="#b20">(Pontiki et al., 2015)</ref>. The opinion terms are annotated by <ref type="bibr" target="#b27">Wang et al. (2016a)</ref>. We use two document-level datasets from <ref type="bibr" target="#b9">(He et al., 2018b)</ref>. One is from the Yelp restaurant domain, and the other is from the Amazon electronics domain. Each contains 30k instances with exactly balanced class labels of pos, neg, and neu. We use the concatenation of the two datasets with domain labels as D dd . We use the Yelp dataset as D ds when D a is either D1 or D3, and use the electronics dataset as D ds when D a is D2.</p><p>Network details. We adopt the multi-layer-CNN structure from <ref type="bibr" target="#b30">(Xu et al., 2018)</ref> as the CNN-based encoders in our proposed network. See Appendix A for implementation details. For word embedding initialization, we concatenate a general-purpose embedding matrix and a domain-specific embedding matrix 7 following <ref type="bibr" target="#b30">(Xu et al., 2018)</ref>. We adopt their released domainspecific embeddings for restaurant and laptop domains with 100 dimensions, which are trained on a large domain-specific corpus using fastText. The general-purpose embeddings are pre-trained Glove vectors <ref type="bibr" target="#b19">(Pennington et al., 2014)</ref> with 300 dimensions.</p><p>One set of important hyper-parameters are the number of CNN layers in the shared encoder and the task-specific encoders. To decide the values of m s , m ae , m as , m ds , m dd , we first investigate 7 For DD, we only look at the general-purpose embeddings by masking out the domain-specific embeddings. how many layers of CNNs would work well for each of the task when training it alone. We denote c o as the optimal number of CNN layers in this case, where o ∈ {ae, as, ds, dd} is the task indicator. We perform AE, AS separately on the training set of D1, and perform DS, DD separately on the document-level restaurant corpus. Crossvalidation is used for selecting c o , which yields 4, 2, 2, 2 for c ae , c as , c ds , c dd . Based on this observation, we made m s , m ae , m as , m ds , m dd equals to 2, 2, 0, 0, 0 respectively, such that m s + m o = c o . Note that there are other configurations satisfying the requirement, for example, m s , m ae , m as , m ds , m dd equals to 1, 3, 1, 1, 1. we select our setting as it involves the smallest set of parameters.</p><p>We tune the maximum number of iterations T in the message passing mechanism by training IMN −d via cross validation on D1. It is set to 2. With T fixed as 2, we then tune r by training IMN via cross validation on D1 and the relevant document-level datasets. It is set to 2 as well.</p><p>We use Adam optimizer with learning rate set to 10 −4 , and we set batch size to 32. Learning rate and batch size are set to conventional values without specific tuning for our task.</p><p>At training phase, we randomly sample 20% of the training data from the aspect-level dataset as the development set and only use the remaining 80% for training. We train the model for a fix number of epoches, and save the model at the epoch with the best F1-I score on the development set for evaluation.</p><p>Evaluation metrics. During testing, we extract aspect (opinion) terms, and predict the sentiment for each extracted aspect term based onŷ ae(T ) and y as(T ) . Since the extracted aspect term may consist of multiple tokens and the sentiment predictions on them could be inconsistent in AS, we only output the sentiment label of the first token as the predicted sentiment for any extracted aspect term.</p><p>We employ five metrics for evaluation, where two measure the AE performance, two measure the AS performance, and one measures the overall performance. Following existing works for AE <ref type="bibr" target="#b30">Xu et al., 2018)</ref>, we use F1 to measure the performance of aspect term extraction and opinion term extraction, which are denoted as F1-a and F1-o respectively. Following existing works for AS <ref type="bibr" target="#b2">(Chen et al., 2017;</ref><ref type="bibr" target="#b9">He et al., 2018b)</ref>, we adopt accuracy and macro-F1 to measure the performance of AS. We denote them as acc-s and F1-s. Since we are solving the integrated task without assuming that gold aspect terms are given, the two metrics are computed based on the correctly extracted aspect terms from AE. We compute the F1 score of the integrated task denoted as F1-I for measuring the overall performance. To compute F1-I, an extracted aspect term is taken as correct only when both the span and the sentiment are correctly identified. When computing F1-a, we consider all aspect terms, while when computing acc-s, F1-s, and F1-I, we ignore aspect terms with conflict sentiment labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Models under Comparison</head><p>Pipeline approach.</p><p>We select two topperforming models from prior works for each of AE and AS, to construct 2 × 2 pipeline baselines. For AE, we use CMLA  and DECNN <ref type="bibr" target="#b30">(Xu et al., 2018)</ref>. CMLA was proposed to perform co-extraction of aspect and opinion terms by modeling their interdependencies. DECNN is the state-of-the-art model for AE. It utilizes a multi-layer CNN structure with both general-purpose and domainspecific embeddings. We use the same structure as encoders in IMN. For AS, we use ATAE-LSTM (denoted as ALSTM for short) <ref type="bibr" target="#b29">(Wang et al., 2016b)</ref> and the model from <ref type="bibr" target="#b9">(He et al., 2018b)</ref> which we denote as dTrans. ALSTM is a representative work with an attention-based LSTM structure. We compare with dTrans as it also utilizes knowledge from document corpora for improving AS performance, which achieves state-of-the-art results.</p><p>Thus, we compare with the following pipeline methods: CMLA-ALSTM, CMLA-dTrans, DECNN-ALSTM, and DECNN-dTrans. We also compare with the pipeline setting of IMN, which trains AE and AS independently (i.e., without parameter sharing, information passing, and document-level corpora). We denote it as PIPELINE. The network structure for AE in PIPELINE is the same as DECNN. During testing of all methods, we perform AE in the first step, and then generate AS predictions on the correctly extracted aspect terms.</p><p>Integrated Approach. We compare with two recently proposed methods that have achieved stateof-the-art results among integrated approaches: MNN <ref type="bibr" target="#b26">(Wang et al., 2018)</ref> and the model from <ref type="bibr" target="#b11">(Li et al., 2019)</ref> which we denote as INABSA (integrated network for ABSA). Both methods model the overall task as a sequence tagging problem with a unified tagging scheme. Since during testing, IMN only outputs the sentiment on the first token of an extracted aspect term to avoid sentiment inconsistency, to enable fair comparison, we also perform this operation on MNN and IN-ABSA. We also show results for a version of IMN that does not use document-level corpora, denoted as IMN −d . The structure of IMN −d is shown as the solid lines in <ref type="figure" target="#fig_0">Figure 1</ref>. It omits the information y ds , a ds i , and a dd i propagated from the documentlevel tasks in Eq.(4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and Analysis</head><p>Main results. <ref type="table" target="#tab_5">Table 3</ref> shows the comparison results. Note that IMN performs co-extraction of aspect and opinion terms in AE, which utilizes additional opinion term labels during training, while the baseline methods except CMLA do not consider this information in their original models. To enable fair comparison, we slightly modify those baselines to perform co-extraction as well, with opinion term labels provided. Further details on model comparison are provided in Appendix B.</p><p>From <ref type="table" target="#tab_5">Table 3</ref>, we observe that IMN −d is able to significantly outperform other baselines on F1-I. IMN further boosts the performance and outperforms the best F1-I results from the baselines by 2.29%, 1.77%, and 2.61% on D1, D2, and D3. Specifically, for AE (F1-a and F1-o), IMN −d performs the best in most cases. For AS (acc-s and F1-s), IMN outperforms other methods by large margins. PIPELINE, IMN −d , and the pipeline methods with dTrans also perform reasonably well on this task, outperforming other baselines by moderate margins. All these models utilize knowledge from larger corpora by either joint training of document-level tasks or using domain-specific embeddings. This suggests that domain-specific knowledge is very helpful, and both joint training and domain-specific embeddings are effective ways to transfer such knowledge.</p><p>We also show the results of IMN −d and IMN when only the general-purpose embeddings (without domain-specific embeddings) are used for initialization. They are denoted as IMN −d /IMN wo DE. IMN wo DE performs only marginally below IMN. This indicates that the knowledge captured by domain-specific embeddings could be similar to that captured by joint training of the document-level tasks. IMN −d is more affected   without domain-specific embeddings, while it still outperforms all other baselines except DECNN-dTrans. DECNN-dTrans is a very strong baseline as it exploits additional knowledge from larger corpora for both tasks. IMN −d wo DE is competitive with DECNN-dTrans even without utilizing additional knowledge, which suggests the effectiveness of the proposed network structure.</p><p>Ablation study. To investigate the impact of different components, we start with a vanilla model which consists of f θs , f θae , and f θas only without any informative message passing, and add other components one at a time. <ref type="table" target="#tab_6">Table 4</ref> shows the results of different model variants. +Opinion transmission denotes the operation of providing additional information P op j to the self-attention layer as shown in Eq.(1). +Message passing-a denotes propagating the outputs from aspect-level tasks only at each message passing iteration. +DS and +DD denote adding DS and DD with parameter sharing only. +Message passing-d denotes involving the document-level information for message passing. We observe that +Message passing-a and +Message passing-d contribute to the performance gains the most, which demonstrates the effectiveness of the proposed message passing mechanism. We also observe that simply adding documentlevel tasks (+DS/DD) with parameter sharing only marginally improves the performance of IMN −d . This again indicates that domain-specific knowledge has already been captured by domain embeddings, while knowledge obtained from DD and DS via parameter sharing could be redundant in this case. However, +Message passing-d is still helpful with considerable performance gains, showing that aspect-level tasks can benefit from knowing predictions of the relevant document-level tasks.</p><p>Impact of T . We have demonstrated the effectiveness of the message passing mechanism. Here, we investigate the impact of the maximum number of iterations T . <ref type="table" target="#tab_9">Table 6</ref> shows the change of F1-I on the test sets as T increases. We find that convergence is quickly achieved within two or three iterations, and further iterations do not provide considerable performance improvement.</p><p>Case study. To better understand in which conditions the proposed method helps, we examine the instances that are misclassified by PIPELINE and INABSA, but correctly classified by IMN.</p><p>For aspect extraction, we find the message passing mechanism is particularly helpful in two scenarios. First, it helps to better recognize uncommon aspect terms by utilizing information from the opinion contexts. As shown in example 1 in  <ref type="table">Table 5</ref>: Case analysis. The "Examples" column contains instances with gold labels. 'The "opinion" and "aspect" columns present the opinion terms and aspect terms with sentiments, generated by the corresponding model.   <ref type="table">Table 5</ref>, PIPELINE and INABSA fail to recognize "build" as it is an uncommon aspect term in the training set while IMN is able to correctly recognize it. We find that when no message passing iteration is performed, IMN also fails to recognize "build". However, when we analyze the predicted sentiment distribution on each token in the sentence, we find that except "durability", only "build" has a strong positive sentiment, while the sentiment distributions on the other tokens are more uniform. This is an indicator that "build" is also an aspect term. IMN is able to aggregate such knowledge with the message passing mechanism, such that it is able to correctly recognize "build" in later iterations. Due to the same reason, the message passing mechanism also helps to avoid extracting terms on which no opinion is expressed. As observed in example 2, both PIPELINE and INABSA extract "Pizza". However, since no opinion is expressed in the given sentence, "Pizza" should not be considered as an aspect term. IMN avoids extracting this kind of terms by aggregating knowledge from opinion prediction and sentiment prediction.</p><p>For aspect-level sentiment, since IMN is trained on larger document-level labeled corpora with balanced sentiment classes, in general it better captures the meaning of domain-specific opinion words (example 3), better captures sentiments of complex expressions such as negation (example 4), and better recognizes minor sentiment classes in the aspect-level datasets (negative and neutral in our cases). In addition, we find that knowledge propagated by the document-level tasks through message passing is helpful. For example, the sentiment-relevant attention weights are helpful for recognizing uncommon opinion words, and which further help on correctly predicting the sentiments of the aspect terms. As observed in example 5, PIPELINE and INABSA are unable to recognize "scratches easily" as the opinion term, and they also make wrong sentiment prediction on the aspect term "aluminum". IMN learns that "scratches" is sentiment-relevant through knowledge from the sentiment-relevant attention weights aggregated via previous iterations of message passing, and is thus able to extract "scratches easily". Since the opinion predictions from AE are sent to the self-attention layer in the AS component, correct opinion predictions further help to infer the correct sentiment towards "aluminum".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose an interactive multi-task learning network IMN for jointly learning aspect and opinion term co-extraction, and aspect-level sentiment classification. The proposed IMN introduces a novel message passing mechanism that allows informative interactions between tasks, enabling the correlation to be better exploited. In addition, IMN is able to learn from multiple training data sources, allowing fine-grained token-level tasks to benefit from document-level labeled corpora. The proposed architecture can potentially be applied to similar tasks such as relation extraction, semantic role labeling, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Opinion Transmission</head><p>To alleviate the problem of unreliable predictions of opinion labels in the early stage of training, we adopt scheduled sampling for opinion transmission at training phase. We send gold opinion labels rather than the predicted ones generated by AE to AS in the probability of i . The probability i depends on the number of epochs i during training, for which we employ an inverse sigmoid decay i = 5/(5 + exp(i/5)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Model Comparison Details</head><p>For CMLA 8 , ALSTM 9 , dTrans 10 , and INABSA 11 , we use the officially released source codes for experiments. For MNN, we re-implement the model following the descriptions in the paper as the source code is not available. We run each baseline multiple times with random initializations and save their predicted results. We use an unified evaluation script for measuring the outputs from different baselines as well as the proposed method.</p><p>The proposed IMN performs co-extraction of aspect terms and opinion terms in AE, which utilizes additional opinion term labels during model training. In the baselines, the two integrated methods MNN and INABSA, and the pipeline methods with DECNN as the AE component do not 8 https://github.com/happywwy/ Coupled-Multi-layer-Attentions 9 https://www.wangyequan.com/ publications/ 10 https://github.com/ruidan/ Aspect-level-sentiment 11 https://github.com/lixin4ever/ E2E-TBSA <ref type="table">Table 7</ref>: Model comparison in a setting without opinion term labels. Average results over 5 runs with random initialization are reported. * indicates the proposed method is significantly better than the other baselines (p &lt; 0.05) based on one-tailed unpaired t-test. take take opinion information during training. To make fair comparison, we add labels {BP, IP} to the original label sets of MNN, INABSA, and DECNN, indicating the beginning of and inside of an opinion term. We train those models on training sets with both aspect and opinion term labels to perform co-extraction as well. In addition, for pipeline methods, we also make the gold opinion terms available to the AS models (ALSTM and dTrans) during training. To make ALSTM and dTrans utilize the opinion label information, we modify their attention layer to assign higher weights to tokens that are more likely to be part of an opinion term. This is reasonable since the objective of the attention mechanism in an AS model is to find the relevant opinion context. The attention weight of the ith token before applying softmax normalization in an input sentence is modified as: a i = a i * P op i</p><p>where a i denotes the attention weight computed by the original attention layer, p op i denotes the probability that the ith token belongs to any opinion term. a i denotes the modified attention weights. At the training phase, since the gold opinion terms are provided, p op i = 1 for the tokens that are part of the gold opinion terms, while p op i = 0 for the other tokens. At the testing phase, p op i is computed based on the predictions from the AE model in the pipeline method. It is computed by summing up the predicted probabilities on the opinion-related labels BP and IP for the ith token. We also present the comparison results in a setting without using opinion term labels in Table 7 12 . In this setting, we modify the proposed IMN and IMN −d to recognize aspect terms only <ref type="bibr">12</ref> We exclude the results of the pipeline methods with CMLA, as CMLA relies on opinion term labels during training. It is difficult to modify it. in AE. The opinion transmission operation, which sends the opinion term predictions from AE to AS, is omitted as well.</p><p>Both IMN −d and IMN still significantly outperform other baselines in most cases under this setting. In addition, when compare the results in <ref type="table">Table 7</ref> and <ref type="table" target="#tab_5">Table 3</ref>, we observe that IMN −d and IMN consistently yield better F1-I scores on all datasets in <ref type="table" target="#tab_5">Table 3</ref>, when opinion term extraction is also considered. Consistent improvements are not observed in other baseline methods when trained with opinion term labels. These findings suggest that knowledge obtained from learning opinion term extraction is indeed beneficial, however, a carefully-designed network structure is needed to utilize such information. IMN is designed to exploit task correlations by explicitly modeling interactions between tasks, and thus it better integrates knowledge obtained from training different tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The overall architecture of IMN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>An aspect-level training instance with gold AE and AS labels.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Require: Integer r &gt; 0 for e ∈ [1, max-pretrain-epochs] do for minibatch B ds , B dd in D ds , D dd do compute L d based on B ds and B dd update θ s , θ ds , θ dd end for end for for e ∈ [1, max-epochs] do for b ∈ [1, batches-per-epoch] do sample B a from D a compute L a based on B a update θ s , θ ae , θ as , θ</figDesc><table><row><cell>end if</cell></row><row><cell>end for</cell></row><row><cell>end for</cell></row><row><cell>minimize the following loss:</cell></row></table><note>re if b is divisible by r then sample B ds , B dd from D ds , D dd compute L d based on B ds and B dd update θ s , θ ds , θ dd</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Dataset statistics with numbers of aspect terms and opinion terms they are semantically relevant. We fix θ ds and θ dd when updating parameters for L a , since we do not want them to be affected by the small number of aspect-level training instances.</figDesc><table><row><cell>4 Experiments</cell></row><row><cell>4.1 Experimental Settings</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>82.45 82.45 83.94 83.94 83.94 83.05 83.92 83.95 84.01 83.50 83.33 F1-o 82.67 82.67 85.60 85.60 85.60 84.55 84.97 85.21 85.64 84.62 85.61 acc-s 77.46 79.58 77.79 80.04 79.56 77.17 79.68 79.65 81.56 * 83.17 * 83.89</figDesc><table><row><cell></cell><cell>Methods</cell><cell>CMLA-ALSTM</cell><cell>CMLA-dTrans</cell><cell>DECNN-ALSTM</cell><cell>DECNN-dTrans</cell><cell>PIPELINE</cell><cell>MNN</cell><cell>INABSA</cell><cell>IMN −d wo DE</cell><cell>IMN −d</cell><cell>IMN wo DE</cell><cell>IMN</cell></row><row><cell>D1</cell><cell>F1-a F1-s</cell><cell cols="8">68.70 72.23 68.50 73.31 69.59 68.45 68.38 69.32</cell><cell>71.90</cell><cell>73.44</cell><cell>75.66</cell></row><row><cell></cell><cell>F1-I</cell><cell cols="11">63.87 65.34 65.26 67.25 66.53 63.87 66.60 66.96 68.32  *  69.11  *  69.54  *</cell></row><row><cell></cell><cell cols="9">F1-a 76.80 76.80 78.38 78.38 78.38 76.94 77.34 76.96</cell><cell>78.46</cell><cell>76.87</cell><cell>77.96</cell></row><row><cell>D2</cell><cell cols="9">F1-o 77.33 77.33 78.81 78.81 78.81 77.77 76.62 76.85 acc-s 70.25 72.38 70.46 73.10 72.29 70.40 72.30 72.89</cell><cell>78.14 73.21</cell><cell cols="2">77.04 74.31  *  75.36  *  77.51</cell></row><row><cell></cell><cell>F1-s</cell><cell cols="8">66.67 69.52 66.78 70.63 68.12 65.98 68.24 67.26</cell><cell>69.92</cell><cell>70.76</cell><cell>72.02  *</cell></row><row><cell></cell><cell>F1-I</cell><cell cols="11">53.68 55.56 55.05 56.60 56.02 53.80 55.88 56.25 57.66  *  57.04  *  58.37  *</cell></row><row><cell></cell><cell cols="9">F1-a 68.55 68.55 68.32 68.32 68.32 70.24 69.40 69.23</cell><cell>69.80</cell><cell>68.23</cell><cell>70.04</cell></row><row><cell>D3</cell><cell cols="10">F1-o 71.07 71.07 71.22 71.22 71.22 69.38 71.43 68.39 72.11  *  acc-s 81.03 82.27 80.32 82.65 82.27 80.79 82.56 81.64 83.38</cell><cell cols="2">70.09 85.90  *  85.64  *  71.94</cell></row><row><cell></cell><cell>F1-s</cell><cell cols="8">58.91 66.45 57.25 69.58 59.53 57.90 58.81 57.51</cell><cell>60.65</cell><cell cols="2">71.67  *  71.76  *</cell></row><row><cell></cell><cell>F1-I</cell><cell cols="9">54.79 56.09 55.10 56.28 55.96 56.57 57.38 56.80 57.91</cell><cell></cell></row></table><note>** 58.82* 59.18 *</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Model comparison. Average results over 5 runs with random initialization are reported. * indicates the proposed method is significantly better than the other baselines (p &lt; 0.05) based on one-tailed unpaired t-test.</figDesc><table><row><cell>Model variants</cell><cell>D1</cell><cell>D2</cell><cell>D3</cell></row><row><cell>Vanilla model</cell><cell cols="3">66.66 55.63 56.24</cell></row><row><cell>+Opinion transmission</cell><cell cols="3">66.98 56.03 56.65</cell></row><row><cell cols="4">+Message passing-a (IMN −d ) 68.32 57.66 57.91</cell></row><row><cell>+DS</cell><cell cols="3">68.48 57.86 58.03</cell></row><row><cell>+DD</cell><cell cols="3">68.65 57.50 58.26</cell></row><row><cell>+Message passing-d (IMN)</cell><cell cols="3">69.54 58.37 59.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>F1-I scores of different model variants. Average results over 5 runs are reported.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>F1 scores with different T values using IMN −d . Average results over 5 runs are reported.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">{B, I}-{POS, NEG, NEU} denotes the beginning and inside of an aspect-term with positive, negative, or neutral sentiment, respectively, and O denotes background words.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We omit the iteration superscript t in the description for simplicity. 4 e.g. "great" and "dreadful" in "Great food but the service is dreadful" are the opinion terms.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Note that we are actually performing aspect and opinion term co-extraction. We still denote this task as AE for simplicity. We believe ABSA is more complete with opinion terms also extracted. Also, the information learned from opinion term extraction could be useful for the other tasks.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is supported by the National Research Foundation Singapore under its AI Singapore Programme grant AISG-RP-2018-006.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Implementation Details <ref type="bibr">CNN-based Encoder</ref> We adopt the multi-layer-CNN structure from <ref type="bibr" target="#b30">(Xu et al., 2018)</ref> as the CNN-based encoders for both the shared CNNs and the task-specific ones in the proposed network. Each CNN layer has many 1Dconvolution filters, and each filter has a fixed kernel size k = 2c + 1, such that each filter performs convolution operation on a window of k word representations, and compute the representation for the ith word along with 2c nearby words in its context.</p><p>Following the settings in the original paper, the first CNN layer in the shared encoder has 128 filters with kernel sizes k = 3 and 128 filters with kernel sizes k = 5. The other CNN layers in the shared encoder and the CNN layers in each task-specific encoder have 256 filters with kernel sizes k = 5 per layer. ReLu is used as the activation function for each CNN layer. Dropout with p = 0.5 is employed after the embedding layer and each CNN layer.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Summarizing opinions: Aspect extraction meets sentiment prediction and they are both weakly supervised</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Angelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Conditional random fields meet deep neural networks for semantic segmentation: Combining probabilistic graphical models with deep learning for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Måns</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="52" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recurrent attention network on memory for aspect sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Aspect-level sentiment classification with heat (hierarchical attention) network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenglin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM on Conference on Information and Knowledge Management</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent Twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Approximation-aware dependency parsing by belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matthew R Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="489" to="501" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An unsupervised neural attention model for aspect extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Wee Sun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahlmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Effective attention modeling for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Wee Sun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahlmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploiting document knowledge for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Wee Sun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahlmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transformation networks for target-oriented sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A unified model for opinion target extraction and target sentiment prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Aspect term extraction with history attention and selective transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimou</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep multi-task learning for aspect term extraction with memory interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention modeling for target sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the European Chapter</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Implicit discourse relation classification via multi-task neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-task sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">PhraseRNN: Phrase recursive neural network for aspect-based sentiment analysis</title>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Thien Hai Nguyen and Kiyoaki Shirai</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SemEval-2015 task 12: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haris</forename><surname>Papageorgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Semantic Evaluation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Suresh Manandhar, and Ion Androutsopoulos</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SemEval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haris</forename><surname>Papageorgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Semantic Evaluation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Ion Androutsopoulos, and Suresh Manandhar</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Opinion word expansion and target extraction through double propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="27" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Effective LSTMs for target-dependent sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to attend via word-aspect associative fusion for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Target-dependent Twitter sentiment classification with rich automatic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy-Tin</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards a one-stop solution to both aspect extraction and sentiment analysis tasks with neural multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feixiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recursive neural conditional random fields for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokui</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Coupled multi-layer attentions for co-extraction of aspect and opinion terms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokui</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention-based LSTM for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Double embeddings and CNN-based sequence labeling for aspect extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised word and dependency path embeddings for aspect term extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaimeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural networks for open domain targeted sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy-Tin</forename><surname>Vo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">D1 D2 D3 F1-a acc-s F1-s F1-I F1-a acc-s F1-s F1-I F1-a acc-s F1-s F1-I 04</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy-Tin</forename><surname>Vo</surname></persName>
		</author>
		<idno>83.05 * 73.30 68.71 * 77.69 75.12 * 71.35 * 58.04 * 69.25 84.53 * 70.85 * 58.18</idno>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Gated neural networks for targeted sentiment analysis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
