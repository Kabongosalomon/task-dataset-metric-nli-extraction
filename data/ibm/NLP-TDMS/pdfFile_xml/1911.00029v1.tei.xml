<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Chirality Nets for Human Pose Regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
							<email>aschwing@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Chirality Nets for Human Pose Regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose Chirality Nets, a family of deep nets that is equivariant to the "chirality transform," i.e., the transformation to create a chiral pair. Through parameter sharing, odd and even symmetry, we propose and prove variants of standard building blocks of deep nets that satisfy the equivariance property, including fully connected layers, convolutional layers, batch-normalization, and LSTM/GRU cells. The proposed layers lead to a more data efficient representation and a reduction in computation by exploiting symmetry. We evaluate chirality nets on the task of human pose regression, which naturally exploits the left/right mirroring of the human body. We study three pose regression tasks: 3D pose estimation from video, 2D pose forecasting, and skeleton based activity recognition. Our approach achieves/matches state-of-the-art results, with more significant gains on small datasets and limited-data settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human pose regression tasks such as human pose estimation, human pose forecasting and skeleton based action recognition, have numerous applications in video understanding, security and humancomputer interaction. For instance, collaborative virtual reality applications rely on accurate pose estimation for which significant advances have been reported in recent years.</p><p>Specifically, recent state-of-the-art approaches use supervised learning to address pose regression and employ deep nets. Input and output of those nets depend on the task: inputs are typically 2D or 3D human pose key-points stacked into a vector; the output may represent human pose key-points for pose estimation or a classification probability for activity recognition. To improve accuracy of those tasks, a variety of deep net architectures have been proposed <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b47">48]</ref>, generally relying on common deep net building blocks, such as, fully connected, convolutional or recurrent layers. Unlike for image datasets, to enlarge the size of human pose datasets, a reflection (left-right flipping) of the pose coordinates as illustrated in step (1) of <ref type="figure" target="#fig_0">Fig. 1</ref> is not sufficient. The chirality of the human pose requires to additionally switch the labeling of left and right as illustrated in step (2) of <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>However, while this two-step data augmentation is conceptually easy to employ during training, we argue that even better accuracy is possible for human pose regression tasks if this pose symmetry is directly built into the deep net. In particular, if confronted with either of the poses illustrated on the left or right hand side of <ref type="figure" target="#fig_0">Fig. 1</ref> the output of a deep net should be equivariant to the transformation, i.e., the output is also transformed in a "predefined way." For example, if the network's output is also a human pose, the output pose should follow the same transformation. On the other hand, for an activity recognition task, the output probability should remain unchanged. The equivariant map, for pose estimation, is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref> and we make the equivariance property more precise later.</p><p>To encode this form of equivariance for human pose regression tasks, we propose "chirality nets." Specifically, the output of a chirality net is guaranteed to be equivariant w.r.t. a transformation composed of reflections and label switching. To build chirality nets, we develop chirality equivariant versions of commonly used layers. Specifically, we design and prove equivariance for versions of fully connected, convolutional, batch-normalization, dropout, and LSTM/GRU layers and elementwise non-linearities such as tanh or soft-sign. The main common design principle for chirality equivariant layers is odd and even symmetric sharing of model parameters. Hence, in addition to being equivariant, transforming a typical deep net into its chiral counterpart results in a reduction of the number of trainable parameters, and lower computation complexity due to the symmetry in the model weights. We find a smaller number of trainable parameters reduces the sample complexity, i.e., the models need less training data.</p><p>We demonstrate the generalization and effectiveness of our approach on three pose regression tasks over four datasets: 3D pose estimation on the Human3.6m <ref type="bibr" target="#b21">[22]</ref> and HumanEva dataset <ref type="bibr" target="#b48">[49]</ref>, 2D pose estimation on the Penn Action dataset <ref type="bibr" target="#b63">[64]</ref> and skeleton-based action recognition on Kinetics-400 dataset <ref type="bibr" target="#b22">[23]</ref>. Our approach achieves state-of-the-art results with guarantees on equivariance, lower number of parameters, and robustness in low-resource settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>First we briefly review invariance and equivariance in machine learning and computer vision as well as human pose regression tasks.</p><p>Invariant and equivariant representation. Hand-crafted invariant and equivariant representations have been utilized widely in computer vision systems for decades, e.g., scale invariance of SIFT <ref type="bibr" target="#b31">[32]</ref>, orientation invariance of HOG <ref type="bibr" target="#b8">[9]</ref>, affine invariance of the Harris detector <ref type="bibr" target="#b35">[36]</ref>, shift-invariant systems in image processing <ref type="bibr" target="#b53">[54]</ref>, etc.</p><p>These properties have also been adapted to learned representations. A widely known property is the translation equivariance of convolutional neural nets (CNN) <ref type="bibr" target="#b27">[28]</ref>: through spatial or temporal parameter sharing, a shifted input leads to a shifted output. Group-equivariant CNNs extend the equivariance to rotation, mirror reflection and translation <ref type="bibr" target="#b6">[7]</ref> by replacing the shift operation with a more general set of transformations. Other representations for building equivariance into deep nets have also been proposed, e.g., the Symmetric Network <ref type="bibr" target="#b11">[12]</ref>, the Harmonic Network <ref type="bibr" target="#b56">[57]</ref> and the Spherical CNN <ref type="bibr" target="#b7">[8]</ref>.</p><p>The aforementioned works focus on deep nets where the input are images. While related, they are not directly applicable to human pose. For example, a reflection with respect to the y-axis in the image domain corresponds to a permutation of the pixel locations, i.e., swapping the pixel intensity between each pixel's reflected counterpart. In contrast, for human pose, where the input is a vector representing the human joints' spatial coordinates, a reflection corresponds to the negation of the value for each of the joints reflected dimension.</p><p>The input representation of deep nets for human pose is more similar to pointsets. Prior work has explored building permutation equivariant deep nets, i.e., any permutation of input elements results in the same permutation of output elements.</p><p>In <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b42">43]</ref>. Both works utilize parameter sharing to achieve permutation equivariance. Following these works, graph nets generalize the family of permutation equivariant networks and demonstrate success on numerous applications <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b30">31]</ref>. For human pose, equivariance to all permutations is too strong of a property. Recall, our aim is to build models equivariant to the chiral symmetry, which only involves a specific permutation, e.g., the switch between left and right joints, shown in step (2) of <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><formula xml:id="formula_0">C h i r a l i t y T r a n s f o r m I n p u t ( x )  I n p u t 2 D P o s e x F θ F θ O u t p u t 3 D P o s e y C h i r a l i t y T r a n s f o r m O u t p u t ( y )   </formula><p>Most relevant to our approach is work by Ravanbakhsh et al. <ref type="bibr" target="#b43">[44]</ref>. Ravanbakhsh et al. <ref type="bibr" target="#b43">[44]</ref> explore which type of equivariance can be achieved through parameter sharing. Their approach captures one specific permutation in the pose symmetric transform, but does not capture the negation from the reflection, shown in <ref type="figure" target="#fig_0">Fig. 1</ref> step <ref type="formula">(1)</ref>. In contrast, our approach considers both operations <ref type="formula">(1)</ref> and <ref type="formula">(2)</ref> jointly, which leads to a different formulation. Lastly, to the best of our knowledge, <ref type="bibr" target="#b43">[44]</ref> only discusses theoretically the construction of equivariant networks. In this work, we design and implement a variety of building blocks for deep nets and demonstrate the benefits on a wide range of practical applications in human pose regression tasks.</p><p>Human pose applications. For 3D pose estimation from images, recent approaches utilize a twostep approach: (1) 2D pose keypoints are predicted given a video; (2) 3D keypoints are estimated given 2D joint locations. The 2D to 3D estimation is formulated as a regression task via deep nets <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42]</ref>. Capturing the temporal information is crucial and has been explored in 3D pose estimation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29]</ref> as well as in action recognition <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b19">20]</ref>, video segmentation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> and learning object dynamics <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37]</ref>. Most recently, Pavllo et al. <ref type="bibr" target="#b41">[42]</ref> propose to use temporal convolutions to better capture the temporal information for 3D pose estimation over previous RNN based methods. They also performed train and test time augmentation based on the chiral-symmetric transformation. For test time augmentation, they compute the output for both the original input and the transformed input, using the average outputs as the final prediction. In contrast to our work, we note that Pavllo et al. <ref type="bibr" target="#b41">[42]</ref> need to transform the output of the transformed input back to the original pose. To carefully assess the benefits of chirality nets, in this work, we closely follow the experiment setup of Pavllo et al. <ref type="bibr" target="#b41">[42]</ref>.</p><p>For 2D keypoint forecasting, we follow the setup of standard temporal modeling: conditioning on past observations to predict the future. To improve temporal modeling, recent works, have utilized different sequence to sequence models for this task <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref>. In this work, we closely follow the experiment setup of Chiu et al. <ref type="bibr" target="#b4">[5]</ref>.</p><p>For action recognition, skeleton based methods have been explored extensively recently <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b47">48]</ref> due to robustness to illumination changes and cluttered background. Here we closely follow the experimental setup of Yan et al. <ref type="bibr" target="#b57">[58]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Chirality Nets</head><p>In the following we first provide the problem formulation for human pose regression, before defining chirality nets, equivariance and the chirality transform. Subsequently we discuss how to develop typical layers such as the fully connected layer, the convolution, etc., which make up chirality nets.</p><p>The Pytorch implementation and unit-tests of the proposed layers are part of the supplementary material. We have also included a short Jupyter notebook demo to illustrate the key concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Chirality nets can be applied to regression tasks on coordinates of joints for human pose related task, i.e., the input corresponds to 2D or 3D coordinates of human joints. For readability, we introduce the input and output representations for a single frame. Note that for our experiments we generalize chirality nets to multiple frames by introducing a time dimension.</p><p>We let x ∈ R |J in |·|D in | denote the chirality net input, where J in is the set of all joints and D in is the dimension index set for an input coordinate. For example, J in = {'right wrist', 'right shoulder', . . .} and D in = {0, 1}, for 2D input joint coordinates. Similarly, we let y ∈ R |J out |·|D out | refer to the chirality net output. Note that the dimension of the spatial coordinates at the input and output may be different, e.g., prediction from 2D to 3D. Also, the number of joints may differ, e.g., when mapping between different key-point sets.</p><p>For human pose regression, the task is to learn the parameters θ of a model F θ by minimizing a loss function, L(θ) = (x,y)∈D (F θ (x), y) over the training dataset D. Hereby, sample loss (F θ (x), y) compares prediction F θ to ground-truth y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Chirality Nets, Chirality Equivariance, and Chirality Transforms</head><p>Chirality nets exhibit chirality equivariance, i.e., their output is transformed in a "predefined manner" given that the chirality transform is applied at the input. Note that the input and output dimensions D in and D out may differ. To define this chirality equivariance, we hence need to consider a pair of transformations, one for the input data, T in , and one for the output data, T out . The corresponding equivariance map is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref> for the task of 2D to 3D pose estimation. Formally, we say a function F θ is chirality equivariant w.</p><formula xml:id="formula_1">r.t. (T in , T out ) if T out (F θ (x)) = F θ (T in (x)) ∀x ∈ R |J in ||D in | .</formula><p>To define the chirality transform on the input data, i.e., T in , we split the set of joints J in into ordered tuples of J in l , J in r , and J in c , each denoting left, right and center joints of the input. Importantly, these tuples are sorted such that the corresponding left/right joints are at corresponding positions in the tuple. We also split the dimension index set D in into D in n and D in p := D in \D in n , indicating the coordinates to, or not to, negate.</p><p>For readability and without loss of generality, assume the dimensions of the input x follow the order</p><formula xml:id="formula_2">of J in l , J in r , J in c , i.e., x = [x l , x r , x c ].</formula><p>Within each vector x (·) , we place the coordinates in the set D in n before the remaining ones, i.e., x l = [x ln , x lp ]. Given this construction of the input x, the reflection illustrated in step (1) of <ref type="figure" target="#fig_0">Fig. 1</ref> is a matrix multiplication with a (|J in ||D in |) × (|J in ||D in |) diagonal matrix T in neg , defined as follows:</p><formula xml:id="formula_3">T in neg = diag([−1 |J in l |·|D in n | , 1 |J in l |·|D in p | , −1 |J in r |·|D in n | , 1 |J in r |·|D in p | , −1 |J in c |·|D in n | , 1 |J in c |·|D in p | ]), where 1 K indicates a vector of ones of length K.</formula><p>The switch operation illustrated in step (2) of <ref type="figure" target="#fig_0">Fig. 1</ref> is a matrix multiplication with a permutation matrix of dimension (|J in ||D in |) × (|J in ||D in |), defined as follows:</p><formula xml:id="formula_4">T in swi =   0 I |J in l |·|D in | 0 I |J in l |·|D in | 0 0 0 0 I |J in c |·|D in |   ,</formula><p>where I K denotes an identity matrix of size K × K.</p><p>Given those matrices, the chirality transform of the input</p><formula xml:id="formula_5">T in (x) is obtained via T in (x) = T in neg T in swi x.</formula><p>The chirality transform of the output, T out , is defined similarly, replacing "in" with "out".</p><p>In the following, we introduce layers that satisfy the (T in , T out ) chirality equivariance property. This enables to construct a chirality net F θ , as the composition of equivariant layers remains equivariant. Note that (T in , T out ) chirality equivariance can be specified separately for every deep net layer which provides additional flexibility. In the following we discuss how to construct layers which satisfy chirality equivariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Chirality Layers</head><p>Fully connected layer. A fully connected layer performs the mapping y = f FC (x; W, b) := W x + b. We achieve equivariance through parameter sharing and odd symmetry:</p><formula xml:id="formula_6">W =         W ln,ln W ln,lp W lp,ln W lp,lp W ln,rn W ln,rp W lp,rn W lp,rp W ln,cn W ln,cp W lp,cn W lp,cp W ln,rn −W ln,rp −W lp,rn W lp,rp W ln,ln −W ln,lp −W lp,ln W lp,lp W ln,cn −W ln,cp −W lp,cn W lp,cp W cn,ln W cn,lp 0 W cp,lp W cn,ln −W cn,lp 0 W cp,lp W cn,cn 0 0 W cp,cp         , b =         b ln b lp −b ln b lp 0 b cp         .</formula><p>We color code the shared parameters using identical colors. Each W (·),(·) denotes a matrix, where the first and the second subscript characterize the dimensions of the output and the input. For example, W ln,rp computes the output's left (l) joint's negated (n) dimensions, from the input's right (r) joint's non-negated, i.e., positive (p), dimensions. Note that W ln,rp is a matrix of dimension |J out l | · |D out n | × |J in r | · |D in p |. We refer to this layer as the chiral fully connected layer. 1D convolution layers <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b27">28]</ref>. Pose symmetric 1D convolution layers can be based on fully connected layers. A 1D convolution is a fully connected layer with shared parameters across the time dimension, i.e., at each time step the computation is the sum of fully connected layers over a window:</p><formula xml:id="formula_7">y t = τ W τ x t−τ + b = τ f FC (x t−τ ; W τ , b).</formula><p>Consequently, we enforce equivariance at each time step by employing the symmetry pattern of fully connected layers at each time slice.</p><p>Element-wise nonlinearities. Nonlinearities are applied element-wise and do not contain parameters. These operations maintain the input dimension, therefore, T out and T in are identical. A nonlinearity f that is an odd function, i.e., f (−x) = −f (x), such as tanh, hardtanh, or soft-sign satisfies the equivariance property. See the following proof:</p><formula xml:id="formula_8">T out (f (x)) = T out neg T out swi (f (x)) elementwise f = T out neg f (T out swi x)) odd func. f = f (T out neg T out swi x) = f (T in (x)) ∀x ∈ R |J in ||D in | .</formula><p>LSTM and GRU layers <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>LSTM and GRU modules which satisfy chirality can be obtained from fully connected layers.</p><p>However, naïvely setting all matrix multiplies within an LSTM to satisfy the equivariance property will not lead to an equivariant LSTM because gates are elementwise multiplied with the cell state. If both gate and cell preserve the negation then the product will not. Therefore, we change the weight sharing scheme for the gates. We set D out n for the gates to be the empty set, i.e., the gates will be invariant to negation at the input, T in neg , but still equivariant to the switch operation, T in swi . With this setup, the product of the gates and the cell's output will preserve the sign, as the gates are invariant to negation and passed through a Sigmoid to be within the range of (0, 1). GRU modules are modified in the same manner.</p><p>Batch-normalization <ref type="bibr" target="#b20">[21]</ref>. A batch normalization layer performs an element-wise standardization, followed by an element-wise affine layer (with learnable parameters γ and β). For γ and β, we follow the the principle applied to fully connected layers.</p><p>Equivariance for µ, and σ is obtained by computing the mean and standard deviation on the "augmented batch" and by keeping track of its running average.</p><p>Dropout <ref type="bibr" target="#b49">[50]</ref>. At test time, dropout scales the input by p, where p is the dropout probability. The equivariance property is satisfied because of the associativity property of a scalar multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Reduction in model parameters, FLOPS, and training/test details</head><p>Model parameters. Our model shares parameters between dimensions representing the left and right joints. For each layer, the number of parameters are reduced by a factor of</p><formula xml:id="formula_9">|(|J in l |+|J in c |)·(|J out l |+|J out c |) |J in |·|J out | . Recall |J in | = |J in l | + |J in r | + |J in c |.</formula><p>The output dimension size is computed similarly.   FLOPS. Chirality nets also have lower FLOPS. Due to the symmetry, instead of multiplying and adding each of the elements independently, we add the symmetric values first before applying a single multiplication per symmetric pair. Concretely, consider w = [w 1 , w 1 ], x = [x 1 , x 2 ], and their inner product w T x. Instead of computing w 1 · x 1 + w 1 · x 2 , we exploit symmetry and use instead w 1 · (x 1 + x 2 ), which removes one multiplication operation. This is a common speed up trick used in symmetric FIR filters <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b59">60]</ref>. The number of multiplications reduces by a factor of</p><formula xml:id="formula_10">|J in l |+|J in c | |J in |</formula><p>. Additionally, baseline models utilize test-time augmentation, which requires two forward passes through the network for each input, whereas the proposed nets only use a single forward pass.</p><p>Training and test details. During training it is important to apply the chirality transform for dataaugmentation, i.e., with 50% probability we apply T in and T out to input and label. This ensures that the mini-batch statistics match our assumption on the chirality, i.e., poses that form a chiral pair are both valid, which is important for the batch-normalization layer. Moreover, during training we use a standard dropout layer. While we could impose dropped units to be chiral equivariant, we found this lead to over-fitting in practice. This is expected as imposing chirality on the added noise reduces the randomness. Importantly, during test no data-augmentation is performed and a single forward pass is sufficient to obtain an 'averaged' result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our approach on a variety of tasks, including 2D to 3D pose estimation, 2D pose forecasting, and skeleton based action recognition. For each task, we describe the dataset, metric, and implementation before discussing the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">2D to 3D pose estimation</head><p>Task. 3D human pose estimation can be decoupled into the tasks of 2D keypoint detection and 2D to 3D pose estimation. We focus on the latter task, i.e., given a sequence of 2D keypoints, the task is to estimate the corresponding 3D human pose. See <ref type="figure" target="#fig_2">Fig. 3 (a)</ref> for an illustration.</p><p>Dataset and metric. We evaluate on two standard datasets, the Human3.6M <ref type="bibr" target="#b21">[22]</ref> and the HumanEva-I <ref type="bibr" target="#b48">[49]</ref>. Human3.6M is a large scale dataset of human motion with 3.6 million video frames. The dataset consists of 11 subjects performing 15 different actions. Following prior work <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42]</ref>, each human pose is represented by a 17-joint skeleton. We use the same train and test subject splits. HumanEva-I is a smaller dataset consisting of four subjects and six actions. To be consistent with Walk Jog Box Avg. App. S1 S2 S3 S1 S2 S3 S1 S2 S3 -  S1 .1% S1 1% S1 5% S1 10% S1 50% S1 100% S15 S156</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPJPE (mm)</head><p>Training Splits Pavllo et al. Ours <ref type="figure">Figure 4</ref>: Comparisons between our approach and <ref type="bibr" target="#b41">[42]</ref> in limited data settings evaluated using Protocol 1 on Human3.6M.</p><p>prior work <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42]</ref>, we use the same train and test splits evaluated over the actions of (walk, jog, and box). For both of these datasets, we consider the setting where we train one model for all actions.</p><p>We report the two standard metrics used in prior work: Protocol 1 (MPJPE) which is the mean perjoint position error between the prediction and ground-truth <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref> and Protocol 2 (P-MPJPE) which is the error, after alignment, between the prediction and ground-truth <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>Implementation details. Our model follows the supervised training procedure and network design of Pavllo et al. <ref type="bibr" target="#b41">[42]</ref>. Our network is the identical temporal convolutional network architecture, where each layer is replaced with its chiral version, i.e., 1D dilated convolution, batch-normalization, and dropout layers. We also replace ReLU non-linearities with Tanh to achieve equivariance. No additional architecture changes were made. For Human3.6M, we use 2D keypoints extracted from CPN <ref type="bibr" target="#b3">[4]</ref> with Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> bounding boxes released by Pavllo et al. <ref type="bibr" target="#b41">[42]</ref>. For HumanEva-I, we use the 2D keypoint detections from Mask R-CNN released by Pavllo et al. <ref type="bibr" target="#b41">[42]</ref>.</p><p>Results. In Tab. 1, we report the performance on the Human3.6M data using Protocol 1 (MPJPE). Our approach outperforms the state-of-the-art <ref type="bibr" target="#b41">[42]</ref> which uses test-time augmentation by 0.1 mm in overall average and achieves the best results in eight out of fifteen sub-categories. For the singleframe models, we observe a more significant reduction in error of 0.4 mm over <ref type="bibr" target="#b41">[42]</ref> with test time augmentation. Additionally, when comparing without test-time augmentation, our approach outperforms by 1 mm. We note that, test-time augmentation employed by Pavllo et al. <ref type="bibr" target="#b41">[42]</ref> involves running the network twice for each input. In contrast, our approach only requires a single forward pass.</p><p>Next, on HumanEva-I dataset, we also observed an increase in performance using Protocol 1. On average, our approach achieves a 32.2mm error. This is a 0.8mm decrease over the current state-ofthe-art of 33.0mm <ref type="bibr" target="#b41">[42]</ref> and a 1.1mm decrease over <ref type="bibr" target="#b41">[42]</ref> without test-time augmentation of 33.3mm.</p><p>We also performed evaluation using Protocol 2 (P-MPJPE). On Human3.6M we observe that our approach performs worse than Pavllo et al. <ref type="bibr" target="#b41">[42]</ref> by 0.3mm. We note that the loss function is chosen to optimize Protocol 1, therefore our models are performing better at what they are optimized for. In Tab. 2, we report the performance on HumanEva-I using Protocol 2 (P-MPJPE). Our model achieves a 0.2 mm reduction in error over Pavllo et al. <ref type="bibr" target="#b41">[42]</ref> on average. Most of the gain is obtained for the boxing action, possibly due to the symmetric nature of the movement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limited data settings.</head><p>A benefit of fewer model parameters is the potential to obtain better models with less data. To confirm this, we perform experiments by varying the amount of training data, starting from 0.1% of subject 1 (S1) to using three subjects S1, S5, S6. The results with comparison to <ref type="bibr" target="#b41">[42]</ref> are shown in <ref type="figure">Fig. 4</ref>. We observe that our approach consistently out-performs <ref type="bibr" target="#b41">[42]</ref> in this low resource settings, except at S1 0.1%. For the reported numbers, we use a batch-size of 64, and all other hyper-parameters are identical between the models. If we further decrease the batch-size to 32 for S1 0.1%, our approach improves to 100.4mm where <ref type="bibr" target="#b41">[42]</ref> improves to 102.3mm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">2D pose forecasting</head><p>Task. 2D pose forecasting is the pose regression task of predicting the future human pose, represented in 2D keypoints, given present and past human pose. See <ref type="figure" target="#fig_2">Fig. 3 (b)</ref> for an illustration.</p><p>Dataset and metric. We evaluate on the Penn Action dataset <ref type="bibr" target="#b63">[64]</ref>. The dataset consists of 2236 videos with 15 actions. Each frame is annotated with 2D keypoints of 13 human joints. We use the  same train and test split as in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. Following Chiu et al. <ref type="bibr" target="#b4">[5]</ref> we consider initial velocity as being part of the input and a single model is used for all actions. For a fair comparison with prior work, we report the 'Percentage of Correct Keypoint' metric with a 0.05 threshold (PCK@0.05), which assesses the accuracy of the predicted keypoints. A predicted keypoint is considered correct if it is within a 0.05 radius of the ground-truth when considering normalized distance.</p><p>Implementation details. Our non-chiral equivariant baseline model is a sequence-to-sequence model based on <ref type="bibr" target="#b33">[34]</ref>. We made several modifications to match the hyperparameters in <ref type="bibr" target="#b4">[5]</ref>, i.e., we used StackedRNN <ref type="bibr" target="#b38">[39]</ref> with 2 layers and added dropout layers. Additionally, we utilize teacher forcing <ref type="bibr" target="#b55">[56]</ref> during training, while prior work did not. We find this to stabilize training and enable the use of the Adam <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b44">45]</ref> optimizer without diverging. We performed data augmentation via the chirality transform, i.e., with 0.5 probability we apply T in and T out to the input and the ground-truth correspondingly. For our pose symmetric model, we replaced all the non-symmetric layers, e.g., fully connected layers and LSTM cells with their corresponding chiral version.</p><p>Results. In Tab. 3, we report the performance of our models and the state-of-the-art. The baseline model without augmentation outperforms the state-of-the-art <ref type="bibr" target="#b4">[5]</ref>. The gain comes from the use of Stacked-LSTM and teacher forcing during training. With additional train and test time data-augmentation, our baseline model further improves. In addition our pose symmetric model outperforms the baseline, in terms of average PCK@0.05. We observe more significant improvements for the first ten prediction steps.  Task. Skeleton based action recognition aims at predicting human action based on skeleton sequences. See <ref type="figure" target="#fig_2">Fig. 3 (c)</ref> for an illustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Skeleton based action recognition</head><p>Dataset and metric. We use the Kinetics-400 dataset <ref type="bibr" target="#b22">[23]</ref> in our experiments. The dataset contains 400 action classes and 306,245 clips in total. Following the experimental setup by <ref type="bibr" target="#b57">[58]</ref>, we use OpenPose <ref type="bibr" target="#b1">[2]</ref> to locate the 18 human body joints. Each joint is represented as (x, y, c), where x and y are the 2D coordinates of the joint and c is the confidence score of the joint given by OpenPose. Following <ref type="bibr" target="#b22">[23]</ref>, we report the classification accuracy at top-1 and top-5.</p><p>Implementation details. Our baseline model, 'Ours-Conv,' follows 'Temporal-Conv' <ref type="bibr" target="#b23">[24]</ref>, modified to have not only temporal convolution but also spatial convolution. The temporal convolution considers the intra-frame information while the spatial convolution considers the inter-frame information.</p><p>For the recognition task, we need chiral invariance, i.e., a chiral pair should be classified as the same action class. To this end, we use a chiral invariance layer where we let both J out r , J out l as well as D out n be empty sets, which means there are no left and right joints but only center joints and there is no dimension that will be negated in the output of the layer after applying chirality transform. Note that the chirality transform exchanges the left and right joints and negates the dimensions in the dimension index set D out n . Given J out r , J out l and D out n are all empty, it's trivial that the output will be chiral invariant. For the chiral invariance model, 'Ours-Conv-Chiral,' we replace all the non-symmetric layers before the chiral invariance layer with their corresponding chiral equivariance version. All the layers after the chiral invariance layer remain identical to the 'Ours-Conv' model. There are in total 10 layers of spatial and temporal convolution and we put the chiral invariance layer at the fourth layer. We use the SGD optimizer with a momentum of 0.9 as in <ref type="bibr" target="#b57">[58]</ref>.</p><p>Results. In Tab. 4, we report the action recognition performance of our model and the skeleton-based approaches. We observe that the baseline model 'Ours-Conv' performs on par with ST-GCN <ref type="bibr" target="#b57">[58]</ref> and the chiral invariant model, 'Ours-Conv-Chiral' outperforms both ST-GCN and Ours-Conv on Top-1 and Top-5 accuracy, achieving the state-of-the-art performance on the Kinetics-400 dataset among skeleton based action recognition methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduce chirality equivariance for pose regression tasks and develop deep net layers that satisfy this property. Through parameter sharing and odd/even symmetry, we design equivariant versions of commonly used layers in deep nets, including fully connected, 1D convolution, LSTM/GRU cells, and batch normalization layers. With these equivariant layers at hand, we build Chirality Nets, which guarantee equivariance from the input to the output. Our models naturally lead to a reduction in trainable parameters and computation due to symmetry. Our experimental results on three human pose regression tasks over four datasets demonstrate state-of-the-art performance and the wide practical impact of the proposed layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material: Pose Symmetric Network for Human Pose Regression A Code and Test Cases</head><p>In the supplemental materials, we have included Pytorch implementation of the proposed layers. Each layer also comes with unit-tests validating the chirality-equivaraince. Please read the README.md for directory structures, usage and required dependencies. There is also a Jupyter notebook and it's HTML output visualizing the concepts introduced in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Description for Equivariant Layers B.1 Equivariant fully connected layers</head><p>Recall, we achieve equivariance through parameter sharing and odd symmetry.</p><p>A fully connected layer performs the mapping y = f FC (x; W, b) := W x + b. Recall, we achieve equivariance through parameter sharing and odd symmetry: </p><formula xml:id="formula_11">W =         W ln,</formula><formula xml:id="formula_12">W cn,cn 0 0 W cp,cp         , b =         b ln b lp −b ln b lp 0 b cp        </formula><p>Here, we prove that the design is chiral-equivariant. Through multiplying out the matrices, we can show W T (x) + b = T (W x + b), as follows:</p><p>Proof:</p><formula xml:id="formula_13">x = [x ln x lp x rn x rp x cn x cp ] T then T (x) = [−x rn x rp −x ln x lp −x cn x cp ] T</formula><p>With linear algebra,  </p><formula xml:id="formula_14">W x+b =        W ln,</formula><formula xml:id="formula_15">) + 0 · (x cp ) + 0 0 · (x ln ) + W cp,lp (x lp ) + 0 · (x rn ) + W cp,lp (x rp ) + 0 · (x cn ) + W cp,cp (x cp ) + b cp        T (W x+b) =        −W ln,rn (x ln ) + W ln,rp (x lp ) − W ln,ln (x rn ) + W ln,lp (x rp ) − W ln,cn (x cn ) + W ln,cp (x cp ) + b ln −W lp,rn (x ln ) + W lp,rp (x lp ) − W ,lp,ln (x rn ) + W lp,lp (x rp ) − W lp,cn (x cn ) + W lp,cp (x cp ) + b lp −W ln,ln (x ln ) − W ln,lp (x lp ) − W ln,rn (x rn ) − W ln,rp (x rp ) − W ln,cn (x cn ) − W ln,cp (x cp ) − b ln W lp,ln (x ln ) + W lp,lp (x lp ) + W lp,rn (x rn ) + W lp,rp (x rp ) + W lp,cn (x cn ) + W lp,cp (x cp ) + b lp −W cn,ln (x ln ) − W cn,lp (x lp ) − W cn,ln (x rn ) + W cn,lp (x rp ) − W cn,cn (x cn ) − 0 · (x cp ) − 0 0 · (x ln ) + W cp,lp (x lp ) + 0 · (x rn ) + W cp,lp (x rp ) + 0 · (x cn ) + W cp,cp (x cp ) + b cp        W T (x)+b =        W ln,</formula><formula xml:id="formula_16">) + 0 · (x cp ) + 0 0 · (−x rn ) + W cp,lp (x rp ) + 0 · (−x ln ) + W cp,lp (x lp ) + 0 · (−x cn ) + W cp,cp (x cp ) + b cp        observe that W T (x) + b = T (W x + b)</formula><p>, which proves the claim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Equivariant 1D convolution layers</head><p>1D convolution layers <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b27">28]</ref>. Pose symmetric 1D convolution layers can be based on fully connected layers. A 1D convolution is a fully connected layer with shared parameters across the time dimension, i.e., at each time step the computation is the sum of fully connected layers over a window:</p><formula xml:id="formula_17">y t = τ W τ x t−τ + b = τ f FC (x t−τ ; W τ , b).</formula><p>Consequently, we enforce equivariance at each time step by employing the symmetry pattern of fully connected layers at each time slice. </p><formula xml:id="formula_18">W τ =         W ln,</formula><formula xml:id="formula_19">        ,</formula><p>for all τ . The bias of a 1D convolution is identical to that of a fully connected layer, i.e., the same bias is added for each time step. Hence the same parameter sharing is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Equivariant LSTM and GRU layers</head><p>LSTM and GRU modules which satisfy chirality can be obtained from fully connected layers. However, naïvely setting all matrix multiplies within an LSTM to satisfy the equivariance property will not lead to an equivariant LSTM because gates are elementwise multiplied with the cell state. If both gate and cell preserve the negation then the product will not. Therefore, we change the weight sharing scheme for the gates. We set D out n for the gates to be the empty set, i.e., the gates will be invariant to negation at the input, T in neg , but still equivariant to the switch operation, T in swi . With this setup, the product of the gates and the cell's output will preserve the sign, as the gates are invariant to negation and passed through a Sigmoid to be within the range of (0, 1). GRU modules are modified in the same manner.</p><p>More formally, the computation in an LSTM module are as follows:</p><formula xml:id="formula_20">i t = σ(W ii x t + b ii + W hi h (t−1) + b hi ) (Input Gate) o t = σ(W io x t + b io + W ho h (t−1) + b ho ) (Output Gate) f t = σ(W if x t + b if + W hf h (t−1) + b hf ) (Forget Gate) g t = tanh(W ig x t + b ig + W hg h (t−1) + b hg ) (Cell State) c t = f t · c (t−1) + i t · g t h t = o t · tanh(c t ) (Recurrent State) ,</formula><p>where σ denotes an element-wise sigmoid non-linearity.</p><p>Observe that the LSTM operations consist of fully connected layers. For the cell state's parameters, e.g., W ig , W hg , b ig , b hg , we follow the weight sharing scheme discussed for fully connected layers.</p><p>Due the to multiplication in the cell state, we redesigned the parameter sharing for the input, output and forget gate, to be invariant to T in neg , by setting D out n to be the empty set: no negation is needed for all dimension. This results in the following parameter sharing scheme for the parameters </p><formula xml:id="formula_21">W ii , b ii , W hi , b hi , W io , b io , W ho , b ho , W if , b if , W hf , b hf : W =   [W lp,</formula><formula xml:id="formula_22">  , b =   [b lp ] [b lp ] [b cp ]   .</formula><p>This LSTM is chirality equivariant, as the computation of the cell state is equivariant. Other computations are linear combinations of chirality equivariant operations, which remains equivariant. We note that the chirality equivariant GRU module is modified by following the same sharing scheme for the gates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Equivariant batch-norm layers</head><p>A batch normalization layer performs an element-wise standardization, followed by an element-wise affine layer (with learnable parameters γ and β):</p><formula xml:id="formula_23">y = f BN (x) := γ · x − µ √ σ 2 + + β.</formula><p>Walk Jog Box Avg. App. S1 S2 S3 S1 S2 S3 S1 S2 S3 -Pavllo <ref type="bibr" target="#b41">[42]</ref> 17.6 12.5 37.6 28.  Equivariance for γ, and β is obtained by following the principle applied to fully connected layers: we achieve equivariance via parameter sharing and odd symmetry:</p><formula xml:id="formula_24">γ = [γ ln γ lp ] [γ ln γ 1p ] [γ cn γ cp ] T and β = [β ln β lp ] [−β ln β lp ] [0 β cp ] T .</formula><p>Equivariance for µ, and σ is obtained by computing the mean and standard deviation on the "augmented batch" and by keeping track of its running average. Formally, given a batch B of data,</p><formula xml:id="formula_25">µ = 1 2|B| x∈B x + T in (x), σ = x∈B (x−µ) 2 +(T in (x)−µ) 2 2|B| . B.5 Dropout.</formula><p>At test time, dropout scales the input by p, where p is the dropout probability. The equivariance property is satisfied because of the associativity property of a scalar multiplication. The input and output dimension and symmetry of a dropout layer are identical. Therefore, T out and T in are identical. From the definition:</p><formula xml:id="formula_26">T out (p · x) = T in (p · x) = T in neg T in swi (p · x) = p · (T in neg T in swi x) = p · (T in (x)) ∀x ∈ R |J in ||D in | .</formula><p>Hence, a dropout layer naturally satisfies the equivariance property. At training-time, we do not enforce equivariance for the dropped units, i.e., we do not jointly drop symmetric units as we found this to prevent overfitting. This is likely application dependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 3D pose estimation</head><p>In Tab. A1, we report the HumanEva-I for multi-action models evaluated on Protocol 1 (MPJPE). Our approach have benefits the most from the Boxing action while maintaing the performance on other actions. We also provide qualitative evaluation in <ref type="figure" target="#fig_0">Fig. A1 and Fig. A2</ref>. We observe that our model successfully estimates 3D poses from 2D key-points. We have also attached animations in the supplemental.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Skeleton based action recognition</head><p>In <ref type="figure" target="#fig_2">Fig. A3</ref>, we show the visualization of the input skeleton sequences computed by OpenPose <ref type="bibr" target="#b1">[2]</ref> and the predicted action class by our chiral invariant skeleton based action recognition model. D Implementation Details D.1 3D pose estimation Implementation details. Our model follows the temporal convolutional architecture proposed by Pavllo et al. <ref type="bibr" target="#b41">[42]</ref>, and replaced all layers with their chiral versions; code for the layers are attached in the supplemental as well. We also changed ReLU to tanh to achieve chiral equivariance. For the temporal models, we follow their 4 blocks design which has the receptive field of 243. For the single frame model, we follow their 3 blocks design. These models all contains 1020 hidden dimensions so it is a factor the number of joints, 17, this is slightly smaller than the 1024 used in <ref type="bibr" target="#b41">[42]</ref>. We also use <ref type="figure" target="#fig_0">Figure A1</ref>: Qualitative visualization of 2D to 3D pose estimation for the action "Walking" on HumanEva-I dataset.</p><p>their data processing and batching stragety as described in Section 5 and Appendix A.5 of <ref type="bibr" target="#b41">[42]</ref>. For training the model, we utilized the Adam optimizer with beta1=0.9 and beta2=0.9999. We decay the batch-normalizations' momentum as suggested in <ref type="bibr" target="#b41">[42]</ref>. Other details follows the publicly available implementation by Pavllo et al. <ref type="bibr" target="#b41">[42]</ref>. We enforced chiral equivariance by choosing the |D out n | to be 1 3 of the hidden dimension. The |D in n | for the input layer is 17 and the |D n | out for the output layer is 17, as one for each joint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 2D pose forecasting</head><p>Implementation details. The non-chiral equivariant baseline is a seq2seq model consisting of an encoder and decoder, which are stacked-LSTMs with hidden size of 1040 and 2 stacked layers. We trained using teacher forcing with the Adam optimizer. The batch-size is 256, and we trained for 30 epochs. Dropout is applied to the LSTMs' hidden layer with drop probability of 0.5. Following prior works, we use max norm gradient clipping of 5, a learning rate of 0.005 with a decay of 0.95 every 2 epochs. The data processing and evaluation setting follows <ref type="bibr" target="#b4">[5]</ref>. Other details follows the publicly available implementation by Chiu et al. <ref type="bibr" target="#b4">[5]</ref>. We enforced chiral equivariance by choosing the |D out n | to be 1 2 of the hidden dimension, as the output is two dimensional per joint. <ref type="figure" target="#fig_1">Figure A2</ref>: Qualitative visualization of 2D to 3D pose estimation for the action "Boxing" on HumanEva-I dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Skeleton-based action recognition</head><p>Implementation details. The non-chiral version of the model, Ours-Conv, follows Temporal-Conv <ref type="bibr" target="#b23">[24]</ref> while we modified the model to have not only temporal convolution but also spatial convolution. There are ten spatial-temporal convolution blocks and each block we first perform spatial convolution and then temporal convolution. The temporal convolution considers the intraframe information while the spatial convolution considers the inter-frame information. For the recognition task, we need chiral invariance, i.e., a chiral pair should be classified as the same action class. To this end, we use a chiral invariance layer where we let both J out r , J out l as well as D out n to be empty sets, which means there are no left and right joints but only center joints and there is no dimension that will be negated in the output of the layer after applying the chirality transform. Note that the chiral transformation exchange the left and right joints and negate the dimension in the index set D out n . Given J out r , J out l and D out n are all empty, it's obvious that the output will be chiral invariance. For the chiral invariance model, Ours-Conv-Chiral, we replace the all the non-symmetric layers before the chiral invariance layer with their corresponding chiral equivariance version. All the layers after the chiral invariance layer remains the same as in the Ours-Conv model. Similar to <ref type="bibr" target="#b23">[24]</ref>, there are in total 10 convolution blocks in Ours-Conv and we put the chiral invariance layer at the fourth layer. Also, we gradually reduce the ratio of the dimension to be negated (|D out n |/|D out |) from</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Push up</head><p>Clean and jerk</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Juggling balls</head><p>Playing piano Jogging <ref type="figure" target="#fig_2">Figure A3</ref>: Visualization of the input skeleton sequences and the corresponding predicted action classes of our method on the Kinetics-400 dataset <ref type="bibr" target="#b22">[23]</ref>. <ref type="bibr" target="#b0">1</ref> 3 to 1 6 at the first layer, from 1 6 to 1 12 at the second layer and from <ref type="bibr">1 12</ref> to 0 at the third layer. We use the SGD optimizer with a momentum of 0.9 as in <ref type="bibr" target="#b57">[58]</ref> with a batch size of 256. We train the model for 90 epochs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of the chirality transformation. The transformation includes two operations, (1) a reflection of the pose, i.e., a negation of the x-coordinates; and (2) a switch of the left / right joint labeling. The ordering of the two operations are interchangeable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of chirality equivariance for the task of 2D to 3D pose estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of pose regression tasks: (a) 2D to 3D pose estimation; (b) 2D pose forecasting; and (c) skeleton-based action recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Approach Dir. Disc. Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg Pavlakos [41] (CVPR'18) 48.5 54.4 54.4 52.0 59.4 65.3 49.9 52.9 65.8 71.1 56.6 52.9 60.9 44.7 47.8 56.2 Yang [59] (CVPR'18) 51.5 58.9 50.4 57.0 62.1 65.4 49.8 52.7 69.2 85.2 57.4 58.4 43.6 60.1 47.7 58.6 Luvizon [33] (CVPR'18) ( ) 49.2 51.6 47.6 50.5 51.8 60.3 48.5 51.7 61.5 70.9 53.7 48.9 57.9 44.4 48.9 53.2 Hossain [17] (ECCV'18)( †, ) 48.4 50.7 57.2 55.2 63.1 72.6 53.0 51.7 66.1 80.9 59.0 57.3 62.4 46.6 49.6 58.3 Lee [29] (ECCV'18)( †, ) 40.2 49.2 47.8 52.6 50.1 75.0 50.2 43.0 55.8 73.9 54.1 55.6 58.2 43.3 43.3 52.8 Pavllo [42] (CVPR'19) 47.1 50.6 49.0 51.8 53.6 61.4 49.4 47.4 59.3 67.4 52.4 49.5 55.3 39.5 42.7 51.8 Pavllo [42] (CVPR'19)( †) 45.9 47.5 44.3 46.4 50.0 56.9 45.6 44.6 58.8 66.8 47.9 44.7 49.7 33.1 34.0 47.7 Pavllo [42] (CVPR'19)( †, ‡) 45.2 46.7 43.3 45.6 48.1 55.1 44.6 44.3 57.3 65.8 47.1 44.0 49.0 32.8 33.9 46.8 Ours, single-frame 47.4 49.9 47.4 51.1 53.8 61.2 48.3 45.9 60.4 67.1 52.0 48.6 54.6 40.1 43.0 51.4 Ours ( †) 44.8 46.1 43.3 46.4 49.0 55.2 44.6 44.0 58.3 62.7 47.1 43.9 48.6 32.7 33.3 46.7</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results on the Human3.6M dataset: reconstruction error using Protocol 1 (MPJPE) in mm. The best result is boldface and the second best is underlined. † indicates temporal models, uses ground-truth bounding box, and ‡ indicates test-time augmentation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Pavlakos [40] 22.3 19.5 29.7 28.9 21.9 23.8 ----Pavlakos [41] 18.8 12.7 29.2 23.5 15.4 14.5 ----Lee [29] 18.6 19.9 30.5 25.7 16.8 17.7 42.8 48.1 53.4 -Pavllo [42] 14.1 10.4 46.8 21.1 13.3 14.0 23.8 34.5 32.3 31.1 Pavllo [42] ( ‡) 13.9 10.2 46.6 20.9 13.1 13.8 23.8 33.7 32.0 30.8 Ours 15.2 10.3 47.0 21.8 13.1 13.7 22.8 31.8 31.0 30.6</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on HumanEva-I for multi-action (MA) models reported in Protocol 2 (P-MPJPE), lower the better. ‡ indicates test time augmentation.</figDesc><table><row><cell>110</cell><cell>108.9</cell><cell></cell><cell></cell></row><row><cell></cell><cell>105.6</cell><cell>99.5</cell><cell></cell></row><row><cell>100</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>93.7</cell><cell></cell></row><row><cell>90</cell><cell></cell><cell>93</cell><cell>85.4</cell></row><row><cell>80</cell><cell></cell><cell>85</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>77.8</cell><cell>71.9</cell></row><row><cell>70</cell><cell></cell><cell></cell><cell></cell><cell>67.1</cell><cell>65.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>68.2</cell></row><row><cell>60</cell><cell></cell><cell></cell><cell></cell><cell>63.9</cell><cell>62</cell><cell>59.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>56.6</cell></row><row><cell>50</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>CVPR'17) 82.4 68.3 58.5 50.9 44.7 40.0 36.4 33.4 31.3 29.5 28.3 27.3 26.4 25.7 25.0 24.5 39.5 3D-PFNet [3](CVPR'17) 79.2 60.0 49.0 43.9 41.5 40.3 39.8 39.7 40.1 40.5 41.1 41.6 42.3 42.9 43.2 43.3 45.5 TP-RNN [5] (WACV'19) 84.5 72.0 64.8 60.3 57.2 55.0 53.4 52.1 50.9 50.0 49.3 48.7 48.3 47.9 47.6 47.3 55.6 Baseline w/o aug. 87.3 75.7 68.5 64.0 61.0 59.1 57.6 56.3 55.4 54.9 54.5 54.5 54.4 54.5 54.6 54.7 60.4 Baseline w/ aug. 86.9 75.2 67.9 63.5 60.4 58.4 57.0 55.8 55.1 54.5 54.1 54.0 53.9 53.9 54.0 54.0 59.9 Baseline w/ aug.( ‡) 87.0 75.5 68.4 64.1 61.0 59.1 57.5 56.3 55.5 55.0 54.7 54.7 54.6 54.7 54.7 54.7 60.5 Ours 87.5 77.0 68.7 64.2 61.2 59.2 57.6 56.5 55.7 55.1 54.7 54.6 54.4 54.5 54.5 54.5 60.6</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Prediction Steps</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Avg.</cell></row><row><cell>Approach</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell><cell>-</cell></row><row><cell>Residual [34] (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results on Penn action dataset, performance reported in terms of PCK@0.05 (higher the better). ( ‡) indicates using test time augmentation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Results of the skeleton based</cell></row><row><cell>action recognition baselines on the</cell></row><row><cell>Kinetics-400 dataset [23] reported in</cell></row><row><cell>Top-1 and Top-5 accuracy.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>ln W ln,lp W lp,ln W lp,lp W ln,rn W ln,rp W lp,rn W lp,rp W ln,cn W ln,cp W lp,cn W lp,cp W ln,rn −W ln,rp −W lp,rn W lp,rp W ln,ln −W ln,lp −W lp,ln W lp,lp W ln,cn −W ln,cp −W lp,cn W lp,cp W cn,ln W cn,lp 0 W cp,lp</figDesc><table /><note>W cn,ln −W cn,lp 0 W cp,lp</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>ln (x ln ) + W ln,lp (x lp ) + W ln,rn (x rn ) + W ln,rp (x rp ) + W ln,cn (x cn ) + W ln,cp (x cp ) + b ln W lp,ln (x ln ) + W lp,lp (x lp ) + W lp,rn (x rn ) + W lp,rp (x rp ) + W lp,cn (x cn ) + W lp,cp (x cp ) + b</figDesc><table /><note>lp W ln,rn (x ln ) − W ln,rp (x lp ) + W ln,ln (x rn ) − W ln,lp (x rp ) + W ln,cn (x cn ) − W ln,cp (x cp ) − b ln −W lp,rn (x ln ) + W lp,rp (x lp ) − W ,lp,ln (x rn ) + W lp,lp (x rp ) − W lp,cn (x cn ) + W lp,cp (x cp ) + b lp W cn,ln (x ln ) + W cn,lp (x lp ) + W cn,ln (x rn ) − W cn,lp (x rp ) + W cn,cn (x cn</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>ln (−x rn ) + W ln,lp (x rp ) + W ln,rn (−x ln ) + W ln,rp (x lp ) + W ln,cn (−x cn ) + W ln,cp (x cp ) + b ln W lp,ln (−x rn ) + W lp,lp (x rp ) + W lp,rn (−x ln ) + W lp,rp (x lp ) + W lp,cn (−x cn ) + W lp,cp (x cp ) + b lp W ln,rn (−x rn ) − W ln,rp (x rp ) + W ln,ln (−x ln ) − W ln,lp (x lp ) + W ln,cn (−x cn ) − W ln,cp (x cp ) − b ln −W lp,rn (−x rn ) + W lp,rp (x rp ) − W ,lp,ln (−x ln ) + W lp,lp (x lp ) − W lp,cn (−x cn ) + W lp,cp (x cp ) + b lp W cn,ln (−x rn ) + W cn,lp (x rp ) + W cn,ln (−x ln ) − W cn,lp (x lp ) + W cn,cn (−x cn</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>ln W lp,lp ] [W lp,rn W lp,rp ] [W lp,cn W lp,cp ] [−W lp,rn W lp,rp ] [−W lp,ln W lp,lp ] [−W lp,cn W lp,cp ] [0 W cp,lp ] [0 W cp,lp ] [0 W cp,cp ]</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>1 19.1 19.2 29.5 44.0 43.1 33.3 Pavllo [42] ( ‡) 17.5 12.3 37.4 27.7 19.0 19.0 27.7 43.4 42.5 33.0 Ours 18.9 12.3 38.1 28.5 18.1 18.2 27.1 40.9 40.2 32.2</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table A1 :</head><label>A1</label><figDesc>Results on HumanEva-I for multi-action (MA) models reported in Protocol 1 (MPJPE), lower the better. ‡ indicates test time augmentation.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This work is supported in part by NSF under Grant No. 1718221 and MRI #1725729, UIUC, Samsung, 3M, Cisco Systems Inc. (Gift Award CG 1377144) and Adobe. We thank NVIDIA for providing GPUs used for this work and Cisco for access to the Arcetri cluster. RY is supported by a Google PhD Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1812.08008</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Forecasting human dynamics from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Action-agnostic human pose forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spherical cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep symmetry networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R I</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MaskRNN: Instance Level Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">VideoMatch: Matching based Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Timeception for complex action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Human3.6m: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Interpretable 3D human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural relational inference for interacting systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Object recognition with gradient-based learning. In Shape, contour and grouping in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Propagating lstm: 3D pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">PIC: Permutation invariant critic for multi-agent deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CORL, 2019. * equal contribution</title>
		<meeting>CORL, 2019. * equal contribution</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">2D/3D pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On human motion prediction using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Scale &amp; affine invariant interest point detectors. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised learning of object structure and dynamics from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Implementing fir filters in flex devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Altera</forename><surname>Note</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Application</surname></persName>
		</author>
		<ptr target="http://www.ee.ic.ac.uk/pcheung/teaching/ee3_dsd/fir.pdf" />
	</analytic>
	<monogr>
		<title level="j">Altera Corporation</title>
		<imprint>
			<date type="published" when="1998-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">How to construct deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">3D human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Equivariance through parameter-sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On the convergence of adam and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">NTU RGB+ D: A large scale dataset for 3D human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with spatial reasoning and temporal stack learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning to fuse 2D and 3D image cues for monocular body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Márquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Foundations of signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kovačević</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Goyal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<title level="m">Phoneme recognition using time-delay neural networks. Backpropagation: Theory, Architectures and Applications</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Harmonic networks: Deep translation and rotation equivariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">3D human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Stable and symmetric filter convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Diverse generation for multi-agent sports games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Adding attentiveness to the neurons in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">From actemes to action: A strongly-supervised representation for detailed action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
