<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ELoPE: Fine-Grained Visual Classification with Efficient Localization, Pooling and Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Hanselmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Human Language Technology and Pattern Recognition Group</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Human Language Technology and Pattern Recognition Group</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ELoPE: Fine-Grained Visual Classification with Efficient Localization, Pooling and Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The task of fine-grained visual classification (FGVC) deals with classification problems that display a small interclass variance such as distinguishing between different bird species or car models. State-of-the-art approaches typically tackle this problem by integrating an elaborate attention mechanism or (part-) localization method into a standard convolutional neural network (CNN). Also in this work the aim is to enhance the performance of a backbone CNN such as ResNet by including three efficient and lightweight components specifically designed for FGVC. This is achieved by using global k-max pooling, a discriminative embedding layer trained by optimizing class means and an efficient bounding box estimator that only needs class labels for training. The resulting model achieves new best stateof-the-art recognition accuracies on the Stanford cars and FGVC-Aircraft datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fine-grained visual classification (FGVC) refers to classification tasks where the differences between the different categories are very subtle. Examples of such tasks are the classification of bird species or differentiating between different car models. The general appearance of the categories is very similar (e.g. all birds have two wings and a beak, cars typically have four wheels) and as result the inter-class variation is small. On the other hand, the intra-class variation can be quite high (e.g. due to different poses). This makes FGVC a very challenging problem that receives a lot of attention in the research community. State-of-the-art approaches typically involve a backbone CNN such as ResNet <ref type="bibr" target="#b10">[11]</ref> or VGG <ref type="bibr" target="#b24">[25]</ref> that is extended by a method that localizes and attends to specific discriminative regions. These methods can become quite complex and sometimes require multiple passes through the backbone CNN.</p><p>In this work we aim to improve the performance of a given backbone CNN with little increase in complexity and requiring just a single pass through the backbone network. Specifically, we propose the following three steps:</p><p>• Global k-max pooling: For FGVC-models, the final convolutional layer often still has a spatial resolution of I × J (e.g. for a ResNet-50 with 448 × 448 input the resolution is <ref type="bibr">14 × 14)</ref>. A single feature vector describing the image can then be obtained by using global average or global max pooling. However, to approximate part-based recognition, we propose to use global k-max pooling, where the average over the k maximal features is computed.</p><p>• Embedding layer: In a typical setup for face verification tasks, the test subjects (i.e. classes) are not known during training, which means a standard softmax classifier can not be trained. CNNs are therefore often used to train a discriminative embedding space in which face images can be compared efficiently and accurately. The embeddings are learned using specifically designed loss functions such as center loss <ref type="bibr" target="#b29">[30]</ref>, triplet loss <ref type="bibr" target="#b22">[23]</ref> or DFF <ref type="bibr" target="#b9">[10]</ref>. We insert such an embedding layer trained with a loss function based similar to <ref type="bibr" target="#b9">[10]</ref> into the backbone CNN as penultimate layer. We show that this greatly improves the performance of the softmax classifier.</p><p>• Localization module: Using bounding boxes to crop the input images typically improves the performance of the classification model. In order to avoid having to rely on human bounding box annotations we train an efficient bounding box detector that can be applied before the image is processed by the backbone CNN. This localization module is lightweight and trained using only the class labels. Bounding box annotations are not needed.</p><p>We evaluate our model on three popular FGVC datasets from different domains. The first dataset is CUB200-2011 <ref type="bibr" target="#b27">[28]</ref> where the task is the classification of bird species. The second dataset is Stanford cars <ref type="bibr" target="#b15">[16]</ref> where different car models are classified and the third is FGVC-Aircraft <ref type="bibr" target="#b20">[21]</ref> for the classification of different aircraft models. We obtain very competitive results on all three datasets and to the best of our knowledge new best state-of-the-art results for the latter two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related work</head><p>As mentioned in the introduction FGVC has received a lot of attention in the research community. As a result, many different approaches have been proposed. Especially using some form of visual attention has been very popular lately <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref>. The work presented in <ref type="bibr" target="#b26">[27]</ref> is of particular relevance since here also an embedding loss is used. However this loss is not used to train an independent embedding layer but to regulate the defined attention mechanism.</p><p>Spatial transformations that extract the discriminative parts of the input can also be seen as a form of attention. For example, the well known spatial transformer introduced in <ref type="bibr" target="#b12">[13]</ref> is capable of learning global transformations (e.g. affine transformations), but is known to be difficult to train and usually needs a second large network to estimate the transformation parameters. In <ref type="bibr" target="#b32">[33]</ref> a module to learn pixelwise translations is proposed. However, this module is applied very late in the network, possibly due to being reliant on high-level features. As a result only the last layers can profit from the localized input. In <ref type="bibr" target="#b23">[24]</ref> an ensemble of networks is learned sequentially, where each network is trained based on a spatial transformation derived from the previous network. This means that each input image needs to be passed through multiple networks. Our localization module fits into the category of spatial transformations (limited to scale and translation), but it is very lightweight and easy to train while still being able to significantly boost the recognition performance.</p><p>The work presented in <ref type="bibr" target="#b18">[19]</ref> proposes to train a gaussian mixture model based on part proposals provided by selective search. However, this requires a looped training procedure with the EM-algorithm.</p><p>Another popular approach is based on bilinear models <ref type="bibr" target="#b19">[20]</ref> which can lead to issues with efficiency due to very high dimensional features and multi-stream architectures. Also other second-order pooling methods such as the work in <ref type="bibr" target="#b16">[17]</ref> often results in very high dimensional features.</p><p>Other approaches include learning global and patch features in an asymmetric multi-stream architecture <ref type="bibr" target="#b28">[29]</ref>, learning a complex sequence of data augmentation steps from the data <ref type="bibr" target="#b2">[3]</ref>, deep layer aggregation <ref type="bibr" target="#b34">[35]</ref> or the training of very large networks (e.g. 557 million parameters) <ref type="bibr" target="#b11">[12]</ref>. It is also possible to boost the performance by obtaining more training data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Overview</head><p>The goal of this work is to improve the performance of a given backbone CNN (e.g. a ResNet <ref type="bibr" target="#b10">[11]</ref>) with lightweight components that do not require multiple passes through the backbone CNN or significantly higher runtime or memory usage during testing. We achieve this goal by adding three components, a localization module, global k-max pooling and an embedding layer. An overview of the resulting model in the testing stage is given in <ref type="figure" target="#fig_0">Figure 1</ref>. The input image that needs to be classified is forwarded through the localization module which estimates the bounding box of the object in the image and returns a cropped image. This cropped image is then forwarded thought the backbone CNN that contains global k-max pooling and the embedding layer at the later stages. The classification result is then given by a softmax classification layer.</p><p>In the training stage the model is trained jointly with a standard cross-entropy loss L CE applied at the classification layer and a specific loss function L e that is applied at the embeddings layer:</p><formula xml:id="formula_0">L = L CE + λL e<label>(1)</label></formula><p>The localization module is trained separate training step (c.f. Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Global K-Max Pooling</head><p>Often global max pooling (GMP) or global average pooling (GAP) is used between the last convolutional layer and the classification layer of a CNN. These pooling operations allow to break down the spatial dimension of the final convolutional layer and obtain a single vector describing the image. For FGVC it has been shown that part-based approaches (e.g. <ref type="bibr" target="#b36">[37]</ref>) can boost the classification performance. For this reason we propose to use global k-max pooling (GKMP). This two-step pooling procedure first applies k-max pooling <ref type="bibr" target="#b13">[14]</ref> at the last convolutional layer which is followed by an averaging operation over the K selected maximal values in each feature map. This way the network can learn features that activate at the K most important parts of the image (during back-propagation the error gets propagated through the K most important parts instead of just one as with GMP or all of them as with GAP). This could be seen as a very simple form of attention.</p><p>The global k-max pooling layer can be defined as follows. Given an input image x, let y ∈ R D×I×J be the output of the last convolutional layer of a CNN, where y has a spatial resolution of I × J (in this work typically 14 × 14) and contains D feature maps. Further, given a Localization module </p><formula xml:id="formula_1">CNN D × I × J K-Max D × 1 × 1 f (x) class scores K-Max Pooling Embedding space</formula><formula xml:id="formula_2">S d = sorted desc({y d,i,j |i ∈ {1, ..., I}, j ∈ {1, ..., J}})<label>(2)</label></formula><p>The global k-max pooling operation for a specified K is then is then defined as:</p><formula xml:id="formula_3">GKM P (y) d = 1 K K k=1 S d,k<label>(3)</label></formula><p>This definition corresponds to the pooling operation used in <ref type="bibr" target="#b5">[6]</ref>, except in <ref type="bibr" target="#b5">[6]</ref> also the K minimal activations are included. However, preliminary experiments suggested that including the minimal activations does not help the recognition performance. Therefore we use global k-max pooling as defined in Equation <ref type="formula" target="#formula_3">(3)</ref>. Note that if we select K = 1 then Equation <ref type="formula" target="#formula_3">(3)</ref> results in the standard GMP, while a choice of K = I · J leads to GAP. In this work we chose K = 4 in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Global k-max pooling with weighted averaging</head><p>GKMP can be extended by including weights in the averaging operation in Equation (3) similar to <ref type="bibr" target="#b30">[31]</ref>:</p><formula xml:id="formula_4">GKM P (y) d = 1 K K k=1 w k · S d,k<label>(4)</label></formula><p>This formulation extends the network with only K parameters that regulate the contribution of the K maximal activations in each feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Embedding layer</head><p>The embedding layer is inserted between GKMP and the classification layer. The idea is to map the images into an discriminative embedding space, where the distances between images of the same class are small, while the distances between images of different classes are large. This concept is known from face verification <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b9">10]</ref> and metric learning <ref type="bibr" target="#b21">[22]</ref>. Different from those two tasks, we do not compare images directly in the embedding space, but use it as an intermediate layer (more specifically as penultimate layer). The classification is done by a standard classification layer trained on the embedding space with crossentropy. We argue that one of the advantages of this approach is that it can help mitigate the issue of limited training data that we often see in FGVC tasks (see <ref type="table" target="#tab_0">Table 1</ref>).</p><p>The embedding space is trained using a specific loss function L e applied directly to output of this intermediate layer. In this work we use a formulation based on optimizing class means <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b9">10]</ref> since this is easy to integrate into the training and does not require any specific batch construction schemes (unlike tuplet-based losses such as the triplet loss <ref type="bibr" target="#b22">[23]</ref> or the NPair loss <ref type="bibr" target="#b25">[26]</ref>). For each class c a feature vector is computed (and updated online during training) that describes the class mean µ c within the embedding space. The goal of the loss function L e is to minimize the distance of each image within a batch to its respective class mean, while maximizing the distance between means of different classes.</p><p>The loss function is composed of two parts:</p><formula xml:id="formula_5">L e = L w + L b<label>(5)</label></formula><p>The first part L w is the within-class (intra-class) loss that minimizes the distances of the images to their class means, while the second part L b is the between-class (inter-class) loss that maximizes the distances between class means.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Within-class loss</head><p>We use the same formulation for the within-class loss as in <ref type="bibr" target="#b9">[10]</ref> (which is derived from the center-loss proposed in <ref type="bibr" target="#b29">[30]</ref>) including an additional l2-normalization. Given classes c ∈ {1, ..., C} and a batch of images x n with n ∈ {1, ..., N } let f (x n ) be the normalized output of the embedding layer. Assuming we are currently in training iteration t the first step is to update the class means using the data points in the current batch and the class means from the previous iteration t − 1</p><formula xml:id="formula_6">µ t c = µ t−1 c − α∆µ t−1 c<label>(6)</label></formula><p>where the hyper-parameter α can be considered the learning rate of the class means. The term ∆µ t−1 c is defined as</p><formula xml:id="formula_7">∆µ t−1 c = N n=1 δ(c n , c)(µ t−1 cn − f (x n )) 1 + N n=1 δ(c n , c)<label>(7)</label></formula><p>and δ(c n , c) is the Kronecker-function:</p><formula xml:id="formula_8">δ(c n , c) = 1 c = c n 0 c = c n<label>(8)</label></formula><p>Note that this update creates a functional dependence between the class means and their corresponding images within the batch which has to be considered during backpropagation <ref type="bibr" target="#b9">[10]</ref>. The within-class loss function is then defined as</p><formula xml:id="formula_9">L w = 1 2N N n=1 f (x n ) − µ t cn 2 2 (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Between-class loss</head><p>The between-class loss maximizes the distances between the updated class means:</p><formula xml:id="formula_10">L b = γ 4|P | (k,c)∈P max m − µ t k − µ t c 2 2 , 0 2<label>(10)</label></formula><p>The terms µ t k and µ t c are the new class means updated with the features f (x n ) from the current batch as computed in Equation <ref type="bibr" target="#b5">(6)</ref>. The margin m defines a threshold for the distances to be penalized and γ controls the contribution of the between-class part to the embedding loss L e . The set P with cardinality |P | contains all class-pairs in the current batch.</p><p>To see why squaring the maximum in Equation <ref type="formula" target="#formula_0">(10)</ref> is important we have a look at the gradient with respect to f (x n ):</p><formula xml:id="formula_11">∂L b ∂f (x n ) = (k,c)∈P 0 m ≤ µ t k − µ t c 2 2 grad(m, µ t k , µ t c ) otherwise<label>(11)</label></formula><p>with</p><formula xml:id="formula_12">grad(m, µ t k , µ t c ) = (m − µ t k − µ t c 2 2 ) d(m,µ t k ,µ t c ) ∂ ∂f (x n ) γ 2|P | µ t k − µ t c 2 2<label>(12)</label></formula><p>and</p><p>The squared maximization leads to the appearance of the distance d(m, µ t k , µ t c ) in the gradient. As a result the gradient gets larger if the distance between two class means gets smaller which encourages the model to focus more on improving the distance of class-pairs with very close means. This should help to reduce classification errors due to the confusion of images from class-pairs with close class means.</p><p>While the formulation of the between-class loss defined above is close to <ref type="bibr" target="#b9">[10]</ref>, the appearance of d(m, µ t k , µ t c ) in the gradient is a key difference, since in <ref type="bibr" target="#b9">[10]</ref> the maximization in the between-class loss is not squared. In addition there are two more differences. First the centers are based on l2normalized features, which makes the choice for the margin m easier, since distances between l2-normalized vectors are restricted by a certain range. The second difference is that in <ref type="bibr" target="#b9">[10]</ref> the set P contains a sampled subset of all class-pairs in the current batch. Since we work with much smaller batch sizes (see Section 6) compared to <ref type="bibr" target="#b9">[10]</ref>, we use all pairs within a batch.</p><p>The embedding layer as defined above is also closely related to the attention regulation proposed in OSME-MAMC <ref type="bibr" target="#b26">[27]</ref>. The latter uses a metric learning loss to learn correlations between the output of different attention branches. If the multiple attention branches were replaced by a single fully connected layer then this method would be equivalent to training an embedding layer with the NPair loss <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Localization module</head><p>It can be observed that cropping the images based on bounding boxes of the objects that need to be recognized can improve the classification performance. Since we do not want to use any annotations apart from the class labels in training (and of course no annotations at all during testing) we need a way to obtain these localizations without any additional annotations if we want to profit from the increased performance the localizations can provide. We design such a localization module based on the following observations.</p><p>The final convolutional layer of a trained classification model typically contains higher activations in positions corresponding the discriminative areas of the image (see <ref type="figure" target="#fig_1">Figure 2)</ref>. By computing the mean over all feature maps a quite accurate heat map can be computed for the object in the image. This heat map can the be used to find the boundaries of the object within the final layer. A similar observation has been made in <ref type="bibr" target="#b37">[38]</ref>, but in contrast to <ref type="bibr" target="#b37">[38]</ref> we do not consider class-specific heat maps due to the small inter-class variations in FGVC.</p><p>The downside with this approach is that we obtain these bounding boxes only at the final layer while the full image needs to be forwarded through all previous layers. One option would be to apply ROI-Pooling after the final layer based on the estimated bounding box. However, this would mean that only the fully connected layers following the last convolutional layer would profit from focusing on the discriminative area of the image. All other layers still have to operate on the full image and we can not fully exploit the potential from using the bounding boxes (see <ref type="figure" target="#fig_4">Figure 5</ref>). An alternative would be to pass the image through the full backbone network, extract the bounding boxes and then train a second, more precise model based on the bounding boxes (similar to <ref type="bibr" target="#b23">[24]</ref>). However, with this approach the image would have to be passed through a full network twice, one pass to obtain the bounding box and a second pass for classification. This is not very efficient. To avoid such a multipass procedure we propose to train a very lightweight localization module that predicts the bounding boxes and is integrated into the backbone network such that an image can be processed in one pass.</p><p>The architecture of the localization module is equivalent to the first few layers of a ResNet-50 (initialized from the trained classification model) until the end of the first residual block including an initial down-sampling layer that resizes the input to a spatial resolution of 64×64. The module has only 220000 parameters and can be added to the classification model without taking up much runtime or memory. The output is of the size 1 × I × J, the same as the mean of the last convolutional layer of the trained classification model. The localization module is trained by feeding an input image through the trained classification model and the localization module. The outputs are compared using the smooth L1 loss <ref type="bibr" target="#b8">[9]</ref> (see <ref type="figure" target="#fig_2">Figure 3</ref>). During back-propagation the weights of the trained classification model are fixed and only the localization module is trained. This way the localization module learns to directly predict the heat maps.</p><p>Examples of the estimated heat maps are given in Figure  Name #Train #Test #Classes images images CUB200-2011 <ref type="bibr" target="#b27">[28]</ref> 5994 5794 200 Stanford cars <ref type="bibr" target="#b15">[16]</ref> 8144 8041 196 FGVC Aircraft <ref type="bibr" target="#b20">[21]</ref> 6667 3333 100 4. The localization module is able to predict the heat maps very accurately, even though less focused on a specific part of the bird (especially in the second and third row).</p><p>To obtain the final bounding box we we process the heat map using min-max normalization and binarization based on a given threshold τ . The bounding box is then the smallest rectangle containing all pixels where the heat map has a value that is greater than τ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental evaluation</head><p>We evaluate our proposed approach on three popular datasets for fine-grained classification, CUB200-2011 <ref type="bibr" target="#b27">[28]</ref> for bird-species classification, Stanford cars <ref type="bibr" target="#b15">[16]</ref> for the classification of car models and FGVC-Aircraft <ref type="bibr" target="#b20">[21]</ref> for the classification of airplane models. The statistics of the three datasets are given in <ref type="table" target="#tab_0">Table 1</ref>. On all datasets we use only the class labels annotations. We do not use any additional annotations such as bounding boxes or part annotations.</p><p>As backbone CNN we select ResNet <ref type="bibr" target="#b10">[11]</ref> where we try the two variants ResNet-50 and ResNet-101 (the analysis in Section 6.2 and 6.3 is done using ResNet-50 as backbone CNN.). The models are pretrained on the ImageNet <ref type="bibr" target="#b4">[5]</ref> from which we remove all images that overlap with the test sets of the three FGVC tasks used for this evaluation.</p><p>Since our added components are all lightweight and do not occupy much memory we are able to train both ResNet variants on a single NVIDIA GeForce R GTX 1080 Ti GPU with 11GB memory with a batch-size of 14 and input images with the spatial resolution 448 × 448. We use this batch-size and input resolution for all experiments. The models are trained with standard back-propagation for 90 epochs with momentum of 0.9 and weight decay of 0.001. The starting learning rate is 0.003 which is reduced by a factor of 10 after 30 epochs. The weighted average pooling (see Section 3.1) is applied as finetuning step for 30 more epochs. The approach is implemented using the torch7 framework <ref type="bibr" target="#b1">[2]</ref>.</p><p>There is a number of hyper-parameters to set for our approach. We perform a very limited search for the hyperparameters on CUB200-2011 using a validation set separated from the training images. The parameters are quite robust and we can use the same set of hyper-parameters in all other experiments. The threshold τ for the localization module is the only exception, where we use a slightly smaller value on the Stanford cars and the FGVC-Aircraft dataset. The exact values for the hyper-parameters are given in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>In Section 6.4, 6.5 and 6.6 we compare to state-of-theart approaches using a similar experimental setting, specifically with the same training data (ImageNet and training set of the FGVC task at hand) and no bounding box or part annotations. Note that for some datasets better results can be achieved by acquiring large amounts of additional training data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>FLOPs Parameters Baseline 1.648 × 10 10 24M Ours 1.654 × 10 10 25M <ref type="table">Table 3</ref>. Computational efficiency comparison between baseline ResNet-50 (input 1x3x448x448) and our approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy[%] GoogleNet-GAP <ref type="bibr" target="#b37">[38]</ref> 41.0 ResNet-50 mean feature map 58.6 Localization module 68.9 <ref type="table">Table 4</ref>. Accuracy of localization if intersection over union (IoU) is at least 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Computational efficiency</head><p>A comparison of the computational efficiency between a baseline ResNet-50 and our approach with ResNet-50 as backbone is given in <ref type="table">Table 3</ref>. With the typical input resolution of 448 × 448 the additional complexity in terms of FLOPs (multiply-adds) caused by our approach is very small (one reason is that the localization module operates on low resolution input (64 × 64)). There is also no large increase in parameters highlighting the efficiency of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Localization module</head><p>In <ref type="figure" target="#fig_4">Figure 5</ref> we analyze the effect on the recognition performance if the image or feature maps are cropped based on ground-truth bounding boxes (ROI-Pooling). We can observe that the earlier the ROI-Pooling is applied the better is the recognition accuracy, confirming our expectation in Section 5.</p><p>The accuracy of the bounding boxes can be calculated by counting a bounding box as correct if the intersection over union (IoU) with the ground truth bounding box is at least 0.5. We can observe in <ref type="table">Table 4</ref> that using the mean over baseline ResNet-50 feature maps of the last convolutional layer already achieves a better accuracy then what is reported in <ref type="bibr" target="#b37">[38]</ref>. However, on top of being much more efficient the localization module is also even more accurate. We argue that this is due to the slightly more general heatmaps generated by the localization module as a result of the approximation (see <ref type="figure" target="#fig_3">Figure 4</ref>).</p><p>In <ref type="figure" target="#fig_5">Figure 6</ref> we show qualitative results of the bounding boxes estimated for validation images by the localization module. We can observe that the localization module is able to find quite accurate bounding boxes. In some cases (right column of <ref type="figure" target="#fig_5">Figure 6</ref>) the localization module over-estimates the bounding box due to other objects in the image such as a branch or a distracting background. Another interesting reason for over-estimated bounding boxes can be multiple instances of the given FGVC domain such as two different airplane models in the same image. However, in each of these cases the classifier still has a good chance of finding the correct class, since the objects are still present in the cropped image. <ref type="table">Table 5</ref> shows how the components contribute to the recognition performance. The two baseline results with GAP and GMP include a fully connected layer with dimension 512 before the classification layer. We can observe in <ref type="table">Table 5</ref> that adding an embedding loss L e and adding the localization module leads to the largest boost in performance at about 2% absolute (86.9%, using ground-truth bounding boxes yields 87.5%). The addition of global k-max pooling, weighted average finetuning and the full embedding loss compared to only the within-class part lead to an improvement of about 0.5% each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Ablation study</head><p>The effect of the value selected for K in GKMP is illustrated in <ref type="figure">Figure 7</ref>. The special case K = 1 is equivalent to GMP and K = 196 is equivalent to GAP (the spatial dimension of the last convolutional layer is 14 × 14). We can observe that a value around 4 seems to be optimal. The improvement compared to GMP is only 0.5% absolute, but using a value larger than 1 also enables us to apply the weighted averaging, which gives another performance boost (see <ref type="table">Table 5</ref>).  <ref type="figure">Figure 7</ref>. Accuracies for different values of K in K-max pooling on CUB200-2011.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">CUB200-2011</head><p>In <ref type="table">Table 6</ref> we compare our approach to other state-ofthe-art methods. In addition to the results reported in <ref type="table">Table 5</ref> we also evaluate our method with ResNet-101 as backbone network. This includes all components (GKMP with weighted average pooling, embedding layer with full embedding loss, localization module). Our best result is with 88.5% very competitive, even though a little less accurate then the best state-of-the-art result is reported in <ref type="bibr" target="#b31">[32]</ref> and <ref type="bibr" target="#b7">[8]</ref>. However, these involve recurrent neural networks which can be computationally expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Stanford cars</head><p>The results on the Stanford cars dataset are given in <ref type="table">Table  7</ref>. Here we run the experiment with ResNet-50 and ResNet-101 with all components included. As mentioned earlier, we use the same hyper-parameters as for CUB200-2011 (apart from the threshold τ ). Again, our approach achieves a very competitive result. In fact, to the best of our knowledge, the accuracy of 95.0% is the best result, even though by a small margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">FGVC-Aircraft</head><p>Similar to the Stanford cars dataset we use all components introduced in the previous sections and the same hyper-parameters as in Stanford cars. Also on FGVC-Aircraft <ref type="table">(Table 8)</ref> we can report a very competitive result which is again to the best of our knowledge the best stateof-the-art result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this work we presented three efficient methods to improve the classification performance of a backbone CNN for fine-grained visual classification. Specifically, we propose a lightweight localization module that relies only on class label annotations during training. We showed that even though the localization module can find reliable bounding boxes and significantly boost the recognition performance. Additionally we propose to use global k-max pooling to ob-  <ref type="table">Table 5</ref>. Results on CUB200-2011 with a ResNet-50. The notation Le = Lw means the embedding layer is trained only with the withinclass part of the embedding loss, while Le = Lw + L b means the embedding layer is trained with the full embedding loss. Weighted average refers to Section 3.1. For comparison we include results obtained with <ref type="bibr" target="#b29">[30]</ref> and <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone CNN Accuracy[%] STN <ref type="bibr" target="#b12">[13]</ref> BN-Inception 84.1 MA-CNN <ref type="bibr" target="#b36">[37]</ref> VGG-19 86.5 GMM <ref type="bibr" target="#b18">[19]</ref> VGG-19 86.3 Spatial RNN <ref type="bibr" target="#b31">[32]</ref> M-Net/D-Net 89.7 Stacked LSTM <ref type="bibr" target="#b7">[8]</ref> GoogleNet 90.4 DLA <ref type="bibr" target="#b34">[35]</ref> DLA-102 85.1 FAL <ref type="bibr" target="#b32">[33]</ref> ResNet-50 84.2 DT-RAM <ref type="bibr" target="#b17">[18]</ref> ResNet-50 86.0 ISE <ref type="bibr" target="#b23">[24]</ref> ResNet-50 87.2 DFL-CNN <ref type="bibr" target="#b28">[29]</ref> ResNet-50 87.4 NTS-Net <ref type="bibr" target="#b33">[34]</ref> ResNet-50 87.5 DCL <ref type="bibr" target="#b0">[1]</ref> ResNet-50 87.8 OSME-MAMC <ref type="bibr" target="#b26">[27]</ref> ResNet-101 86.5 iSQRT-COV <ref type="bibr" target="#b16">[17]</ref> ResNet-101 88.7 Ours</p><p>ResNet-50 87.4 Ours</p><p>ResNet-101 88.5 <ref type="table">Table 6</ref>. Comparison of our approach with other state-of-the-art methods on CUB200-2011.</p><p>tain a global vector describing the image. This approximates part-based modeling and can further be improved by learning weights to regulate the contribution of the maximal values in each feature map. Finally, we project the image descriptor into a discriminative embedding space from which the classification layer makes the classification. As an intermediate layer of the full classification network the embedding space is trained jointly with the full network and a specific loss function that optimizes class means. We evaluate our approach on three popular FGVC tasks and achieve competitive results on all three. In fact, on Stanford cars and FGVC-Aircraft we can report new best classification accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone CNN Accuracy[%] MA-CNN <ref type="bibr" target="#b36">[37]</ref> VGG-19 92.8 GMM <ref type="bibr" target="#b18">[19]</ref> VGG-19 93.5 DFL-CNN <ref type="bibr" target="#b28">[29]</ref> VGG-16 93.8 Spatial RNN <ref type="bibr" target="#b31">[32]</ref> M-Net/D-Net 93.4 DLA <ref type="bibr" target="#b34">[35]</ref> DLA-X-60-C 94.1 DT-RAM <ref type="bibr" target="#b17">[18]</ref> ResNet-50 93.1 ISE <ref type="bibr" target="#b23">[24]</ref> ResNet-50 94.1 NTS-Net <ref type="bibr" target="#b33">[34]</ref> ResNet-50 93.9 DCL <ref type="bibr" target="#b0">[1]</ref> ResNet-50 94.5 GPipe <ref type="bibr" target="#b11">[12]</ref> AmoebaNet-B 94.8 AutoAugm <ref type="bibr" target="#b2">[3]</ref> Inception-v4 94.8 OSME-MAMC <ref type="bibr" target="#b26">[27]</ref> ResNet-101 93.0 iSQRT-COV <ref type="bibr" target="#b16">[17]</ref> ResNet-101 93.3 Ours</p><p>ResNet-50 94.5 Ours</p><p>ResNet-101 95.0 <ref type="table">Table 7</ref>. Comparison of our approach with other state-of-the-art methods on Stanford cars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone CNN Accuracy[%] MA-CNN <ref type="bibr" target="#b36">[37]</ref> VGG-19 89.9 GMM <ref type="bibr" target="#b18">[19]</ref> VGG-19 90.5 DFL-CNN <ref type="bibr" target="#b28">[29]</ref> VGG-16 92.0 Spatial RNN <ref type="bibr" target="#b31">[32]</ref> M-Net/D-Net 88.4 DLA <ref type="bibr" target="#b34">[35]</ref> DLA-X-60 92.9 ISE <ref type="bibr" target="#b23">[24]</ref> ResNet-50 90.9 NTS-Net <ref type="bibr" target="#b33">[34]</ref> ResNet-50 91.4 DCL <ref type="bibr" target="#b0">[1]</ref> ResNet-50 93.0 GPipe <ref type="bibr" target="#b11">[12]</ref> AmoebaNet-B 92.9 AutoAugm <ref type="bibr" target="#b2">[3]</ref> Inception-v4 92.7 iSQRT-COV <ref type="bibr" target="#b16">[17]</ref> ResNet-101 91.4 Ours</p><p>ResNet-50 93.4 Ours</p><p>ResNet-101 93.5 <ref type="table">Table 8</ref>. Comparison of our approach with other state-of-the-art methods on FGVC-Aircraft.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview of the proposed model including a lightweight localization module, global k-max pooling and an embedding layer. specific d ∈ {1, ..., D} the sorted vector S d contains the values at y d sorted in descending order:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Min-max normalized activations of the last convolutional layer of a trained model. The three images in the middle are the activations in a specific feature map, while the last image shows the mean over all feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Training procedure for the localization module: The localization module learns to directly predict the heat maps by minimizing the mean squared error with the heat maps generated by the backbone CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>True mean (middle) of the trained classification model compared to estimated mean (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Accuracies for ROI-Pooling with ground-truth bounding boxes applied at different depths of a CNN on CUB200-2011 (ResNet-50 with GAP and no embedding learning).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Examples of bounding boxes detected by the localization module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Datasets used for the evaluation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Hyper-parameters used in the experiments. The threshold τ is 0.3 on CUB200-2011 and 0.2 on the other two datasets.</figDesc><table><row><cell>Hyper-</cell><cell>Reference</cell><cell>Value</cell></row><row><cell>parameter</cell><cell></cell><cell></cell></row><row><cell>α</cell><cell>Equation (6)</cell><cell>0.5</cell></row><row><cell>λ</cell><cell>Equation (1)</cell><cell>2.0</cell></row><row><cell>γ</cell><cell>Equation (10)</cell><cell>16.0</cell></row><row><cell>m</cell><cell>Equation (10)</cell><cell>0.75</cell></row><row><cell>K</cell><cell>Equation (3)</cell><cell>4</cell></row><row><cell>τ</cell><cell>Section (5)</cell><cell>0.3/0.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Destruction and construction learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS workshop</title>
		<imprint>
			<publisher>Granada</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale fine-grained categorization and domain-specific transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4109" to="4118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weldon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly supervised complementary parts models for fine-grained image classification from the bottom up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on computer vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep fisher faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hanselmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1811.06965</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Comparison of midlevel feature coding approaches and pooling strategies in visual concept detection. Computer vision and image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="479" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of noisy data for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="301" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards faster training of global covariance pooling networks by iterative matrix square root normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamic computational time for visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1199" to="1209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fine-grained image classification with gaussian mixture layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="53356" to="53367" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on computer vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05939</idno>
		<title level="m">Metric learning with adaptive density discrimination</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Increasingly specialized ensemble of convolutional neural networks for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Natale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Messelodi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="594" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class npair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-attention multi-class constraint for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="805" to="821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>- port CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Re</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a cnn for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4148" to="4157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning weighted geometric pooling for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3805" to="3809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep attention-based spatially recursive networks for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1791" to="1802" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attend and align: Improving deep representations with feature alignment layer for person retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2148" to="2153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Diversified visual attention networks for fine-grained object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1245" to="1256" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5209" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
