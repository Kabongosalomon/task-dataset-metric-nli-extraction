<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attentive Pooling Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dos</forename><surname>Cicero</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CICERONS@US.IBM.COM</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CICERONS@US.IBM.COM</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">MINGTAN@US.IBM.COM</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
							<affiliation key="aff2">
								<address>
									<postBox>BINGXIA@US.IBM.COM</postBox>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">ZHOU@US.IBM.COM IBM Watson, T.J. Watson Research Center</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attentive Pooling Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose Attentive Pooling (AP), a two-way attention mechanism for discriminative model training. In the context of pair-wise ranking or classification with neural networks, AP enables the pooling layer to be aware of the current input pair, in a way that information from the two input items can directly influence the computation of each other's representations. Along with such representations of the paired inputs, AP jointly learns a similarity measure over projected segments (e.g. trigrams) of the pair, and subsequently, derives the corresponding attention vector for each input to guide the pooling. Our two-way attention mechanism is a general framework independent of the underlying representation learning, and it has been applied to both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) in our studies. The empirical results, from three very different benchmark tasks of question answering/answer selection, demonstrate that our proposed models outperform a variety of strong baselines and achieve state-of-the-art performance in all the benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural networks (NN) with attention mechanisms have recently proven to be successful at different computer vision (CV) and natural language processing (NLP) tasks such as image captioning <ref type="bibr" target="#b20">(Xu et al., 2015)</ref>, machine translation <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref> and factoid question answering . However, most recent work on neural attention models have focused on one-way attention mechanisms based on recurrent neural networks designed . for generation tasks.</p><p>Another important family of machine learning tasks are centered around pair-wise ranking or classification, which have a broad set of applications, including but not limited to, question answering, entailment, paraphrasing and any other pair-wise matching problems. The current state-ofthe-art models usually include NN-based representation for the input pair, followed by a discriminative ranking or classification models. For example, a convolution (or a RNN) and a max-pooling is used to independently construct distributed vector representations of the input pair, followed by a large-margin training <ref type="bibr" target="#b8">(Hu et al., 2014;</ref><ref type="bibr" target="#b19">Weston et al., 2014;</ref><ref type="bibr" target="#b12">Shen et al., 2014;</ref><ref type="bibr" target="#b2">dos Santos et al., 2015)</ref>.</p><p>The key contribution of this work is that we propose Attentive Pooling (AP), a two-way attention mechanism, that significantly improves such discriminative models' performance on pair-wise ranking or classification, by enabling a joint learning of the representations of both inputs as well as their similarity measurement.</p><p>Specifically, AP enables the pooling layer to be aware of the current input pair, in a way that information from the two input items can directly influence the computation of each other's representations. The main idea in AP consists of learning a similarity measure over projected segments (e.g. trigrams) of the two items in the input pair, and using the similarity scores between the segments to compute attention vectors in both directions. Next, the attention vectors are used to perform pooling.</p><p>There are a few key benefits of our model.</p><p>• Thanks to the two-way attention, our model projects the paired inputs, even though they may not be always semantically comparable for some applications (e.g., questions and answers in question answering), into a common representation space that they can be compared in a more plausible way.</p><p>• Our model is effective in matching pairs of inputs with significant length variations.</p><p>• The two-way attention mechanism is independent of the underlying representation learning. For example, AP can be applied to both CNNs and RNNs, which is in contrast to the one-way attention used in the generation models mostly based on recurrent nets.</p><p>In this work, we perform an extensive number of experiments on applying attentive pooling CNNs (AP-CNN) and biLSTMs (AP-biLSTM) for the answer selection task. In this task, given a question q and an candidate answer pool P = {a 1 , a 2 , · · · , a p } for this question, the goal is to search for and select the candidate answer a ∈ P that correctly answers q. We perform experiments with three publicly available benchmark datasets, which vary in data scale, complexity and length ratios between question and answers: InsuranceQA, TREC-QA and WikiQA. For the three datasets, AP-CNN and AP-biLSTM respectively outperform the CNN and the biLSTM that do not use attention.</p><p>Additionally, AP-CNN achieves state-of-the-art results for the three datasets.</p><p>Our experimental results also demonstrate that attentive pooling makes the CNN more robust to large input texts. This is an important finding, since recent work have demonstrated that, in the context of semantically equivalent question retrieval, CNN based representations do not scale well with the size of the input text (dos . Additionally, as AP-CNN does not rely only on the final vector representation to capture interactions between the input question and answer, it requires much less convolutional filters than the regular CNN. It means that AP-CNN-based representations are more compact, which can help to speed up the training process.</p><p>Although we demonstrate experimental results for NLP tasks only, AP is a general method that can be also applied to different types of NNs that perform matching of two inputs. Therefore, we believe that AP can be useful for different applications, such as computer vision and bioinformatics.</p><p>This paper is organized as follows. In Section 2, we describe two NN architectures for answer selection that have been recently proposed in the literature. In Section 3, we detail the attentive pooling approach. In Section 4, we discuss some related work. Sections 5 and 6 detail our experimental setup and results, respectively. In Section 7 we present our final remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Neural Networks for Answer Selection</head><p>Different neural network architectures have been recently proposed to perform matching of semantically related text segments <ref type="bibr" target="#b25">(Yu et al., 2014;</ref><ref type="bibr" target="#b8">Hu et al., 2014;</ref><ref type="bibr" target="#b2">dos Santos et al., 2015;</ref><ref type="bibr" target="#b15">Wang &amp; Nyberg, 2015;</ref><ref type="bibr" target="#b11">Severyn &amp; Moschitti, 2015;</ref><ref type="bibr" target="#b14">Tan et al., 2015)</ref>. In this section we briefly review two NN architectures that have previously been applied to the answer selection task: QA-CNN <ref type="bibr" target="#b4">(Feng et al., 2015)</ref> and QA-biLSTM <ref type="bibr" target="#b14">(Tan et al., 2015)</ref>. Given a pair (q, a) consisting of a question q and a candidate answer a, both networks score the pair by first computing fixed-length independent continuous vector representations r q and r a , and then computing the cosine similarity between these two vectors.</p><p>In <ref type="figure" target="#fig_0">Figure 1</ref> we present a joint illustration of these two neural networks. The first layer in both QA-CNN and QA-biLSTM transforms each input word w into a fixed-size real-valued word embedding r w ∈ R d . Word embeddings (WEs) are encoded by column vectors in an embedding matrix W 0 ∈ R d×|V | , where V is a fixed-sized vocabulary and d is the dimention of the word embeddings. Given the input pair (q, a), where the question q contains M tokens and the candidate answer a contains L tokens, the output of the first layer consists of two sequences of word embeddings q emb = {r w1 , ..., r w M } and a emb = {r w1 , ..., r w L }.</p><p>Next, QA-CNN and QA-biLSTM use different approaches to process these sequences. While QA-CNN process both q emb and a emb using a convolution, QA-biLSTM uses a Bidirectional Long Short-Term Memory RNN <ref type="bibr" target="#b7">(Hochreiter &amp; Schmidhuber, 1997)</ref> to process these sequences. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Convolution</head><p>Given the sequence q emb = {r w1 , ..., r w M }, let us define the matrix Z q = [z 1 , ..., z M ] as a matrix where each column contains a vector z m ∈ R dk that is the concatenation of a sequence of k word embeddings centralized in the mth word of the question. The output of the convolution with c filters over the question q is computed as follows:</p><formula xml:id="formula_0">Q = W 1 Z q + b 1<label>(1)</label></formula><p>where each column m in Q ∈ R c×M contains features extracted in a context window around the m-th word of q. The matrix W 1 and the vector b 1 are parameters to be learned. The number of convolutional filters c, and the size of the word context window k are hyper-parameters to be chosen by the user.</p><p>In a similar manner, and using the same NN parameters W 1 and b 1 , we compute A ∈ R c×L , the output of the convolution over the candidate answer a.</p><formula xml:id="formula_1">A = W 1 Z a + b 1 (2) 2.2. Bidirectional LSTM (biLSTM)</formula><p>Our LSTM implementation is similar to the one in <ref type="bibr" target="#b5">(Graves et al., 2013)</ref> with minor modification. Given the sequence q emb = {r w1 , ..., r w M }, the hidden vector h(t) (with size H) at the time step t is updated as follows:</p><formula xml:id="formula_2">i t = σ(W i r wt + U i h(t − 1) + b i ) (3) f t = σ(W f r wt + U f h(t − 1) + b f ) (4) o t = σ(W o r wt + U o h(t − 1) + b o ) (5) C t = tanh(W m r wt + U m h(t − 1) + b m ) (6) C t = i t * C t + f t * C t−1 (7) h t = o t * tanh(C t )<label>(8)</label></formula><p>In the LSTM architecture, there are three gates (input i, forget f and output o), and a cell memory vector c. σ is the sigmoid function. The input gate can determine how incoming vectors r wt alter the state of the memory cell. The output gate can allow the memory cell to have an effect on the outputs. Finally, the forget gate allows the cell to remember or forget its previous state. W ∈ R H×d , U ∈ R H×H and b ∈ R H×1 are the network parameters.</p><p>Single direction LSTMs suffer a weakness of not utilizing the contextual information from the future tokens. Bidirectional LSTM utilizes both the previous and future context by processing the sequence on two directions, and generate two independent sequences of LSTM output vectors. One processes the input sequence in the forward direction, while the other processes the input in the reverse direction.</p><p>The output at each time step is the concatenation of the two output vectors from both directions, ie.</p><formula xml:id="formula_3">h t = − → h t ← − h t .</formula><p>We define c = 2 × H for the notation consistency with the previous subsection. After computing the hidden state h t for each time step t, we generate the matrices Q ∈ R c×M and A ∈ R c×L , where the j-th column in Q (A) corresponds to j-th hidden state h j that is computed by the biLSTM when processing q (a). The same network parameters are used to process both questions and candidate answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Scoring and Training Procedure</head><p>Given the matrices Q and A, we compute the vector representations r q ∈ R c and r a ∈ R c by applying a column-wise max-pooling over Q and A, followed by a non-linearity. Formally, the j-th elements of the vectors r q and r a are compute as follows:</p><formula xml:id="formula_4">[r q ] j = tanh max 1&lt;m&lt;M [Q j,m ] (9) [r a ] j = tanh max 1&lt;l&lt;L [A j,l ]<label>(10)</label></formula><p>The last layer in QA-CNN and QA-biLSTM scores the input pair (q,a) by computing the cosine similarity between the two representations:</p><formula xml:id="formula_5">s(q, a) = r q .r a r q r a<label>(11)</label></formula><p>Both networks are trained by minimizing a pairwise ranking loss function over the training set D. The input in each round is two pairs (q, a + ) and (q, a − ), where a + is a ground truth answer for q, and a − is an incorrect answer. As in <ref type="bibr" target="#b19">(Weston et al., 2014;</ref><ref type="bibr" target="#b8">Hu et al., 2014)</ref>, we define the training objective as a hinge loss:</p><formula xml:id="formula_6">L = max{0, m − s θ (q, a + ) + s θ (q, a − )}<label>(12)</label></formula><p>where m is constant margin, s θ (q, a + ) and s θ (q, a − ) are scores generated by the network with parameter set θ. During training, for each question we randomly sample 50 negative answers from the entire answer set, but only use the one with the highest score to update the model.</p><p>We use stochastic gradient descent (SGD) to minimize the loss function with respect to θ. The backpropagation algorithm is used to compute the gradients of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Attentive Pooling Networks for Answer Selection</head><p>Attentive pooling is an approach that enables the pooling layer to be aware of the current input pair, in a way that information from the question q can directly influence the computation of the answer representation r a , and vice versa. The main idea consists of learning a similarity measure over the projected segments in the input pairs, and uses the similarity scores between the segments to compute attention vectors. When AP is applied to CNN, which we call AP-CNN, the network learns the similarity measure over the convolved input sequences. When AP is applied to biLSTM, which we call AP-biLSTM, the network learns the similarity measure over the hidden states produced by the biLSTM when processing the two input sequences. We use a similarity measure that has a bilinear form but followed by a non-linearity.</p><p>In <ref type="figure" target="#fig_1">Fig. 2</ref>, we illustrate the application of AP over the output of the convolution or the biLSTM to construct the representations r q and r a . Consider the input pair (q, a) where the question has size M and the answer has size L 1 . After we compute the matrices Q ∈ R c×M and A ∈ R c×L , either by convolution or biLSTM, we compute the matrix G ∈ R M ×L as follows:</p><formula xml:id="formula_7">G = tanh Q T U A<label>(13)</label></formula><p>where U ∈ R c×c is a matrix of parameters to be learned by the NN. When the convolution is used to compute Q and A, the matrix G contains the scores of a soft alignment between the convolved k-size context windows of q and a. When the biLSTM is used to compute Q and A, the matrix G contains the scores of a soft alignment between the hidden vectors of each token in q and a.</p><p>Next, we apply column-wise and row-wise max-poolings over G to generate the vectors g q ∈ R M and g a ∈ R L , respectively. Formally, the j-th elements of the vectors g q and g a are computed as follows:</p><formula xml:id="formula_8">[g q ] j = max 1&lt;m&lt;M [G j,m ]<label>(14)</label></formula><formula xml:id="formula_9">[g a ] j = max 1&lt;l&lt;L [G l,j ]<label>(15)</label></formula><p>We can interpret each element j of the vector g a as an importance score for the context around the j-th word in the candidate answer a with regard to the question q. Likewise, each element j of the vector g q can be interpreted as the importance score for the context around the j-th word in the question q with regard to the candidate answer a.</p><p>Next, we apply the softmax function to the vectors g q and g a to create attention vectors σ q and σ a . For instance, the j-th element of the vector σ q is computed as follows:</p><formula xml:id="formula_10">[σ q ] j = e [g q ]j 1&lt;l&lt;M e [g q ] l<label>(16)</label></formula><p>1 <ref type="figure" target="#fig_1">In Fig. 2</ref>, q has a size of five and a has a size of seven.</p><p>Finally, the representations r q and r a are computed as the dot product between the attention vectors σ q and σ a and the output of the convolution (or biLSTM) over q and a, respectively:</p><formula xml:id="formula_11">r q = Qσ q (17) r a = Aσ a<label>(18)</label></formula><p>Like in QA-CNN and QA-biLSTM, the final score is also computed using the cosine similarity between r q and r a . We use SGD to train AP-CNN and AP-biLSTM by minimizing the same pairwise loss function used in QA-CNN and QA-biLSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>Traditional work on answer selection have normally used feature engineering, linguistic tools, or external resources <ref type="bibr" target="#b23">(Yih et al., 2013;</ref><ref type="bibr" target="#b16">Wang &amp; Manning, 2010;</ref><ref type="bibr" target="#b17">Wang et al., 2007)</ref>. Recently, deep learning (DL) approaches have been exploited for this task and achieved significant outperformance compared to traditional non-DL methods. For example, in <ref type="bibr" target="#b25">(Yu et al., 2014;</ref><ref type="bibr" target="#b4">Feng et al., 2015;</ref><ref type="bibr" target="#b11">Severyn &amp; Moschitti, 2015)</ref>, the authors generate the representations of questions and answers separately, and score a QA pair using a similarity metric on top of these representations.</p><p>In <ref type="bibr" target="#b15">Wang &amp; Nyberg (2015)</ref>, first a joint feature vectors is learned from a joint long short-term memory (LSTM) model connecting questions and answers, and then the task is converted into a learning-to-rank problem.</p><p>At the same time, attention-based systems have shown very promising results on a variety of NLP tasks, such as machine translation <ref type="bibr" target="#b0">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b13">Sutskever et al., 2014)</ref>, caption generation <ref type="bibr" target="#b20">(Xu et al., 2015)</ref> and factoid question answering . Such models learn to focus their attention to specific parts of their input.</p><p>Some recently-proposed approaches introduce attention mechanisms in the answer selection task. <ref type="bibr" target="#b14">Tan et al. (2015)</ref> developed an attentive reader based on bidirectional long short-term memory, which emphasizes certain part of the answer according to the question embedding. Unlike <ref type="bibr" target="#b14">(Tan et al., 2015)</ref>, in which attention is imposed only on answer embedding generation, AP-CNN and AP-biLSTM consider the interdependence between questions and answers.</p><p>In the context of two-way attention, two very recent work are related to ours. <ref type="bibr" target="#b10">Rocktäschel et al. (2015)</ref>, propose a two-way attention method that is inspired by bidirectional LSTMs that read a sequence and its reverse for improved encoding. Their approach, which is designed for RNNs only, differs in many aspects from the approach described in this work, which can be easily applied for CNNs and RNNs. <ref type="bibr" target="#b24">Yin et al. (2015)</ref> present a two-way attention mechanism that is tailored to CNNs. Some of the main differ- ences between their approach and this work are: (1) they use a simple Euclidean distance to compute the interdependence between the two input texts, while in this work we apply similarity metric learning, which has the potential to learn better ways to measure the interaction between segments of the input items;</p><p>(2) the models in <ref type="bibr" target="#b24">(Yin et al., 2015)</ref> compute the attention vector using sum-pooling over the alignment matrix and use the convolutional outputs updated by the attention as the input for another level of convolutional layer. In this work we use max-pooling over the alignment matrix plus softmax, in order to explicitly create an attention vector that is used to perform the pooling. Experimental results show that such difference yields substantial improvement of performance on WikiQA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>We apply AP-CNN, AP-biLSTM, QA-CNN and QA-biLSTM to three different answer selection datasets: Insur-anceQA, TREC-QA and WikiQA. These datasets contain text of different domains and have different caracteristics. <ref type="table" target="#tab_0">Table 1</ref> presents some statistics about the datasets, including the number of questions in each set, average length of questions (M) and answers (L), average number of candidate answers in the dev/test sets and the average ratio between the lengths of questions and their ground-truth answers.</p><p>InsuranceQA 2 is a recently released large-scale non-factoid QA dataset from the insurance domain. This dataset provides a training set, a validation set, and two test sets. We do not see obvious categorical differentiation between questions of the two test sets. For each question in dev/test sets, there is a set of 500 candidate answers, which include the ground-truth answers and randomly selected negative answers. More details can be found in <ref type="bibr" target="#b4">(Feng et al., 2015)</ref>.</p><p>TREC-QA 3 was created by <ref type="bibr" target="#b17">Wang et al. (2007)</ref> based on Text REtrieval Conference (TREC) QA track (8-13) data. We follow the exact approach of train/dev/test questions selection in <ref type="bibr" target="#b15">(Wang &amp; Nyberg, 2015)</ref>, in which all questions with only positive or negative answers are removed. Finally, we have 1162 training questions, 65 development questions and 68 test questions.</p><p>WikiQA 4 is an open domain question-answering dataset.</p><p>We use the subtask that assumes that there is at least one correct answer for a question. The corresponding dataset consists of 20,360 question/candidate pairs in train, 1,130 pairs in dev and 2,352 pairs in test. We adopt the standard setup of only considering questions that have correct answers for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Word Embeddings</head><p>In order to fairly compare our results with the ones in previous work, we use two different sets of pre-trained word embeddings. For the InsuranceQA dataset, we use the 100dimensional vectors that were trained by <ref type="bibr" target="#b4">Feng et al. (2015)</ref> using word2vec <ref type="bibr" target="#b9">(Mikolov et al., 2013)</ref>. Following <ref type="bibr" target="#b15">Wang &amp; Nyberg (2015)</ref>, <ref type="bibr" target="#b14">Tan et al. (2015)</ref> and <ref type="bibr" target="#b24">Yin et al.(2015)</ref>, for the TREC-QA and the WikiQA datasets we use the 300dimensional vectors that were trained using word2vec and are publicly available on the website of this tool 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Neural Networks Setup</head><p>In <ref type="table" target="#tab_2">Table 2</ref>, we show the selected hyperparameter values, which were tuned using the validation sets. We try to use as much as possible the same hyperparameters for all the three datasets. The size of the word embeddings is different due to the different pre-trained versions that we used for InsuranceQA and the other two datasets. We use a context window of size 3 for InsuranceQA, while we set this parameter to 4 for TREC-QA and WikiQA. Using the selected hyperparameters, the best results are normally achieved using between 15 and 25 training epochs. For AP-CNN, AP-biLSTM and QA-LSTM, we also use a learning rate schedule that decreases the learning rate λ according to the training epoch t. Following dos Santos &amp; Zadrozny (2014), we http://cs.jhu.edu/˜xuchen/packages/ jacana-qa-naacl2013-data-results.tar.bz2 4 The data is obtained from <ref type="bibr" target="#b21">(Yang et al., 2015)</ref> 5 https://code.google.com/p/word2vec/ set the learning rate for epoch t, λ t , using the equation:</p><formula xml:id="formula_12">λ t = λ t .</formula><p>In our experiments, the four NN architectures QA-CNN, AP-CNN, QA-biLSTM and AP-biLSTM are implemented using Theano <ref type="bibr" target="#b1">(Bergstra et al., 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">InsuranceQA</head><p>In <ref type="table" target="#tab_1">Table 3</ref>, we present the experimental results of the four NNs for the InsuranceQA dataset. The results are in terms of accuracy, which is equivalent to precision at top one.</p><p>On the bottom part of this table, we can see that AP-CNN outperforms QA-CNN by a large margin in both test sets, as well as in the dev set. AP-biLSTM also outperforms the QA-biLSTM in all the three sets. AP-CNN and AP-biLSTM have similar performance.</p><p>On the top part of <ref type="table" target="#tab_1">Table 3</ref> we present the results of two state-of-the-art systems for this dataset. In <ref type="bibr" target="#b4">(Feng et al., 2015)</ref>, the authors present a CNN architecture that is similar to QA-CNN, but that uses a different similarity metric instead of cosine similarity. In <ref type="bibr" target="#b14">(Tan et al., 2015)</ref>, the authors use a biLTSM architecture that employs unidirectional attention. Both AP-CNN and AP-biLSTM outperform the state-of-the-art systems. One important characteristic of AP-CNN is that it requires less convolutional filters than QA-CNN. For the Insur-anceQA dataset, AP-CNN uses 10x less filters (400) than QA-CNN (4000). Using 800 filters in AP-CNN produces very similar results as using 400. On the other hand, as also found in <ref type="bibr" target="#b4">(Feng et al., 2015)</ref>, QA-CNN requires at least 2000 filters to achieve more than 60% accuracy on Insur- In figures 3 and 4, we plot the aggregated accuracy of AP-CNN and QA-CNN for answers up to a certain length for the Test1 and Test2 sets, respectively. We can see in both plots that the performance of both system is better for shorter answers. However, while the performance of QA-CNN continues to drop as larger answers are considered, the performance of AP-CNN seems to be stable after reaching a length of ∼90 tokens. These results give support to our hypothesis that attentive pooling helps the CNN to become robust to larger input texts. In <ref type="table" target="#tab_3">Table 4</ref>, we present the experimental results of the four NNs for the TREC-QA dataset. The results are in terms of mean average precision (MAP) and mean reciprocal rank (MRR), which are the metric normally used in previous work with the same dataset. We use the official trec eval scorer to compute MAP and MRR. We can see in <ref type="table" target="#tab_3">Table 4</ref> that AP-CNN outperforms QA-CNN by a large margin in both metrics. AP-biLSTM outperforms the QA-biLSTM, but its performance is not as good as the of AP-CNN.</p><p>On the top part of <ref type="table" target="#tab_3">Table 4</ref> we present the results of three recent work that use TREC-QA as a benchmark. In <ref type="bibr" target="#b15">(Wang &amp; Nyberg, 2015)</ref>, the authors present an LTSM architecture for answer selection. Their best result consists of a combination of LSTM and the BM25 algorithm. In <ref type="bibr" target="#b11">(Severyn &amp; Moschitti, 2015)</ref>, the authors propose an NN architecture where the representations created by a convolutional layer are the input to similarity measure learning. <ref type="bibr" target="#b18">Wang &amp; Ittycheriah (2015)</ref> propose a word-alignment-based method that is suitable for the FAQ-based QA task. AP-CNN outperforms the state-of-the-art systems in both metrics, MAP and MRR. <ref type="table" target="#tab_4">Table 5</ref> shows the experimental results of the four NNs for the WikiQA dataset. Like in the other two datasets, AP-CNN outperforms QA-CNN, and AP-biLSTM outperforms the QA-biLSTM. The difference of performance between AP-CNN and QA-CNN is smaller than the one for the InsuranceQA dataset. We believe it is because the average size of the answers in WikiQA <ref type="formula">(25)</ref> is much smaller  <ref type="formula">(95)</ref>. It is expected that attentive pooling bring more impact to the datasets that have larger answer/question lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">WikiQA</head><p>In <ref type="table" target="#tab_4">Table 5</ref> we also present the results of two recent work that use WikiQA as a benchmark. <ref type="bibr" target="#b21">Yang et al. (2015)</ref>, present a bigram CNN model with average pooling. In <ref type="bibr" target="#b24">(Yin et al., 2015)</ref>, the authors propose an attention-based CNN. In order to make a fair comparison, in <ref type="table" target="#tab_4">Table 5</ref> we include Yin et al.'s result that use word embeddings only 6 . AP-CNN outperforms these two systems in both metrics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Attentive Pooling Visualization</head><p>Figures 5 and 6 depict two heat maps of two test questions from InsuranceQA that were correctly answered by AP-CNN and whose answers are more than 100 words long. The stronger the color of a word in the question (answer), the larger the attention weight in σ q (σ a ) of the trigram centered at that word. As we can see in the pictures, the attentive pooling mechanism is indeed putting more focus on the segments of the answer that have some interaction with the question, and vice-verse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>We present attentive pooling, a two-way attention mechanism for discriminative model training. The main contributions of the paper are: (1) AP is more general than <ref type="bibr">6</ref> Yin et al. <ref type="bibr" target="#b24">(Yin et al., 2015)</ref> report 0.6921(MAP) and 0.7108(MRR) when using handcrafted features in addition to word embeddings. recently proposed two-way attention mechanism because: (a) it learns how to compute interactions between the items in the input pair; and (b) it can be applied to both CNNs and RNNs;</p><p>(2) we demonstrate that AP can be effectively used with CNNs and biLSTM in the context of the answer selection task, using three different benchmark datasets; (3) our experimental results demonstrate that AP helps the CNN to cope with large input texts; (4) we present new state-of-theart results for InsuranceQA and TREC-QA datasets. (5) for the WikiQA dataset our results are the best reported so far for methods that do not use handcrafted features. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Joint illustration of QA-CNN and QA-biLSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Attentive Pooling Networks for Answer Selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Aggregated accuracy for answers up to a certain length in the InsuranceQA Test1 set 6.2. TREC-QA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Aggregated accuracy for answers up to a certain length in the InsuranceQA Test2 set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Attention heat map from AP-CNN for a correctly selected answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Attention heat map from AP-CNN for a correctly selected answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Answer Selection Datasets. DATASET TRAIN DEV TEST AVG. M AVG. L AVG. # CAND. ANS. AVG. L/M</figDesc><table><row><cell cols="4">INSURANCEQA 12887 1000 1800X2</cell><cell>7</cell><cell>95</cell><cell>500</cell><cell>13.8</cell></row><row><cell>TREC-QA</cell><cell>1162</cell><cell>65</cell><cell>68</cell><cell>8</cell><cell>28</cell><cell>38</cell><cell>4.2</cell></row><row><cell>WIKIQA</cell><cell>873</cell><cell>126</cell><cell>243</cell><cell>6</cell><cell>25</cell><cell>9</cell><cell>5.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Accuracy of different systems for InsuranceQA</figDesc><table><row><cell>System</cell><cell cols="3">Dev Test1 Test2</cell></row><row><cell cols="2">(FENG ET AL., 2015) 65.4</cell><cell>65.3</cell><cell>61.0</cell></row><row><cell>(TAN ET AL., 2015)</cell><cell>68.4</cell><cell>68.1</cell><cell>62.2</cell></row><row><cell>QA-CNN</cell><cell>61.6</cell><cell>60.2</cell><cell>56.1</cell></row><row><cell>QA-BILSTM</cell><cell>66.6</cell><cell>66.6</cell><cell>63.7</cell></row><row><cell>AP-CNN</cell><cell>68.8</cell><cell>69.8</cell><cell>66.3</cell></row><row><cell>AP-BILSTM</cell><cell>68.4</cell><cell>71.7</cell><cell>66.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Neural Network Hyper-Parameters</figDesc><table><row><cell cols="2">Hyp. Hyperpar. Name</cell><cell cols="4">AP-CNN QA-CNN AP-biLSTM QA-biLSTM</cell></row><row><cell>d</cell><cell>WORD EMB. SIZE</cell><cell>100/300</cell><cell>100/300</cell><cell>100/300</cell><cell>100/300</cell></row><row><cell>c</cell><cell>CONV. FILTERS / HID.VEC. SIZE</cell><cell>400</cell><cell>4000</cell><cell>141X2</cell><cell>141X2</cell></row><row><cell>k</cell><cell>CONTEXT WINDOW SIZE</cell><cell>3/4</cell><cell>2</cell><cell>1</cell><cell>1</cell></row><row><cell>mbs</cell><cell>MINIBATCH SIZE</cell><cell>20</cell><cell>1</cell><cell>20</cell><cell>20</cell></row><row><cell>m</cell><cell>LOSS MARGIN</cell><cell>0.5</cell><cell>0.009</cell><cell>0.2</cell><cell>0.1</cell></row><row><cell>λ</cell><cell>INIT. LEARNING RATE</cell><cell>1.1</cell><cell>0.05</cell><cell>1.1</cell><cell>1.1</cell></row><row><cell cols="3">anceQA. AP-CNN needs less filters because it does not</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">rely only on the final vector representation to capture in-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">teractions between the input question and answer. As a re-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">sult, although AP-CNN has a more complex architecture,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">its training time is two times faster than QA-CNN. Using</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">a Tesla K20Xm, our Theano implementation of AP-CNN</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">takes about 16 minutes to complete one epoch (training +</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">inference over validation set) for InsuranceQA, which con-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">sists on processing 1.5 million text segments.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Performance of different systems for TREC-QA</figDesc><table><row><cell>System</cell><cell>MAP</cell><cell>MRR</cell></row><row><cell>WANG &amp; NYBERG (2015)</cell><cell cols="2">0.7134 0.7913</cell></row><row><cell cols="3">SEVERYN &amp; MOSCHITTI (2015) 0.7460 0.8080</cell></row><row><cell>WANG &amp; ITTYCHERIAH (2015)</cell><cell cols="2">0.7460 0.8200</cell></row><row><cell>QA-BILSTM</cell><cell cols="2">0.6750 0.7723</cell></row><row><cell>QA-CNN</cell><cell cols="2">0.7147 0.8070</cell></row><row><cell>AP-BILSTM</cell><cell cols="2">0.7132 0.8032</cell></row><row><cell>AP-CNN</cell><cell>0.7530</cell><cell>0.8511</cell></row><row><cell>than in InsuranceQA</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Performance of different systems for WikiQA</figDesc><table><row><cell>System</cell><cell>MAP</cell><cell>MRR</cell></row><row><cell cols="3">YANG ET AL. (2015) 0.6520 0.6652</cell></row><row><cell>YIN ET AL. (2015)</cell><cell cols="2">0.6600 0.6770</cell></row><row><cell>QA-BILSTM</cell><cell cols="2">0.6557 0.6695</cell></row><row><cell>QA-CNN</cell><cell cols="2">0.6701 0.6822</cell></row><row><cell>AP-BILSTM</cell><cell cols="2">0.6705 0.6842</cell></row><row><cell>AP-CNN</cell><cell>0.6886</cell><cell>0.6957</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">git clone https://github.com/shuzi/insuranceQA.git 3 The data is obtained from<ref type="bibr" target="#b22">(Yao et al., 2013)</ref> </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Piero Molino for creating the script used to produce the text heat maps presented in this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frédéric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warde</forename><forename type="middle">-</forename><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference</title>
		<meeting>the Python for Scientific Computing Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning hybrid representations to retrieve semantically equivalent questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cicero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luciano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dasha</forename><surname>Bogdanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning, JMLR: W&amp;CP volume</title>
		<meeting>the 31st International Conference on Machine Learning, JMLR: W&amp;CP volume<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Applying deep learning to answer selection: A study and an open task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowen</surname></persName>
		</author>
		<idno>arXiv preprint:1508.01585</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Geoffrey. Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinton</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1684" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baotian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhengdong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Reasoning about entailment with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Kociský</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno>abs/1509.06664</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to rank short text pairs with convolutional deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A latent semantic model with convolutional-pooling structure for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiaodong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jianfeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Mesnil</surname></persName>
		</author>
		<idno>978-1- 4503-2598-1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management</title>
		<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Lstm-based deep learning models for nonfactoid answer selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cicero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowen</surname></persName>
		</author>
		<idno>abs/1511.04108</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A long short-term memory model for answer sentence selection in question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Probabilistic tree-edit models with structured latent variables for textual entailment and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of the 23rd International Conference on Computational Linguistics (COLING)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">What is the jeopardy model? a quasi-synchronous grammar for qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitamura</forename><surname>Teruko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.02628</idno>
		<title level="m">Faq-based question answering via word alignment</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename></persName>
		</author>
		<title level="m">Semantic embeddings from hashtags. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyunghyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-11" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wikiqa: A challenge dataset for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Answer extraction as sequence tagging with tree edit distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Question answering using enhanced lexical semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pastusiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguist (ACL)</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguist (ACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">ABCNN: attention-based convolutional neural network for modeling sentence pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowen</surname></persName>
		</author>
		<idno>abs/1512.05193</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning for answer sentence selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
