<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Static Features for Temporally Consistent 3D Human Pose and Shape from a Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ECE &amp; ASRI</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ECE &amp; ASRI</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><forename type="middle">Yong</forename><surname>Chang</surname></persName>
							<email>juyong.chang@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">ECE</orgName>
								<orgName type="institution">Kwangwoon University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
							<email>kyoungmu@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">ECE &amp; ASRI</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond Static Features for Temporally Consistent 3D Human Pose and Shape from a Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>input TCMR (Ours) VIBE <ref type="figure">Figure 1</ref>: VIBE [15], the state-of-the-art video-based 3D human pose and shape estimation method, outputs very different 3D human poses per frame, although the frames have subtle differences. Our TCMR produces clearly more temporally consistent and smooth 3D human motion. This is a video figure that is best viewed by Adobe Reader.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Despite the recent success of single image-based 3D human pose and shape estimation methods, recovering temporally consistent and smooth 3D human motion from a video is still challenging. Several video-based methods have been proposed; however, they fail to resolve the single image-based methods' temporal inconsistency issue due to a strong dependency on a static feature of the current frame. In this regard, we present a temporally consistent mesh recovery system (TCMR). It effectively focuses on the past and future frames' temporal information without being dominated by the current static feature. Our TCMR significantly outperforms previous video-based methods in temporal consistency with better per-frame 3D pose and shape accuracy. We also release the codes 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Various methods have been proposed to analyze humans from images, ranging from estimating a simplistic 2D skeleton to recovering 3D human pose and shape. Despite the recent improvements, estimating 3D human pose and shape from images is still a challenging task, especially in the monocular case due to depth ambiguity, limited training data, and complexity of human articulations.</p><p>Most of the previous methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref> attempt to recover 3D human pose and shape from a single image. They are generally based on parametric 3D hu-man mesh models, such as SMPL <ref type="bibr" target="#b18">[19]</ref>, and directly regress the model parameters from the input image. Although single image-based methods predict a reasonable output from a static image, they tend to produce temporally inconsistent and unsmooth 3D motion when applied to a video per frame. The temporal instability is from inconsistent 3D pose errors for consecutive frames. For example, the errors could occur in different 3D directions, or the following frames' pose outputs could remain relatively the same, not reflecting the motion.</p><p>Several methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref> have been proposed to extend the single image-based methods to the video case effectively. They feed a sequence of images to the pretrained single image-based 3D human pose and shape estimation networks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref> to obtain a sequence of static features. All input frames' static features are passed to a temporal encoder, which encodes a temporal feature for each input frame. Then, a body parameter regressor outputs SMPL parameters for each frame from the temporal feature of the corresponding time step.</p><p>Although the above works quantitatively improved the per-frame 3D pose accuracy and motion smoothness, their qualitative results still suffer from the temporal inconsistency aforementioned, as shown in <ref type="figure">Figure 1</ref>. We argue that the failure comes from a strong dependency on the static feature of the current frame. For terminological convenience, we use a word current to indicate the time step of a target frame where SMPL parameters to be estimated. The first reason for the strong dependency is a residual connection between the current frame's static and temporal features. While the residual connection has been widely verified to facilitate a learning process, naively applying it to the temporal encoding can hinder the system from learning useful temporal information. Given that the static feature is extracted by the pretrained network <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref>, it contains a strong cue for the SMPL parameters of the current frame. Thus, the residual connection's identity mapping of the static feature can make the SMPL parameter regressor heavily depend on it and leverage the temporal feature marginally. This procedure can constrain the temporal encoder from encoding more meaningful temporal features. The second reason is the temporal encoding that takes static features from all frames, which include a current static feature. The current static feature has the largest potential to affect the current temporal feature, from which SMPL parameters are predicted. This phenomenon is caused by the current static feature having the most crucial information for 3D human pose and shape of a current frame. Although the dominance will increase the per-frame accuracy of 3D pose and shape estimation, it can prevent the temporal encoder from fully exploiting the past and future frames' temporal information. Taken together, the existing video-based methods have a strong preference for the current static fea-ture, and suffer from the temporal inconsistency issue as single image-based methods do.</p><p>In this work, we propose a temporally consistent mesh recovery system (TCMR). It is designed to resolve the strong dependency on the current static feature for temporally consistent and smooth 3D human motion output from a video. First, although we follow the previous video-based works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref> to encode a temporal feature of the current frame, we remove the residual connection between the static and temporal features. Moreover, we introduce Pose-Forecast, which consists of two temporal encoders, to forecast a current pose from the past and future frames without the current frame. The temporal features from PoseForecast are free from the current static feature; however, they contain essential temporal information of the past and future frames to forecast a current pose. The temporal features from PoseForecast are integrated with the current temporal feature, which is extracted from all input frames, to predict current SMPL parameters. The parameters estimated from the integrated temporal feature are the final output in inference time. By removing the strong dependency on the current static feature, our SMPL parameter regressor can have more chance to focus on the past and future frames without being dominated by the current frame.</p><p>Despite its simplicity, we observed that our newly designed temporal architecture is highly effective on obtaining the temporally consistent and smooth 3D human motion. It also improves the accuracy of the 3D pose and shape per frame by utilizing better temporal information. We show that the proposed TCMR outperforms the previous videobased methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref> on various 3D video benchmarks, especially in temporal consistency.</p><p>Our contributions can be summarized as follows.</p><p>• We present a temporally consistent mesh recovery system (TCMR), which produces temporally consistent and smooth 3D human motion from a video. It effectively leverages temporal information from the past and future frames without being dominated by the static feature of the current frame.</p><p>• Despite its simplicity, TCMR not only improves the temporal consistency of 3D human motion but also increases per-frame 3D pose and shape accuracy compared to a baseline method.</p><p>• TCMR outperforms previous video-based methods in temporal consistency by a large margin while achieving better per-frame 3D pose and shape accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Single image-based 3D human pose and shape estimation. Most of the current single image-based 3D human pose and shape estimation methods are based on the modelbased approach, which predicts parameters of a predefined 3D human mesh model, SMPL <ref type="bibr" target="#b18">[19]</ref>. Kanazawa et al. <ref type="bibr" target="#b11">[12]</ref> proposed an end-to-end trainable human mesh recovery (HMR) system that uses adversarial loss to make their output 3D human mesh anatomically plausible. Pavlakos et al. <ref type="bibr" target="#b26">[27]</ref> used 2D joint heatmaps and silhouette as cues for predicting accurate SMPL parameters. Omran et al. <ref type="bibr" target="#b23">[24]</ref> proposed a similar system, which uses human part segmentation as a cue for regressing SMPL parameters. Pavlakos et al. <ref type="bibr" target="#b25">[26]</ref> proposed a system that uses multi-view color consistency to supervise a network using multi-view geometry.</p><p>Kolotouros et al. <ref type="bibr" target="#b15">[16]</ref> introduced a self-improving system that consists of an SMPL parameter regressor and an iterative fitting framework <ref type="bibr" target="#b1">[2]</ref>. Georgakis et al. <ref type="bibr" target="#b8">[9]</ref> incorporated hierarchical kinematic prior on a human body to a network. Conversely, the model-free approach estimates the shape directly instead of regressing the model parameters. Varol et al. <ref type="bibr" target="#b33">[34]</ref> proposed BodyNet, which estimates 3D human shape in the 3D volumetric space. Kolotouros et al. <ref type="bibr" target="#b16">[17]</ref> designed a graph convolutional human mesh regression system. Their graph convolutional network takes a template human mesh in a rest pose as input and predicts mesh vertex coordinates using image features from ResNet <ref type="bibr" target="#b9">[10]</ref>. Moon and Lee <ref type="bibr" target="#b22">[23]</ref> introduced a lixel-based 1D heatmap to localize mesh vertices in a fully convolutional manner. Choi et al. <ref type="bibr" target="#b6">[7]</ref> proposed a graph convolutional network that recovers 3D human pose and mesh from a 2D human pose.</p><p>Despite moderate performance on a static image, the single image-based works suffer from temporal inconsistency (e.g., sudden change of poses), when applied to a video. Video-based 3D human pose and shape estimation. HMMR <ref type="bibr" target="#b12">[13]</ref> extracts static features and encodes them to a temporal feature using a 1D fully convolutional temporal encoder. It learns temporal context representation to reduce the 3D prediction's temporal inconsistency by predicting 3D poses in the nearby past and future frames. Doersch et al. <ref type="bibr" target="#b7">[8]</ref> trained their network on a sequence of optical flow and 2D poses to make their network generalize well to unseen videos. Sun et al. <ref type="bibr" target="#b32">[33]</ref> proposed a skeletondisentangling framework, which separates 3D human pose and shape estimation into multi-level spatial and temporal subproblems. They enforced the network to order shuffled frames to encourage temporal feature learning. VIBE <ref type="bibr" target="#b14">[15]</ref> encodes static features from the input frames into a temporal feature by using a bi-directional gated recurrent unit (GRU) <ref type="bibr" target="#b5">[6]</ref>, and feeds it to an SMPL parameter regressor. A motion discriminator is introduced to encourage the regressor to produce plausible 3D human motion. MEVA <ref type="bibr" target="#b19">[20]</ref> addresses the problem in a coarse-to-fine manner. Their system initially estimates the coarse 3D human motion using a variational motion estimator (VME), and predicts the residual motion with a motion residual regressor (MRR).</p><p>Temporally consistent 3D human motion from a video. Although there have been many methods for video-based 3D human motion estimation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref>, most of them showed their results only qualitatively, and did not report numerical evaluation on temporal consistency. After the HMMR <ref type="bibr" target="#b12">[13]</ref> introduced the 3D pose acceleration error for the temporal consistency and smoothness of human motion, the following works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20]</ref> have reported the error metric. HMMR and VIBE <ref type="bibr" target="#b14">[15]</ref> lowered the acceleration error compared with the single image-based methods. However, they revealed a trade-off between per-frame accuracy and temporal consistency. The HMMR outputs smoother 3D human motion but provides low per-frame 3D pose accuracy. Conversely, the VIBE <ref type="bibr" target="#b14">[15]</ref> shows high perframe 3D pose accuracy; however, the output is temporally inconsistent in quantitative metrics and qualitative results compared with HMMR.</p><p>In this regard, MEVA <ref type="bibr" target="#b19">[20]</ref> attempts to establish the balance between the per-frame 3D pose accuracy and the temporal smoothness. Although it provides better results in both metrics, the qualitative results still expose unsmooth 3D motion. The reason is that the system strongly depends on the current static feature to estimate the current 3D pose and shape. First, MEVA uses a residual connection between the current frames' static and temporal features. In addition, the current temporal feature, which is used to refine initial 3D pose and shape by MRR, is encoded from static features of all frames, which include the current frame. This procedure can make the temporal feature dominated by the current static feature. As a result, the refinement is significantly driven by the current static feature, and the 3D errors from consecutive frames appear inconsistent. On the contrary, our TCMR is deliberately designed to reduce the strong dependency on the static feature. The residual connection is removed, and PoseForecast forecasts additional temporal features from past and future frames without a current frame. Our approach alleviates the dependency and provides temporally consistent and accurate 3D human motions in both qualitative and quantitative manners.</p><p>Forecasting 3D human poses from images. Recently, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> proposed to predict a person's future 3D human poses from RGB input. Chao et al. <ref type="bibr" target="#b4">[5]</ref> leveraged a recurrent neural network (RNN) to forecast a sequence of 2D poses from a static image, and estimate 3D poses from the predicted 2D poses. The HMMR <ref type="bibr" target="#b12">[13]</ref> predicts the current, future, and past 3D poses from a current input image using a hallucinator. It hallucinates the past and future 3D poses from a current frame and is self-supervised by the output of the 1D fully convolutional temporal encoder. Zhang et al. <ref type="bibr" target="#b36">[37]</ref> proposed a neural autoregressive framework that takes past video frames as input to predict future 3D motion. Yuan et al. <ref type="bibr" target="#b35">[36]</ref> adopted deep reinforcement learning to forecast future 3D human poses from egocentric videos.  <ref type="figure">Figure 2</ref>: The overall pipeline of TCMR. The gold-colored output Θ int is used in inference time, which is regressed from the integrated temporal feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GRU GRU</head><p>Although the objective of the above methods is to forecast future 3D poses, our system aims to learn useful temporal features free from a current static feature by the forecasting. <ref type="figure">Figure 2</ref> shows the overall pipeline of our TCMR. We provide descriptions of each part in the system as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TCMR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Temporal encoding from all frames</head><p>Given a sequence of T RGB frames I 1 , . . . , I T , ResNet <ref type="bibr" target="#b9">[10]</ref>, pretrained by Kolotouros et al. <ref type="bibr" target="#b15">[16]</ref>, extracts a static image feature per frame. Then, a global average pooling is applied on the ResNet outputs, which become</p><formula xml:id="formula_0">f 1 , . . . , f T , where f • ∈ R 2048 .</formula><p>The network weights of the ResNet are shared for all frames.</p><p>From the extracted static features of all input frames, we compute the current frame's temporal feature using a bi-directional GRU, which consists of two uni-directional GRUs. We denote the bi-directional GRU as G all . The current frame is defined as a T /2 th frame among T input frames. The two uni-directional GRUs extract temporal features from the input static features in the opposite time directions. The initial inputs of the two GRUs are f 1 and f T , respectively, and the initial hidden states of them are initialized as zero tensors. Then, they recurrently updates their hidden states by aggregating the static features from the next frames f 2 , . . . , f T /2 and f T −1 , . . . , f T /2 , respectively. The concatenated hidden states of the GRUs at the current frame become the current temporal feature from all input frames g all ∈ R 2048 . Unlike VIBE <ref type="bibr" target="#b14">[15]</ref>, we do not add residual connection between f T /2 and g all , such that the current temporal feature will not be dominated by f T /2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Temporal encoding by PoseForecast</head><p>PoseForecast forecasts additional temporal features for the current target pose from the past and future frames by employing two additional GRUs, denoted as G past and G future , respectively. The past and future frames are defined as 1, . . . , ( T /2 − 1)th frames and ( T /2 + 1), . . . , T th frames, respectively. The initial input of the G past is f 1 , and the initial hidden state is initialized as a zero tensor. Then, it recurrently updates its hidden state by aggregating the static features from the next frames f 2 , . . . , f T /2 −1 . The final hidden state of the G past becomes the temporal feature from the past frames g past ∈ R 1024 . Similarly, G future takes f T as an initial input with a zero-initialized hidden state, and re-currently updates its hidden state by aggregating the static features from the next frames f T −1 , . . . , f T /2 +1 . The final hidden state of the G future becomes the temporal feature from the future frames g future ∈ R 1024 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporal feature integration</head><p>We integrate the extracted temporal features from all frames g all , from the past frames g past , and from the future frames g future for the final 3D mesh estimation, as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. For the integration, we pass each temporal feature to ReLU activation function and a fully connected layer to change the size of the channel dimension to 2048. The outputs of the fully connected layer are denoted as g all , g past , and g future . Then, the output features are resized to 256 by a shared fully connected layer and concatenated. The concatenated feature is passed to several fully connected layers, followed by the softmax activation function, which produces attention values a = (a all , a past , a future ) ∈ R 3 . The attention values represent how much the system should give a weight for the feature integration. The final integrated temporal feature is obtained by g int = a all g all + a past g past + a future g future .</p><p>In the training stage, we pass g past , g future , and g int to the SMPL parameter regressor, which outputs Θ past , Θ future , and Θ int from each input temporal feature, respectively. The regressor is shared for all outputs. Θ • denotes a union of SMPL parameter set {θ • , β • } and weak-perspective camera parameter set {s • , t • }. θ, β, s, and t represent SMPL pose parameter, identity parameter, scale, and translation, respectively. In the testing stage, we only pass g int to the parameter regressor and use Θ int as the final 3D human mesh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss functions</head><p>For the training, we supervise all three outputs Θ past , Θ future , and Θ int with current frame groundtruth. L2 loss between predicted and groundtruth SMPL parameters and 2D/3D joint coordinates are used, following VIBE <ref type="bibr" target="#b14">[15]</ref>. The 3D joint coordinates are obtained by forwarding the SMPL parameters to the SMPL layer, and the 2D joint coordinates are obtained by projecting the 3D joint coordinates using the predicted camera parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation details</head><p>Following VIBE <ref type="bibr" target="#b14">[15]</ref>, we set the length of the input sequence T to 16 and the input video frame rate to 25-30 frames per second and initialize the backbone and regressor with the pretrained SPIN <ref type="bibr" target="#b15">[16]</ref>. The weights are updated by the Adam optimizer <ref type="bibr" target="#b13">[14]</ref> with a mini-batch size of 32. The human body region is cropped using a groundtruth box in both of training and testing stages following previous works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>. The cropped image is resized to 224×224. Inspired by Sarandi et al. <ref type="bibr" target="#b31">[32]</ref>, we occlude the cropped image with various objects for data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>input</head><p>-res +PF (Ours) +res -PF <ref type="figure">Figure 4</ref>: Qualitative comparison between our TCMR (middle) and the baseline (right). TCMR learns more useful temporal features, and provides a more accurate 3D pose and temporally consistent 3D motion. res denotes the residual connection and PF is the abbreviation for PoseForecast. This is a video figure that is best viewed by Adobe Reader.</p><p>The occlusion augmentation reduces both pose and acceleration errors approximately by 1mm. Following <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref>, we precompute the static features from the cropped images by ResNet <ref type="bibr" target="#b9">[10]</ref> to save training time and memory. All the 3D rotations of θ are initially predicted in the 6D rotational representation of Zhou et al. <ref type="bibr" target="#b38">[39]</ref>, and converted to the 3D axis-angle rotations. The initial learning rate is set to 5 −5 and reduced by a factor of 10, when the 3D pose accuracy does not improve after every 5 epochs. We train the network for 30 epochs with one NVIDIA RTX 2080Ti GPU. PyTorch <ref type="bibr" target="#b24">[25]</ref> is used for code implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation metrics and datasets.</head><p>Evaluation metrics. We report the per-frame and temporal evaluation metrics. For the per-frame evaluation, we use mean per joint position error (MPJPE), Procrustes-aligned MPJPE (PA-MPJPE), and mean per vertex position error (MPVPE). The position errors are measured in millimeter (mm) between the estimated and groundtruth 3D coordinates after aligning the root joint. Particularly, we use PA-MPJPE as the main metric for per-frame accuracy, since it excludes the effect of outputs' scale ambiguity on errors. For the temporal evaluation, we use the acceleration error proposed in HMMR <ref type="bibr" target="#b12">[13]</ref>. The acceleration error computes an average of the difference between the predicted and groundtruth acceleration of each joint in (mm/s 2 ).</p><p>Datasets. We use 3DPW <ref type="bibr" target="#b34">[35]</ref>, Human3.6M <ref type="bibr" target="#b10">[11]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b20">[21]</ref>, InstaVariety <ref type="bibr" target="#b12">[13]</ref>, Penn Action <ref type="bibr" target="#b37">[38]</ref>, and Pose-Track <ref type="bibr" target="#b0">[1]</ref> for training, following VIBE <ref type="bibr" target="#b14">[15]</ref>. 3DPW is the only in-the-wild dataset that contains accurate groundtruth SMPL parameters. 3DPW, Human3.6M, MPI-INF-3DHP are also used for evaluation. More details are in the supplementary material.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation study</head><p>In this study, we show how each component of our temporal architecture reduces the dependency of the model on a current static feature, and make it focus on temporal features from the past and future. We take the same baseline used in VIBE <ref type="bibr" target="#b14">[15]</ref>. The baseline has a single bi-directional GRU that encodes temporal features from all input frames and a residual connection between the static and temporal features as VIBE. It also predicts each 3D pose and shape for all input frames in a single feed-forward, but does not use the motion discriminator. We use 3DPW <ref type="bibr" target="#b34">[35]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b20">[21]</ref>, InstaVariety <ref type="bibr" target="#b12">[13]</ref>, and Penn Action <ref type="bibr" target="#b37">[38]</ref> for training, and 3DPW for evaluation. Effectiveness of residual connection removal. To analyze the effect of the residual connection between the static and temporal features, we compare the models with and without it. As shown in <ref type="table" target="#tab_0">Table 1</ref>, removing the residual connection decreases the acceleration error significantly, which indicates a considerable improvement in temporal consistency and smoothness of 3D human motion. This finding verifies that the identity mapping of the current static feature inside the residual connection hinders a model from learning meaningful temporal features. Moreover, the increased temporal consistency of 3D motion improves the per-frame 3D pose accuracy. <ref type="figure">Figure 4</ref> illustrates how the enhanced temporal consistency contributes to better per-frame 3D pose estimation. The sudden change of poses, caused by the inaccurate 3D pose estimation on specific frames, is disappeared. The above comparisons clearly validate the effectiveness of removing the residual connection in terms of both per-frame and temporal metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of PoseForecast</head><p>We compare the models with and without PoseForecast to verify the effectiveness of forecasting current temporal features only from the past and future frames. On the basis of the results in <ref type="table" target="#tab_0">Table 1</ref>, PoseForecast consistently improves per-frame and temporal metrics regardless of the residual connection. Particularly, the acceleration error consistently decreases by over 11% . Thus, the temporal encoding that takes all frames with the current frame may be suboptimal, and forecasting the current temporal features from the past and future frames is beneficial for temporally consistent 3D human motion.</p><p>To further validate the forecasting, we compare our Pose-Forecast with its variations. First, we show the effectiveness of taking past and future frames without a current frame in <ref type="table" target="#tab_1">Table 2</ref>. As the table shows, additionally taking current frames increases the acceleration error by 33%. Thus, maintaining the temporal features free from the current static feature is important for temporally consistent and smooth 3D human motion. Second, we validate the effectiveness of supervising the predicted SMPL parameters from Pose-Forecast (i.e., Θ past and Θ future ) with groundtruth of the current frame in <ref type="table" target="#tab_2">Table 3</ref>. As shown in the table, supervising the predicted parameters with the current groundtruth provides better per-frame 3D pose accuracy and temporal consistency than the other supervisions. When we supervise the predicted parameters with the groundtruth of T /2 − 1th and T /2 + 1th frames (the second row), the acceleration error increases by 10%. The performance degrades, because the temporal features of PoseForecast are encoded from the input including the static features of the target frames (i.e., T /2 −1th and T /2 +1th frames). As verified in <ref type="table" target="#tab_1">Table 2</ref>, including the target static feature hinders PoseForecast from learning useful temporal information for temporally consistent and smooth 3D human motion. The encoded temporal feature is likely to be dominated by the target static feature and marginally leverage temporal information from other frames. When no supervision is observed (the first row), both 3D pose accuracy and temporal consistency decrease compared with ours. Hence, designing our PoseFrecast to forecast the current SMPL parameters by supervising it with the current target (the third row) facilitates the network to learn more useful temporal features.</p><p>In summary, the above comparisons show that forecasting current temporal features from past and future frames is effective for temporally consistent 3D human motion by reducing the strong dependency on a current static feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with state-of-the-art methods</head><p>Comparison with video-based methods. We compare our TCMR with previous state-of-the-art video-based methods <ref type="table" target="#tab_7">Table 4</ref>: Evaluation of state-of-the-art methods on on 3DPW <ref type="bibr" target="#b34">[35]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b20">[21]</ref>, and Human3.6M <ref type="bibr" target="#b10">[11]</ref>. All methods except HMMR <ref type="bibr" target="#b12">[13]</ref> do not use Human3.6M SMPL parameters from Mosh <ref type="bibr" target="#b17">[18]</ref>, but use 3DPW train set for training following MEVA <ref type="bibr" target="#b19">[20]</ref>. The number of input frames are following the protocols of the papers.  <ref type="figure">Figure 5</ref>: Comparison between the acceleration errors of the proposed TCMR, MEVA <ref type="bibr" target="#b19">[20]</ref>, and VIBE <ref type="bibr" target="#b14">[15]</ref>. Our TCMR shows clearly lower acceleration errors along the time step than previous methods, which indicates temporally consistent 3D motion output. The previous methods reveal extreme acceleration error spikes compared to our TCMR.  <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref> that report the acceleration error in <ref type="table" target="#tab_7">Table 4</ref>.</p><p>On the basis of the study of Luo et al. <ref type="bibr" target="#b19">[20]</ref>, all methods, except HMMR <ref type="bibr" target="#b12">[13]</ref> are trained on the train set including 3DPW <ref type="bibr" target="#b34">[35]</ref>, but do not leverage Human3.6M <ref type="bibr" target="#b10">[11]</ref> SMPL parameters obtained from Mosh <ref type="bibr" target="#b17">[18]</ref> for supervision. The numbers of VIBE <ref type="bibr" target="#b14">[15]</ref> are from MEVA <ref type="bibr" target="#b19">[20]</ref>, but we validated them independently. As shown in the table, our proposed system outperforms the previous video-based methods on all benchmarks both in per-frame 3D pose accuracy and temporal consistency. These results prove that our system effectively leverages temporal information of the past and future by resolving the system's strong dependency on a current static feature. Although MEVA <ref type="bibr" target="#b19">[20]</ref> also improves the per-frame and temporal metrics, the model consumes nearly 6 times more input frames during training and testing, and provides worse results than ours. In addition, MEVA requires at least 90 input frames, which means that it can not be trained and tested on short videos. <ref type="figure">Figure 5</ref> describes the clear advantage of our TCMR on the temporal consistency among video-based methods. The previous methods expose numerous spikes, which represent unstable and unsmooth 3D motion estimation. Our TCMR provides relatively low acceleration errors along the time step, which indicates temporally consistent 3D motion output. The figure's acceleration errors are measured on a sequence of the 3DPW validation set that has a diverse motion.</p><p>To further confirm the effectiveness of the proposed system on temporal consistency, we compare our TCMR with VIBE <ref type="bibr" target="#b14">[15]</ref> and MEVA <ref type="bibr" target="#b19">[20]</ref> with an average filter applied as post-processing in <ref type="table" target="#tab_4">Table 5</ref>. Average filtering is performed by spherical linear interpolation in the quaternions of estimated SMPL <ref type="bibr" target="#b18">[19]</ref> pose parameters following MEVA. The numbers of other methods are from MEVA. As shown in the table, our system outperforms other methods even when they are applied with the average filtering. Moreover, the results imply that the average filtering can decrease the perframe 3D pose accuracy by smoothing out the details of 3D human motion. However, each component of our TCMR decreases the acceleration error while improving the perframe 3D pose accuracy, as shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>In summary, our newly designed system significantly outperforms the previous state-of-the-art methods in temporal consistency and smoothness of 3D human motion without any post-processing while also increasing the per-frame 3D pose accuracy. Note that the comparison in <ref type="table" target="#tab_7">Table 4</ref> and 5 is the fairest comparison between the video-based methods, since all methods, except HMMR <ref type="bibr" target="#b12">[13]</ref>, used the same training datasets. Comparison with single image-based and video-based methods. We compare our system with previous 3D pose and shape estimation methods, including single imagebased methods in <ref type="table" target="#tab_5">Table 6</ref>. None of the methods are trained on 3DPW <ref type="bibr" target="#b34">[35]</ref>. For evaluation on Human3.6M <ref type="bibr" target="#b10">[11]</ref>, we use the frontal view images following <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16]</ref>, whereas all views are tested in <ref type="table" target="#tab_7">Table 4</ref> and 5. In addition, to confirm the acceleration error of VIBE <ref type="bibr" target="#b14">[15]</ref> on MPI-INF-3DHP <ref type="bibr" target="#b20">[21]</ref> and Human3.6M, we re-evaluate the model using the pretrained weights provided in the official code repository.</p><p>As shown in the table, our method outperforms all the previous methods on 3DPW, a challenging in-the-wild benchmark, and MPI-INF-3DHP in per-frame 3D pose accuracy (PA-MPJPE) and temporal consistency. Especially the temporal consistency is largely improved compared with single image-based methods. While VIBE decreases the acceleration error of SPIN <ref type="bibr" target="#b15">[16]</ref> by 9% and is defeated by Pose2Mesh <ref type="bibr" target="#b6">[7]</ref> in the temporal consistency, our system provides over 3 times better performance than both SPIN and Pose2Mesh in 3DPW. Moreover, VIBE gives a higher acceleration error than I2L-MeshNet <ref type="bibr" target="#b22">[23]</ref> but our TCMR outperforms it by a wide margin in Human3.6M.</p><p>We provide qualitative comparison with VIBE <ref type="bibr" target="#b14">[15]</ref> and MEVA <ref type="bibr" target="#b19">[20]</ref> on 3DPW, qualitative results of TCMR on Internet videos, and failure cases in this link 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We present TCMR, a novel and powerful system that estimates a 3D human mesh from a RGB video. Previous video-based methods suffer from the temporal inconsistency issue because of the strong dependency on the static feature of the current frame. We resolve the issue by removing the residual connection between the static and temporal features, and employing PoseForecast that forecasts the current temporal feature from the past and future frames. In comparison with the previous video-based methods, the proposed TCMR provides highly temporally consistent 3D motion and a more accurate 3D pose per frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material for</head><p>Beyond Static Features for Temporally Consistent 3D Human Pose and Shape from a Video</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">More qualitative results</head><p>We provide more qualitative results in the online video 3 , which consists of three parts. The first part shows the qualitative results of our TCMR on in-the-wild videos that have fast and diverse motions from 3DPW <ref type="bibr" target="#b34">[35]</ref>. We also provide the outputs rendered from the opposite view. The second part compares the proposed TCMR with VIBE <ref type="bibr" target="#b14">[15]</ref> and MEVA <ref type="bibr" target="#b19">[20]</ref>. The results are rendered on a plain background with a fixed camera to clearly compare the temporal consistency and smoothness of 3D human motion following MEVA <ref type="bibr" target="#b19">[20]</ref>. The fixed camera has the fixed weakperspective camera parameters s and t, which are set to one and zero, respectively. The last part provides the results of TCMR on Internet videos. The bounding boxes of people in the videos are tracked by a multi-person tracker that uses YOLOv3 <ref type="bibr" target="#b29">[30]</ref>. With the cropped images from the bounding boxes, our TCMR processes 41 frames per second (fps) for the video 4 with 5 people. A single NVIDIA RTX 2080Ti GPU is used for the test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Human evaluation.</head><p>We surveyed 50 people to pick the most realistic motion from TCMR, MEVA, and VIBE outputs on 20 sequences of 3DPW <ref type="bibr" target="#b34">[35]</ref> validation and test sets. TCMR, MEVA, and VIBE got 69%, 26%, and 5% votes, respectively. The result is coherent with the acceleration error results of the three methods in the main manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Attention values in feature integration.</head><p>During the temporal feature integration, the past and future temporal features are weighted more than the current temporal feature, and the variation range of each attention value is ±20%. The past and future temporal features' attention values tend to become larger when the current pose is difficult or the motion is fast. The attached videos 56 plot the attention values of the past, future, and current temporal features on two sequences of 3DPW <ref type="bibr" target="#b34">[35]</ref>. The values are written at the top-right of frames, and the sum is always 1. As the video shows, the attention value of the current temporal feature does not drop below 0.4 when a subject is walking in slow motion, whereas the value overall stays below 0.4 when a subject is playing basketball with fast movement and complex poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Datasets</head><p>3DPW. 3DPW <ref type="bibr" target="#b34">[35]</ref> is captured from in-the-wild and contains 3D human pose and shape annotations. It consists of 60 videos and 51K video frames in total, which are captured with a phone at 30 fps. IMU sensors are leveraged to acquire the groundtruth 3D human pose and shape. We follow the official split protocol to train and test our model, where train, validation, test sets consist of 24, 12, 24 videos, respectively. Also, we report MPVPE on 3DPW because it only has groundtruth 3D shape among the datasets we used. We use 14 joints defined by Human3.6M <ref type="bibr" target="#b10">[11]</ref> for evaluating PA-MPJPE and MPJPE following the previous works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. Human3.6M. Human3.6M <ref type="bibr" target="#b10">[11]</ref> is a large-scale indoor 3D human pose benchmark, which consists of 15 action categories and 3.6M video frames. Following <ref type="bibr" target="#b14">[15]</ref>, our TCMR is trained on 5 subjects (S1, S5, S6, S7, S8) and tested on 2 subjects (S9, S11). We subsampled the dataset to 25 fps (originally 50 fps) for training and evaluation on the acceleration error. 14 joints defined by Human3.6M are used for computing PA-MPJPE and MPJPE. MPI-INF-3DHP. MPI-INF-3DHP <ref type="bibr" target="#b20">[21]</ref> is a 3D benchmark mostly captured from indoor environment. The train set has 8 subjects, 16 videos per subject, and 1.3M video frames captured at 25 fps in total. It exploits a marker-less motion capture system and provides 3D human pose annotations.</p><p>The test set contains 6 subjects performing 7 actions in both the indoor and outdoor environment. The positional errors (i.e., PA-MPJPE and MPJPE) of TCMR are measured on the valid frames, which are composed of every 10th frame approximately, using 17 joints defined by MPI-INF-3DHP. The acceleration error is computed using all frames. InstaVariety. InstaVariety is a 2D human dataset curated by HMMR <ref type="bibr" target="#b12">[13]</ref>, whose videos are collected from Instagram using 84 motion-related hashtags. There are 28K videos with an average length of 6 seconds, and OpenPose <ref type="bibr" target="#b3">[4]</ref> is used to acquire pseudo-groundtruth 2D pose annotations. Penn Action. Penn Action <ref type="bibr" target="#b37">[38]</ref> contains 2.3K video sequences of 15 different sports actions. It has a total of 77K video frames annotations for 2D human poses, bounding boxes, and action categories. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Effect of pretrained ResNet</head><p>Due to lack of video data, our TCMR and previous video-based methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref> employ ResNet <ref type="bibr" target="#b9">[10]</ref> pretrained by the single image-based 3D human pose and shape estimation methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref> to extract static features from input frames. The pretrained ResNet is trained on largescale in-the-wild 2D human pose datasets and provides reliable static features. However, it is also one reason for the strong dependency of the system on the current static feature. The current static feature extracted by the pretrained ResNet already contains a strong cue on the current 3D human pose and shape, leading the system to leverage temporal information marginally.</p><p>In this regard, an alternative to our TCMR, one could train models from scratch without using the ResNet pretrained by <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref> to extract static features to reduce the strong dependency. <ref type="table" target="#tab_6">Table 7</ref> compares our TCMR, the baseline (the third row), and the models that do no use the ResNet pretrained by SPIN <ref type="bibr" target="#b15">[16]</ref>. As the table shows, the models that do no use the ResNet pretrained by SPIN <ref type="bibr" target="#b15">[16]</ref> reveal very high per-frame 3D pose errors. This indicates that training the models with only video data in the current literature is not sufficient for accurate 3D human pose estimation. The interesting part is that the model using ResNet with random initialization provides the highest 3D pose error but the lowest acceleration error among the models without our TCMR. While the high pose error attributes to the lack of train data, the low acceleration error implies that the strong cue of the current static feature adversely affects the temporal consistency of 3D human motion.</p><p>In summary, with the insufficient video data in the current literature, the proposed TCMR significantly improves the temporal consistency of 3D human motion by reducing the strong dependency on the current static feature. It also preserves the per-frame 3D pose accuracy by leveraging the ResNet pretrained on large-scale in-the-wild 2D hu- <ref type="table">Table 8</ref>: Performance comparison between two networks taking different input fps on 3DPW <ref type="bibr" target="#b34">[35]</ref>. The numbers in the second row are from  <ref type="table">Table 8</ref> shows the effect of input fps. The acceleration error doubles when input fps reduces by half, whereas the accuracy remains relatively the same. The result indicates that TCMR can still fix invalid poses using relatively sparse temporal information. The result also implies that temporally dense information is critical for temporal consistency of outputs, which is intuitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.">Effect of input fps</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.">Pose2Mesh with temporal smoothing</head><p>We performed temporal smoothing on Pose2Mesh <ref type="bibr" target="#b6">[7]</ref>, the state-of-the-art single image-based 3D human pose and shape estimation method. Pose2Mesh wins the first in MPJPE, MPVPE, and acceleration error and the second in PA-MPJPE among single image-based methods according to <ref type="table" target="#tab_5">Table 6</ref> of the main manuscript. Pose2Mesh with eurofilter achieves PA-MPJPE 58.6, MPJPE 89.6, acceleration error 12.9 on 3DPW. TCMR still outperforms the smoothed Pose2Mesh by nearly twice in temporal consistency without any post-processing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Temporal feature integration to estimate 3D human mesh for the current frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison between different temporal architectures. All networks estimate only on the middle frame of the input sequence.</figDesc><table><row><cell>remove residual</cell><cell cols="2">PoseForecast PA-MPJPE↓ Accel↓</cell></row><row><cell></cell><cell>55.6</cell><cell>29.2</cell></row><row><cell></cell><cell>55.0</cell><cell>24.9</cell></row><row><cell></cell><cell>54.2</cell><cell>8.7</cell></row><row><cell>(Ours)</cell><cell>53.9</cell><cell>7.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison between PoseForecast that takes a current frame and that does not take a current frame.</figDesc><table><row><cell>PoseForecast input</cell><cell cols="2">PA-MPJPE↓ Accel↓</cell></row><row><cell>w. current frame</cell><cell>53.8</cell><cell>10.3</cell></row><row><cell>wo. current frame (Ours)</cell><cell>53.9</cell><cell>7.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison between different supervision on estimated SMPL parameters from the PoseForecast.</figDesc><table><row><cell>PoseForecast supervision target</cell><cell cols="2">PA-MPJPE↓ Accel↓</cell></row><row><cell>none</cell><cell>55.1</cell><cell>8.3</cell></row><row><cell>GT of past and future frames</cell><cell>54.1</cell><cell>8.5</cell></row><row><cell>GT of current frame (Ours)</cell><cell>53.9</cell><cell>7.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>MPJPE ↓ MPJPE ↓ MPVPE ↓ Accel ↓ PA-MPJPE ↓ MPJPE ↓ Accel ↓ PA-MPJPE ↓ MPJPE ↓ Accel ↓ input frames</figDesc><table><row><cell></cell><cell></cell><cell>3DPW</cell><cell></cell><cell></cell><cell cols="2">MPI-INF-3DHP</cell><cell></cell><cell></cell><cell>Human3.6M</cell><cell></cell><cell>number of</cell></row><row><cell>method PA-HMMR [13]</cell><cell>72.6</cell><cell>116.5</cell><cell>139.3</cell><cell>15.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>56.9</cell><cell>-</cell><cell>-</cell><cell>20</cell></row><row><cell>VIBE [15]</cell><cell>57.6</cell><cell>91.9</cell><cell>-</cell><cell>25.4</cell><cell>68.9</cell><cell>103.9</cell><cell>27.3</cell><cell>53.3</cell><cell>78.0</cell><cell>27.3</cell><cell>16</cell></row><row><cell>MEVA [20]</cell><cell>54.7</cell><cell>86.9</cell><cell>-</cell><cell>11.6</cell><cell>65.4</cell><cell>96.4</cell><cell>11.1</cell><cell>53.2</cell><cell>76.0</cell><cell>15.3</cell><cell>90</cell></row><row><cell>TCMR (Ours)</cell><cell>52.7</cell><cell>86.5</cell><cell>103.2</cell><cell>6.8</cell><cell>63.5</cell><cell>97.6</cell><cell>8.5</cell><cell>52.0</cell><cell>73.6</cell><cell>3.9</cell><cell>16</cell></row><row><cell></cell><cell>(1)</cell><cell></cell><cell></cell><cell>(2)</cell><cell></cell><cell></cell><cell>(3)</cell><cell></cell><cell>(4)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">(a) sampled frames of 'courtyard_basketball_01' in order</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">acceleration error spikes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>acceleration error (mm/s 2</cell><cell>(1)</cell><cell></cell><cell>(2)</cell><cell></cell><cell>(3)</cell><cell></cell><cell>(4)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>time step</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">(b) acceleration error plot of 'courtyard_basketball_01'</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison between ours and previous methods applied with average filtering on 3DPW<ref type="bibr" target="#b34">[35]</ref>.</figDesc><table><row><cell>method</cell><cell cols="3">PA-MPJPE↓ MPJPE↓ Accel↓</cell></row><row><cell>VIBE [15]</cell><cell>57.6</cell><cell>91.9</cell><cell>25.4</cell></row><row><cell>+ Avg. filter</cell><cell>57.8</cell><cell>91.6</cell><cell>13.5</cell></row><row><cell>MEVA [20]</cell><cell>54.7</cell><cell>86.9</cell><cell>11.6</cell></row><row><cell>+ Avg. filter</cell><cell>55.5</cell><cell>87.7</cell><cell>8.2</cell></row><row><cell>TCMR (Ours)</cell><cell>52.7</cell><cell>86.5</cell><cell>6.8</cell></row><row><cell>+ Avg. filter</cell><cell>55.0</cell><cell>88.7</cell><cell>6.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Evaluation of state-of-the-art methods on 3DPW<ref type="bibr" target="#b34">[35]</ref>, MPI-INF-3DHP<ref type="bibr" target="#b20">[21]</ref>, and Human3.6M<ref type="bibr" target="#b10">[11]</ref>. All methods do not use 3DPW<ref type="bibr" target="#b34">[35]</ref> on training. 'single image' or 'video' denotes whether the input of a method is a single image or a video.MPJPE  ↓ MPVPE ↓ Accel ↓ PA-MPJPE ↓ MPJPE ↓ Accel ↓ PA-MPJPE ↓ MPJPE ↓ Accel ↓</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>3DPW</cell><cell></cell><cell></cell><cell></cell><cell>MPI-INF-3DHP</cell><cell></cell><cell></cell><cell>Human3.6M</cell><cell></cell></row><row><cell cols="3">method PA-MPJPE ↓ single image HMR [12] 76.7 GraphCMR [17] 70.2 SPIN [16] 59.2 I2L-MeshNet [23] 57.7 Pose2Mesh [7] 58.3</cell><cell>130.0 -96.9 93.2 88.9</cell><cell>--116.4 110.1 106.3</cell><cell>37.4 -29.8 30.9 22.6</cell><cell>89.8 -67.5 --</cell><cell>124.2 -105.2 --</cell><cell>-----</cell><cell>56.8 50.1 41.1 41.1 46.3</cell><cell>88.0 --55.7 64.9</cell><cell>--18.3 13.4 23.9</cell></row><row><cell></cell><cell>HKMR [9]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>59.6</cell><cell>-</cell></row><row><cell></cell><cell>HMMR [13]</cell><cell>72.6</cell><cell>116.5</cell><cell>139.3</cell><cell>15.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>56.9</cell><cell>-</cell><cell>-</cell></row><row><cell>video</cell><cell>Doersch et al. [8] Sun et al. [33] VIBE [15]</cell><cell>74.7 69.5 56.5</cell><cell>--93.5</cell><cell>--113.4</cell><cell>--27.1</cell><cell>--63.4</cell><cell>--97.7</cell><cell>--29.0</cell><cell>-42.4 41.5</cell><cell>-59.1 65.9</cell><cell>--18.3</cell></row><row><cell></cell><cell>TCMR (Ours)</cell><cell>55.8</cell><cell>95.0</cell><cell>111.3</cell><cell>6.7</cell><cell>62.8</cell><cell>96.5</cell><cell>9.5</cell><cell>41.1</cell><cell>62.3</cell><cell>5.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Comparison between different models using ResNet with different initialization to extract static features. All models use the same SMPL parameter regressor pretrained by SPIN<ref type="bibr" target="#b15">[16]</ref>. 3K videos and 46K annotated frames in total. The videos are captured at different fps, varying around 25 fps. We use 792 videos from the official train set, which has 2D pose annotations for 30 frames in the middle of the video.</figDesc><table><row><cell>ResNet initialization</cell><cell>remove residual</cell><cell>PoseForecast</cell><cell>PA-MPJPE↓</cell><cell>Accel↓</cell></row><row><cell>ResNet with random initialization</cell><cell></cell><cell></cell><cell>126.5</cell><cell>24.3</cell></row><row><cell>ResNet pretrained on ImageNet [31]</cell><cell></cell><cell></cell><cell>103.7</cell><cell>65.5</cell></row><row><cell>ResNet from SPIN [16]</cell><cell></cell><cell></cell><cell>55.6</cell><cell>29.2</cell></row><row><cell>ResNet from SPIN [16] (TCMR. Ours.)</cell><cell></cell><cell></cell><cell>53.9</cell><cell>7.7</cell></row><row><cell>PoseTrack. PoseTrack [1] is a 2D benchmark for multi-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>person pose estimation and tracking in videos. It contains</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc>of the main manuscript.</figDesc><table><row><cell>input fps</cell><cell>PA-MPJPE↓</cell><cell>Accel↓</cell></row><row><cell>15</cell><cell>53.5</cell><cell>15.3</cell></row><row><cell>30</cell><cell>52.7</cell><cell>7.1</cell></row></table><note>man pose datasets to extract useful static features.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.youtube.com/watch?v=WB3nTnSQDII</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.youtube.com/watch?v=WB3nTnSQDII 4 https://www.youtube.com/watch?v=Opry3F6aB1I 5 https://youtu.be/dFQ6hkfkwz0 6 https://youtu.be/otdL5WVjwPg</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Posetrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Exploiting spatial-temporal relationships for 3D pose estimation via graph convolutional networks. ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><forename type="middle">Magnenat</forename><surname>Thalmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Forecasting human dynamics from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pose2Mesh: Graph convolutional network for 3D human pose and mesh recovery from a 2D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sim2real transfer learning for 3D human pose estimation: motion to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Hierarchical kinematic human mesh recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Georgakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Košecká</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">End-to-end recovery of human shape and pose. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<title level="m">Learning 3D human dynamics from video. CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">VIBE: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Convolutional mesh regression for single-image human shape reconstruction. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Mosh: Motion and shape capture from sparse markers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">SMPL: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3D human motion estimation via motion compression and refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Golestaneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACCV</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">I2L-MeshNet: Image-to-Lixel prediction network for accurate 3D human pose and mesh estimation from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Neural Body Fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">TexturePose: Supervising human mesh estimation with texture consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to estimate 3D human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3D human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">How robust is 3d human pose estimation to occlusion? IROS workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">István</forename><surname>Sárándi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timm</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeleton-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">BodyNet: Volumetric inference of 3D human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gul</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ego-pose estimation and forecasting as real-time pd control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
		<title level="m">Predicting 3d human dynamics from video. ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">From actemes to action: A strongly-supervised representation for detailed action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos G</forename><surname>Derpanis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<title level="m">On the continuity of rotation representations in neural networks. CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
