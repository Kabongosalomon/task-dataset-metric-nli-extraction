<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Alignment Network: A convolutional neural network for robust face alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Kowalski</surname></persName>
							<email>m.kowalski@ire.pw.edu.pl</email>
							<affiliation key="aff0">
								<orgName type="institution">Warsaw University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Naruniec</surname></persName>
							<email>j.naruniec@ire.pw.edu.pl</email>
							<affiliation key="aff0">
								<orgName type="institution">Warsaw University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Trzcinski</surname></persName>
							<email>t.trzcinski@ii.pw.edu.pl</email>
							<affiliation key="aff0">
								<orgName type="institution">Warsaw University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Alignment Network: A convolutional neural network for robust face alignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose Deep Alignment Network (DAN), a robust face alignment method based on a deep neural network architecture. DAN consists of multiple stages, where each stage improves the locations of the facial landmarks estimated by the previous stage. Our method uses entire face images at all stages, contrary to the recently proposed face alignment methods that rely on local patches. This is possible thanks to the use of landmark heatmaps which provide visual information about landmark locations estimated at the previous stages of the algorithm. The use of entire face images rather than patches allows DAN to handle face images with large variation in head pose and difficult initializations. An extensive evaluation on two publicly available datasets shows that DAN reduces the state-of-theart failure rate by up to 70%. Our method has also been submitted for evaluation as part of the Menpo challenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of face alignment is to localize a set of predefined facial landmarks (eye corners, mouth corners etc.) in an image of a face. Face alignment is an important component of many computer vision applications, such as face verification <ref type="bibr" target="#b26">[27]</ref>, facial emotion recognition <ref type="bibr" target="#b23">[24]</ref>, humancomputer interaction <ref type="bibr" target="#b5">[6]</ref> and facial motion capture <ref type="bibr" target="#b10">[11]</ref>.</p><p>Most of the face alignment methods introduced in the recent years are based on shape indexed features <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b28">29]</ref>. In these approaches image features, such as SIFT <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35]</ref> or learned features <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29]</ref>, are extracted from image patches extracted around each of the landmarks. The features are then used to iteratively refine the estimates of landmark locations. While those approaches can be successfully applied to face alignment in many photos, their performance on the most challenging datasets <ref type="bibr" target="#b21">[22]</ref> leaves room for improvement <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b28">29]</ref>. We believe that this is due to the fact that for the most difficult images the features extracted at disjoint patches do not provide enough information and can lead the method into a local minimum.</p><p>In this work, we address the above shortcoming by proposing a novel face alignment method which we dub Deep Alignment Network (DAN). It is based on a multistage neural network where each stage refines the landmark positions estimated at the previous stage, iteratively improving the landmark locations. The input to each stage of our algorithm (except the first stage) are a face image normalized to a canonical pose and an image learned from the dense layer of the previous stage. To make use of the entire face image during the process of face alignment, we additionally input at each stage a landmark heatmap, which is a key element of our system.</p><p>A landmark heatmap is an image with high intensity values around landmark locations where intensity decreases with the distance from the nearest landmark. The convolutional neural network can use the heatmaps to infer the current estimates of landmark locations in the image and thus refine them. An example of a landmark heatmap can be seen in <ref type="figure">Figure 1</ref> which shows an outline of our method. By using landmark heatmaps, our DAN algorithm is able to reduce the failure rate on the 300W public test set by a large margin of 72% with respect to the state of the art.</p><p>To summarize, the three main contributions of this work are the following:</p><p>1. We introduce landmark heatmaps which transfer the information about current landmark location estimates between the stages of our method. This improvement allows our method to make use of the entire image of a face, instead of local patches, and avoid falling into local minima.</p><p>2. The resulting robust face alignment method we propose in this paper reduces the failure rate by 60% on <ref type="figure">Figure 1</ref>. A diagram showing an outline of the proposed method. Each stage of the neural network refines the landmark location estimates produced by the previous stage, starting with an initial estimate S0. The connection layers form a link between the consecutive stages of the network by producing the landmark heatmaps Ht, feature images Ft and a transform Tt which is used to warp the input image to a canonical pose. By introducing landmark heatmaps and feature images we can transmit crucial information, including the landmark location estimates, between the stages of our method.</p><p>the 300W private test set <ref type="bibr" target="#b21">[22]</ref> and 72% on the 300-W public test set <ref type="bibr" target="#b21">[22]</ref> compared to the state of the art.</p><p>3. Finally, we publish both the source code of our implementation of the proposed method and the models used in the experiments.</p><p>The remainder of the paper is organized in the following manner. In section 2 we give an overview of the related work. In section 3 we provide a detailed description of the proposed method. Finally, in section 4 we perform an evaluation of DAN and compare it to the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Face alignment has a long history, starting with the early Active Appearance Models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref>, moving to Constrained Local Models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b0">1]</ref> and recently shifting to methods based on Cascaded Shape Regression (CSR) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b12">13]</ref> and deep learning <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>In CSR based methods, the face alignment begins with an initial estimate of the landmark locations which is then refined in an iterative manner. The initial shape S 0 is typically an average face shape placed in the bounding box returned by the face detector <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref>. Each CSR iteration is characterized by the following equation:</p><formula xml:id="formula_0">S t+1 = S t + r t (φ(I, S t )),<label>(1)</label></formula><p>where S t is the estimate of landmark locations at iteration t, r t is a regression function which returns the update to S t given a feature φ extracted from image I at the landmark locations. The main differences between the variety of CSR based methods introduced in the literature lie in the choice of the feature extraction method φ and the regression method r t . For instance, Supervised Descent Method (SDM) <ref type="bibr" target="#b31">[32]</ref> uses SIFT <ref type="bibr" target="#b18">[19]</ref> features and a simple linear regressor. LBF <ref type="bibr" target="#b20">[21]</ref> takes advantage of sparse features generated from binary trees and intensity differences of individual pixels. LBF uses Support Vector Regression <ref type="bibr" target="#b8">[9]</ref> for regression which, combined with the sparse features, leads to a very efficient method running at up to 3000 fps.</p><p>Coarse to Fine Shape Searching (CFSS) <ref type="bibr" target="#b34">[35]</ref>, similarly to SDM, uses SIFT features extracted at landmark locations. However the regression step of CSR is replaced with a search over the space of possible face shapes which goes from coarse to fine over several iterations. This reduces the probability of falling into a local minimum and thus improves convergence.</p><p>MIX <ref type="bibr" target="#b29">[30]</ref> also uses SIFT for feature extraction, while regression is performed using a mixture of experts, where each expert is specialized in a certain part of the space of face shapes. Moreover MIX, warps the input image before each iteration so that the current estimate of the face shape matches a predefined canonical face shape.</p><p>Mnemonic Descent Method (MDM) <ref type="bibr" target="#b28">[29]</ref> fuses the feature extraction and regression steps of CSR into a single Recurrent Neural Network that is trained end-to-end. MDM also introduces memory into the process which allows information to be passed between CSR iterations.</p><p>While all of the above mentioned methods perform face alignment based only on local patches, there are some methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b9">10]</ref> that estimate initial landmark positions using the entire face image and use local patches for refinement. In contrast, DAN localizes the landmarks based on the entire face image at all of its stages.</p><p>The use of heatmaps for face alignment related tasks precedes the proposed method. One method that uses heatmaps is <ref type="bibr" target="#b2">[3]</ref>, where a neural network outputs predictions in the form of a heatmap. In contrast, the proposed method uses heatmaps solely as a means for transferring information between stages.</p><p>The development of novel methods contributes greatly in advancing face alignment. However it cannot be overlooked that the publication of several large scale datasets of annotated face images <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> also had a crucial role in both improving the state of the art and the comparability of face alignment methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Alignment Network</head><p>In this section, we describe our method, which we call the Deep Alignment Network (DAN). DAN is inspired by the Cascade Shape Regression (CSR) framework, just like CSR our method starts with an initial estimate of the face shape S 0 which is refined over several iterations. However, in DAN we substitute each CSR iteration with a single stage of a deep neural network which performs both feature extraction and regression. The major difference between DAN and approaches based on CSR is that DAN extracts features from the entire face image rather than the patches around landmark locations. This is achieved by introducing additional input to each stage, namely a landmark heatmap which indicates the current estimates of the landmark positions within the global face image and transmits this information between the stages of our algorithm. An outline of the proposed method is shown in <ref type="figure">Figure 1</ref>.</p><p>Therefore, each stage of DAN takes three inputs: the input image I which has been warped so that the current landmark estimates are aligned with the canonical shape S 0 , a landmark heatmap H t and a feature image F t which is generated from a dense layer connected to the penultimate layer of the previous stage t − 1. The first stage only takes the input image as the initial landmarks are always assumed to be the average face shape S 0 located in the middle of the . Tt+1 is subsequently used to transform the input image I and St. The transformed shape Tt+1(St) is then used to generate the landmark heatmap Ht+1. The feature image Ft+1 is generated using the fc1 dense layer of the current stage t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>image.</head><p>A single stage of DAN consists of a feed-forward neural network which performs landmark location estimation and connection layers that generate the input for the next stage. The details of the feed-forward network are described in subsection 3.1. The connection layers consist of the Transform Estimation layer, the Image Transform layer, Landmark Transform layer, Heatmap Generation layer and Feature Generation layer. The structure of the connection layers is shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>The transform estimation layer generates the transform T t+1 , where t is the number of the stage. The transformation is used to warp the input image I and the current landmark estimates S t so that S t is close to the canonical shape S 0 . The transformed landmarks T t+1 (S t ) are passed to the heatmap generation layer. The inverse transform T −1 t+1 is used to map the output landmarks of the consecutive stage back into the original coordinate system.</p><p>The details of the Transform Estimation, Image Transform and Landmark Transforms layer are described in subsection 3.2. The Heatmap Generation and Feature Image layers are described in sections 3.3, 3.4. Section 3.5 details <ref type="table">Table 1</ref>. Structure of the feed-forward part of a Deep Alignment Network stage. The kernels are described as height × width × depth, stride.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head><p>Shape the training procedure.</p><formula xml:id="formula_1">-in Shape-out Kernel conv1a 112×112×1 112×112×64 3×3×1,1 conv1b 112×112×64 112×112×64 3×3×64,1 pool1 112×112×64 56×56×64 2×2×1,2 conv2a 56×56×64 56×56×128 3×3×64,1 conv2b 56×56×128 56×56×128 3×3×128,1 pool2 56×56×128 28×28×128 2×2×1,2 conv3a 28×28×128 28×28×256 3×3×128,1 conv3b 28×28×256 28×28×256 3×3×256,1 pool3 28×28×256 14×14×256 2×2×1,2 conv4a 14×14×256 14×14×512 3×3×256,1 conv4b 14×14×512 14×14×512 3×3×512,1 pool4 14×14×512 7×7×512 2×2×1,2 fc1 7×7×512 1×1×256 - fc2 1×1×256 1×1×136 -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feed-forward neural network</head><p>The structure of the feed-forward part of each stage is shown in <ref type="table">Table 1</ref>. With the exception of max pooling layers and the output layer, every layer takes advantage of batch normalization and uses Rectified Linear Units (ReLU) for activations. A dropout <ref type="bibr" target="#b25">[26]</ref> layer is added before the first fully connected layer. The last layer outputs the update ∆S t to the current estimate of the landmark positions.</p><p>The overall shape of the feed-forwad network was inspired by the network used in <ref type="bibr" target="#b24">[25]</ref> for the ImageNet ILSVRC 2014 competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Normalization to canonical shape</head><p>In DAN the input image I is transformed for each stage so that the current estimates of the landmarks are aligned with the canonical face shape S 0 . This normalization step allows the further stages of DAN to be invariant to a given family of transforms. This in turn simplifies the alignment task and improves accuracy.</p><p>The Transform Estimation layer of our network is responsible for estimating the parameters of transform T t+1 at the output of stage t. As input the layer takes the output of the current stage S t . Once T t+1 is estimated the Image Transform and the Landmark Transform layers transform the image I and landmarks S t to the canonical pose. The image is transformed using bilinear interpolation. Note that for the first stage of DAN the normalization step is not necessary since the input shape is always the average face shape S 0 , which is also the canonical face shape.</p><p>Since the input image is transformed, the output of every stage has to be transformed back to match the original image, the output of any DAN stage is thus:</p><formula xml:id="formula_2">S t = T −1 t (T t (S t−1 ) + ∆S t ),<label>(2)</label></formula><p>where ∆S t is the output of the last layer of stage t and T −1 t is the inverse of transform T t A similar normalization step has been previously proposed in <ref type="bibr" target="#b29">[30]</ref> with the use of affine transforms. In our implementation we chose to use similarity transforms as they do not cause non-uniform scaling and skewing of the output image. <ref type="figure" target="#fig_1">Figure 3</ref> shows examples of images before and after the transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Landmark heatmap</head><p>The landmark heatmap is an image where the intensity is highest in the locations of landmarks and it decreases with the distance to the closest landmark. Thanks to the use of landmark heatmaps the Convolutional Neural Network can infer the landmark locations estimated by the previous stage. In consequence DAN can perform face alignment based on entire facial images.</p><p>At the input to a DAN stage the landmark heatmap is created based on the landmark estimates produced by the previous stage and transformed to the canonical pose: T t (S t−1 ). The heatmap is generated using the following equation:</p><formula xml:id="formula_3">H(x, y) = 1 1 + min si∈Tt(St−1) ||(x, y) − s i || ,<label>(3)</label></formula><p>where H is the heatmap image and s i is the i-th landmark of T t (S t−1 ). In our implementation the heatmap values are only calculated in a circle of radius 16 around each landmark to improve performance. Note that similarly to normalization, this step is not necessary at the input of the first stage, since the input shape is always assumed to be S 0 , which would result in an identical heatmap for any input. An example of a face image and a corresponding landmark heatmap is shown in <ref type="figure" target="#fig_1">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Feature image layer</head><p>The feature image layer F t is an image created from a dense layer connected to the fc1 layer (see <ref type="table">Table 1</ref>) of the previous stage t − 1. Such a connection allows any information learned by the preceding stage to be transferred to the consecutive stage. This naturally complements the heatmap which transfers the knowledge about landmark locations learned by the previous stage.</p><p>The feature image layer is a dense layer which has 3136 units with ReLU activations. The output of this dense layer is reshaped to a 56×56 2D layer and upscaled to 112×112, which is the input shape of DAN stages. We use the smaller 56×56 image rather than 112×112 since it showed similar results in our experiments, with considerably less parameters. <ref type="figure" target="#fig_1">Figure 3</ref> shows an example of a feature image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training procedure</head><p>The stages of DAN are trained sequentially. The first stage is trained by itself until the validation error stops improving. Subsequently the connection layers and the second stage are added and trained. This procedure is repeated until further stages stop reducing the validation error.</p><p>While many face alignment methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b20">21]</ref> learn a model that minimizes the Sum of Squared Errors of the landmark locations, DAN minimizes the landmark location error normalized by the distance between the pupils:</p><formula xml:id="formula_4">min ∆St ||T −1 t (T t (S t−1 ) + ∆S t ) − S * || d ipd ,<label>(4)</label></formula><p>where S * is a vector of ground truth landmark locations, T t is the transform that normalizes the input image and shape for stage t and d ipd is the distance between the pupils of S * . The use of this error is motivated by the fact that it is a far more common <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35]</ref> benchmark for face alignment methods than the Sum of Squared Errors. Thanks to the fact that all of the layers used in DAN are differentiable DAN can also be trained end-to-end. In order to evaluate end-to-end training in DAN we have experimented with several approaches. Pre-training the first stage for several epochs followed by training of the entire network yielded similar accuracy to the proposed approach but the training was significantly longer. Training the entire network from scratch yielded results significantly inferior to the proposed approach.</p><p>While we did not manage to obtain improved results with end-to-end training we believe that it is possible with a better training strategy. We leave the creation of such a strategy for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we perform an extensive evaluation of the proposed method on several public datasets as well as the test set of the Menpo challenge <ref type="bibr" target="#b32">[33]</ref> to which we have submitted our method. The following paragraphs detail the datasets, error measures and implementation. Section 4.1 compares our method with the state of the art, while section 4.2 shows our results in the Menpo challenge. Section 4.3 discusses the influence of the number of stages on the performance of DAN.</p><p>Datasets In order to evaluate our method we perform experiments on the data released for the 300W competition <ref type="bibr" target="#b21">[22]</ref> and the recently introduced Menpo challenge <ref type="bibr" target="#b32">[33]</ref> dataset.</p><p>The 300W competition data is a compilation of images from five datasets: LFPW <ref type="bibr" target="#b1">[2]</ref>, HELEN <ref type="bibr" target="#b16">[17]</ref>, AFW <ref type="bibr" target="#b35">[36]</ref>, IBUG <ref type="bibr" target="#b21">[22]</ref> and 300W private test set <ref type="bibr" target="#b21">[22]</ref>. The last dataset was originally used for evaluating competition entries and at that time was private to the organizers of the competition, hence the name. Each image in the dataset is annotated with 68 landmarks <ref type="bibr" target="#b22">[23]</ref> and accompanied by a bounding box generated by a face detector. We follow the most established approach <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">31]</ref> and divide the 300-W competition data into training and testing parts. The training part consists of the AFW dataset as well as training subsets of LFPW and HELEN, which results in a total of 3148 images. The test data consists of the remaining datasets: IBUG, 300W private test set, test sets of LFPW, HELEN. In order to facilitate comparison with previous methods we split this test data into four subsets:</p><p>• the common subset which consists of the test subsets of LFPW and HELEN (554 images),</p><p>• the challenging subset which consists of the IBUG dataset (135 images),</p><p>• the 300W public test set which consists of the test subsets of LFPW and HELEN as well as the IBUG dataset (689 images),</p><p>• the 300W private test set (600 images).</p><p>The annotation for the images in the 300W public test set were originally published for the 300W competition as part of its training set. We use them for testing as it became a common practice to do so in the recent years <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>The Menpo challenge dataset consists of semi-frontal and profile face image datasets. In our experiments we only use the semi-frontal dataset. The dataset consists of training and testing subsets containing 6679 and 5335 images respectively. The training subset consists of images from the FDDB <ref type="bibr" target="#b11">[12]</ref> and AFLW <ref type="bibr" target="#b14">[15]</ref> datasets. The image were annotated with the same set of 68 landmarks as the 300W competition data but no face detector bounding boxes. The annotations of the test subset have not been released.</p><p>Error measures Several measures of face alignment error for an individual face image have been recently introduced:</p><p>• the mean distance between the localized landmarks and the ground truth landmarks divided by the interocular distance (the distance between the outer eye corners) <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref>,</p><p>• the mean distance between the localized landmarks and the ground truth landmarks divided by the interpupil distance (the distance between the eye centers) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b21">22]</ref>,</p><p>• the mean distance between the localized landmarks and the ground truth landmarks divided by the diagonal of the bounding box <ref type="bibr" target="#b32">[33]</ref>.</p><p>In our work, we report our results using all of the above measures. For evaluating our method on the test datasets we use three metrics: the mean error, the area under the cumulative error distribution curve (AUC α ) and the failure rate.</p><p>Similarly to <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b28">29]</ref>, we calculate AUC α as the area under the cumulative distribution curve calculated up to a threshold α, then divided by that threshold. As a result the range of the AUC α values is always 0 to 1. Following <ref type="bibr" target="#b28">[29]</ref>, we consider each image with an inter-ocular normalized error of 0.08 or greater as failure and use the same threshold for AUC 0.08 . In all the experiments we test on the full set of 68 landmarks.</p><p>Implementation We train two models, DAN which is trained on the training subset of the 300W competition data and DAN-Menpo which is trained on both the above mentioned dataset and the Menpo challenge training set. Data augmentation is performed by mirroring around the Y axis as well as random translation, rotation and scaling, all sampled from normal distributions. During data augmentation a total of 10 images are created from each input image in the training set.</p><p>Both models (DAN and DAN-Menpo) consist of two stages. Training is performed using Theano 0.9.0 <ref type="bibr" target="#b27">[28]</ref> and Lasagne 0.2 <ref type="bibr" target="#b7">[8]</ref>. For optimization we use Adam stochastic optimization <ref type="bibr" target="#b13">[14]</ref> with an initial step size of 0.001 and mini batch size of 64. For validation we use a random subset of 100 images from the training set.</p><p>The Python implementation runs at 73 fps for images processed in parallel and at 45 fps for images processed sequentially on a GeForce GTX 1070 GPU. We believe that the processing speed can be further improved by optimizing the implementation of some of our custom layers, most notably the Image Transform layer.</p><p>To enable reproducible research, we release the source code of our implementation as well as the models used in the experiments 1 . The published implementation also contains an example of face tracking with the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with state-of-the-art</head><p>We compare the DAN model with state-of-the-art methods on all of the test sets of the 300W competition data. We also show results for the DAN-Menpo model but do not perform comparison since at the moment there are no published methods that use this dataset for training. For each test set we initialize our method using the face detector bounding boxes provided with the datasets. <ref type="table" target="#tab_0">Tables 2 and 3</ref> show the mean error, AUC 0.08 and the failure rate of the proposed method and other methods on the 300W public test set. <ref type="table">Table 4</ref> shows the mean error, the AUC 0.08 and failure rate on the 300W private test set.</p><p>All of the experiments performed on the two most difficult test subsets (the challenging subset and the 300W private test set) show state-of-the-art results, including:</p><p>• a failure rate reduction of 60% on the 300W private test set,</p><p>• a failure rate reduction of 72% on the 300W public test set,</p><p>• a 9% improvement of the mean error on the challenging subset.</p><p>This shows that the proposed DAN is particularly suited for handling difficult face images with a high degree of occlusion and variation in pose and illumination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on the Menpo challenge test set</head><p>In order to evaluate the proposed method on the Menpo challenge test dataset we have submitted our results to the challenge and received the error scores from the challenge organizers. The Menpo test data differs from the other datasets we used in that it does not include any bounding boxes which could be used to initialize face alignment. For that reason we have decided to use a two step face alignment procedure, where the first step serves as an initialization for the second step.</p><p>The first step performs face alignment using a square initialization bounding box placed in the middle of the image with a size set to a percentage of image height. The second step takes the result of the first step, transforms the landmarks and the image to the canonical face shape and   In order to determine the optimal size of the bounding boxes in the first step we ran DAN on a small subset of the Menpo test set for several bounding box sizes. The optimal size was determined using a method that would estimate the face alignment error of a given set of landmarks and an image. Said method extracts HOG [?] features at each of the landmarks and uses a linear model to estimate the error. The method was trained on the 300W training set using ridge regression. The chosen bounding box size was 46% of the image height. <ref type="figure" target="#fig_4">Figure 6</ref> and <ref type="table" target="#tab_2">Table 5</ref> show the CED curve, mean error, AU C 0.03 and failure rate for the DAN-Menpo model on the Menpo test set. In all cases the errors are calculated using the diagonal of the bounding box normalization, used by the challenge organizers. For the AUC and the failure rate we have chosen a threshold of 0.03 of the bounding box diagonal as it is approximately equivalent to 0.08 of the interocular distance used in the previous chapter. <ref type="figure" target="#fig_3">Figure 5</ref> shows examples of images from the Menpo test set and corresponding results produced by our method. Note that even though DAN was trained primarily on semifrontal images it can handle fully profile images as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Further evaluation</head><p>In this subsection we evaluate several DAN models with a varying number of stages on the 300W private test set. All of the models were trained identically to the DAN model from section 4.1. <ref type="table" target="#tab_3">Table 6</ref> shows the results of our eval-   uation. The addition of the second stage increases the AUC 0.08 by 20% while the mean error and failure rate are reduced by 14% and 56% respectively. The addition of a third stage does not bring significant benefit in any of the metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we introduced the Deep Alignment Network -a robust face alignment method based on convo- lutional neural networks. Contrary to the recently proposed face alignment methods, DAN performs face alignment based on entire face images, which makes it highly robust to large variations in both initialization and head pose. Using entire face images instead of local patches extracted around landmarks is possible thanks to the use of novel landmark heatmaps which transmit the information about landmark locations between DAN stages. Extensive evaluation performed on two challenging, publicly available datasets shows that the proposed method improves the stateof-the-art failure rate by a significant margin of over 70%. Future research includes investigation of new strategies for training DAN in an end-to-end manner. We also plan to introduce learning into the estimation of the transform T t that normalizes the shapes and images between stages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>A diagram showing an outline of the connection layers. The landmark locations estimated by the current stage St are first used to estimate the normalizing transform Tt+1 and its inverse T −1 t+1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Selected images from the IBUG dataset and intermediate results after 1 stage of DAN. The columns show: the input image I, the input image normalized to canonical shape using transform T2, the landmark heatmap showing T2(S1), the corresponding feature image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>The 9 worst results on the challenging subset (IBUG dataset) in terms of inter-ocular error produced by the DAN model. Only the first 7 images have an error of more than 0.08 inter-ocular distance and can be considered failures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Results of our submission to the Menpo challenge on some of the difficult images of the Menpo test set. The blue squares denote the initialization bounding boxes. The images were cropped to better visualize the results, in the original images the bounding boxes are always located in the center.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>The Cumulative Error Distribution curve for the DAN-Menpo model on the Menpo test set. The Point-to-Point error is shown as percentage of the bounding box diagonal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Mean error of face alignment methods on the 300W public test set and its subsets. All values are shown as percentage of the normalization metric.</figDesc><table><row><cell>Method</cell><cell>Common subset</cell><cell>Challenging subset</cell><cell>Full set</cell></row><row><cell cols="3">inter-pupil normalization</cell><cell></cell></row><row><cell>ESR [4]</cell><cell>5.28</cell><cell>17.00</cell><cell>7.58</cell></row><row><cell>SDM [32]</cell><cell>5.60</cell><cell>15.40</cell><cell>7.52</cell></row><row><cell>LBF [21]</cell><cell>4.95</cell><cell>11.98</cell><cell>6.32</cell></row><row><cell>cGPRT [18]</cell><cell>-</cell><cell>-</cell><cell>5.71</cell></row><row><cell>CFSS [35]</cell><cell>4.73</cell><cell>9.98</cell><cell>5.76</cell></row><row><cell>Kowalski et al.</cell><cell>4.62</cell><cell>9.48</cell><cell>5.57</cell></row><row><cell>[16]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RAR [31]</cell><cell>4.12</cell><cell>8.35</cell><cell>4.94</cell></row><row><cell>DAN</cell><cell>4.42</cell><cell>7.57</cell><cell>5.03</cell></row><row><cell>DAN-Menpo</cell><cell>4.29</cell><cell>7.05</cell><cell>4.83</cell></row><row><cell cols="3">inter-ocular normalization</cell><cell></cell></row><row><cell>MDM [29]</cell><cell>-</cell><cell>-</cell><cell>4.05</cell></row><row><cell>Kowalski et al.</cell><cell>3.34</cell><cell>6.56</cell><cell>3.97</cell></row><row><cell>[16]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DAN</cell><cell>3.19</cell><cell>5.24</cell><cell>3.59</cell></row><row><cell>DAN-Menpo</cell><cell>3.09</cell><cell>4.88</cell><cell>3.44</cell></row><row><cell cols="3">bounding box diagonal normalization</cell><cell></cell></row><row><cell>DAN</cell><cell>1.35</cell><cell>2.00</cell><cell>1.48</cell></row><row><cell>DAN-Menpo</cell><cell>1.31</cell><cell>1.87</cell><cell>1.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>AUC and failure rate of face alignment methods on the 300W public test set. Results of face alignment methods on the 300W private test set. Mean error is shown as percentage of the inter-ocular distance.</figDesc><table><row><cell cols="2">Method</cell><cell cols="2">AUC 0.08 Failure (%)</cell></row><row><cell></cell><cell cols="2">inter-ocular normalization</cell><cell></cell></row><row><cell cols="2">ESR [4]</cell><cell>43.12</cell><cell>10.45</cell></row><row><cell cols="2">SDM [32]</cell><cell>42.94</cell><cell>10.89</cell></row><row><cell cols="2">CFSS [35]</cell><cell>49.87</cell><cell>5.08</cell></row><row><cell cols="2">MDM [29]</cell><cell>52.12</cell><cell>4.21</cell></row><row><cell cols="2">DAN</cell><cell>55.33</cell><cell>1.16</cell></row><row><cell cols="2">DAN-Menpo</cell><cell>57.07</cell><cell>0.58</cell></row><row><cell>Method</cell><cell cols="3">Mean error AUC 0.08 Failure (%)</cell></row><row><cell></cell><cell cols="2">inter-ocular normalization</cell><cell></cell></row><row><cell>ESR [4]</cell><cell>-</cell><cell>32.35</cell><cell>17.00</cell></row><row><cell>CFSS [35]</cell><cell>-</cell><cell>39.81</cell><cell>12.30</cell></row><row><cell>MDM [29]</cell><cell>5.05</cell><cell>45.32</cell><cell>6.80</cell></row><row><cell>DAN</cell><cell>4.30</cell><cell>47.00</cell><cell>2.67</cell></row><row><cell>DAN-</cell><cell>3.97</cell><cell>50.84</cell><cell>1.83</cell></row><row><cell>Menpo</cell><cell></cell><cell></cell><cell></cell></row></table><note>creates a bounding box around the transformed landmarks. The transformed image and bounding box are used as input to face alignment. An inverse transform is later applied to get landmark coordinates for the original image.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Results of the proposed method on the semi-frontal subset of the Menpo test set. Mean error is shown as percentage of the bounding box diagonal.</figDesc><table><row><cell>Method</cell><cell cols="3">Mean error AUC 0.03 Failure (%)</cell></row><row><cell cols="4">bounding box diagonal normalization</cell></row><row><cell>DAN-</cell><cell>1.38</cell><cell>56.20</cell><cell>1.74</cell></row><row><cell>Menpo</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>Results of the proposed method with a varying number of stages on the 300W private test set. Mean error is shown as percentage of the inter-ocular distance.</figDesc><table><row><cell># of stages</cell><cell cols="3">Mean error AUC 0.08 Failure (%)</cell></row><row><cell></cell><cell cols="2">inter-ocular normalization</cell><cell></cell></row><row><cell>1</cell><cell>5.02</cell><cell>39.04</cell><cell>6.17</cell></row><row><cell>2</cell><cell>4.30</cell><cell>47.00</cell><cell>2.67</cell></row><row><cell>3</cell><cell>4.32</cell><cell>47.08</cell><cell>2.67</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/MarekKowalski/ DeepAlignmentNetwork</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>The work presented in this article was supported by The National Centre for Research and Development grant number DOB-BIO7/18/02/2015. The results obtained in this work were a basis for developing the software for the grant sponsor, which is different from the software published with this paper.</p><p>We thank NVIDIA for donating a Titan X Pascal GPU which was used to train the proposed neural network.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust discriminative response map fitting with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3444" to="3451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Localizing parts of faces using a consensus of exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2930" to="2940" />
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Two-stage convolutional part heatmap regression for the 1st 3d face alignment in the wild (3dfaw) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshop, 14th European Conference on</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Realtime eye gaze tracking for gaming design and consumer electronics systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Corcoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petrescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bigioi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Consumer Electronics</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="347" to="355" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Feature detection and tracking with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schlter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Snderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nouri</surname></persName>
		</author>
		<title level="m">First release</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Support vector regression machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="155" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Approaching human level facial landmark localization by deep learning. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unconstrained realtime facial performance capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2015 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1675" to="1683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fddb: A benchmark for face detection in unconstrained settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>UM- CS-2010-009</idno>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Representations, 3rd International Conference on</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, realworld database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First IEEE International Workshop on Benchmarking Facial Image Analysis Technologies</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Face alignment using k-cluster regression forests with weighted splitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Naruniec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 12th European Conference on</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="679" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Face alignment using cascade gaussian process regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2015 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Active appearance models revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="164" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops, 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="397" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A semi-automatic methodology for facial landmark annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Worshops, 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic analysis of facial affect: A survey of registration, representation, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sariyanidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1113" to="1133" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. Computing Research Repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theano</forename><surname>Development Team</surname></persName>
		</author>
		<idno>abs/1605.02688</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>arXiv eprints</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2016 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Robust Face Alignment Using a Mixture of Invariant Experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tambe</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="825" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Robust Facial Landmark Detection via Recurrent Attentive-Refinement Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kassim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The menpo facial landmark localisation challenge: A step closer to the solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 1st Faces in-the-wild Workshop</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Extensive facial landmark localization with coarse-to-fine convolutional network cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops, 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2015 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2012 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
