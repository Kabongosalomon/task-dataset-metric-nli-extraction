<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanqiu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Cai</surname></persName>
							<email>jycai@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wang</surname></persName>
							<email>jiewangx@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graph embedding, which aims to represent entities and relations as low dimensional vectors (or matrices, tensors, etc.), has been shown to be a powerful technique for predicting missing links in knowledge graphs. Existing knowledge graph embedding models mainly focus on modeling relation patterns such as symmetry/antisymmetry, inversion, and composition. However, many existing approaches fail to model semantic hierarchies, which are common in real-world applications. To address this challenge, we propose a novel knowledge graph embedding model-namely, Hierarchy-Aware Knowledge Graph Embedding (HAKE)which maps entities into the polar coordinate system. HAKE is inspired by the fact that concentric circles in the polar coordinate system can naturally reflect the hierarchy. Specifically, the radial coordinate aims to model entities at different levels of the hierarchy, and entities with smaller radii are expected to be at higher levels; the angular coordinate aims to distinguish entities at the same level of the hierarchy, and these entities are expected to have roughly the same radii but different angles. Experiments demonstrate that HAKE can effectively model the semantic hierarchies in knowledge graphs, and significantly outperforms existing state-of-the-art methods on benchmark datasets for the link prediction task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs are usually collections of factual triples-(head entity, relation, tail entity), which represent human knowledge in a structured way. In the past few years, we have witnessed the great achievement of knowledge graphs in many areas, such as natural language processing , question answering <ref type="bibr" target="#b5">(Huang et al. 2019)</ref>, and recommendation systems <ref type="bibr" target="#b22">(Wang et al. 2018)</ref>.</p><p>Although commonly used knowledge graphs contain billions of triples, they still suffer from the incompleteness problem that a lot of valid triples are missing, as it is impractical to find all valid triples manually. Therefore, knowledge graph completion, also known as link prediction in knowledge graphs, has attracted much attention recently. Link prediction aims to automatically predict missing links between entities based on known links. It is a challenging task as we * Equal contribution. † Corresponding author.</p><p>not only need to predict whether there is a relation between two entities, but also need to determine which relation it is. Inspired by word embeddings <ref type="bibr" target="#b11">(Mikolov et al. 2013</ref>) that can well capture semantic meaning of words, researchers turn to distributed representations of knowledge graphs (aka, knowledge graph embeddings) to deal with the link prediction problem. Knowledge graph embeddings regard entities and relations as low dimensional vectors (or matrices, tensors), which can be stored and computed efficiently. Moreover, like in the case of word embeddings, knowledge graph embeddings can preserve the semantics and inherent structures of entities and relations. Therefore, other than the link prediction task, knowledge graph embeddings can also be used in various downstream tasks, such as triple classification <ref type="bibr" target="#b8">(Lin et al. 2015)</ref>, relation inference <ref type="bibr" target="#b4">(Guo, Sun, and Hu 2019)</ref>, and search personalization <ref type="bibr" target="#b14">(Nguyen et al. 2019)</ref>.</p><p>The success of existing knowledge graph embedding models heavily relies on their ability to model connectivity patterns of the relations, such as symmetry/antisymmetry, inversion, and composition . For example, TransE <ref type="bibr" target="#b1">(Bordes et al. 2013)</ref>, which represent relations as translations, can model the inversion and composition patterns. DistMult <ref type="bibr" target="#b25">(Yang et al. 2015)</ref>, which models the threeway interactions between head entities, relations, and tail entities, can model the symmetry pattern. RotatE , which represents entities as points in a complex space and relations as rotations, can model relation patterns including symmetry/antisymmetry, inversion, and composition. However, many existing models fail to model semantic hierarchies in knowledge graphs.</p><p>Semantic hierarchy is a ubiquitous property in knowledge graphs. For instance, WordNet <ref type="bibr" target="#b12">(Miller 1995)</ref> contains the triple [arbor/cassia/palm, hypernym, tree], where "tree" is at a higher level than "arbor/cassia/palm" in the hierarchy. Freebase <ref type="bibr" target="#b0">(Bollacker et al. 2008)</ref> contains the triple [England, /location/location/contains, Pontefract/Lancaster], where "Pontefract/Lancaster" is at a lower level than "England" in the hierarchy. Although there exists some work that takes the hierarchy structures into account <ref type="bibr" target="#b24">(Xie, Liu, and Sun 2016;</ref><ref type="bibr" target="#b26">Zhang et al. 2018)</ref>, they usually require additional data or process to obtain the hierarchy information. Therefore, it is still challenging to find an approach that is capable of modeling the semantic hierarchy automatically and effectively.</p><p>In this paper, we propose a novel knowledge graph embedding model-namely, Hierarchy-Aware Knowledge Graph Embedding (HAKE). To model the semantic hierarchies, HAKE is expected to distinguish entities in two categories: (a) at different levels of the hierarchy; (b) at the same level of the hierarchy. Inspired by the fact that entities that have the hierarchical properties can be viewed as a tree, we can use the depth of a node (entity) to model different levels of the hierarchy. Thus, we use modulus information to model entities in the category (a), as the size of moduli can reflect the depth. Under the above settings, entities in the category (b) will have roughly the same modulus, which is hard to distinguish. Inspired by the fact that the points on the same circle can have different phases, we use phase information to model entities in the category (b). Combining the modulus and phase information, HAKE maps entities into the polar coordinate system, where the radial coordinate corresponds to the modulus information and the angular coordinate corresponds to the phase information. Experiments show that our proposed HAKE model can not only clearly distinguish the semantic hierarchies of entities, but also significantly and consistently outperform several state-of-the-art methods on the benchmark datasets.</p><p>Notations Throughout this paper, we use lower-case letters h, r, and t to represent head entities, relations, and tail entities, respectively. The triplet (h, r, t) denotes a fact in knowledge graphs. The corresponding boldface lower-case letters h, r and t denote the embeddings (vectors) of head entities, relations, and tail entities. The i-th entry of a vector h is denoted as [h] i . Let k denote the embedding dimension.</p><p>Let • : R n × R n → R n denote the Hadamard product between two vectors, that is,</p><formula xml:id="formula_0">[a • b] i = [a] i · [b] i ,</formula><p>and · 1 , · 2 denote the 1 and 2 norm, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we will describe the related work and the key differences between them and our work in two aspects-the model category and the way to model hierarchy structures in knowledge graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Category</head><p>Roughly speaking, we can divide knowledge graph embedding models into three categories-translational distance models, bilinear models, and neural network based models. <ref type="table" target="#tab_0">Table 1</ref> exhibits several popular models.</p><p>Translational distance models describe relations as translations from source entities to target entities. TransE <ref type="bibr" target="#b1">(Bordes et al. 2013)</ref> supposes that entities and relations satisfy h + r ≈ t, where h, r, t ∈ R n , and defines the corresponding score function as f r (h, t) = − h+r−t 1/2 . However, TransE does not perform well on 1-N, N-1 and N-N relations <ref type="bibr" target="#b21">(Wang et al. 2014)</ref>. TransH <ref type="bibr" target="#b21">(Wang et al. 2014</ref>) overcomes the many-to-many relation problem by allowing entities to have distinct representations given different relations.</p><p>The score function is defined as f r (h, t) = − h ⊥ +r−t ⊥ 2 , where h ⊥ and t ⊥ are the projections of entities onto relationspecific hyperplanes. ManifoldE <ref type="bibr" target="#b23">(Xiao, Huang, and Zhu 2016)</ref> deals with many-to-many problems by relaxing the hypothesis h + r ≈ t to h + r − t 2 2 ≈ θ 2 r for each valid triple. In this way, the candidate entities can lie on a manifold instead of exact point. The corresponding score function is defined as f r (h, t) = −( h+r−t 2 2 −θ 2 r ) 2 . More recently, to better model symmetric and antisymmetric relations, Ro-tatE  defines each relation as a rotation from source entities to target entities in a complex vector space. The score function is defined as</p><formula xml:id="formula_1">f r (h, t) = − h • r − t 1 , where h, r, t ∈ C k and |[r] i | = 1.</formula><p>Bilinear models product-based score functions to match latent semantics of entities and relations embodied in their vector space representations. RESCAL <ref type="bibr">(Nickel, Tresp, and Kriegel 2011)</ref> represents each relation as a full rank matrix, and defines the score function as f r (h, t) = h M r t, which can also be seen as a bilinear function. As full rank matrices are prone to overfitting, recent works turn to make additional assumptions on M r . For example, DistMult <ref type="bibr" target="#b25">(Yang et al. 2015)</ref> assumes M r to be a diagonal matrix, and ANAL-OGY <ref type="bibr" target="#b9">(Liu, Wu, and Yang 2017)</ref> supposes that M r is normal. However, these simplified models are usually less expressive and not powerful enough for general knowledge graphs. Differently, ComplEx <ref type="bibr" target="#b20">(Trouillon et al. 2016)</ref> extends DistMult by introducing complex-valued embeddings to better model asymmetric and inverse relations. HolE <ref type="bibr" target="#b15">(Nickel, Rosasco, and Poggio 2016)</ref> combines the expressive power of RESCAL with the efficiency and simplicity of DistMult by using the circular correlation operation.</p><p>Neural network based models have received greater attention in recent years. For example, MLP <ref type="bibr" target="#b3">(Dong et al. 2014)</ref> and NTN <ref type="bibr" target="#b17">(Socher et al. 2013</ref>) use a fully connected neural network to determine the scores of given triples. ConvE <ref type="bibr" target="#b2">(Dettmers et al. 2018)</ref> and ConvKB <ref type="bibr" target="#b13">(Nguyen et al. 2018)</ref> employ convolutional neural networks to define score functions. Recently, graph convolutional networks are also introduced, as knowledge graphs obviously have graph structures <ref type="bibr" target="#b16">(Schlichtkrull et al. 2018</ref>).</p><p>Our proposed model HAKE belongs to the translational distance models. More specifically, HAKE shares similarities with RotatE , in which the authors claim that they use both modulus and phase information. However, there exist two major differences between RotatE and HAKE. Detailed differences are as follows.</p><p>(a) The aims are different. RotatE aims to model the relation patterns including symmetry/antisymmetry, inversion, and composition. HAKE aims to model the semantic hierarchy, while it can also model all the relation patterns mentioned above.</p><p>(b) The ways to use modulus information are different. Ro-tatE models relations as rotations in the complex space, which encourages two linked entities to have the same modulus, no matter what the relation is. The different moduli in RotatE come from the inaccuracy in training. Instead, HAKE explicitly models the modulus information, which significantly outperforms RotatE in distin- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Score Function f r (h, t) Parameters TransE <ref type="bibr" target="#b1">(Bordes et al. 2013</ref> <ref type="bibr" target="#b23">(Xiao, Huang, and Zhu 2016)</ref> −</p><formula xml:id="formula_2">) − h + r − t 1/2 h, r, t ∈ R k TransR (Lin et al. 2015) − M r h + r − M r t 2 h, t ∈ R d , r ∈ R k , M r ∈ R k×d ManifoldE</formula><formula xml:id="formula_3">( h + r − t 2 2 − θ 2 r ) 2 h, r, t ∈ R k RotatE (Sun et al. 2019) − h • r − t 2 h, r, t ∈ C k , |r i | = 1 RESCAL (Nickel, Tresp, and Kriegel 2011) h M r t h, t ∈ R k , M r ∈ R k×k DistMult (Yang et al. 2015) h diag(r)t h, r, t ∈ R k ComplEx (Trouillon et al. 2016) Re(h diag(r)t) h, r, t ∈ C k ConvE (Dettmers et al. 2018) f (vec(f ([r,h] * ω))W)t h, r, t ∈ R k HAKE − h m • r m − t m 2 − λ sin((h p + r p − t p )/2) 1 h m , t m ∈ R k ,r m ∈ R k + , h p , r p , t p ∈ [0, 2π) k , , λ ∈ R</formula><p>guishing entities at different levels of the hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Ways to Model Hierarchy Structures</head><p>Another related problem is how to model hierarchy structures in knowledge graphs. Some recent work considers the problem in different ways. <ref type="bibr" target="#b7">Li et al. (2016)</ref> embed entities and categories jointly into a semantic space and designs models for the concept categorization and dataless hierarchical classification tasks. <ref type="bibr" target="#b26">Zhang et al. (2018)</ref> use clustering algorithms to model the hierarchical relation structures. <ref type="bibr" target="#b24">Xie, Liu, and Sun (2016)</ref> proposed TKRL, which embeds the type information into knowledge graph embeddings. That is, TKRL requires additional hierarchical type information for entities.</p><p>Different from the previous work, our work (a) considers the link prediction task, which is a more common task for knowledge graph embeddings;</p><p>(b) can automatically learn the semantic hierarchy in knowledge graphs without using clustering algorithms;</p><p>(c) does not require any additional information other than the triples in knowledge graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed HAKE</head><p>In this section, we introduce our proposed model HAKE. We first introduce two categories of entities that reflect the semantic hierarchies in knowledge graphs. Afterwards, we introduce our proposed HAKE that can model entities in both of the categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two Categories of Entities</head><p>To model the semantic hierarchies of knowledge graphs, a knowledge graph embedding model must be capable of distinguishing entities in the following two categories.</p><p>(a) Entities at different levels of the hierarchy. For example, "mammal" and "dog", "run" and "move".</p><p>(b) Entities at the same level of the hierarchy. For example, "rose" and "peony", "truck" and "lorry".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchy-Aware Knowledge Graph Embedding</head><p>To model both of the above categories, we propose a hierarchy-aware knowledge graph embedding model-HAKE. HAKE consists of two parts-the modulus part and the phase part-which aim to model entities in the two different categories, respectively. <ref type="figure">Figure 1</ref> gives an illustration of the proposed model. To distinguish embeddings in the different parts, we use e m (e can be h or t) and r m to denote the entity embedding and relation embedding in the modulus part, and use e p (e can be h or t) and r p to denote the entity embedding and relation embedding in the phase part.</p><p>The modulus part aims to model the entities at different levels of the hierarchy. Inspired by the fact that entities that have hierarchical property can be viewed as a tree, we can use the depth of a node (entity) to model different levels of the hierarchy. Therefore, we use modulus information to model entities in the category (a), as moduli can reflect the depth in a tree. Specifically, we regard each entry of h m and t m , that is, [h m ] i and [t m ] i , as a modulus, and regard each entry of r m , that is, [r] i , as a scaling transformation between two moduli. We can formulate the modulus part as follows:</p><formula xml:id="formula_4">h m • r m = t m , where h m , t m ∈ R k , and r m ∈ R k + . The corresponding distance function is: d r,m (h m , t m ) = h m • r m − t m 2 .</formula><p>Note that we allow the entries of entity embeddings to be negative but restrict the entries of relation embeddings to be positive. This is because that the signs of entity embeddings can help us to predict whether there exists a relation between two entities. For example, if there exists a relation r between h and t 1 , and no relation between h and t 2 , then (h, r, t 1 ) is a positive sample and (h, r, t 2 ) is a negative sample. Our goal is to minimize d r (h m , t 1,m ) and maximize d r (h m , t 2,m ), so as to make a clear distinction between positive and negative samples. For the positive sample, [h] i and [t 1 ] i tend to share the same sign, as [r m ] i &gt; 0. For the negative sample, the signs of [h m ] i and [t 2,m ] i can be different if we initialize their signs randomly. In this way, <ref type="figure">Figure 1</ref>: Simple illustration of HAKE. In a polar coordinate system, the radial coordinate aims to model entities at different levels of the hierarchy, and the angular coordinate aims to distinguish entities at the same level of the hierarchy.</p><formula xml:id="formula_5">d r (h m , t 2,m ) is more ! " # + % # " &amp; ∘ % &amp; Device Support Shelf Bookend Source Lamp Light ( " % # % &amp; ! |" # + % # − ( # | |" &amp; ∘ % &amp; −( &amp; |</formula><p>likely to be larger than d r (h m , t 1,m ), which is exactly what we desire. We will validate this argument by experiments in Section 4 of the supplementary material. Further, we can expect the entities at higher levels of the hierarchy to have smaller modulus, as these entities are more close to the root of the tree.</p><p>If we use only the modulus part to embed knowledge graphs, then the entities in the category (b) will have the same modulus. Moreover, suppose that r is a relation that reflects the same semantic hierarchy, then [r] i will tend to be one, as h • r • r = h holds for all h. Hence, embeddings of the entities in the category (b) tend to be the same, which makes it hard to distinguish these entities. Therefore, a new module is required to model the entities in the category (b).</p><p>The phase part aims to model the entities at the same level of the semantic hierarchy. Inspired by the fact that points on the same circle (that is, have the same modulus) can have different phases, we use phase information to distinguish entities in the category (b). Specifically, we regard each entry of h p and t p , that is, [h p ] i and [t p ] i as a phase, and regard each entry of r p , that is, [r p ] i , as a phase transformation. We can formulate the phase part as follows:</p><formula xml:id="formula_6">(h p + r p )mod 2π = t p , where h p , r p , t p ∈ [0, 2π) k .</formula><p>The corresponding distance function is:</p><formula xml:id="formula_7">d r,p (h p , t p ) = sin((h p + r p − t p )/2) 1 ,</formula><p>where sin(·) is an operation that applies the sine function to each element of the input. Note that we use a sine function to measure the distance between phases instead of using h p + r p − t p 1 , as phases have periodic characteristic. This distance function shares the same formulation with that of pRotatE .</p><p>Combining the modulus part and the phase part, HAKE maps entities into the polar coordinate system, where the radial coordinate and the angular coordinates correspond to the modulus part and the phase part, respectively. That is, HAKE maps an entity h to [h m ; h p ], where h m and h p are generated by the modulus part and the phase part, respectively, and [ · ; · ] denotes the concatenation of two vectors. Obviously, ([h m ] i , [h p ] i ) is a 2D point in the polar coordinate system. Specifically, we formulate HAKE as follows:</p><formula xml:id="formula_8">h m • r m = t m , where h m , t m ∈ R k , r m ∈ R k + , (h p + r p )mod 2π = t p , where h p , t p , r p ∈ [0, 2π) k .</formula><p>The distance function of HAKE is:</p><formula xml:id="formula_9">d r (h, t) = d r,m (h m , t m ) + λd r,p (h p , t p ),</formula><p>where λ ∈ R is a parameter that learned by the model. The corresponding score function is</p><formula xml:id="formula_10">f r (h, t) = d r (h, t) = −d r,m (h, t) − λd r,p (h, t).</formula><p>When two entities have the same moduli, then the modulus part d r,m (h m , t m ) = 0. However, the phase part d r,p (h p , t p ) can be very different. By combining the modulus part and the phase part, HAKE can model the entities in both the category (a) and the category (b). Therefore, HAKE can model semantic hierarchies of knowledge graphs.</p><p>When evaluating the models, we find that adding a mixture bias to d r,m (h, t) can help to improve the performance of HAKE. The modified d r,m (h, t) is given by:</p><formula xml:id="formula_11">d r,m (h, t) = h m • r m + (h m + t m ) • r m − t m 2 ,</formula><p>where −r m &lt; r m &lt; 1 is a vector that have the same dimension with r m . Indeed, the above distance function is equivalent to</p><formula xml:id="formula_12">d r,m (h, t) = h m • ((r m + r m )/(1 − r m )) − t m 2 ,</formula><p>where / denotes the element-wise division operation. If we let r m ← (r m + r m )/(1 − r m ), then the modified distance function is exactly the same as the original one when compare the distances of different entity pairs. For notation convenience, we still use d r,m (h, t) = h m • r m − t m 2 to represent the modulus part. We will conduct ablation studies on the bias in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>To train the model, we use the negative sampling loss functions with self-adversarial training :</p><formula xml:id="formula_13">L = − log σ(γ − d r (h, t)) − n i=1 p(h i , r, t i ) log σ(d r (h i , t i ) − γ),</formula><p>where γ is a fixed margin, σ is the sigmoid function, and (h i , r, t i ) is the ith negative triple. Moreover,</p><formula xml:id="formula_14">p(h j , r, t j |{(h i , r i , t i )}) = exp αf r (h j , t j ) i exp αf r (h i , t i )</formula><p>is the probability distribution of sampling negative triples, where α is the temperature of sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Analysis</head><p>This section is organized as follows. First, we introduce the experimental settings in detail. Then, we show the effectiveness of our proposed model on three benchmark datasets. Finally, we analyze the embeddings generated by HAKE, and show the results of ablation studies. The code of HAKE is available on GitHub at https://github.com/MIRALab-USTC/KGE-HAKE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Settings</head><p>We evaluate our proposed models on three commonly used knowledge graph datasets-WN18RR <ref type="bibr" target="#b19">(Toutanova and Chen 2015)</ref>, FB15k-237 <ref type="bibr" target="#b2">(Dettmers et al. 2018)</ref>, and YAGO3-10 <ref type="bibr" target="#b10">(Mahdisoltani, Biega, and Suchanek 2013)</ref>. Details of these datasets are summarized in <ref type="table" target="#tab_1">Table 2.</ref> WN18RR, FB15k-237, and YAGO3-10 are subsets of WN18 <ref type="bibr" target="#b1">(Bordes et al. 2013</ref>), FB15k <ref type="bibr" target="#b1">(Bordes et al. 2013)</ref>, and YAGO3 <ref type="bibr">(Mahdisoltani, Biega, and</ref> Suchanek 2013), respectively. As pointed out by <ref type="bibr" target="#b19">Toutanova and Chen (2015)</ref> and <ref type="bibr" target="#b2">Dettmers et al. (2018)</ref>, WN18 and FB15k suffer from the test set leakage problem. One can attain the state-of-the-art results even using a simple rule based model. Therefore, we use WN18RR and FB15k-237 as the benchmark datasets. Evaluation Protocol Following <ref type="bibr" target="#b1">Bordes et al. (2013)</ref>, for each triple (h, r, t) in the test dataset, we replace either the head entity h or the tail entity t with each candidate entity to create a set of candidate triples. We then rank the candidate triples in descending order by their scores. It is worth noting that we use the "Filtered" setting as in <ref type="bibr" target="#b1">Bordes et al. (2013)</ref>, which does not take any existing valid triples into accounts at ranking. We choose Mean Reciprocal Rank (MRR) and Hits at N (H@N) as the evaluation metrics. Higher MRR or H@N indicate better performance. Training Protocol We use Adam <ref type="bibr" target="#b6">(Kingma and Ba 2015)</ref> as the optimizer, and use grid search to find the best hyperparameters based on the performance on the validation datasets. To make the model easier to train, we add an additional coefficient to the distance function, i.e., d r (h, t) =</p><formula xml:id="formula_15">λ 1 d r,m (h m , t m ) + λ 2 d r,p (h p , t p ), where λ 1 , λ 2 ∈ R.</formula><p>Baseline Model One may argue that the phase part is unnecessary, as we can distinguish entities in the category (b) by allowing [r] i to be negative. We propose a model-ModEthat uses only the modulus part but allow [r] i &lt; 0. Specifically, the distance function of ModE is</p><formula xml:id="formula_16">d r (h, t) = h • r − t 2 , where h, r, t ∈ R k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head><p>In this part, we show the performance of our proposed models-HAKE and ModE-against existing state-of-theart methods, including TransE <ref type="bibr" target="#b1">(Bordes et al. 2013</ref>), DistMult <ref type="bibr" target="#b25">(Yang et al. 2015)</ref>, ComplEx <ref type="bibr" target="#b20">(Trouillon et al. 2016)</ref>, ConvE <ref type="bibr" target="#b2">(Dettmers et al. 2018)</ref>, and RotatE . <ref type="table" target="#tab_2">Table 3</ref> shows the performance of HAKE, ModE, and several previous models. Our baseline model ModE shares similar simplicity with TransE, but significantly outperforms it on all datasets. Surprisingly, ModE even outperforms more complex models such as DistMult, ConvE and Complex on all datasets, and beats the state-of-the-art model-RotatEon FB15k-237 and YAGO3-10 datasets, which demonstrates the great power of modulus information. <ref type="table" target="#tab_2">Table 3</ref> also shows that our HAKE significantly outperforms existing state-ofthe-art methods on all datasets.</p><p>WN18RR dataset consists of two kinds of relations: the symmetric relations such as similar to, which link entities in the category (b); other relations such as hypernym and member meronym, which link entities in the category (a). Actually, RotatE can model entities in the category (b) very well ). However, HAKE gains a 0.021 higher MRR, a 2.4% higher H@1, and a 2.4% higher H@3 against RotatE, respectively. The superior performance of HAKE compared with RotatE implies that our proposed model can better model different levels in the hierarchy. FB15k-237 dataset has more complex relation types and fewer entities, compared with WN18RR and YAGO3-10. Although there are relations that reflect hierarchy in FB15k-237, there are also lots of relations, such as "/location/location/time zones" and "/film/film/prequel", that do not lead to hierarchy. The characteristic of this dataset accounts for why our proposed models doesn't outperform the previous state-of-the-art as much as that of WN18RR and YAGO3-10 datasets. However, the results also show that our models can gain better performance so long as there exists semantic hierarchies in knowledge graphs. As almost all knowledge graphs have such hierarchy structures, our model is widely applicable.</p><p>YAGO3-10 datasets contains entities with high relationspecific indegree <ref type="bibr" target="#b2">(Dettmers et al. 2018)</ref>. For example, the link prediction task (?, hasGender, male) has over 1000 true answers, which makes the task challenging. Fortunately, we can regard "male" as an entity at higher level of the hierarchy and the predicted head entities as entities at lower level. In this way, YAGO3-10 is a dataset that clearly has semantic hierarchy property, and we can expect that our proposed models is capable of working well on this dataset. Table 3 validates our expectation. Both ModE and HAKE significantly outperform the previous state-of-the-art. Notably, HAKE gains a 0.050 higher MRR, 6.0% higher H@1 and 4.6% higher H@3 than RotatE, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis on Relation Embeddings</head><p>In this part, we first show that HAKE can effectively model the hierarchy structures by analyzing the moduli of relation embeddings. Then, we show that the phase part of HAKE can help us to distinguish entities at the same level of the hierarchy by analyzing the phases of relation embeddings.</p><p>In <ref type="figure" target="#fig_0">Figure 2</ref>, we plot the distribution histograms of moduli of six relations. These relations are drawn from WN18RR, FB15k-237, and YAGO3-10. Specifically, the relations in <ref type="figure" target="#fig_0">Figures 2a, 2c, 2e and 2f</ref> are drawn from WN18RR. The relation in <ref type="figure" target="#fig_0">Figure 2d</ref> is drawn from FB15k-237. The relation in <ref type="figure" target="#fig_0">Figure 2b</ref> is drawn from YAGO3-10. We divide the relations in <ref type="figure" target="#fig_0">Figure 2</ref> into three groups.</p><p>(A) Relations in <ref type="figure" target="#fig_0">Figures 2c and 2d</ref> connect the entities at the same level of the semantic hierarchy;  <ref type="bibr" target="#b13">Nguyen et al. (2018)</ref> and <ref type="bibr" target="#b18">Sun et al. (2019)</ref>, respectively. Other results are taken from <ref type="bibr" target="#b2">Dettmers et al. (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WN18RR</head><p>FB15k-237 YAGO3-10 MRR H@1 H@3 H@10 MRR H@1 H@3 H@10 MRR H@1 H@3 H@10   <ref type="figure" target="#fig_0">Figures 2a and 2b</ref> represent that tail entities are at higher levels than head entities of the hierarchy;</p><p>(C) Relations in <ref type="figure" target="#fig_0">Figures 2e and 2f</ref> represent that tail entities are at lower levels than head entities of the hierarchy.</p><p>As described in the model description section, we expect entities at higher levels of the hierarchy to have small moduli. The experiments validate our expectation. For both ModE and HAKE, most entries of the relations in the group (A) take values around one, which leads to that the head entities and tail entities have approximately the same moduli. In the group (B), most entries of the relations take values less than one, which results in that the head entities have smaller moduli than the tail entities. The cases in the group (C) are contrary to that in the group (B). These results show that our model can capture the semantic hierarchies in knowledge graphs. Moreover, compared with ModE, the relation embeddings' moduli of HAKE have lower variances, which shows that HAKE can model hierarchies more clearly.</p><p>As mentioned above, relations in the group (A) reflect the same semantic hierarchy, and are expected to have the moduli of about one. Obviously, it is hard to distinguish entities linked by these relations only using the modulus part. In <ref type="figure" target="#fig_1">Figure 3</ref>, we plot the phases of the relations in the group (A). The results show that the entities at the same level of the hierarchy can be distinguished by their phases, as many phases have the values of π.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis on Entity Embeddings</head><p>In this part, to further show that HAKE can capture the semantic hierarchies between entities, we visualize the embeddings of several entity pairs.</p><p>We plot the entity embeddings of two models: the previous state-of-the-art RotatE and our proposed HAKE. RotatE regards each entity as a group of complex numbers. As a complex number can be seen as a point on a 2D plane, we <ref type="table">Table 4</ref>: Ablation results on WN18RR, FB15k-237 and YAGO3-10 datasets. The symbols m, p, and b represent the modulus part, the phase part, and the mixture bias term, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WN18RR</head><p>FB15k-237 YAGO3-10 m p b MRR H@1 H@3 H@10 MRR H@1 H@3 H@10 MRR H@1 H@3 H@10 . <ref type="bibr">240 .047 .404 .527 .258 .121 .333 .508 .476 .374 .541 .658 .465 .423 .480 .550 .324 .226 .361 .519 .480 .383 .532 .664 .496 .449 .517 .584 .336 .239 .373 .533 .522 .429 .581 .693 .497 .452 .516 .582 .346 .250 .381 .542 .545 .462 .596</ref> .694 can plot the entity embeddings on a 2D plane. As for HAKE, we have mentioned that it maps entities into the polar coordinate system. Therefore, we can also plot the entity embeddings generated by HAKE on a 2D plane based on their polar coordinates. For a fair comparison, we set k = 500. That is, each plot contains 500 points, and the actual dimension of entity embeddings is 1000. Note that we use the logarithmic scale to better display the differences between entity embeddings. As all the moduli have values less than one, after applying the logarithm operation, the larger radii in the figures will actually represent smaller modulus. <ref type="figure" target="#fig_2">Figure 4</ref> shows the visualization results of three triples from the WN18RR dataset. Compared with the tail entities, the head entities in <ref type="figure" target="#fig_2">Figures 4a, 4b</ref>, and 4c are at lower levels, similar levels, higher levels in the semantic hierarchy, respectively. We can see that there exist clear concentric circles in the visualization results of HAKE, which demonstrates that HAKE can effectively model the semantic hierarchies. However, in RotatE, the entity embeddings in all three subfigures are mixed, making it hard to distinguish entities at different levels in the hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>In this part, we conduct ablation studies on the modulus part and the phase part of HAKE, as well as the mixture bias item. <ref type="table">Table 4</ref> shows the results on three benchmark datasets.</p><p>We can see that the bias can improve the performance of HAKE on nearly all metrics. Specifically, the bias improves the H@1 score of 4.7% on YAGO3-10 dataset, which illustrates the effectiveness of the bias.</p><p>We also observe that the modulus part of HAKE does not perform well on all datasets, due to its inability to distinguish the entities at the same level of the hierarchy. When only using the phase part, HAKE degenerates to the pRotatE model . It performs better than the modulus part, because it can well model entities at the same level of  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>To model the semantic hierarchies in knowledge graphs, we propose a novel hierarchy-aware knowledge graph embedding model-HAKE-which maps entities into the polar coordinate system. Experiments show that our proposed HAKE significantly outperforms several existing state-ofthe-art methods on benchmark datasets for the link prediction task. A further investigation shows that HAKE is capable of modeling entities at both different levels and the same levels in the semantic hierarchies.</p><p>x m • r 1,m • r 2,m = x m ⇒ r 1,m = r −1 2,m , (r 1,p + r 2,p )mod 2π = 0.</p><p>Proposition 3. HAKE can infer the composition pattern.</p><p>Proof. If r 1 (x, z), r 2 (x, y) and r 3 (y, z) hold, we have </p><formula xml:id="formula_17">                 x m • r 1,m = z m , x m • r 2,m = y m , y m • r 3,m = z m , (x p + r 1,p )mod 2π = z p , (x p + r 2,p )mod 2π = y p , (y p + r 3,p )mod 2π = z p . Then we have x m • r 2m • r 3m • z m = x m • r 1m • z m ⇒ r 1m = r 2m • r 3m , (r 1p − r 2p − r 3p )mod 2π = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Analysis on Negative Entity Embeddings</head><p>We denote the linked entity pairs as the set of entity pairs linked by some relation, and denote the unlinked entity pairs as the set of entity pairs that no triple contains in the train/valid/test dataset. It is worth noting that the unlinked paris may contain valid triples, as the knowledge graph is incomplete. For both the linked and the unlinked entity pairs, we count the embedding entries of two entities that have different signs. <ref type="figure">Figure 5</ref> shows the result.</p><p>For the linked entity pairs, as we expected, most of the entries have the same sign. Due to the large amount of unlinked entity pairs, we randomly sample a part of them for plotting. For the unlinked entity pairs, around half of the entries have different signs, which is consistent with the random initialization. The results support our hypothesis that the negative signs of entity embeddings can help our model to distinguish positive and negative triples. <ref type="figure">Figure 6</ref> shows the modulus of entity embeddings. We can observe that RotatE encourages the modulus of embeddings to be the same, as the relations are modeled as rotations in a complex space. Compared with RotatE, the modulus of entity embeddings in HAKE are more dispersed, making it to have more potential to model the semantic hierarchies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis on Moduli of Entity Embeddings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Results on Semantic Hierarchies</head><p>In this part, we visualize more triples from WN18RR. We plot the head and tail entities on 2D planes using the same method as that in the main text. The visualization results are in <ref type="figure" target="#fig_4">Figure 7</ref>, where the subcaptions demonstrate the corresponding triples. The figures show that, compared with Ro-tatE, our HAKE model can better model the entities both in different hierarchies and in the same hierarchy. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Distribution histograms of moduli of some relations. The relations are drawn from WN18RR, FB15k-237 and YAGO3-10 dataset. The relation in (d) is /celebrities/celebrity/celebrity friends/celebrities/friendship/friend. Let friend denote the relation for simplicity. (B) Relations in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Distribution histograms of phases of two relations that reflect the same hierarchy. The relations in Figure (a) and (b) are drawn from WN18RR and FB15k-237, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of the embeddings of several entity pairs from WN18RR dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Illustration of the negative modulus of linked and unlinked entity pairs. For each pair of entities h and t, if there is a link between h and t, we label them as "Linked". Otherwise, we label them as "Unlinked". The x-axis represents the number of i that [h m ] i and [t m ] i have different signs, the y-axis represents the frequency. Histograms of the modulus of entity embeddings. Compared with RotatE, the modulus of entity embeddings in HAKE are more dispersed, making it to have more potential to model the semantic hierarchies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of several entity embeddings from WN18RR dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Details of several knowledge graph embedding models, where • denotes the Hadamard product, f denotes a activation function, * denotes 2D convolution, and ω denotes a filter in convolutional layers.· denotes conjugate for complex vectors in ComplEx model and 2D reshaping for real vectors in ConvE model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of datasets. The symbols #E and #R denote the number of entities and relations, respectively. #TR, #VA, and #TE denote the size of train set, validation set, and test set, respectively.</figDesc><table><row><cell>Dataset</cell><cell>#E</cell><cell>#R</cell><cell>#TR</cell><cell>#VA</cell><cell>#TE</cell></row><row><cell cols="6">WN18RR FB15k-237 14,541 237 40,493 11 YAGO3-10 123,182 37 1,079,040 5,000 86,835 3,034 272,115 17,535 20,466 3,134 5,000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Evaluation results on WN18RR, FB15k-237 and YAGO3-10 datasets. Results of TransE and RotatE are taken from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="6">: Comparison results with TKRL models (Xie, Liu, and Sun 2016) on FB15k dataset. RHE, WHE, RHE+STC, and WHE+STC are four versions of TKRL model , of which the results are taken from the original paper.</cell></row><row><cell></cell><cell cols="5">HAKE RHE WHE RHE+STC WHE+STC</cell></row><row><cell>H@10</cell><cell>.884</cell><cell>.694</cell><cell>.696</cell><cell>.731</cell><cell>.734</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>the hierarchy. However, our HAKE model significantly outperforms the modulus part and the phase part on all datasets, which demonstrates the importance to combine the two parts for modeling semantic hierarchies in knowledge graphs.HAKE and TKRLs on FB15k dataset. The best performance of TKRL is .734 obtained by the WHE+STC version, while the H@10 score of our HAKE model is .884. The results show that HAKE significantly outperforms TKRL, though it does not require additional information.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Other Related Work</head><p>We compare our models with TKRL models <ref type="bibr" target="#b24">(Xie, Liu, and Sun 2016)</ref>, which also aim to model the hierarchy structures. For the difference between HAKE and TKRL, please refer to the Related Work section. <ref type="table">Table 5</ref> shows the H@10 scores of</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this appendix, we will provide analysis on relation patterns, negative entity embeddings, and moduli of entity embeddings. Then, we will give more visualization results on semantic hierarchies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Analysis on Relation Patterns</head><p>In this section, we prove that our HAKE model can infer the (anti)symmetry, inversion and composition relation patterns. Detailed propositions and their proofs are as follows.</p><p>Proposition 1. HAKE can infer the (anti)symmetry pattern.</p><p>Proof. If r(x, y) and r(y, x) hold, we have</p><p>(r p + r p )mod 2π = 0. Otherwise, if r(x, y) and ¬r(y, x) hold, we have r m • r m = 1, (r p + r p )mod 2π = 0.</p><p>Proposition 2. HAKE can infer the inversion pattern.</p><p>Proof. If r 1 (x, y) and r 2 (y, x) hold, we have       </p><p>x m • r 1,m = y m , y m • r 2,m = x m , (x p + r 1,p )mod 2π = y p , (y p + r 2,p )mod 2π = x p . Then, we have</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Durán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pasquale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pontus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowledge vault: A web-scale approach to probabilistic knowledge fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to exploit long-term relational dependencies in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Joint embedding of hierarchical categories and entities for concept categorization and dataless classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sycara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Analogical inference for multi-relational embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Yago3: A knowledge base from multilingual wikipedias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A novel embedding model for knowledge base completion based on convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Capsule Network-based Embedding Model for Knowledge Graph Completion and Search Personalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A threeway model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI. Nickel, M.; Tresp, V.; and Kriegel</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Workshop on CVSC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ripplenet: Propagating user preferences on the knowledge graph for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">From one point to a manifold: Knowledge graph embedding for precise link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with hierarchical types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding with hierarchical relation structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ERNIE: Enhanced language representation with informative entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
