<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Strong Baseline and Batch Normalization Neck for Deep Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">A Strong Baseline and Batch Normalization Neck for Deep Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Person ReID</term>
					<term>Baseline</term>
					<term>Tricks</term>
					<term>BNNeck</term>
					<term>Deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This study proposes a simple but strong baseline for deep person re-identification (ReID). Deep person ReID has achieved great progress and high performance in recent years. However, many state-of-the-art methods design complex network structures and concatenate multi-branch features. In the literature, some effective training tricks briefly appear in several papers or source codes. The present study collects and evaluates these effective training tricks in person ReID. By combining these tricks, the model achieves 94.5% rank-1 and 85.9% mean average precision on Market1501 with only using the global features of ResNet50. The performance surpasses all existing global-and part-based baselines in person ReID. We propose a novel neck structure named as batch normalization neck (BNNeck). BNNeck adds a batch normalization layer after global pooling layer to separate metric and classification losses into two different feature spaces because we observe they are inconsistent in one embedding space. Extended experiments show that BNNeck can boost the baseline, and our baseline can improve the performance of existing state-of-the-art methods. Our codes and models are available at: https://github.com/michuanhaohao/reid-strongbaseline</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Person re-identification (ReID) is widely applied in video surveillance and criminal investigation applications <ref type="bibr" target="#b1">[2]</ref>. Person ReID with deep neural networks has progressed and achieved high performance in recent years <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>. Apart from many novel and effective ideas being proposed, the improvement of baseline model plays a key role. For example, Jian et al. <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> proposed a baseline that greatly promoted development of underwater saliency detection. The importance of baseline model should not be ignored. However, few works <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> have focused on the design of an effective baseline. The performance of such baselines has gradually become obsolete due to the rapid development of person ReID. In the literature, some effective training tricks or refinements briefly appear in several papers or source codes. In the present study, we design a strong and effective baseline for person ReID by collecting and evaluating such effective training tricks. This study has three motivations. First, we survey papers published on ECCV and CVPR in 2018. As shown in <ref type="figure" target="#fig_0">Fig.  1</ref>, many previous works were expanded on poor baselines. Only two in 23 baselines surpassed 90% rank-1 accuracy on Market1501. The rank-1 accuracies of four baselines were even lower than 80%. Achieving improvements on poor baselines cannot strictly demonstrate the effectiveness of some methods. Thus, a strong baseline is crucial in promoting research development.</p><p>Second, we discover that the improvements of some works were mainly from training tricks rather than methods themselves. So they were unfairly compared with other state-ofthe-art methods. However, the training tricks were understated in the paper; thus, readers ignored them, thereby exaggerating the effectiveness of the method. We suggest that reviewers consider these tricks when commenting on academic papers.</p><p>Third, the industry prefers simple but effective models over concatenating many local features in the inference stage. In order to achieve great performance, researchers in the academia always combine several local features or use semantic information from pose estimation or segmentation models. Nevertheless, such methods bring extra consumption. Large features also greatly reduce the speed of the retrieval process. Thus, we use tricks to improve the capability of the ReID model and only use global features extracted by the model. arXiv:1906.08332v2 [cs.CV] 7 Jan 2020</p><p>On the basis of the aforementioned considerations, the motivations of designing a strong baseline are summarized as follows:</p><p>• For the academia, we survey many works published on top conferences and discover that most of them were expanded on poor baselines. We aim to provide a strong baseline for researchers to achieve high accuracies in person ReID. • For the community, we aim to provide references to reviewers regarding tricks that will affect the performance of the ReID model. We suggest that reviewers consider these tricks when comparing the performance of different methods. • For the industry, we aim to provide effective tricks for acquiring improved models without extra consumption. Many effective training tricks have been presented in papers or open-sourced projects. We collect tricks and evaluate each of them on ReID datasets. After numerous experiments, we select six tricks to introduce in this study. We propose a novel bottleneck structure, namely, batch normalization neck (BNNeck). As classification and metric losses are inconsistent in the same embedding space, BNNeck optimizes these two losses in two different embedding spaces. In addition, person ReID task mainly focuses on ranking performance, such as cumulative match characteristic (CMC) curve and mAP, but ignores the clustering effect, such as intra-class compactness and inter-class separability. However, clustering effect is important to some special tasks, such as object tracking, which must decide on a distance threshold to separate positive samples from negative ones. An easy approach to overcome this problem is to train the model with center loss. Finally, we add the tricks into a widely used baseline to obtain our modified baseline (the backbone is ResNet50), which achieves 94.5% and 85.9% mAP on Market1501.</p><p>To determine whether these tricks are generally useful or not, we design extended experiments from three aspects. First, we follow the cross-domain ReID settings in which the models are trained and evaluated on different datasets. Crossdomain experiments can show whether the tricks boost the models or simply suppress overfitting in the training dataset. Second, we evaluate all tricks with different backbones, such as ResNet18, SeResNet50, and IBNNet-50. All backbones achieve improvements from our training tricks. Third, we reproduce some state-of-the-art methods on our modified baseline. Experimental results show that our baseline obtains better performance than those reported in published papers. Although our baseline achieves surprising performance, some methods remain effective on our baseline. Thus, our baseline can be a strong baseline for the ReID community.</p><p>There are four main differences between the original version and this paper. 1)We add a new section Related Works to introduce the developments of deep person ReID. 2) We perform a lot of extended experiments to explain the effectiveness of BNNeck. 3) We explain why we introduce center loss and perform some extended experiments to analyze the effect of center loss. 4) We evaluate different backbones and some stateof-the-art methods on our baseline.</p><p>The contributions of this study are summarized as follows:</p><p>• We collect effective training tricks for person ReID. The improvements from each trick are evaluated on two widely used datasets. • We observe the inconsistency between ID loss and triplet and propose a novel neck structure, namely, BNNeck. • We observe that the ReID task ignores intra-class compactness and inter-class separability and claim that center loss can compensate for it. • We proposed a strong ReID baseline. With ResNet50 backbone, it achieves 94.5% and 85.9% mAP on Mar-ket1501. To our best knowledge, this result is the best performance acquired by global features in person ReID. • We design extended experiments to demonstrate that our baseline can be a strong baseline for the ReID community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>This section focuses on deep learning baseline for person ReID. In addition, existing approaches compared with our strong baseline for deep person ReID are introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Baseline for Deep Person ReID</head><p>Recent studies on person ReID mostly focus on building deep convolutional neural networks (CNNs) to represent the features of person images in an end-to-end learning manner. GoogleNet <ref type="bibr" target="#b9">[10]</ref>, ResNet <ref type="bibr" target="#b10">[11]</ref>, DenseNet <ref type="bibr" target="#b11">[12]</ref>, etc are widely used backbone networks. The baselines can be classified into two main genres in accordance with the loss function, i.e. classification loss and metric loss. For classification loss, Zheng et al. <ref type="bibr" target="#b7">[8]</ref> proposed ID-discriminative embedding (IDE) to train the re-ID model as image classification which is fine-tuned from the ImageNet <ref type="bibr" target="#b12">[13]</ref> pre-trained models. Classification loss is also called ID loss in person ReID because IDE is trained by classification loss. However, ID loss requires an extra fully connected (FC) layer to predict the logits of person IDs in the training stage. In the inference stage, such FC layer is removed, and the feature from the last pooling layer is used as the representation vector of the person image.</p><p>Different from ID loss, metric loss regards the ReID task as a clustering or ranking problem. The most widely used baseline based on metric learning is training model with triplet loss <ref type="bibr" target="#b13">[14]</ref>. A triplet includes there images, i.e. anchor, positive, and negative samples. The anchor and positive samples belong to the same person ID, whereas the negative sample belongs to a different person ID. Triplet loss minimizes the distance from the anchor sample to the positive sample and maximizes the distance from the anchor sample to the negative one. However, triplet loss is greatly influenced by the sample triplets. Inspired by FaceNet <ref type="bibr" target="#b14">[15]</ref>, Hermans et al. proposed an online hard example mining for triplet loss (TriHard loss). Most current methods are expanded on the TriHard baseline. Combining ID loss with TriHard loss is also a popular manner of acquiring a strong baseline <ref type="bibr" target="#b2">[3]</ref>.</p><p>Apart from designing different losses, some works focus on building effective baseline model for deep person ReID. In <ref type="bibr" target="#b8">[9]</ref>, three good practices were proposed to build an effective CNN baseline toward person ReID. Their most important practice is adding a batch normalization (BN) layer after the global pooling layer. Similar to these models, the baseline uses a global feature for image representation. Sun et al. <ref type="bibr" target="#b4">[5]</ref> proposed part-based convolutional baseline (PCB). Given an image input, PCB outputs a convolutional descriptor consisting of several part-level features. Both baselines have achieved good performance in person ReID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Some Existing Approaches for Deep person ReID</head><p>On the basis of the aforementioned baselines, many methods have been proposed in the past few years. We divide these works into striped-based, pose-guided, mask-guided, attentionbased, GAN-based, and re-ranking methods.</p><p>Stripe-based methods, which divide the image into several stripes and extract local features for each stripe, play an important role in person ReID. Inspired by PCB, the typical methods includes AlignedReID++ <ref type="bibr" target="#b2">[3]</ref>, MGN <ref type="bibr" target="#b15">[16]</ref>, SCPNet <ref type="bibr" target="#b16">[17]</ref>, etc. Stripe-based local features are effective in boosting the performance of the ReID model. However, they always encounter the problem of pose misalignment.</p><p>Pose-guided methods [18]- <ref type="bibr" target="#b20">[21]</ref> use an extra pose/skeleton estimation model to acquire human pose information. Pose information can exactly align corresponding parts of two person images. However, an extra model brings additional computation consumption. A trade off between the performance and speed of the model is important.</p><p>Mask-guided models <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref> use mask as external cues to remove the background clutters in pixel level and contain body shape information. For example, Song et al. <ref type="bibr" target="#b21">[22]</ref> proposed a mask-guided contrastive attention model that applies binary segmentation masks to learn features separately from the body and background regions. Kalayeh et al. <ref type="bibr" target="#b22">[23]</ref> proposed SPReID, which uses human semantic parsing to harness local visual cues for person ReID. Mask-guided models extremely rely on accurate pedestrian segmentation model.</p><p>Attention-based methods [25]- <ref type="bibr" target="#b27">[28]</ref> involve an attention mechanism to extract additional discriminative features. In comparison with pixel-level masks, attention region can be regraded as an automatically learned high-level 'mask'. A popular model is Harmonious Attention CNN (HA-CNN) model proposed by Li et al. <ref type="bibr" target="#b25">[26]</ref>. HA-CNN combines the learning of soft pixel and hard regional attentions along with simultaneous optimization of feature representations. An advantage of attention-based models is that they do not require a segmentation model to acquire mask information.</p><p>GAN-based methods <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b33">[34]</ref> address the limited data for person ReID. Zheng et al. <ref type="bibr" target="#b28">[29]</ref> first used GAN <ref type="bibr" target="#b34">[35]</ref> to generate images for enriching ReID datasets. The GAN model randomly generates unlabeled and unclear images. On the basis of <ref type="bibr" target="#b28">[29]</ref>, PTGAN <ref type="bibr" target="#b29">[30]</ref> and CamStyle <ref type="bibr" target="#b30">[31]</ref> were proposed to bridge domain and camera gaps for person ReID, respectively. Qian et al. <ref type="bibr" target="#b31">[32]</ref> proposed PNGAN for obtaining a new pedestrian feature and transforming a person into normalized poses. The final feature is obtained by combining the pose-independent features with original ReID features. With the development of GAN, many ganbased methods have been proposed to generate high quality for supervised and unsupervised person ReID tasks. Zhao et al. <ref type="bibr" target="#b33">[34]</ref> integrated GAN with a widely used triplet loss method.</p><p>Re-ranking methods <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b38">[39]</ref> are post-processing strategies for image retrieval. In general, person ReID simply uses Euclidean or cosine distances in the retrieval stage. Zhong et al. <ref type="bibr" target="#b35">[36]</ref> a k-reciprocal encoding method with the Jaccard distance of probe and gallery images to re-rank the ReID results. Shen et al. <ref type="bibr" target="#b36">[37]</ref> proposed a deep group-shuffling random walk (DGRW) network for fully utilizing the affinity information between gallery images in training and testing processes. In the retrieval stage, DGRW can be regarded as a re-ranking method. Re-ranking is a critical step in improving retrieval accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR STRONG BASELINE AND TRAINING TRICKS</head><p>A widely used baseline for the academia and industry is present in <ref type="figure" target="#fig_2">Fig. 2a</ref>. For convenience, such baseline is called standard baseline. Since we have introduced it in [1], the details of such standard baseline can be found in <ref type="bibr" target="#b0">[1]</ref> and our open source code.</p><p>In addition, this section introduces some effective training tricks in person ReID. Our proposed BNNeck structure is discussed in detail. The intra-class compactness and interclass separability problem for person ReID is also raised. Most tricks can be expanded on the standard baseline without changing the model architecture. Compared with our previous papaer <ref type="bibr" target="#b0">[1]</ref>, this paper does not add any new contents for the first four tricks. So we will introduce them simply and mainly focus on our proposed BNNeck and center loss. <ref type="figure" target="#fig_2">Fig. 2b</ref> shows training strategies and model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Warmup Learning Rate</head><p>Learning rate has a great effect on the performance of a ReID model. Standard baseline is initially trained with a large and constant learning rate. In <ref type="bibr" target="#b39">[40]</ref>, a warmup strategy was applied to bootstrap the network for enhanced performance. In practice, we spend 10 epochs, thereby linearly increasing the learning rate from 3.5 × 10 −5 to 3.5 × 10 −4 . The learning rate is decayed to 3.5 × 10 −5 and 3.5 × 10 −6 at 40th and 70th epochs, respectively. The learning rate lr(t) at epoch t is compute as follows:</p><formula xml:id="formula_0">lr(t) =        3.5 × 10 −4 × t 10 if t ≤ 10 3.5 × 10 −4 if 10 &lt; t ≤ 40 3.5 × 10 −5 if 40 &lt; t ≤ 70 3.5 × 10 −6</formula><p>if 70 &lt; t ≤ 120</p><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Random Erasing Augmentation</head><p>In person ReID, persons in the images are sometimes occluded by other objects. To address the occlusion problem and improve the generalization capability of ReID models, Zhong et al. <ref type="bibr" target="#b40">[41]</ref> proposed a new data augmentation approach, namely, random erasing augmentation (REA). REA randomly masks a rectangle region of the training image with a manually set probability p. All hyper-parameters are set same with <ref type="bibr" target="#b40">[41]</ref> and <ref type="bibr" target="#b0">[1]</ref>. Some examples are shown in <ref type="figure" target="#fig_3">Fig. 3</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Label Smoothing</head><p>A basic baseline in person ReID is the IDE <ref type="bibr" target="#b7">[8]</ref> network, whose last layer outputs the ID prediction logits of images. Given an image, we denote y as truth ID label and p i as ID prediction logits of class i. The ID loss is computed as follows:</p><formula xml:id="formula_1">L(ID) = N i=1 −q i log (p i ) q i = 0, y = i q i = 1, y = i<label>(2)</label></formula><p>However, in person ReID, person IDs of the testing set do not appear in the training set. So it is important to prevent from overfitting training IDs for the ReID model. A widely used technique to prevent overfitting for a classification task is Label smoothing (LS) proposed in <ref type="bibr" target="#b41">[42]</ref>. The construction of q i is changed to:</p><formula xml:id="formula_2">q i = 1 − N −1 N ε if i = y ε/N otherwise,<label>(3)</label></formula><p>where ε is a constant to encourage the model to be less confident on the training set. In this study, ε is set to be 0.1.</p><p>When the training set is not large, LS can significantly improve the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Last Stride</head><p>A high spatial resolution always enriches feature granularity. In PCB <ref type="bibr" target="#b4">[5]</ref>, the last spatial down-sampling operation of the backbone network is removed to enlarge the spatial size of the feature map. For convenience, the last spatial down-sampling operation in the backbone network is denoted as the last stride. The last stride equals to 2 for ResNet50 backbone. When fed into an image with 256×128 size, it outputs a feature map with a spatial size of 8×4. If last stride is changed from 2 to 1, then we can obtain a feature map with increased spatial size <ref type="bibr">(16 × 8)</ref>. This manipulation only slightly increases the computation cost and does not involve extra training parameters. However, an increased spatial resolution brings significant improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. BNNeck</head><p>As shown in <ref type="figure" target="#fig_5">Fig. 4(a)</ref>, many state-of-the-art methods combined ID and triplet losses to constrain the same feature f . Combining these two losses always let the model achieve  better performance. However, the better performance let us ignore that the targets of these two losses are inconsistent in the embedding space. <ref type="figure">Fig. 5</ref>(a) presents that ID loss constructs several hyperplanes to separate the embedding space into different subspaces. The features of each class are distributed affinely in different subspaces. So ,cosine distance is more suitable than Euclidean distance for the model optimized by ID loss in the inference stage. However, triplet loss is computed by Euclidean distance and enhances intra-class compactness and inter-class separability in the Euclidean space. As shown in 5(b), The features of triplet loss appear a cluster distribution. If we use both losses to optimize a feature space simultaneously, then their goals may be inconsistent. During training, a possible problem is that one loss is reduced, whereas the other loss oscillates or even increases, as shown in <ref type="figure">Fig.7</ref>. Finally, triplet loss may influence the clear decision surfaces of ID loss, and ID loss may reduce the intra-class compactness of triplet loss. The feature distribution is tadpole shaped. Therefore, directly combining these two losses can boost the performance, but it is not the best way.</p><p>Xiong et al. <ref type="bibr" target="#b8">[9]</ref> added a BN layer <ref type="bibr" target="#b42">[43]</ref> iongbetween feature and ID loss, which is same as <ref type="figure" target="#fig_7">Fig. 8(d)</ref>. The authors claimed that the BN layer overcomes the overfitting and boosts the performance of IDE baseline. However, we consider that the BN layer can smoothen the feature distribution in the embedding space. For ID loss ( <ref type="figure">Fig. 5(a)</ref>), the BN layer will enhance the intra-class compactness. The BN layer can improve the performance of ID loss because the features close to the affine center lack clear decision surfaces and are difficult to distinguish. Nevertheless, such layer increases the cluster radius of intra-class feature for triplet loss. Thus, the decision surfaces of 5(e)(f) are stricter than those of <ref type="figure">Fig. 5(b)</ref></p><formula xml:id="formula_3">(c).</formula><p>To overcome this problem, we design a structure, namely, BNNeck, as shown in <ref type="figure" target="#fig_5">Fig. 4(b)</ref>. BNNeck adds a BN layer after features and before classifier FC layers. The BN and FC layers are initialized through Kaiming initialization proposed in <ref type="bibr" target="#b43">[44]</ref>. The feature before the BN layer is denoted as f t . We let f t pass through a BN layer to acquire the feature f i . In the training stage, f t and f i are used to compute triplet and ID losses, respectively. <ref type="figure" target="#fig_5">Fig. 4(g)</ref> shows that f t not only can keep a compact distribution from but also acquires ID knowledge from ID loss. Affected by the BN layer and ID loss, the distribution of f i is tadpole shaped. In comparison with 4(c), f i has clear decision surfaces because of the weaker influence of the triple loss. Additional details are introduced in Section IV-C.</p><p>In the inference stage, we select f i to perform the person ReID task. Cosine distance metric can achieve better performance than Euclidean distance metric. Experimental results in <ref type="table">Table.</ref> I show that BNNeck can improve the performance of the ReID model by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Center Loss</head><p>Person ReID is always regarded as a retrieval/ranking task. The evaluation protocols, i.e. CMC curve and mAP, are determined by the ranking results but ignore the clustering effect. However, for some ReID applications, such as tracking task, an important step is to decide on a distance threshold to separate positive and negative objects. As shown in <ref type="figure">Fig. 6</ref>, two cases can acquire the same ranking results, but probe2 is easy for the tracking task because of its intra-class compactness of positive pairs.</p><p>Focusing on relative distance, triplet loss is computed as:</p><formula xml:id="formula_4">L T ri = [d p − d n + α] + ,<label>(4)</label></formula><p>where d p and d n are feature distances of positive and negative pairs. α is the margin of triplet loss, and [z] + equals max(z, 0). In this study, α is set to 0.3. However, the triplet loss only considers the difference between d p and d n and ignores their absolute values. For instance, when d p = 0.3 and d n = 0.5, the triplet loss is 0.1. For another case, when d p = 1.3 and d n = 1.5, the triplet loss also is 0.1. Triplet loss is determined by two randomly sampled person IDs. Ensuring that d p &lt; d n in the entire training dataset is difficult. In addition, intra-class compactness is ignored.</p><p>To compensate for the drawbacks of the triplet loss, we involve center loss <ref type="bibr" target="#b44">[45]</ref> intraining, simultaneously learns a center for deep features of each class and penalizes the distances between the deep features and their corresponding class centers. The center loss function is formulated as follows:</p><formula xml:id="formula_5">L C = 1 2 B j=1 f tj − c yj 2 2 ,<label>(5)</label></formula><p>where y j is the label of the jth image in a mini-batch. c yj denotes the y i th class center of deep features, and B is the batch size number. The formulation effectively characterizes the intra-class variations. Minimizing center loss increases intra-class compactness. Our model includes three losses as follows: . Visualized demonstration that ranking task ignores the intra-class compactness of positive pairs. Blue circles and red crosses represent positive and negative samples, respectively. In the direction of the arrow, the feature distance of two samples is increasing. Although two cases can acquire the same ranking results, probe2 is easy for the tracking task that must decide on a threshold to separate positive and negative samples.</p><formula xml:id="formula_6">L = L ID + L T riplet + βL C<label>(6)</label></formula><p>where β is the balanced weight of center loss. In our baseline, β is set to be 0.0005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT A. Datasets</head><p>We evaluate our models on Market1501 <ref type="bibr" target="#b45">[46]</ref> and DukeMTMC-reID <ref type="bibr" target="#b46">[47]</ref> datasets, because both datasets are widely used and large scale. Following the previous works, we use rank-1 accuracy and mAP for evaluation on both datasets.</p><p>Market1501 contains 32,217 images of 1,501 labeled persons of six camera views. The training set has 12,936 images from 751 identities, and the testing set has 19,732 images from 750 identities. In testing, 3,368 hand-drawn images from 750 identities are used as queries to retrieve the matching persons in the database. Single-query evaluation is used in this study.</p><p>DukeMTMC-reID is a new large-scale person ReID dataset and collects 36,411 images from 1,404 identities of eight camera views. The training set has 16,522 images from 702 identities, and the testing set has 19,889 images from other 702 identities. Single-query evaluation is used in this study. Baseline-S only reaches 87.7% and 79.7% rank-1 accuracies on Market1501 and DukeMTMC-reID, respectively. The performance of standard baseline is similar to most baselines reported in other papers. Warmup strategy, REA, LS, stride <ref type="figure">Fig. 7</ref>. ID and triplet loss curves of different models on Market1501 and DukeMTMC-reID datasets. We train the models with Neck3, BNNeck3, and our proposed BNNeck, respectively. Black ovals mark the inconsistency between ID and triplet losses. We show that BNNeck suppresses the inconsistency and smoothens the triplet loss curve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Influences of Each Trick</head><formula xml:id="formula_7">(a) (b) (c) (d) (e) (f)</formula><p>change, BNNeck, and center loss are individually added to the model training process. The designed BNNeck boosts performance to a greater extent than other tricks, especially on DukeMTMC-reID. Finally, the baseline acquires 94.5% rank-1 accuracy and 85.9% mAP on Market1501 with these training tricks. On DukeMTMC-reID, the baseline achieves 86.4% rank-1 and 76.4% mAP accuracy . Thus, these training tricks boost the performance of the standard baseline by over 10% mAP. In order to achieve such improvement, our strong baseline only involves an extra BN layer and do not increase training time.</p><p>We also explore the the effectiveness of these tricks for cross domain ReID, the results can be present in II. In overall, most of these tricks apart from REA are also effective for cross domain ReID. We infer that by REA masking the regions of training images, the model learns additional knowledge in the source domain and performs poorly in the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis of BNNeck</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Different neck structures:</head><p>To discuss the effectiveness of our BNNeck, we design several different neck structures, as shown as <ref type="figure" target="#fig_7">Fig. 8</ref>. In addition, some ablation studies also are analysed in <ref type="table">Table III</ref>. Neck3 outperforms Neck1 and Neck2. In addition, BNNeck2 is worse than Neck2, but BNNeck1 is better than Neck1. Our BNNeck achieves the best performance on two benchmarks. In summary, we present the following observations/conclusions. 1) Without the BN layer, integrating ID and triplet losses is better than only using one loss. 2) The BN layer is effective for ID loss but is invalid for triplet loss.</p><p>3) Our BNNeck that sets triplet loss before the BN layer is a reasonable neck structure.</p><p>2) Inconsistency between ID loss and Triplet loss: To verify that ID and triplet losses are inconsistent in the same feature space, we train the models with Neck3, BNNeck3, and our proposed BNNeck. <ref type="figure" target="#fig_7">Fig. 8</ref> shows that these three neck structures use ID and triplet losses to optimize the same feature. <ref type="figure">Fig. 7</ref> presents the training loss curves of 500 iterations. In <ref type="figure">Fig. 7a and 7d</ref>, the triplet loss initially increases and then decays in the loss curves marked by black ovals, showing a clear confrontation between triplet and ID losses. In comparison with Neck3, BNNeck3 adds a BN layer after f. In <ref type="figure">Figs. 7b and 7e</ref>, the BN layer weakens but does not eliminate the inconsistency. However, for BNNeck in <ref type="figure">Figs.  7c and 7f</ref>, the inconsistency is suppressed, and the triplet loss curves are smoothened. In conclusion, the BN layer can weaken the inconsistency between the losses, and separating them into two different feature spaces is important.</p><p>3) Visualization of feature distribution: To analyze the distribution of the different features in <ref type="figure" target="#fig_7">Fig. 8</ref>, we train models in MNIST dataset. The visualization has considerable noise because the number of person IDs on ReID benchmark is large, and the number of images from each person ID is small. By contrast, MNIST only has 10 categories, and each  <ref type="figure" target="#fig_7">Fig. 8(g)</ref>. The feature dimension is set to 2 for the best view. The BN layer will smoothen the feature.  category consists of thousands of samples, making the feature distribution clear and robust. <ref type="figure" target="#fig_8">Fig. 9</ref> shows the results. ID and triplet losses have two different feature distributions.</p><p>When integrating these two losses in <ref type="figure" target="#fig_8">Fig. 9c</ref>, the clustered distribution is stretched to be tadpole shaped. The distributions of (dsimf) are more gaussian than those of (asimc) because of the BN effect. <ref type="figure" target="#fig_8">Figs. 9g and 9h</ref> show that our BNNeck separates triplet and ID losses into two different feature spaces. The feature distribution of triplet loss remains clustered, and that of ID loss has clear decision surfaces similar to Figs. 9a and 9b.</p><p>We summarize our conclusions or observations as follows: 1) The feature distributions of ID and triplet losses are affined and clustered, i.e., they are inconsistent. 2) The feature distribution of ID+Triplet loss is tadpole shaped. 3) The BN layer can smoothen/normalize the feature distribution and enhance the intra-class compactness for ID loss but reduce it for triplet loss. 4) We separate triplet and ID losses into two different  and suitable feature spaces. 4) Two feature space of BNNeck: Although the results on MNIST in <ref type="figure" target="#fig_8">Fig. 9</ref> can efficiently support our conclusion, image classification and person ReID are two different tasks. We perform statistical analysis on the norm distribution of f t and f i in BNNeck on Market1501 dataset. The mean value µ and standard deviation σ of feature norm are calculated. To analyze the separability of feature distribution, Coefficient of Variation C.V. = µ/σ is also present. As shown in <ref type="figure" target="#fig_0">Fig.  10</ref>, f i and f t are distributed differently in the feature space. f t is compactly and gaussian distributed in an annular space because it is directly optimized by triplet loss. However, we consider f i as a tadpole-shaped distribution because ID loss stretches intra-class distribution. The maximum value of f i is 48.70, whereas the µ is 18.62. C.V. of f t is 0.043, but C.V. of f i reaches 0.98, which demonstrates that f i is distributed more discretely than f t . In conclusion, BNNeck provide two different and suitable feature spaces for triplet loss and ID loss.</p><p>5) Metric space for BNNeck: We evaluate the performance of two different features (f t and f i ) with Euclidean and cosine distance metrics. All models are trained without center loss in <ref type="table">Table.</ref> IV. We observe that cosine distance metric performs better than Euclidean distance metric for f t . As ID loss directly constrains the features followed the BN layer, f i can be clearly separated by several hyperplanes. The cosine distance can measure the angle between feature vectors; thus, cosine distance metric is more suitable than Euclidean distance metric for f i . However, f t is simultaneously close to triplet loss and constrained by ID loss. The two types of metrics achieve similar performance for f t .</p><p>Overall, BNNeck significantly improve the performance of ReID models. We select f i with cosine distance metric to perform the retrieval in the inference stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Analysis of Center loss</head><p>We discuss the influence of center loss on intra-class compactness. We consider that average intra-class distance cannot fully represent the intra-class compactness because it ignores inter-class distance. For convenience, the average intraclass and inter-class distances are denoted as D p and D n , respectively. Inspired by <ref type="bibr" target="#b47">[48]</ref>, the ratio of D p to D n is used to measure the clustering effect of feature distribution. The ratio is computed as R = D p /D n . We set β to different values and evaluate rank-1, mAP, and R of the models. For the feature f t constrained directly by center loss, R decreases as β increases. With β increasing from 0 to 0.5, R is reduced from 0.407 to 0.311 on Market1501 and from 0.424 to 0.363 on DukeMTMC-reID. Hence, center loss can improve intra-class compactness and inter-class separability, thereby bringing a clear boundary between positive and negative samples. When β is set to 0.5, f t can acquire the best clustering effect but obtains the worse rank-1 and mAP accuracies. However, the BN layer destroys such clustering effect. For feature f i , the value of R is almost not influenced by β. On the basis of these observations, we arrive at the following conclusions: (1) Center loss boosts intra-class compactness and inter-class separability. (2) The BN layer can destroy the effect of center loss. (3) Increasing the weight of center loss may reduce ranking performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison to Other Baselines</head><p>We compare our strong baseline with other effective baselines, such as IDE <ref type="bibr" target="#b7">[8]</ref>, TriNet <ref type="bibr" target="#b48">[49]</ref>, AWTL <ref type="bibr" target="#b49">[50]</ref> and PCB <ref type="bibr" target="#b4">[5]</ref>. PCB is a part-based baseline for person ReID. <ref type="table" target="#tab_4">Table VI</ref> presents the performance of these baselines. The experimental results show that our baseline outperforms IDE, TriNet, and AWTL by a large margin. PCB integrates multi-part features and GP uses effective tricks, and both of them achieves great performance. However, our baseline surpasses them by over 7.1% mAP on both datasets. To our best knowledge, our baseline is the strongest baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Comparison to State-of-the-Arts</head><p>We compare our strong baseline with some state-of-theart methods in <ref type="table">Table.</ref> VII. All methods have been divided  into different types. Pyramid <ref type="bibr" target="#b50">[51]</ref> achieves surprising performance on two datasets, but it concatenates 21 local features of different scales. When only the global feature is utilized, Pyramid obtains 92.8% rank-1 accuracy and 82.1% mAP on Market1501. Our strong baseline can reach 94.5% rank-1 accuracy and 85.9% mAP on Market1501. BFE <ref type="bibr" target="#b51">[52]</ref> obtains similar performance to our strong baseline, but it combines features of two branches. Among all methods that only use global features, our strong baseline outperforms AWTL <ref type="bibr" target="#b49">[50]</ref> by more than 10% mAP on both Market1501 and DukeMTMC-reID. To our best knowledge, our baseline achieves the best performance when only global features are used. Our method, which does not require human semantic information, local features or attention modules, is simpler than other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Baseline Meets State-of-the-Arts</head><p>We reproduce some popular state-of-the-art methods with our strong baseline. Given numerous outstanding methods are available, we cannot try all of them and select only several typical models such as k-reciprocal re-ranking <ref type="bibr" target="#b35">[36]</ref>, PCB <ref type="bibr" target="#b4">[5]</ref>, AligedReID++ <ref type="bibr" target="#b2">[3]</ref>, CamStyle <ref type="bibr" target="#b30">[31]</ref>, and MGN <ref type="bibr" target="#b15">[16]</ref>. For a fair comparison, we use the same losses as the paper reported to train the models. For instance, AlignedReID++ only uses ID and triplet losses, and we do not use center loss to reproduce it. However, as k-reciprocal re-ranking is a post-processing method of global features, three losses are used to improve its performance. <ref type="table" target="#tab_4">Table VIII</ref> shows the details and results, wherein the values in parentheses are the results reported by authors in their papers. In addition, we present the performance of the baselines (with BNNeck) trained by different losses as a reference.</p><p>Our baseline boosts the performance of k-reciprocal reranking, PCB, AligedReID++, and CamStyle by a large mar-gin. The mAP of k-reciprocal re-ranking achieves +30.6% on Market1501, demonstrating that the performance of baselines is important for methods. In addition, our MGN achieves similar performance to <ref type="bibr" target="#b15">[16]</ref> because its accuracies are too high to improve, and <ref type="bibr" target="#b15">[16]</ref> uses BNNeck1 structure. Integrating multiple part features can reduce the effect of global features and limit the effectiveness of baselines for PCB and MGN. However, PCB and MGN still obtain better performance than Baseline2, i.e., part-based methods are effective for our baseline.</p><p>However, CamStyle(Our) outperforms CamStyle <ref type="bibr" target="#b30">[31]</ref> but not Baseline1. Our baseline can be a strong baseline for the ReID community because it can boost the performance of some methods, and other methods based on it may be ineffective. To some extent, our baseline efficiently filters effective methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Performance of Different Backbones</head><p>All aforementioned models apply ResNet50 as backbones for clear ablation studies and comparison with other methods.</p><p>Models with different backbones, such as ResNet, SeRes-Net, SeResNeXt, and IBNNet, are evaluated because backbones have a great influence on their performance. As shown in <ref type="table">Table IX</ref>, deep and large backbones can achieve high performance. For example, ResNet101 outperforms ResNet18 by 2.8% and 9.3% in Rank-1 and mAP accuracy on Market1501, respectively. In addition, the channel attention of SeNet and group convolution of ResNeXt can enhance the performance by a slight margin. IBN-Net50 <ref type="bibr" target="#b53">[54]</ref>, which replaces the BN layers with instance BN layers for ResNet50, is also effective for our baseline. Specifically, IBN-Net50-a is suitable for standard ReID task and obtains 95.0% and 90.1% rank-1 accuracies on Market1501 and DukeMTMC-reID, respectively. However, IBN-Net50-b achieves 50.1% rank-1 and 29.8% mAP for Market1501→DukeMTMCReID (M→D) and 61.7% rank-1 and 32.0% mAP DukeMTMCReID1501→Market1501 (D→M).</p><p>For comparison, IBN-Net50-a achieves 40.0% rank-1 and 25.1% mAP for M→D and 52.9% rank-1 and 25.1% mAP (D→M). In conclusion, IBN-Net-a and IBN-Net50-b are suitable for the same domain task and the cross-domain task, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS AND OUTLOOKS</head><p>In this study, we propose a strong baseline for person ReID with only adding an extra BN layer for standard baseline. Our strong baseline achieves 94.5% rank-1 accuracy and 85.9% mAP on Market1501. To our best knowledge, this result is the best performance achieved by the global features of a single backbone. We evaluate each trick of our baseline on same-and cross-domain ReID tasks. In addition, some state-of-the-art methods can be effectively extended on our baseline. We hope that this work can promote ReID research in the academia and industry.</p><p>We observe the inconsistency between ID and triplet losses in previous ReID baselines. To address this problem, we propose a BNNeck to separate both losses into two different  feature spaces. Extended experiments show that the BN layer can enhance and reduce the intra-class compactness for ID and triplet losses, respectively. Furthermore, ID loss is suitable for optimizing the feature.</p><p>We emphasize that the evaluation of ReID task ignores the clustering effect of representation features. However, the clustering effect is important to some ReID applications, such as tracking task wherein an important step is deciding on a distance threshold to separate positive and negative objects. A simple way to address this problem is using center loss to train the model. Center loss can boost the clustering effect of features, but may reduce the ranking performance of ReID models.</p><p>In the future, we will explore additional tricks and effective methods based on this strong baseline. In comparison with face recognition, person ReID still has room for further exploration. In addition, some confusions remain, such as why REA reduces the cross-domain performance in our baseline. Points wherein the conclusion is unclear are worth researching.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Performance of different baselines on Market1501. Our strong baseline are compared with other baselines published on ECCV2018 and CVPR2018.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The pipeline of our modified baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Pipelines of the standard baseline and our modified baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Examples of random erasing augmentation. The first row shows the five original training images. The second row presents the processed images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Designed BNNeck. In the inference stage, we select f i following the BN layer to perform the retrieval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Comparison between standard neck and our designed BNNeck.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 . 2 Fig. 6</head><label>526</label><figDesc>Two-dimensional visualization of sample distribution in the embedding space supervised by different losses and neck structures. (a∼g) correspond to (a∼g) inFig. 8. Points of different colors represent embedding features from different person IDs. The yellow dotted lines stand for decision surfaces. For better understanding, we make some overexpression compared toFig. 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Different neck structures for ablation study. (a∼c) are standard neck structures, and (d∼f) add an additional BN layer. Different losses includes L ID , L T ri , and L ID + L T ri . (g) is our proposed BNNeck that separates triplet and ID losses into different feature spaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>(d)ID loss+BN (e)Triplet loss+BN (f)ID+Triplet loss+BN (h)ID loss+BNNeck (a)ID loss (b)Triplet loss (c)ID+Triplet loss (g)Triplet loss+BNNeck 2D visualization of feature distribution in the embedding space supervised by different losses and neck structures on MNIST dataset. (a∼f) correspond to (a∼f) in Figs. 8. (g) and (h) are related to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>C. V. = 0.098 f i (BNNeck) (b) f i (after BN) Histograms of feature norm for ft and f i in BNNeck on Market1501. µ, σ, C.V. are mean value, standard deviation, and Coefficient of Variation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV ABLATION</head><label>IV</label><figDesc>STUDY OF BNNECK. f (W/O BNNECK) IS BASELINE WITHOUT BNNECK. BNNECK INCLUDES FEATURES ft AND f i . WE EVALUATE THEIR PERFORMANCE WITH EUCLIDEAN AND COSINE DISTANCE.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table V</head><label>V</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">Market1501</cell><cell></cell><cell cols="3">DukeMTMC</cell></row><row><cell>Feature</cell><cell>β</cell><cell cols="2">r = 1 mAP</cell><cell>R</cell><cell cols="2">r = 1 mAP</cell><cell>R</cell></row><row><cell></cell><cell>0</cell><cell>94.2</cell><cell>85.5</cell><cell>0.407</cell><cell>85.7</cell><cell>74.4</cell><cell>0.424</cell></row><row><cell></cell><cell>0.0005</cell><cell>93.9</cell><cell>85.7</cell><cell>0.405</cell><cell>86.5</cell><cell>75.1</cell><cell>0.420</cell></row><row><cell>ft</cell><cell>0.005</cell><cell>94.2</cell><cell>85.7</cell><cell>0.394</cell><cell>86.2</cell><cell>75.4</cell><cell>0.417</cell></row><row><cell></cell><cell>0.05</cell><cell>94.4</cell><cell>85.4</cell><cell>0.365</cell><cell>86.4</cell><cell>74.9</cell><cell>0.403</cell></row><row><cell></cell><cell>0.5</cell><cell>92.6</cell><cell>81.1</cell><cell>0.311</cell><cell>85.5</cell><cell>72.2</cell><cell>0.363</cell></row><row><cell></cell><cell>0</cell><cell>94.1</cell><cell>85.7</cell><cell>0.590</cell><cell>86.2</cell><cell>75.9</cell><cell>0.568</cell></row><row><cell></cell><cell>0.0005</cell><cell>94.5</cell><cell>85.9</cell><cell>0.589</cell><cell>86.4</cell><cell>76.4</cell><cell>0.564</cell></row><row><cell>f i</cell><cell>0.005</cell><cell>94.3</cell><cell>85.9</cell><cell>0.595</cell><cell>86.8</cell><cell>76.4</cell><cell>0.566</cell></row><row><cell></cell><cell>0.05</cell><cell>94.3</cell><cell>85.7</cell><cell>0.592</cell><cell>86.7</cell><cell>76.5</cell><cell>0.560</cell></row><row><cell></cell><cell>0.5</cell><cell>94.1</cell><cell>84.7</cell><cell>0.593</cell><cell>87.4</cell><cell>76.9</cell><cell>0.554</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE V</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">EVALUATION WITH DIFFERENT WEIGHTS OF CENTER LOSS β. R IS THE</cell></row><row><cell cols="8">RATIO OF INTRA-CLASS DISTANCE TO INTER-CLASS DISTANCE.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Market1501</cell><cell cols="2">DukeMTMC</cell><cell></cell></row><row><cell></cell><cell>Baseline</cell><cell>Loss</cell><cell cols="4">r = 1 mAP r = 1 mAP</cell><cell></cell></row><row><cell></cell><cell>IDE [8]</cell><cell>ID</cell><cell>79.5</cell><cell>59.9</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell cols="2">TriNet [49]</cell><cell>Tri</cell><cell>84.9</cell><cell>69.1</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell cols="2">AWTL [50]</cell><cell>Tri</cell><cell>89.5</cell><cell>75.7</cell><cell>79.8</cell><cell>63.4</cell><cell></cell></row><row><cell></cell><cell>GP [9]</cell><cell>ID</cell><cell>91.7</cell><cell>78.8</cell><cell>83.4</cell><cell>68.8</cell><cell></cell></row><row><cell></cell><cell>PCB [5]</cell><cell>ID</cell><cell>92.3</cell><cell>77.4</cell><cell>81.7</cell><cell>66.9</cell><cell></cell></row><row><cell></cell><cell>Our</cell><cell>ID+Tri</cell><cell>94.5</cell><cell>85.9</cell><cell>86.4</cell><cell>76.4</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE VI</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">COMPARISON OF DIFFERENT BASELINES ON MARKET1501 AND</cell></row><row><cell cols="8">DUKEMTMC-REID DATASETS. ID AND TRI STANDS FOR ID LOSS AND</cell></row><row><cell></cell><cell cols="5">TRIPLET-BASED LOSS, RESPECTIVELY.</cell><cell></cell><cell></cell></row><row><cell>presents</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>the results.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII COMPARISON</head><label>VII</label><figDesc>OF STATE-OR-THE-ART METHODS. N f IS THE NUMBER OF FEATURES USED IN THE INFERENCE STAGE.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>L ID , L T ri , L C k-reciprocal<ref type="bibr" target="#b8">[9]</ref> CVPR17 95.<ref type="bibr" target="#b3">4</ref>(77.1) 94.2(63.6) 90.3(-) 89.1(-) L ID , L T ri , L C PCB [5] ECCV18 94.0(92.3) 84.0(77.4) 88.6(81.7) 77.2(66.1) L ID , L T ri AligedReID++ [3] PR19 94.3(91.8) 86.5(79.1) 86.5(82.1) 76.9(69.7) L ID , L T ri CamStyle [31] TIP19 93.3(88.1) 81.0(68.7) 80.3(75.3) 60.1(53.5) L ID MGN [16] ACMMM19 95.3(95.7) 86.3(86.9) 89.2(88.7) 78.9(78.4) L ID , L T ri</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Market1501</cell><cell cols="2">DukeMTMC</cell><cell></cell></row><row><cell>Method</cell><cell>Reference</cell><cell>r = 1</cell><cell>mAP</cell><cell>r = 1</cell><cell>mAP</cell><cell>Loss</cell></row><row><cell>Baseline1</cell><cell>BNNeck1</cell><cell>93.1</cell><cell>83.9</cell><cell>85.2</cell><cell>74.0</cell><cell>L ID</cell></row><row><cell>Baseline2</cell><cell>BNNeck</cell><cell>94.1</cell><cell>85.7</cell><cell>86.2</cell><cell>75.9</cell><cell>L ID , L T ri</cell></row><row><cell>Baseline3</cell><cell>BNNeck</cell><cell>94.5</cell><cell>85.9</cell><cell>86.4</cell><cell>76.4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII PERFORMANCE</head><label>VIII</label><figDesc>OF SOME STATE-OF-THE-ART METHODS REPRODUCED BY OUR STRONG BASELINE. THE VALUES IN PARENTHESES ARE THE RESULTS REPORTED BY AUTHORS.</figDesc><table><row><cell></cell><cell cols="2">Market1501</cell><cell cols="2">DukeMTMC</cell></row><row><cell>Backbone</cell><cell cols="4">r = 1 mAP r = 1 mAP</cell></row><row><cell>ResNet18</cell><cell>91.7</cell><cell>77.8</cell><cell>82.5</cell><cell>68.8</cell></row><row><cell>ResNet34</cell><cell>92.7</cell><cell>82.7</cell><cell>86.4</cell><cell>73.6</cell></row><row><cell>ResNet50</cell><cell>94.5</cell><cell>85.9</cell><cell>86.4</cell><cell>76.4</cell></row><row><cell>ResNet101</cell><cell>94.5</cell><cell>87.1</cell><cell>87.6</cell><cell>77.6</cell></row><row><cell>SeResNet50</cell><cell>94.4</cell><cell>86.3</cell><cell>86.4</cell><cell>76.5</cell></row><row><cell>SeResNet101</cell><cell>94.6</cell><cell>87.3</cell><cell>87.5</cell><cell>78.0</cell></row><row><cell>SeResNeXt50</cell><cell>94.9</cell><cell>87.6</cell><cell>88.0</cell><cell>78.3</cell></row><row><cell>SeResNeXt101</cell><cell>95.0</cell><cell>88.0</cell><cell>88.4</cell><cell>79.0</cell></row><row><cell>IBN-Net50-a</cell><cell>95.0</cell><cell>88.2</cell><cell>90.1</cell><cell>79.1</cell></row><row><cell>IBN-Net50-b</cell><cell>93.5</cell><cell>83.9</cell><cell>86.4</cell><cell>73.5</cell></row><row><cell></cell><cell cols="2">TABLE IX</cell><cell></cell><cell></cell></row><row><cell cols="5">PERFORMANCE OF OUR BASELINE WITH DIFFERENT BACKBONE.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Incremental re-identification by cross-direction and cross-ranking adaption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Alignedreid++: Dynamically matching local information for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="53" to="61" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mancs: A multi-task attentional network with curriculum sampling for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="365" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The extended marine underwater environment database and baseline evaluations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-M</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="425" to="437" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Integrating qdwd with pattern distinctness and local contrast for underwater saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-M</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of visual communication and image representation</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="31" to="41" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A discriminatively learned cnn embedding for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Communications, and Applications (TOMM)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Good practices on building effective cnn baseline model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth International Conference on Graphics and Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11069</biblScope>
			<biblScope unit="page">110690</biblScope>
		</imprint>
	</monogr>
	<note>ICGIP 2018</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE confer</title>
		<meeting>the IEEE confer</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end comparative attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3492" to="3506" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scpnet: Spatial-channel parallelism network for joint holistic and partial person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spindle net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1077" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Glad: Global-localalignment descriptor for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="420" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A pose-sensitive embedding for person re-identification with expanded cross neighborhood re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pose invariant embedding for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mask-guided contrastive attention model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1179" to="1188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Human semantic parsing for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gökmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Kamasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1062" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Maskreid: A mask based deep ranking neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03864</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dual attention matching network for context-aware feature sequence based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5363" to="5372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Diversity regularized spatiotemporal attention for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="369" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention-aware compositional network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2119" to="2128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3754" to="3762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Camstyle: A novel data augmentation method for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1176" to="1190" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pose-normalized image generation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="650" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2138" to="2147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An adversarial approach to hard triplet generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="501" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Re-ranking person reidentification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep groupshuffling random walk for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2265" to="2274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Person reidentification via ranking aggregation of similarity pulling and dissimilarity pushing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2553" to="2566" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Re-ranking via metric fusion for object retrieval and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="740" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spherereid: Deep hypersphere manifold embedding for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, IEEE International Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision workshop on Benchmarking Multi-Target Tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Range loss for deep face recognition with long-tailed training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5409" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Features for multi-target multi-camera tracking and re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6036" to="6046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pyramidal person re-identification via multi-loss dynamic training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8514" to="8522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Batch feature erasing for person re-identification and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07130</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Svdnet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3800" to="3808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Two at once: Enhancing learning and generalization capacities via ibn-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="464" to="479" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
