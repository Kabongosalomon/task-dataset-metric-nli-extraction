<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain Adaptive Transfer Learning on Visual Attention Aware Data Augmentation for Fine-grained Visual Categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashiq</forename><surname>Imran</surname></persName>
							<email>ashiq.imran@mavs.uta.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Texas at Arlington</orgName>
								<address>
									<settlement>Arlington</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilis</forename><surname>Athitsos</surname></persName>
							<email>athitsos@uta.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Texas at Arlington</orgName>
								<address>
									<settlement>Arlington</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Domain Adaptive Transfer Learning on Visual Attention Aware Data Augmentation for Fine-grained Visual Categorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Domain Adaptation · Transfer Learning · Fine-Grained Vi- sual Categorization · Visual Attention</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-Grained Visual Categorization (FGVC) is a challenging topic in computer vision. It is a problem characterized by large intra-class differences and subtle inter-class differences. In this paper, we tackle this problem in a weakly supervised manner, where neural network models are getting fed with additional data using a data augmentation technique through a visual attention mechanism. We perform domain adaptive knowledge transfer via fine-tuning on our base network model. We perform our experiment on six challenging and commonly used FGVC datasets, and we show competitive improvement on accuracies by using attention-aware data augmentation techniques with features derived from deep learning model InceptionV3, pre-trained on large scale datasets. Our method outperforms competitor methods on multiple FGVC datasets and showed competitive results on other datasets. Experimental studies show that transfer learning from large scale datasets can be utilized effectively with visual attention based data augmentation, which can obtain state-of-the-art results on several FGVC datasets. We present a comprehensive analysis of our experiments. Our method achieves state-of-the-art results in multiple fine-grained classification datasets including challenging CUB200-2011 bird, Flowers-102, and FGVC-Aircrafts datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks have provided state-of-the-art results in many domains in computer vision. However, having a big training set is very important for the performance of deep neural networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref>. Data augmentation techniques have been gaining popularity in deep learning and are extensively used to address the scarcity of training data. Data augmentation has led to promising results in various computer vision tasks <ref type="bibr" target="#b14">[15]</ref>. There are different data augmentation methods for deep models, like image flipping, cropping, scaling, rotation, translation, color distortion, adding Gaussian noise, and many more. arXiv:2010.03071v1 [cs.CV] 6 Oct 2020</p><p>Previous methods mostly choose random images from the dataset and apply the above operations to enlarge the amount of training data. However, applying random cropping to generate new training examples can have undesirable consequences. For example, if the size of the cropped region is not large enough, it may consist entirely of background, and not contain any part of the labeled object. Moreover, this generated data might reduce accuracy and negatively affect the quality of the extracted features. Consequently, the disadvantages of random cropping might cancel out its advantages. More specific features need to be provided to the model to make data augmentation more productive.</p><p>In Fine-Grained Visual Categorization (FGVC), same-class items may have variation in the pose, scale, or rotation. FGVC contains subtle differences among classes in a sub-category of an object, which includes the model of the cars, type of the foods or the flowers, species of the birds or dogs, and type of the aircrafts. These differences are what make FGVC a challenging problem, as there are significant intra-class differences among the sub-categories, and at the same time, items from different classes may look similar. In contrast with regular object classification techniques, FGVC aims to solve the identification of particular subcategories from a given category <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Convolutional Neural Networks (CNNs) have been extensively used for various applications in computer vision. To achieve good performance with CNNs, typically we need large amounts of labeled data. However, it is a tedious process to collect labeled fine-grained datasets. That is why there are not many FGVC datasets, and existing datasets are not as large compared to standard image recognition datasets like ImageNet <ref type="bibr" target="#b6">[7]</ref>. Normally, a model pre-trained on large scale datasets such as ImageNet is used, and that model is then fine-tuned using data from an FGVC dataset. Typically, FGVC datasets are not too big, so it becomes critical to design methods that can compensate for the limited amount of data. In this paper, we investigate some techniques that allow the model to learn features more effectively, and that perform well on large scale datasets with fine-grained categories.</p><p>Generally, there are two domains involved in fine-tuning a network. One is the source domain, which typically includes large scale image datasets like ImageNet <ref type="bibr" target="#b6">[7]</ref>, where initial models are pre-trained. Another is the target domain, where data is used to fine-tune the pre-trained models. In this paper, the target domain is FGVC datasets, and we are interested in developing techniques that can boost accuracy on these type of datasets. Modern FGVC methods use pre-trained networks with ImageNet dataset to a large extent. We explore the possibility of achieving better accuracy than what has been achieved so far using ImageNet. A model first learns useful features from a large amount of training data, and is then fine-tuned on a more evenly-distributed subset to balance the efforts of the network among different categories and transfer the already learned features.</p><p>In short, our research tries to address two questions: 1) What approaches beyond transfer learning do we need to take to boost the performance on FGVC datasets? 2) How can we determine which large scale source domain we choose, given that the target domain is FGVC?</p><p>We calculate the domain similarity score between the source and target domains. This score gives us a clear picture of selecting the source domain for transfer learning to achieve better accuracy in the target domain. Then, we focus on a visual attention guided network for data augmentation. As FGVC datasets are relatively smaller in size, we leverage the feature learning from fine-tuning as well as data augmentation to achieve better accuracy. The performance of the combination of these two strategies outperforms the baseline approach.</p><p>In summary, the main contributions of this work are:</p><p>1. We propose a simple yet effective improvement over the recently proposed Weakly Supervised Data Augmentation Network (WS-DAN) <ref type="bibr" target="#b11">[12]</ref>, which is used for generating attention maps to extract sequential local features to tackle the FGVC challenge. A domain similarity score can play a vital role before applying transfer learning. Based on the score, we decide which source domain is necessary to use for transfer learning. Then, we can employ WS-DAN <ref type="bibr" target="#b11">[12]</ref> to achieve better results among FGVC datasets. 2. We demonstrate a domain adaptive transfer learning approach, that combines with visual attention based data augmentation, and that can achieve state-of-the-art results on CUB200-2011 <ref type="bibr" target="#b27">[28]</ref>, and Flowers-102 <ref type="bibr" target="#b19">[20]</ref>, and FGVC-Aircrafts <ref type="bibr" target="#b18">[19]</ref> datasets. Additionally, we match the current state-of-the-art accuracy on Stanford Cars <ref type="bibr" target="#b13">[14]</ref>, Stanford Dogs <ref type="bibr" target="#b12">[13]</ref> datasets. 3. We present the relationship of top-1 accuracy and domain score on six commonly used FGVC datasets. We illustrate the effect of image resolution in transfer learning in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we present a brief overview of data augmentation, fine-grained visual categorization, visual attention mechanism and transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Augmentation</head><p>Machine learning theory suggests that a model can be more generalized and robust if it has been trained on a dataset with higher diversity. However, it is a very difficult and time-consuming task to collect and label all the images which involve these variations <ref type="bibr" target="#b33">[34]</ref>. Data augmentation methods are proposed to address this issue by adding the amount and diversity of training samples. Various methods have been proposed focusing on random spatial image augmentation, specifically involving in rotation variation, scale variation, translation, and deformation, etc. <ref type="bibr" target="#b11">[12]</ref>. Classical augmentation methods are widely adopted in deep learning techniques. The main drawback of random data augmentation is low model accuracy. Additionally, it suffers from generating a lot of unavoidable noisy data. Various methods have been proposed to consider data distribution rather than random data augmentation. A search space based data augmentation method has been proposed <ref type="bibr" target="#b4">[5]</ref>. It can automatically search for improving data augmentation policies in order to obtain better validation accuracy. In contrast, we leverage WS-DAN <ref type="bibr" target="#b11">[12]</ref>, which generates augmented data from visual attention features of the image. Peng et al. proposed a method for human pose estimation, by introducing an augmentation network whose task is to generate hard data online, thus improving the robustness of models <ref type="bibr" target="#b20">[21]</ref>. Nevertheless, their augmentation system is complicated and less accurate compared to the network that we experimented with. Additionally, attention-aware data segmentation is more simple and proven effective in terms of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fine-Grained Visual Categorization</head><p>Fine-grained Visual Categorization (FGVC) is a challenging problem in the field of computer vision. Normally, object classification is used for categorize different objects in the image, such as humans, animals, cars, trees, etc. In contrast, fine-grained image classification concentrates more on detecting sub-categories of a given category, like various types of birds, dogs or cars. The purpose of FGVC is to find subtle differences among various categories of a dataset. It presents significant challenges for building a model that generalizes patterns. FGVC is useful in a wide range of applications such as image captioning <ref type="bibr" target="#b1">[2]</ref>, image generation <ref type="bibr" target="#b3">[4]</ref>, image search engines, and so on.</p><p>Various methods have been developed to differentiate fine-grained categories. Due to the remarkable success of deep learning, most of the recognition works depend on the powerful convolutional deep features. Several methods were proposed to solve large scale real problems <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">26]</ref>. However, it is relatively hard for the basic models to focus on very precise differences of an object's parts without adding special modules <ref type="bibr" target="#b11">[12]</ref>. A weakly supervised learning-based approach was adapted to generate class-specific location maps by using pooling methods <ref type="bibr" target="#b17">[18]</ref>. Adversarial Complementary Learning (ACoL) <ref type="bibr" target="#b34">[35]</ref> is a weakly supervised approach to identify entire objects by training two adversarial complementary classifiers, which aims at locating several parts of objects and detects complementary regions of the same object. However, their method fails to accurately locate the parts of the objects due to having only two complementary regions. On the contrary, our proposed approach depends on attention-guided data augmentation and domain adaptive transfer learning. Our method extracts finegrained discriminative features and provides a generalization of domain features to achieve state-of-the-art performance in terms of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Attention</head><p>Attention mechanisms have been getting a lot of popularity in the deep learning area. Visual attention has been already used for FGVC. Xiao et al. proposed a two-way attention method (object-level attention and part-level attention) to train domain-specific deep networks <ref type="bibr" target="#b30">[31]</ref>. Fu et al. proposed an approach that can predict the location of one attention area and extract corresponding features <ref type="bibr" target="#b8">[9]</ref>. However, this method can only focus on a local object's parts at the same time.</p><p>Zheng et al. addressed this issue and introduced Multi-Attention CNN (MA-CNN) <ref type="bibr" target="#b35">[36]</ref>, which can simultaneously focus on multiple body parts. However, selected parts of the object are limited and the number of selected parts is fixed (2 or 4), which might hamper accuracy.</p><p>The works mentioned above mostly focus on object localization. In contrast, our research concentrates more on data augmentation with visual attention, which has not been much explored. We use the attention mechanism for data augmentation purposes. Moreover, the benefit of guided attention based data augmentation <ref type="bibr" target="#b11">[12]</ref> helps the network to locate object precisely, which helps our trained model learn about closer object details and hence, improve the predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Transfer Learning</head><p>The purpose of transfer learning is to improve the performance of a learning algorithm by utilizing knowledge that is acquired from previously solved similar problems. CNNs have been widely used for transfer learning. They are mostly used in the form of pre-trained networks that serve as feature extractors <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Considerable amounts of effort have been made to understand transfer learning <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b2">3]</ref>. Initial weights for a certain network can be obtained from an already-trained network even if the network is used for different tasks <ref type="bibr" target="#b31">[32]</ref>. Some prior work has shown some results on transfer learning and domain similarity <ref type="bibr" target="#b5">[6]</ref>. Their contribution mostly addresses the effect of image resolution on large scale datasets and choosing different subsets of datasets to boost accuracy. In our work, we show that domain adaptive transfer learning can be useful if we also incorporate visual attention based data augmentation.</p><p>Unlike previous works, our proposed technique takes account of domain adaptive transfer learning between the source and target domains. Then, it incorporates the attention-driven approach for data augmentation. Our main goal is to guide the training model to learn relevant features from the source domain and augment data with the visual attention of the target domain. The combination of two processes can be useful to achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Domain Adaptive Transfer Learning (DATL)</head><p>In our research, we explore the way of determining similarity between the source and target domains. Additionally, we describe the attention aware data augmentation technique, WS-DAN in detail. We consider different types of large scale datasets to find out the similarity score between large scale datasets and FGVC datasets. Then, we compute domain similarity score firstly. Based on the domain similarity score we choose large scale datasets for transfer learning and then we perform WS-DAN to evaluate the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Domain Similarity</head><p>Generally, transfer learning performs better if it has been trained on bigger datasets. Chen et al. showed that transfer learning performance increases logarithmically with the number of data <ref type="bibr" target="#b24">[25]</ref>. In our work, we observe that using a bigger dataset does not always provide a more accurate result. Yosinski et al. <ref type="bibr" target="#b31">[32]</ref> mentions that there is some correlation between the transferability of a network from the source task to the target task and the distance between the source and target tasks. Furthermore, they show fine-tuning on a pre-trained network towards a target task can boost performance. Our domain adaptive transfer learning approach is inspired from Cui et al. <ref type="bibr" target="#b5">[6]</ref> who introduce a method which can calculate domain similarity by the Earth Mover's Distance (EMD) <ref type="bibr" target="#b21">[22]</ref>. Furthermore, they show transfer learning can be treated as moving image sets from the source domain S to the target domain T . The domain similarity <ref type="bibr" target="#b5">[6]</ref> can be defined</p><formula xml:id="formula_0">d(S, T ) = EM D(S, T ) = m,n i=1,j=1 f i,j d i,j m,n i=1,j=1 f i,j (1) where s i is i-th category in S and t j is j-th in T , d i,j = ||g(s i ) − g(t j )|| , feature extractor g(.)</formula><p>of an image and the optimal flow f i,j computes total work as a EMD minimization problem. Finally, the similarity is calculated as:</p><formula xml:id="formula_1">sim(S, T ) = e −γd(S,T )<label>(2)</label></formula><p>where γ is a regularization constant of value 0.01. Domain similarly score can be calculated between the source and target domain. In our approach, we use large scale datasets as source domains, and target domains are selected from six commonly used FGVC datasets. After calculating the similarity score, we choose top k categories with the highest domain similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attention Aware Data Augmentation</head><p>In our method, we consider using the Weakly Supervised Data Augmentation Network (WS-DAN) <ref type="bibr" target="#b11">[12]</ref>. Firstly, we extract features of the image I and feature maps F ∈ R H×W ×C , where H, W, and C correspond to height, width, and number of channels of a feature layer. Then, we generate attention maps A ∈ R H×W ×M from feature maps, where M is the number of attention maps. One more critical component is bi-linear attention pooling, which is used to extract features from part objects. Element-wise multiplication between feature maps and attention maps is computed to get part-feature maps, and then, pooling operation is applied on part-feature maps afterward. Randomly generated data from augmentation is not much efficient. However, attention maps can be handy for data augmentation. This way model can be guided to focus on essential parts of the data and augment those data to the network. With an augmentation map, part's region can be zoomed, and detailed features can be extracted. This process is called attention cropping. Attention maps can represent similar object's part. Attention dropping can be applied to the network to distinguish multiple object's part. Both attention cropping and attention dropping are controlled through a threshold value. During the training process, no bounding box or keypoints based annotation is available. For each particular training image, attention maps are generated to represent the distinguishable part of object. Attention, guided data augmentation component, is responsible for selecting attention maps efficiently utilizing attention cropping and attention dropping. Bilinear Attention Pooling (BAP) is used to extract features from the object's parts. Element-wise multiplication between the feature maps and attention map are used to generate a part feature matrix. In the last step, the original data, along with attention generated augmented data, are trained as input data.</p><p>During the testing process, in the beginning, the object's categories probability and attention maps are produced from input images. Then, the selected part of the object can be enlarged to refine the category's probability. The final prediction is evaluated as the average of those two probabilities. The process of final prediction <ref type="bibr" target="#b11">[12]</ref> is presented as Algorithm 1.</p><p>The training process is illustrated in <ref type="figure">Figure 1</ref>. During training process, no bounding box or keypoints based annotation are available. For each particular training image, attention maps are generated to represent the distinguishable part of object. Attention guided data augmentation component is responsible to select attention maps efficiently utilizing attention cropping and attention dropping. Bilinear Attention Pooling (BAP) is used to extract feature from object's parts. Element-wise multiplication between the feature maps and attention map is used to generate part feature matrix. In the last step, the original data along with attention generated augmented data are trained as input data. <ref type="figure" target="#fig_2">Figure 2</ref> shows the illustration of testing process. Firstly, the object's categories probability and attention maps are produced from input images. Then, selected part of the object can be enlarged to refine the categories probability. The final prediction is evaluated as the average of those two probabilities. <ref type="figure">Fig. 1</ref>. Weakly Supervised Data Augmentation Network <ref type="bibr" target="#b11">[12]</ref> Training Process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Visualization of Augmented Data</head><p>We visualize the attention-guided data augmentation in CUB200-2011, Food-101, Flowers-102, Stanford Car, Stanford Dog and FGVC-Aircraft respectively in <ref type="figure" target="#fig_3">Figure 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss Function</head><p>The loss function of the network is derived from center loss <ref type="bibr" target="#b29">[30]</ref>, which has been proposed to tackle face recognition issues. Here, we adopt attention regularization loss <ref type="bibr" target="#b11">[12]</ref> for the attention learning process. The idea is to minimize the intra-class variations while keeping the features of inter-class features differentiable. So, penalizing the features variation that belong to same part of object which is important for fine-grained category. The loss function can be defined as:</p><formula xml:id="formula_2">L A = M k=1 ||f k − c k || 2 2 (3)</formula><p>where M is number of attention maps, f k is the part feature and c k is its part's feature center of kth object. c k can be updated by moving average and initialized as zero, and the update rate is β .  </p><formula xml:id="formula_3">c k ← c k + β(f k − c k ) (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we show comprehensive experiments to verify the effectiveness of our approach. Firstly, we calculate the domain similarity score using EMD <ref type="bibr" target="#b21">[22]</ref> to demonstrate the relationship between the source and target domains.</p><p>Then we compare our model with the state-of-the-art methods on six publicly available fine-grained visual categorization datasets. Following this, we perform additional experiments to demonstrate the effect of image resolution on transfer learning. We compare input images in the iNaturalist dataset from 299 × 299 to 448 × 448 to observe the effect in terms of accuracy. We have trained the baseline inceptionV3 model with iNaturalist datasets. Additionally, we combine both iNaturalist and imageNet dataset to make a bigger dataset. We perform detailed experimental studies with different types of large scale datasets and <ref type="figure">Fig. 4</ref>. Visual attention on image, Attention Maps, Feature Maps, Attention Dropping on Food-101 dataset (left to right respectively) <ref type="bibr" target="#b11">[12]</ref>. apply the WS-DAN method to observe the impact. The training loss curve and top-1 accuracy curve are presented in <ref type="figure" target="#fig_8">Figures 9 and 10</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We present a detailed overview of the datasets that we use for our experiments. ImageNet: The ImageNet [7] contains 1.28 million training images and 50 thousand validation images along with 1,000 categories.</p><p>iNaturalist(iNat) : The iNat dataset, introduced in 2017 <ref type="bibr" target="#b26">[27]</ref>, contains more than 665,000 training and around 10000 test images from more than 5000 natural fine-grained categories. Those categories include different types of mammals, birds, insects, plants, and more. This dataset is quite imbalanced and varies a lot in terms of the number of images per category.</p><p>Fine-grained object classification datasets: <ref type="table">Table 1</ref> summarizes the information of each dataset in detail.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>In our experiment, we used Tensorflow <ref type="bibr" target="#b0">[1]</ref> to train all the models on multiple Nvidia Geforce GTX 1080Ti GPUs. The machine has Intel Core-i7-5930k CPU@ 3.50GHz x 12 processors with 64GB of memory. During training, we adopted Inception v3 <ref type="bibr" target="#b25">[26]</ref> as the backbone network. We employed WS-DAN <ref type="bibr" target="#b11">[12]</ref> technique to perform experiments to demonstrate the effectiveness of transfer learning. For all the datasets, we used Stochastic Gradient Descent (SGD) with a momentum of 0.9, the number of epoch 80, mini-batch size 12. The initial learning rate was set to 0.001, with exponential decay of 0.8 after every 2 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>When training a CNN, input images are often preprocessed to match a specific size. Higher resolution images usually contain essential information and precise details that are important to visual recognition. We compare results on six FGVC datasets with different sizes of image resolution of the iNat dataset. In summary, images with higher resolution yields better accuracy except for the Stanford Dogs dataset. <ref type="figure">Figure 11</ref> represents the effect of transfer learning with various sizes of image resolution on iNat dataset.  In <ref type="table">Table 3</ref>, we present the top-1 accuracy of the target domains on various source domains. These results show the impact of transfer learning from a pretrained model. Large scale datasets are essential for getting improved accuracy when transfer learning is conducted. ImageNet dataset is much larger than iNat dataset; still, it shows worse accuracy in the CUB200-2011 dataset. So, we cannot conclude that using a bigger dataset with transfer learning can always yield better results. Moreover, the domain similarity score also supports this hypothesis. Hence, transfer learning can be effective if the target domain can be trained with similar source domain.</p><p>We compare our method with state-of-the-art baselines on six commonly used fine-grained categorization datasets. The summary of the comparison is presented in <ref type="table">Table 4</ref>. In <ref type="table">Table 2</ref>, we show the domain similarity score between the source and various target domains. We visually represent the relationship between the top-1 accuracy and the domain similarity score. We can observe from <ref type="figure" target="#fig_2">Figure 12</ref> that the domain similarity score positively correlated with transfer learning accuracy between large scale datasets and FGVC datasets. Each  iNat pre-trained model. It means that we cannot attain good accuracy on target domains by just using a larger (combined) source domain. Our work demonstrates that a domain similarity score can be useful for identifying which large scale dataset to employ. That way, the model can learn essential features for the target dataset from large source training sets. Furthermore, we can employ attention aware data augmentation techniques to achieve state-of-the-art accuracy on several FGVC datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we describe a simple technique that takes attention mechanism as a data augmentation technique. Attention maps are guided to focus on the object's parts and encourage multiple attention. We demonstrate that domain adaptive transfer learning plays a vital role in boosting performance. Depending on the domain similarity score, we can choose which source datasets to pre-train on to get better accuracy. We show that combining similarity-based selection of source datasets with attention-based augmentation technique can achieve stateof-the-art results in multiple fine-grained visual classification datasets. We also  analyze the effect of image resolution on transfer learning between the source and target domains. In future work, we are planning to explore the various factors on transfer learning to boost performance. We like to leverage variational auto encoder and GAN to generate augmented data which can be passed to the model to check the performance. Additionally, we want to compare different types of source datasets and try to control the variability in the number of training images to show the impact. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 2 : 3 : 4 :</head><label>1234</label><figDesc>Attention Aware Fine-grained Categorization Input: Trained model with WS-DAN and Raw Image I Output: Classification Accuracy 1: Calculate coarse-grained probability p1 : p1 = W (I) and generate attention maps A Calculate object map Am from A and obtain bounding box B from Am Zoom in the region B as I b Predict fine-grained probability p2 : p2 = W (I b ) 5: Calculate final probability p = (0.5) * (p1 + p2) 6: return p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>-8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Weakly Supervised Data Augmentation Network<ref type="bibr" target="#b11">[12]</ref> Testing Process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Visual attention on image, Attention Maps, Feature Maps, Attention Dropping on CUB200-2011 dataset (left to right respectively)<ref type="bibr" target="#b11">[12]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Visual attention on image, Attention Maps, Feature Maps, Attention Dropping on Flowers-102 dataset (left to right respectively)<ref type="bibr" target="#b11">[12]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Visual attention on image, Attention Maps, Feature Maps, Attention Dropping on Stanford Car dataset (left to right respectively)<ref type="bibr" target="#b11">[12]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Visual attention on image, Attention Maps, Feature Maps, Attention Dropping on Stanford Dog dataset (left to right respectively)<ref type="bibr" target="#b11">[12]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Visual attention on image, Attention Maps, Feature Maps, Attention Dropping on FGVC-Aircraft dataset (left to right respectively)<ref type="bibr" target="#b11">[12]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Training loss on CUB200-2011 dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Accuracy on CUB200-2011 dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Correlation between transfer learning accuracy and domain similarity score between the source and target domain. Each colored line represents a target domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Six commonly used FGVC datasets. With the right selection of source domain, better transfer learning performance can be achieved. For example, the domain similarity score between iNat and CUB200-2011 is around 0.65, which is the reason it shows higher accuracy (91.2) when iNat is used as pre-training the source domain compared to others. For Flowers-102 dataset, the accuracy is 98.9 with iNat as the source domain which has the highest domain simiarity score 0.54, among other source domains. Similarly, Stanford Cars, Stanford Dogs and Aircrafts dataset show higher domain similarity score supports better accuracy. Only for the Food101 dataset, the accuracy from transfer learning remains similar while domain similarity changes. We believe this is due to having a large number of training images in Food101. Consequently, the target domain contains enough data and transfer learning is not as useful. We can observe that both ImageNet and iNat are highly biased, achieving dramatically different transfer learning accuracy on target datasets. Intriguingly, when we transfer networks trained on the combined ImageNet + iNat dataset and perform WS-DAN<ref type="bibr" target="#b11">[12]</ref> method over it, we got better results in Food-101 dataset. The resulted accuracy of the combination of ImageNet and iNat, fell in-between ImageNet andFig. 11. Effect of transfer learning with different sizes of image resolution on iNat dataset. Comparison on domain similarity score between source datasets and target datasets</figDesc><table><row><cell>Datasets</cell><cell cols="3">Objects Classes Training Test</cell></row><row><cell cols="2">CUB200-2011 Bird</cell><cell>200</cell><cell>5,994 5,794</cell></row><row><cell cols="3">FGVC-Aircraft Aircraft 100</cell><cell>6,667 3,333</cell></row><row><cell>Stanford Cars</cell><cell>Car</cell><cell>196</cell><cell>8,144 8,041</cell></row><row><cell cols="2">Stanford Dogs Dog</cell><cell>120</cell><cell>12,000 8,580</cell></row><row><cell cols="3">Flowers-102 Flowers 102</cell><cell>2,040 6,149</cell></row><row><cell>Food-101</cell><cell>Food</cell><cell>101</cell><cell>75,750 25,250</cell></row><row><cell cols="2">marker represents a source domain.</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Table 3. Comparison to different types of FGVC datasets. Each row represents a network pre-trained on source domain for transfer learning and each column represents top-1 image classification accuracy by fine-tuning on the target domain.Method CUB200 2011Stanford Cars Aircrafts Food</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Ashiq Imran, and Vassilis AthitsosTable 4. Comparison in terms of accuracy with existing FGVC methods.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Aircrafts Food</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was partially supported by National Science Foundation grant IIS-1565328. Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the authors, and do not necessarily reflect the views of the National Science Foundation.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method CUB200 2011</head><p>Stanford Cars</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep compositional captioning: Describing novel object categories without paired training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Factors of transferability for a generic convnet representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1790" to="1802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cvae-gan: fine-grained image generation through asymmetric training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2745" to="2754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Autoaugment: Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large scale fine-grained categorization and domain-specific transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4109" to="4118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fine-grained classification via mixture of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mccool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Corke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">See better before looking closer: Weakly supervised data augmentation network for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09891</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Novel dataset for fgvc: Stanford dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">San Diego: CVPR Workshop on FGVC</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d object representations for finegrained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards faster training of global covariance pooling networks by iterative matrix square root normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="947" to="955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06772</idno>
		<title level="m">Improved bilinear pooling with cnns</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<title level="m">Fine-grained visual classification of aircraft</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Jointly optimize data augmentation and network training: Adversarial data augmentation in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2226" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The earth mover&apos;s distance as a metric for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="121" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cnn features off-theshelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural activation constellations: Unsupervised part model discovery with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1143" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8769" to="8778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The caltech-ucsd birds</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a cnn for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4148" to="4157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The application of two-level attention models in deep convolutional neural network for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="842" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<title level="m">How transferable are features in deep neural networks? In: Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A survey on deep learning of small sample in biomedical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00473</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1325" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5209" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
