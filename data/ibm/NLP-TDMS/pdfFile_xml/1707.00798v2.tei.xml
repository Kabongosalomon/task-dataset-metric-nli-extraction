<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Representation Learning with Part Loss for Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantao</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
							<email>jtli@ict.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>qi.tian@utsa.edu</email>
						</author>
						<title level="a" type="main">Deep Representation Learning with Part Loss for Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning discriminative representations for unseen person images is critical for person Re-Identification (ReID). Most of current approaches learn deep representations in classification tasks, which essentially minimize the empirical classification risk on the training set. As shown in our experiments, such representations easily get overfitted on a discriminative human body part among the training set. To gain the discriminative power on unseen person images, we propose a deep representation learning procedure named Part Loss Networks (PL-Net), to minimize both the empirical classification risk and the representation learning risk. The representation learning risk is evaluated by the proposed part loss, which automatically detects human body parts, and computes the person classification loss on each part separately. Compared with traditional global classification loss, simultaneously considering part loss enforces the deep network to learn representations for different parts and gain the discriminative power on unseen persons. Experimental results on three person ReID datasets, i.e., Mar-ket1501, CUHK03, VIPeR, show that our representation outperforms existing deep representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person Re-Identification (ReID) targets to identify a probe person appeared under multiple cameras. More specifically, person ReID can be regarded as a challenging zero-shot learning problem, because the training and test sets do not share any person in common. Therefore, person ReID requires discriminative representations to depict unseen person images.</p><p>Existing approaches conquer this challenge by either seeking discriminative metrics <ref type="bibr" target="#b47">[49,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b57">59,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b44">46,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b57">59,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b48">50]</ref>, or generating discriminative features <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b59">61</ref>]. Inspired by the success of Convolutional Neural Network (CNN) in large-scale visual classification <ref type="bibr" target="#b16">[18]</ref>, lots of approaches have been proposed to generate representations based on CNN <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr" target="#b0">2,</ref><ref type="bibr" target="#b58">60,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b31">33]</ref>. For example, several works <ref type="bibr" target="#b43">[45,</ref><ref type="bibr" target="#b56">58,</ref><ref type="bibr" target="#b41">43]</ref> employ deep clas-  Notwithstanding the success of these approaches, we argue that representations learned by current classification models are not optimal for zero-shot learning problems like person ReID. Most of current deep classification models learn representations by minimizing the classification loss on the training set. This conflicts with the objective of representation learning in person ReID, i.e., gaining high discriminative power to unseen person images. Different optimization objectives make current deep representations perform promisingly on classification tasks, but might not be optimal to depict and distinguish unseen person images.</p><p>Observations from our experiments are consistent with the above discussions. As shown in <ref type="figure" target="#fig_1">Fig. 1(a)</ref>, the representations generated by deep classification model mainly focus on one body region, i.e., the upper body, and ignore the other body parts. This seems reasonable because on the training set, the upper body conveys more distinct clothing cues than the other parts. In order to decrease the classification loss on training data, deep network tends to focus on upper body and ignore the others. However, the other body parts like head, lower-body, and foot are potential to be meaningful for depicting other unseen persons. Ignoring those parts is potential to increases the risk of representation learning for unseen data.</p><p>The above observations motivate us to study more reliable deep representations for person ReID. We are inspired by the structural risk minimization principle in SVM <ref type="bibr" target="#b7">[9]</ref>, which imposes more strict constraint by maximizing the classification margin. Similarly, we enforce the network to learn better representation with extra representation learning risk minimization. Specifically, the representation learning risk is evaluated by the proposed part loss, which automatically generates K parts for an image, and computes the person classification loss on each part separately. In other words, the network is trained to focus on every body part and learn representations for each of them. As illustrated in <ref type="figure" target="#fig_1">Fig. 1(b)</ref>, minimizing the person part loss guides the deep network to learn discriminative representations for different body parts. In other words, part loss avoids overfitting on a specific body part, thus decreases the representation learning risk for unseen data.</p><p>We propose part loss networks (PL-Net) structure that can be optimized accordingly. As shown in <ref type="figure">Fig. 2</ref>, part loss networks is composed of a baseline network and an extension to compute the person part loss. It is trained to simultaneously minimize the part loss and the global classification loss. Experiments on three public datasets, i.e., Market1501, CUHK03, VIPeR show PL-Net learns more reliable representations and obtains promising performance in comparison with state-of-the-arts. It also should be noted that, PL-Net is easy to repeat because it only has one important parameter to tune, i.e., the number of generated parts K.</p><p>Most of previous person ReID works directly train deep classification models to extract image representations. To our best knowledge, this work is an original effort discussing the reasons why such representations are not optimal for person ReID. Representation learning risk and part loss are hence proposed to learn more reliable deep representations to depict unseen person images. The proposed PL-Net is simple but shows promising performance in comparison with the state-of-the-arts. It may also inspire future research on zero-shot learning for person ReID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The promising performance of CNN on ImageNet classification indicates that classification network extracts discriminative image features. Therefore, several works <ref type="bibr" target="#b43">[45,</ref><ref type="bibr" target="#b56">58,</ref><ref type="bibr" target="#b41">43]</ref> fine-tuned the classification networks on target datasets as feature extractors for person ReID. For example, Xiao et al. <ref type="bibr" target="#b43">[45]</ref> propose a novel dropout strategy to train a classification model with multiple datasets jointly. Wu et al. <ref type="bibr" target="#b41">[43]</ref> combine the hand-crafted histogram features and deep features to fine-tune the classification network.</p><p>Besides of classification network, siamese network and triplet network are two popular networks for person ReID. The siamese network takes a pair of images as input, and is trained to verify the similarity between those two images <ref type="bibr" target="#b58">[60,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b0">2,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b32">34]</ref>  <ref type="bibr" target="#b14">[16]</ref> for part localization, and propose Multli-Scale Context-Aware Network to infer representations on the generated local parts.</p><p>By analyzing the difference between image classification and person ReID, we find that the representations learned by existing deep classification models are not optimal for person ReID. Therefore, we consider extra representation learning risk and person part loss for deep representation learning. Our work also considers local parts cues for representation learning. Different from existing algorithms <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b17">19]</ref>, part loss networks (PL-Net) automatically detects human parts and does not need extra annotation or detectors, thus is more efficient and easier to implement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation</head><p>Given a probe person image I q , person ReID targets to return the images containing the identical person in I q from a gallery set G. We denote the gallery set as</p><formula xml:id="formula_0">G = {I i }, i ∈ [1, m],</formula><p>where m is the total number of person images. Person ReID can be tackled by learning a discriminative feature representation f for each person image from a training set T . Therefore, the probe image can be identified by matching its f q against the gallery images.</p><p>Suppose the training set contains n labeled images from C persons, we denote the training set as T = {I i , y i }, i ∈ [1, n], y i ∈ [1, C], where I i is the i-th image and y i is its person ID label. Note that, person ReID assumes the training and gallery sets contain distinct persons. Therefore, person ReID can be regarded as a zero-shot learning problem, i.e., the ID of probe person is not included in the training set.</p><p>Currently, some methods <ref type="bibr" target="#b43">[45,</ref><ref type="bibr" target="#b58">60,</ref><ref type="bibr" target="#b41">43]</ref> fine-tune a classification-based CNN to generate the feature representation. The feature representation learning can be formulated as updating the CNN network parameter θ by minimizing the empirical classification risk of representation f on T through back prorogation. We denote the empirical classification risk on T as,</p><formula xml:id="formula_1">J = 1 n [ n i=1 L g (ŷ i )],<label>(1)</label></formula><p>whereŷ i is the predicted classification score for the i-th training sample, and L g (·) computes the classification loss for each training image. We use the superscript g to denote it is computed on the global image. The predicted classification scoreŷ i can be formulated as, i.e.,</p><formula xml:id="formula_2">y i = Wf i + b,<label>(2)</label></formula><p>where W denotes the parameter of the classifier in CNN, e.g., the weighting matrix in the fully connected layer. Given a new image I q , its representation f q is hence extracted by CNN with the updated parameter θ, i.e.,</p><formula xml:id="formula_3">f q = CNN θ (I q ).<label>(3)</label></formula><p>It can be inferred from Eq. (1) and Eq. (2) that, to improve the discriminative power of f i during training, a possible way is to restrict the classification ability of W. In another word, a weaker W would enforce the network to learn more discriminative f i to minimize the classification error. This motivates us to introduce a baseline CNN network with weaker classifiers. Details of this network will be given in Sec. <ref type="bibr">3.2</ref> It also can be inferred from Eq. (1) that, minimizing the empirical classification risk on T results in a discriminative representation f for classifying the seen categories in T . For example in <ref type="figure" target="#fig_1">Fig. 1(a)</ref>, the learned representations focus on discriminative parts for training set. However, such representations lack the ability to describe other parts like head, lower-body, and foot which could be meaningful to distinguish an unseen person. Therefore, more parts should be depicted by the network to minimize the risk of representation learning for unseen data.</p><p>Therefore, we propose to consider the representation learning risk, which tends to make the CNN network learn discriminative representation for each part of the human body. We denote the representation of each body part as f k , k ∈ [1, K], where K is the total number of parts. The representation learning risk P can be formulated as, . <ref type="figure">Figure 2</ref>. Overview of part loss networks (PL-Net), which is composed of a baseline network and a part loss computation extension. "GAP" denotes the Global Average Pooling. Given an input image, we firstly extract its feature maps X , then compute the global loss and person part loss based on X . The person part loss is computed on K parts generated with an unsupervised method.</p><formula xml:id="formula_4">P = 1 K K k=1 1 n [ n i=1 L p (ŷ k i )],<label>(4)</label></formula><p>where L p (·) computes the part loss, i.e., the classification loss on each part.ŷ k i is the predicted person classification score for the i-th training sample by the representation of k-th part.ŷ k i is computed with,</p><formula xml:id="formula_5">y k i = W k f k i + b k ,<label>(5)</label></formula><p>where W k denotes the classifier for the representation of the k-th part. The representation learning risk monitors the network and enforces it to learn discriminative representation for each part. It shares a certain similarity with the structural risk minimization principle in SVM <ref type="bibr" target="#b7">[9]</ref>, which also imposes more strict constraints to enforce the classifier to learn better discriminative power.</p><p>The final part loss networks (PL-Net) model could be inferred by minimizing the empirical classification risk and the representation learning risk simultaneously, i.e., θ = arg min(J + P).</p><p>In the following parts, we proceed to introduce the part loss networks and the computation of part loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Part Loss Networks</head><p>Most of the deep learning-based person ReID methods treat the Alexnet <ref type="bibr" target="#b16">[18]</ref>, GoogLeNet <ref type="bibr" target="#b35">[37]</ref>, and Residual-50 <ref type="bibr" target="#b12">[14]</ref> as the baseline network. Given an image, these networks firstly use several convolutional layers to generate the feature representation, then infer fully-connected layers for classification. Therefore, these networks essentially consist of feature extraction and classification modules.</p><p>As discussed in Sec. 3.1, weaker classifiers should be used to improve the discriminative power of the learned representations. Moreover, the massive parameters in fullyconnected layers may make the network prone to overfitting, especially for small-scale person ReID training sets.</p><p>We thus propose a simpler classifier in our baseline network. Our baseline network replaces the fully-connected layers with a convolutional layer and a Global Average Pooling (GAP) layer <ref type="bibr" target="#b21">[23]</ref>. As shown in <ref type="figure">Fig. 2</ref>, the convolutional layer directly generates C activation maps explicitly corresponding to C classes. Then GAP hence generates the classification score for each class, i.e.,</p><formula xml:id="formula_7">s c = 1 W × H H h=1 W w=1 C c (h, w),<label>(7)</label></formula><p>where s c is the average response of the c-th activation map C c with size W × H, and C c (h, w) denotes the activation value on the location (h, w) on C c . s c is hence regarded as the classification score for the c-th class. As GAP contains no parameter to learn, it avoids over-fitting and makes the network more compact. We replace FC with GAP because GAP has weak discriminative power and thus needs a powerful feature to ensure the classification accuracy. This encourages the end-to-end training to better focus on feature learning. According to Eq. <ref type="formula" target="#formula_6">(6)</ref>, our representation is learned to minimize both the empirical classification risk and the representation learning risk. The empirical classification risk is evaluated by the classification loss on the training set. The representation learning risk is evaluated by counting the classification loss on each body part. We thus extend the baseline network accordingly to make it can be optimized by these two types of supervisions. The overall network is shown in <ref type="figure">Fig. 2</ref>. During training, it computes a person part loss and a global loss with two branches.</p><p>Specifically, part loss networks (PL-Net) processes the input image and generates feature maps. We denote the feature maps of the last convolutional layer before the classification module as X ∈ R Z×H×W . For example, Z=1024, H=16, W =8 when we input 512 × 256 sized image into the baseline network modified from GoogLeNet <ref type="bibr" target="#b35">[37]</ref>. After obtaining X , the global loss is calculated as,</p><formula xml:id="formula_8">L g (ŷ i ) = − C c=1 1{y i = c} log eŷ i C l=1 eŷ l .<label>(8)</label></formula><p>The part loss is computed on each automatically generated part to minimize the representation learning risk. The network first generates K person parts based on X in an unsupervised way. Then part loss is computed on each part by counting its classification loss. The following part gives details of the unsupervised part generation and part loss computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Person Part Loss Computation</head><p>Person parts can be extracted by various methods. For instance, detection models could be trained with part annotations to detect and extract part locations. However, those CNN CNN feature maps generated by average pooling all feature maps <ref type="figure">Figure 3</ref>. Examples of CNN feature maps and generated saliency maps. The saliency map generated on all feature maps focuses on one part and suppresses the activations on other parts. The four saliency maps on the right side are generated by average pooling four clusters of feature maps, respectively. They clearly indicate different part locations. methods <ref type="bibr" target="#b49">[51,</ref><ref type="bibr" target="#b53">55]</ref> require extra annotations that are hard to collect. We thus propose an unsupervised part generation algorithm that can be optimized together with the representation learning procedure.</p><p>Previous work <ref type="bibr" target="#b39">[41]</ref> shows that simply average pooling the feature maps of convolutional layers generates a saliency map. The saliency essentially denotes the "focused" regions by the neural network. <ref type="figure">Fig. 3</ref> shows several feature maps generated by a CNN trained in the classification task. It can be observed that, the lower part of the body has substantially stronger activations. There exist some feature maps responding to the other parts like head and upper body, but their responses are substantially weaker. As illustrated in <ref type="figure">Fig. 3</ref>, simply average pooling all feature maps shows the discriminative region and suppresses the other regions.</p><p>Although the responses on different parts are seriously imbalanced, they still provide cues of different part locations. By clustering feature maps based on the locations of their maximum responses, we can collect feature maps depicting different body parts. Individually average pooling those feature map clusters indicates the part locations. As shown in <ref type="figure">Fig. 3</ref>, the four saliency maps on the right side focus on head, upper body, lower body, and foot, respectively. This might be because the appearances of head, lower body, and foot differs among training persons, thus CNN still learns certain neurons to depict them.</p><p>The above observation motivates our unsupervised part generation. Assume that we have got the feature map X , we first compute the position of maximum activation on each feature map, denoted as (h z , w z ), z ∈ [1, Z], where X z (h, w) is the activation value on location (h, w) in the z-th channel of X . We then cluster those locations (h, w) into K groups using L2 distance. As the images in current person ReID datasets are cropped and coarsely aligned, we could simply perform clustering only according to the vertical location h.</p><formula xml:id="formula_9">(h z , w z ) = arg max h,w X z (h, w),<label>(9)</label></formula><p>After grouping all feature maps into K clusters, we generate one part bounding box from each cluster. Specifically, we average pooling the feature maps in each cluster and apply the max-min normalization to produce the saliency map. A threshold, e.g., 0.5, is set to turn each saliency map into a binary image. In other words, we consider a pixel as foreground if its value is larger than the threshold. For each binary image, we treat its minimum enclosing rectangle as the part bounding box. This procedure is illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p><p>After obtaining the part bounding boxes, we proceed to compute the part loss. Inspired by Fast R-CNN <ref type="bibr" target="#b10">[12]</ref>, we employ the RoI pooling to convert the responses of X inside each part bounding box into a new feature map X k ∈ R Z×H ×W with a fixed spatial size, e.g., H = W = 4 in this work. Based on those feature maps, we compute the part loss L p (·) for k-th part with a similar procedure of global loss computation, i.e.,</p><formula xml:id="formula_10">L p (ŷ k l ) = − C c=1 1{y i = c} log eŷ k i C l=1 eŷ k l .<label>(10)</label></formula><p>Similar to the notations in Eq. (4),ŷ k i is the predicted person classification score of the i-th training sample based on the representation of its k-th part.</p><p>The generated parts are updated on each iteration of network training. It should be noted that, the accuracy of our unsupervised part generation is related with the representation learning performance. For example in <ref type="figure">Fig. 3</ref>, if more neurons are trained to depict parts like head and foot during representation learning, more feature maps would focus on these parts. This in turn improves the feature maps clustering and results in more accurate bounding boxes for these parts. During this procedure, the part generation and representation learning can be jointly optimized.</p><p>Examples of generated parts are shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. As shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, the bounding boxes cover important body parts. For the case with K=4, the generated four parts coarsely cover the head, upper body, lower body, and legs, respectively. For the case that K=8, most of generated parts distribute on the human and cover more detailed parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Person ReID</head><p>On the testing phase, we extract feature representation from the trained part loss networks for person ReID. We use the feature maps X to generate the global and part representations for similarity computation.</p><p>Given a person image I, we firstly resize it to the size of 512 × 256, then input it into the network to obtain feature maps X . We hence compute the global representation f (g) with Eq. (11),</p><formula xml:id="formula_11">f (g) = [f 1 , ..., f z , ...f Z ],<label>(11)</label></formula><formula xml:id="formula_12">f z = 1 W × H H h=1 W w=1 X z (h, w).<label>(12)</label></formula><p>For the part representation, we obtain the feature maps after RoI pooling for each part, denoted as X k ∈ R Z×4×4 , k ∈ [1, K]. For each X k , we calculate the part description f k in similar way with Eq. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We verify the proposed part loss networks (PL-Net) on three datasets: VIPeR <ref type="bibr" target="#b11">[13]</ref>, CUHK03 <ref type="bibr" target="#b18">[20]</ref>, and Mar-ket1501 <ref type="bibr" target="#b54">[56]</ref>. VIPeR <ref type="bibr" target="#b11">[13]</ref> contains 632 identities appeared under two cameras. For each identity, there is one image for each camera. The dataset is split randomly into equal halves and cross camera search is performed to evaluate the algorithms. CUHK03 <ref type="bibr" target="#b18">[20]</ref> consists of 14,097 cropped images from 1,467 identities. For each identity, images are captured from two cameras and there are about 5 images for each view. Two ways are used to produce the cropped images, i.e., human annotation and detection by Deformable Part Model (DPM) <ref type="bibr" target="#b9">[11]</ref>. Our evaluation is based on the human annotated images. We use the standard experimental setting <ref type="bibr" target="#b18">[20]</ref> to select 1,367 identities for training, and the rest 100 for testing.</p><p>Market1501 <ref type="bibr" target="#b54">[56]</ref> contains 32,668 images from 1,501 identities, and each image is annotated with a bounding box detected by DPM. Each identity is captured by at most six cameras. We use the standard training, testing, and query split provided by the authors in <ref type="bibr" target="#b54">[56]</ref>. The Rank-1, Rank-5, Rank-10 accuracies are evaluated for VIPeR and CUHK03. For Market1501, we report the Rank-1 accuracy and mean Average Precision (mAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We use Caffe <ref type="bibr" target="#b15">[17]</ref> to implement and train the part loss networks (PL-Net). The baseline network is modified from second version of GoogLeNet <ref type="bibr" target="#b13">[15]</ref>. Following the in-ception5b/output layer, an 1 × 1 convolutional layer with the output of C channels is used to generate the category confidence map. For the training, we use the pre-trained model introduced in [1] to initialize the PL-Net, and use a step strategy with mini-batch Stochastic Gradient Descent (SGD) to train the neural networks on Tesla K80 GPU. Parameters like the maximum number of iterations, learning rate, step size, and gamma are set as 50,000, 0.001, 2500, and 0.75, respectively. For the person images, we first resize their size to 512 × 256, and then feed their into the PL-Net for training and testing.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance of Learned Representations</head><p>Accuracy of Part Generation: One key component of our representation learning is the person part generation. As existing person ReID datasets do not provide part annotations, it is hard to quantify the results. To demonstrate that our generated parts are reasonable, we compare the representations learned by CNN trained with part loss using the generated parts and fixed grid parts, respectively. As shown on the left side of <ref type="figure" target="#fig_6">Fig. 6</ref>, we generate grid parts by equally dividing an image into horizontal stripes following previous works <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b44">46]</ref>. In <ref type="figure" target="#fig_6">Fig. 6</ref>, the generated parts get substantially higher accuracy than the fixed grid parts for K = 4 and 8, respectively. This conclusion is reasonable, because the generated parts cover most of the human body and filter the clustered backgrounds. It also can be observed that, part representations extracted from the center parts of human body, e.g., parts with index =4 and 5 for K=8, get higher accuracies. This might be because the center of human body generally presents more distinct clothing cues. Table 1 compares the final global-part representations learned with fixed grid parts and our generated parts. It is clear that, our generated parts perform substantially better.</p><p>Validity of Part Loss: This experiment shows that part loss helps to minimize the representation learning risk and improve the descriptive power of CNN. We firstly show the effects of part loss computed with fixed grid parts. We equally divide an image into stripes, then learn part representations on them with and without part loss, respectively. We compare the ReID performance on Market1501. <ref type="figure" target="#fig_7">Fig. 7</ref> clearly shows that more discriminative part representations can be learned with part loss for K =4 and 8, respectively.  Besides using fixed grid part, we further perform experiments to show the validity of part loss computed on generated parts. Comparisons with similar settings are shown in <ref type="figure" target="#fig_8">Fig. 8</ref>, where part loss also constantly improves the performance. Those two experiments show that, part loss enforces the network to learn more discriminative representations for different body parts, thus better avoids overfitting and decreases the representation learning risk for unseen person images.</p><p>Performance of Global Representation: This experiment verifies the effects of part loss to the global representation. As shown in <ref type="figure">Fig. 2</ref>, the global representation is computed on X , which is also affected by the part loss. Experimental results on Market1501 are shown in <ref type="table" target="#tab_2">Table 2</ref>, where K=0 means no part is generated, thus part loss is not considered. From <ref type="table" target="#tab_2">Table 2</ref>, we could observe that part loss also boosts the global representation, e.g., the mAP and Rank-1 accuracy constantly increase with larger K. This phenomenon can be explained by the saliency maps in <ref type="figure" target="#fig_1">Fig. 1 (b)</ref>, which shows the global representation learned with part loss focuses on larger body regions. We thus conclude that, part loss also boosts the global representation to focus on more body parts.</p><p>Performance of Final Representation: K is the only parameter for part loss. We thus test the performance of the final representation with different K. As shown in <ref type="figure" target="#fig_9">Fig. 9</ref>, the final representation performs better with larger K, which extracts more detailed parts. This is consistent with the observation in <ref type="table" target="#tab_2">Table 2</ref>. This also partially validates our part generation algorithm and part loss. Therefore, we set K=8 in the following experiments.</p><p>Discussions on Part Loss: For Peron ReID, it is hard to directly model unseen person images because they are not  given during training. We thus propose the part loss to decrease the representation learning risk on unseen person images. Part loss is a strict constraint, i.e., it is difficult to predict person ID from a single body part. By posting this strict constraints, we enforce the network to learn discriminative features for different parts, thus avoid overfitting on a specific part on the training set. As shown in the above experiments, the performance of a single part feature in <ref type="figure" target="#fig_7">Fig. 7</ref> and <ref type="figure" target="#fig_8">Fig. 8</ref> is not high. However, their concatenation achieves promising performance in <ref type="figure" target="#fig_9">Fig. 9</ref>. Our part loss is computed with Eq. (10), i.e., compute the ID classification error on each part separately. Another possible solution is first to concatenate part representations then compute the ID classification with the fused features. We have compared those two methods and summarize the results in <ref type="table" target="#tab_3">Table 3</ref>. As shown in the comparison, part loss computed with Eq. (10) performs better than the other solution, e.g., 67.17%vs 64.72%. This might be because Eq. <ref type="bibr" target="#b8">(10)</ref> better ensures the quality of each learned part feature, thus is more effective in decreasing the representation learning risk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State-of-the-art</head><p>In this section, we compare the proposed part loss networks (PL-Net) with existing ones on the Market1501, CUHK03, and VIPeR. <ref type="table" target="#tab_4">Table 4</ref> shows the comparison on Market1501 in the terms of mAP and Rank-1 accuracy. As shown in <ref type="table" target="#tab_4">Table 4</ref>, the proposed method achieves the mAP of 69.3% and Rank-1 accuracy 88.2%, which both outperform existing methods. As shown in <ref type="table" target="#tab_4">Table 4</ref>, by adding the part loss, the global and part representation achieve 4% and 7.1% improvements in mAP over the baseline network, respectively. This makes the global and part representations already perform better than existing methods. By combining the global and part representations, PL-Net further boosts the performance. On CUHK03, the comparisons with existing methods are summarized in <ref type="table">Table 5</ref>. As shown in <ref type="table">Table 5</ref>, the global and part representations improve the baseline network by 8.1% and 9.85% on Rank-1 accuracy, respectively. The proposed PL-Net achieves 82.75%, 96.59%, and 98.59% for the for Rank-1, Rank-5, and Rank-10 accuracies, respectively. This substantially outperforms most of the compared methods. Note that, the SpindelNet <ref type="bibr" target="#b49">[51]</ref> and PDC <ref type="bibr" target="#b33">[35]</ref> are learned with extra human landmark annotations, thus leverages more detailed annotations than our method, and DLPAR <ref type="bibr" target="#b50">[52]</ref> has a higher baseline performance, e.g., 82.4% <ref type="bibr" target="#b50">[52]</ref> vs 72.85% for our baseline.</p><p>The comparisons on VIPeR are summarized in <ref type="table">Table 6</ref>. As VIPeR dataset contains fewer training images, it is hard to learn a robust deep representation. Therefore, deep learning-based methods <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b33">35]</ref> achieve lower performance than metric learning methods <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b48">50]</ref>. As shown in <ref type="table">Table 6</ref>, simply using the generated representation obtains the Rank-1 accuracy of 47.47%, which is lower than some metric learning methods <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b48">50]</ref>. However, it outperforms most of recent deep learning based methods, e.g., DeepReID <ref type="bibr" target="#b18">[20]</ref>, LSTM Siamese <ref type="bibr" target="#b38">[40]</ref>, Gated Siamese <ref type="bibr" target="#b37">[39]</ref>, and MuDeep <ref type="bibr" target="#b31">[33]</ref>. Some recent deep learning based methods <ref type="bibr" target="#b49">[51,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b50">52]</ref> perform better than ours. Note that, SpindelNet <ref type="bibr" target="#b49">[51]</ref> and PDC <ref type="bibr" target="#b33">[35]</ref> leverage extra annotations during training. Also, the training set of DLPAR <ref type="bibr" target="#b50">[52]</ref> is larger than ours, i.e., the combination of CUHK03 and VIPeR. Our learned representation is capable of combining with other features to further boost the performance. By combining the traditional LOMO <ref type="bibr" target="#b19">[21]</ref> feature, we improve the Rank-1 accuracy to 56.65%, which is the highest among all of the compared works.</p><p>From the above comparisons, we summarize : 1) part loss improves the baseline network and results in more discriminative global and part representations, and 2) the combined final representation is learned only with person ID annotations but outperforms most of existing works on the three datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This paper shows that, the traditional deep classification models are trained with empirical classification risk on the training set. This makes those deep models not optimal for representation learning in person ReID, which can be regarded as a zero-shot learning problem. We thus propose to minimize the representation learning risk to infer more discriminative representations for unseen person images. The person part loss is computed to evaluate the representation learning risk. Person part loss firstly generates K body parts in an unsupervised way, then optimizes the classification loss for each part separately. In this way, part loss network learns discriminative representations for different parts. Extensive experimental results on three public datasets demonstrate the advantages of our method. This work explicitly infers parts based on the given parameter K. More implicit ways to minimize the representation learning risk will be explored in our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Saliency maps of CNN learned in traditional classification network (a), and part loss networks (PL-Net) (b). The salient region reveals the body part that the CNN representation focuses on. Representations of our PL-Net are more discriminative to different parts. sification model to learn representations. More detailed reviews on deep learning based person ReID will be given in Sec. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of the procedure for unsupervised person part generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Samples of generated part bounding boxes. The first and second row correspond to K = 4 and K = 8, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc><ref type="bibr" target="#b9">(11)</ref>. The final representation is the concatenation of global and part representations, i.e., f = [f (g) , f 1 , ..., f K ].<ref type="bibr" target="#b11">(13)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Performance comparison of representations learned on generated parts and fixed grid parts on Market1501.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Performance of part representations learned with and without part loss on Market1501. We use fixed grid parts in this experiment with K=4 and 8, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Performance of part representation learned with and without part loss on Market1501.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Performance of final representation on Market1501 and CUHK03 with different K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. Ahmed et al.<ref type="bibr" target="#b0">[2]</ref> and Zheng et al.<ref type="bibr" target="#b58">[60]</ref> employ the siamese network to infer the description and a corresponding similarity metric simultaneously. Shi et al.<ref type="bibr" target="#b32">[34]</ref> replace the Euclidean distance with Mahalanobis distance in the siamese network. Varior et al.<ref type="bibr" target="#b38">[40]</ref> combine the LSTM and siamese network for person ReID.</figDesc><table><row><cell>Some other works [36, 6, 27] employ triplet networks to</cell></row><row><cell>learn the representation for person ReID. Cheng et al. [6]</cell></row><row><cell>propose a multi-channel parts-based CNN model for per-</cell></row><row><cell>son ReID. Liu et al. [27] propose an end-to-end Com-</cell></row><row><cell>parative Attention Network to generate image description.</cell></row><row><cell>Su et al. [36] propose a semi-supervised network trained by</cell></row><row><cell>triplet loss to learn human semantic attributes.</cell></row><row><cell>Recently, many works generate deep representation from</cell></row><row><cell>local parts [35, 51, 19, 52]. For example, Su et al. [35], and</cell></row><row><cell>Zhao et al. [51] firstly extract human body parts with four-</cell></row><row><cell>teen body joints, then fuse the features extracted on body</cell></row><row><cell>parts. Different from [35] and [51], Li et al. [19] employ</cell></row><row><cell>Spatial Transform Network (STN)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Performance of final representations learned with our generated parts vs. fixed grid parts with K=8 on Market1501.</figDesc><table><row><cell>Part</cell><cell cols="2">mAP(%) Rank-1 (%)</cell></row><row><cell>Grid Part</cell><cell>67.99</cell><cell>86.96</cell></row><row><cell>Generated Part</cell><cell>69.3</cell><cell>88.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Performance of global representation on Market1501 with different K. K=0 means the part loss is not considered.</figDesc><table><row><cell>K</cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>8</cell></row><row><cell>mAP(%)</cell><cell cols="4">61.9 62.0 64.46 65.91</cell></row><row><cell cols="3">Rank-1 Acc.(%) 81.5 81.9</cell><cell>84</cell><cell>85.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell cols="7">mAP achieved by different ways of part loss computation</cell></row><row><cell cols="7">on Market1501. "Concat." denotes part loss computed with con-</cell></row><row><cell cols="7">catenated part features. "Final", "Global", "P-k" denotes the final,</cell></row><row><cell cols="6">global, and k-th part representations. K is set as 4.</cell><cell></cell></row><row><cell>Methods</cell><cell>Final</cell><cell>Global</cell><cell>P-1</cell><cell>P-2</cell><cell>P-3</cell><cell>P-4</cell></row><row><cell>Concat.</cell><cell>64.72</cell><cell>63.36</cell><cell cols="4">21.80 38.55 37.78 19.39</cell></row><row><cell cols="2">Part Loss 67.17</cell><cell>64.46</cell><cell cols="4">25.43 42.24 45.19 32.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparison on Market1501 with single query.</figDesc><table><row><cell>Methods</cell><cell cols="2">mAP(%) Rank-1 (%)</cell></row><row><cell>LOMO+XQDA [21] CVPR15</cell><cell>22.22</cell><cell>43.79</cell></row><row><cell>TMA [29] ECCV16</cell><cell>22.31</cell><cell>47.92</cell></row><row><cell>DNS [50] CVPR16</cell><cell>35.68</cell><cell>61.02</cell></row><row><cell>SSM [3] CVPR17</cell><cell>68.80</cell><cell>82.21</cell></row><row><cell>LSTM SCNN [40] ECCV16</cell><cell>35.31</cell><cell>61.60</cell></row><row><cell>Gated SCNN [39] ECCV16</cell><cell>39.55</cell><cell>65.88</cell></row><row><cell>SpindleNet [51] CVPR17</cell><cell>-</cell><cell>76.9</cell></row><row><cell>MSCAN [19] CVPR17</cell><cell>57.53</cell><cell>80.31</cell></row><row><cell>DLPAR [52] ICCV17</cell><cell>63.4</cell><cell>81.0</cell></row><row><cell>P2S [62] CVPR17</cell><cell>44.27</cell><cell>70.72</cell></row><row><cell>CADL [22] CVPR17</cell><cell>55.58</cell><cell>80.85</cell></row><row><cell>PDC [35] ICCV17</cell><cell>63.41</cell><cell>84.14</cell></row><row><cell>Baseline Network</cell><cell>61.9</cell><cell>81.5</cell></row><row><cell>Global Representation</cell><cell>65.9</cell><cell>85.6</cell></row><row><cell>Part Representation</cell><cell>69</cell><cell>88.0</cell></row><row><cell>PL-Net</cell><cell>69.3</cell><cell>88.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Comparison with existing methods on CUHK03. Comparison with existing methods on VIPeR.</figDesc><table><row><cell>Methods</cell><cell cols="3">Rank-1 Rank-5 Rank-10</cell></row><row><cell>DeepReID [20] CVPR14</cell><cell>20.65</cell><cell>51.50</cell><cell>66.5</cell></row><row><cell>LSTM SCNN [40] ECCV16</cell><cell>57.3</cell><cell>80.1</cell><cell>88.3</cell></row><row><cell>Gated SCNN [39] ECCV16</cell><cell>61.8</cell><cell>88.1</cell><cell>92.6</cell></row><row><cell>DNS [50] CVPR16</cell><cell>62.55</cell><cell>90.05</cell><cell>94.80</cell></row><row><cell>GOG [30] CVPR16</cell><cell>67.3</cell><cell>91.0</cell><cell>96.0</cell></row><row><cell>DGD [45] CVPR16</cell><cell>72.58</cell><cell>95.21</cell><cell>97.72</cell></row><row><cell>SSM [3] CVPR17</cell><cell>76.6</cell><cell>94.6</cell><cell>98.0</cell></row><row><cell>SpindleNet [51] CVPR17</cell><cell>88.5</cell><cell>97.8</cell><cell>98.6</cell></row><row><cell>MSCAN [19] CVPR17</cell><cell>74.21</cell><cell>94.33</cell><cell>97.54</cell></row><row><cell>DLPAR [52] ICCV17</cell><cell>85.4</cell><cell>97.6</cell><cell>99.4</cell></row><row><cell>MuDeep [33] ICCV17</cell><cell>76.87</cell><cell>96.12</cell><cell>98.41</cell></row><row><cell>PDC [35] ICCV17</cell><cell>88.70</cell><cell>98.61</cell><cell>99.24</cell></row><row><cell>Baseline Network</cell><cell>72.85</cell><cell>89.53</cell><cell>94.82</cell></row><row><cell>Global Representation</cell><cell>80.95</cell><cell>95.86</cell><cell>98.16</cell></row><row><cell>Local Representation</cell><cell>82.7</cell><cell>96.6</cell><cell>98.59</cell></row><row><cell>PL-Net</cell><cell>82.75</cell><cell>96.59</cell><cell>98.6</cell></row><row><cell>Methods</cell><cell cols="3">Rank-1 Rank-5 Rank-10</cell></row><row><cell>DNS [50] CVPR16</cell><cell>41.01</cell><cell>69.81</cell><cell>81.61</cell></row><row><cell>TMA [29] ECCV16</cell><cell>48.19</cell><cell>87.65</cell><cell>93.54</cell></row><row><cell>GOG [30] CVPR16</cell><cell>49.72</cell><cell>88.67</cell><cell>94.53</cell></row><row><cell>Null [50] CVPR16</cell><cell>51.17</cell><cell>90.51</cell><cell>95.92</cell></row><row><cell>SCSP [4] CVPR16</cell><cell>53.54</cell><cell>91.49</cell><cell>96.65</cell></row><row><cell>SSM [3] CVPR17</cell><cell>53.73</cell><cell>91.49</cell><cell>96.08</cell></row><row><cell>DeepReID [20] CVPR14</cell><cell>19.9</cell><cell>49.3</cell><cell>64.7</cell></row><row><cell>Gated Siamese [39] ECCV16</cell><cell>37.8</cell><cell>66.9</cell><cell>77.4</cell></row><row><cell>LSTM Siamese [40] ECCV16</cell><cell>42.4</cell><cell>68.7</cell><cell>79.4</cell></row><row><cell>SpindleNet [51] CVPR17</cell><cell>53.8</cell><cell>74.1</cell><cell>83.2</cell></row><row><cell>MuDeep [33] ICCV17</cell><cell>43.03</cell><cell>74.36</cell><cell>85.76</cell></row><row><cell>DLPAR [52] ICCV17</cell><cell>48.7</cell><cell>74.7</cell><cell>85.1</cell></row><row><cell>PDC [35] ICCV17</cell><cell>51.27</cell><cell>74.05</cell><cell>84.18</cell></row><row><cell>Baseline Network</cell><cell>34.81</cell><cell>61.71</cell><cell>72.47</cell></row><row><cell>Global Representation</cell><cell>44.30</cell><cell>69.30</cell><cell>79.11</cell></row><row><cell>Local Representation</cell><cell>44.94</cell><cell>72.47</cell><cell>80.70</cell></row><row><cell>PL-Net</cell><cell>47.47</cell><cell>72.47</cell><cell>80.70</cell></row><row><cell>PL-Net+LOMO [21]</cell><cell>56.65</cell><cell>82.59</cell><cell>89.87</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scalable person re-identification on supervised smoothed manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Similarity learning with spatial constraints for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Similarity learning on an explicit polynomial kernel feature map for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Person re-identification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Custom pictorial structures for re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stoppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bmvc</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A two stream siamese convolutional neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tahboub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Delp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PETSW</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deep context-aware features over body and latent parts for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="384" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Consistent-aware deep learning for person re-identification in a camera network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5771" to="5780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Person re-identification by iterative re-weighted sparse ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1629" to="1642" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pop: Person re-identification post-rank optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Person reidentification: What features are important?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">End-to-end comparative attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04404</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Domain transfer support vector ranking for person re-identification without target camera label information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal model adaptation for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Martinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Micheloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hierarchical gaussian descriptor for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Local fisher discriminant analysis for pedestrian re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pedagadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Orwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Velastin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boghossian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised cross-dataset transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multiscale deep learning architectures for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Embedding deep metric for person re-identification: A study against large variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Posedriven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep attributes driven multi-camera person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Multiregion bilinear convolutional neural networks for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.05300</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A siamese long short-term memory architecture for human reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Selective convolutional descriptor aggregation for fine-grained image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04994</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Personnet: Person re-identification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.07255</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An enhanced deep feature representation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A comprehensive study on cross-view gait based human identification with deep cnns. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Person reidentification using kernel-based metric learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Salient color names for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep metric learning for practical person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cross-view asymmetric metric learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning a discriminative null space for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spindle net: Person re-identification with human body region guided feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Deeply-learned partaligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning mid-level filters for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Pose invariant embedding for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07732</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Query-adaptive late fusion for image search and person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Person re-identification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Reidentification by relative distance comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="653" to="668" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">A discriminatively learned cnn embedding for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05666</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Efficient online local metric adaptation via negative samples for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2420" to="2428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Point to set similarity based deep feature learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
