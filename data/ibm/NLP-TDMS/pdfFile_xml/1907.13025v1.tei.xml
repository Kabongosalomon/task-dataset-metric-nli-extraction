<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SkeleMotion: A New Representation of Skeleton Joint Sequences Based on Motion Information for 3D Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Caetano</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Smart Sense Laboratory</orgName>
								<orgName type="institution">Universidade Federal de Minas Gerais</orgName>
								<address>
									<settlement>Belo Horizonte</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Sena</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Smart Sense Laboratory</orgName>
								<orgName type="institution">Universidade Federal de Minas Gerais</orgName>
								<address>
									<settlement>Belo Horizonte</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franois</forename><surname>Brmond</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">INRIA</orgName>
								<address>
									<settlement>Sophia Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jefersson</forename><forename type="middle">A Dos</forename><surname>Santos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Smart Sense Laboratory</orgName>
								<orgName type="institution">Universidade Federal de Minas Gerais</orgName>
								<address>
									<settlement>Belo Horizonte</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Robson Schwartz</surname></persName>
							<email>william@dcc.ufmg.brfrancois.bremond@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Smart Sense Laboratory</orgName>
								<orgName type="institution">Universidade Federal de Minas Gerais</orgName>
								<address>
									<settlement>Belo Horizonte</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SkeleMotion: A New Representation of Skeleton Joint Sequences Based on Motion Information for 3D Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Due to the availability of large-scale skeleton datasets, 3D human action recognition has recently called the attention of computer vision community. Many works have focused on encoding skeleton data as skeleton image representations based on spatial structure of the skeleton joints, in which the temporal dynamics of the sequence is encoded as variations in columns and the spatial structure of each frame is represented as rows of a matrix. To further improve such representations, we introduce a novel skeleton image representation to be used as input of Convolutional Neural Networks (CNNs), named SkeleMotion. The proposed approach encodes the temporal dynamics by explicitly computing the magnitude and orientation values of the skeleton joints. Different temporal scales are employed to compute motion values to aggregate more temporal dynamics to the representation making it able to capture longrange joint interactions involved in actions as well as filtering noisy motion values. Experimental results demonstrate the effectiveness of the proposed representation on 3D action recognition outperforming the state-of-the-art on NTU RGB+D 120 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human action recognition plays an important role in various applications, for instance surveillance systems can be used to detect and prevent abnormal or suspicious actions, health care systems can be used to monitor elderly people on their daily living activities and robot and human interactions.</p><p>Over the last decade, significant progress on the action recognition task has been achieved with the design of discriminative representations employed to the image and video domains on RGB data or optical flow. Such information is based on appearance or motion analysis. Due to the development of low-cost RGB-D sensors (e.g., Kinect), it becomes possible to employ depth information as well as human skeleton joints to perform 3D action recognition. Compared to RGB and optical flow, skeleton data has the advantages of being computationally efficient since the data size is smaller. Moreover, skeleton data are robust to illumination changes, robust to background noise and invariant to camera views <ref type="bibr" target="#b5">[6]</ref>.</p><p>Many works for 3D action recognition have focused on designing handcrafted feature descriptors <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b2">3]</ref> to encode skeleton data while adopting Dynamic Time Warping (DTW), Fourier Temporal Pyramid (FTP) or Hidden Markov Model (HMM) to model temporal dynamics in the sequences. Nowadays, large efforts have been directed to the employment of deep neural networks. These architectures learn hierarchical layers of representations to perform pattern recognition and have demonstrated impressive results on many pattern recognition tasks (e.g., image classification <ref type="bibr" target="#b11">[12]</ref> and face recognition <ref type="bibr" target="#b23">[24]</ref>). For instance, Recurrent Neural Networks (RNNs) with Long-Short Term Memory (LSTM) have been employed to model skeleton data for 3D action recognition <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36]</ref>. Although RNN approaches present excellent results in 3D action recognition task due to their power of modeling temporal sequences, such structures lack the ability to efficiently learn the spatial relations between the skeleton joints <ref type="bibr" target="#b33">[34]</ref>.</p><p>To take advantage of the spatial relations, a hierarchical structure was proposed by Du et al. <ref type="bibr" target="#b3">[4]</ref>. The authors represent each skeleton sequence as 2D arrays, in which the temporal dynamics of the sequence is encoded as variations in columns and the spatial structure of each frame is represented as rows. Then, the representation is fed to a Convolutional Neural Network (CNN) which has the natural abil-ity of learning structural information from 2D arrays. Such type of representations are very compact encoding the entire video sequence in one single image.</p><p>To further improve the representation of skeleton joints for 3D action recognition, in this paper we introduce a novel skeleton image representation to be used as input of CNNs named SkeleMotion. The proposed approach encodes temporal dynamics by explicitly using motion information computing the magnitude and orientation values of the skeleton joints. To that end, different temporal scales are used to filter noisy motion values as well as aggregating more temporal dynamics to the representation making it being able to capture long-range joint interactions involved in actions. Moreover, the method takes advantage of a structural organization of joints that preserves spatial relations of more relevant joint pairs. To perform action classification, we train a tiny CNN architecture with only three convolutional layers and two fully-connected layers. Since the network is shallow and takes as input a compact representation for each video, it is extremely fast to train.</p><p>In the literature, many works employed or improved the skeleton image representation for 3D action recognition <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b1">2]</ref>. However, none of the methods model explicit motion information (i.e., magnitude and orientation) in multiple temporal scales, as the proposed approach does. Similar to our approach, the works of <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> were the only ones that tried to encode motion on skeleton images, however they employed a naive approach by computing difference of motion joints on consecutive frames.</p><p>According to the experimental results, our proposed skeleton image representation can handle skeleton based 3D action recognition very well. Moreover, SkeleMotion representation achieves the state-of-the-art performance on the large scale NTU RGB+D 120 <ref type="bibr" target="#b14">[15]</ref> dataset when combined with a spatial structural joint representation.</p><p>The code of our SkeleMotion representation is publicly available to facilitate future research 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we present a literature review of works that are close to the idea proposed in our approach by employing different representations based on skeleton images.</p><p>As the forerunner of skeleton image representations, Du et al. <ref type="bibr" target="#b3">[4]</ref> represent the skeleton sequences as a matrix. Each row of such matrix corresponds to a chain of concatenated skeleton joint coordinates from the frame t. Hence, each column of the matrix corresponds to the temporal evolution of the joint j. At this point, the matrix size is J × T × 3, where J is the number of joints for each skeleton, T is the total frame number of the video sequence and 3 is the num-1 https://github.com/carloscaetano/skeleton-images ber coordinate axes (x, y, z). The values of this matrix are quantified into an image (i.e., linearly rescaled to a [0, 255]) and normalized to handle the variable-length problem. In this way, the temporal dynamics of the skeleton sequence is encoded as variations in rows and the spatial structure of each frame is represented as columns. Finally, the authors use their representation as input to a CNN model composed by four convolutional layers and three max-pooling layers. After the feature extraction, a feed-forward neural network with two fully-connected layers is employed for classification.</p><p>Wang et al. <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b29">30]</ref> present a skeleton representation to represent both spatial configuration and dynamics of joint trajectories into three texture images through color encoding, named Joint Trajectory Maps (JTMs). The authors apply rotations to the skeleton data to mimicking multi-views and also for data enlargement to overcome the drawback of CNNs usually being not view invariant. JTMs are generated by projecting the trajectories onto the three orthogonal planes. To encode motion direction in the JTM, they use a hue colormap function to "color" the joint trajectories over the action period. They also encode the motion magnitude of joints into saturation and brightness claiming that changes in motion results in texture in the JMTs. Finally, the authors individually fine-tune three AlexNet <ref type="bibr" target="#b11">[12]</ref> CNNs (one for each JTM) to perform classification.</p><p>To overcome the problem of the sparse data generated by skeleton sequence video, Ke et al. <ref type="bibr" target="#b9">[10]</ref> represent the temporal dynamics of the skeleton sequence by generating four skeleton representation images. Their approach is closer to Du et al. <ref type="bibr" target="#b3">[4]</ref> method, however they compute the relative positions of the joints to four reference joints by arranging them as a chain and concatenating the joints of each body part to the reference joints resulting onto four different skeleton representations. According to the authors, such structure incorporate different spatial relationships between the joints. Finally, the skeleton images are resized and each channel of the four representations is used as input to a VGG19 <ref type="bibr" target="#b25">[26]</ref> pre-trained architecture for feature extraction.</p><p>To encode motion information on skeleton image representation, Li et al. <ref type="bibr" target="#b12">[13]</ref> proposed the skeleton motion image. Their approach is created similar to Du et al. <ref type="bibr" target="#b3">[4]</ref> skeleton image representation, however each matrix cell is composed by joint difference computation between two consecutive frames. To perform classification, the authors used Du et al. <ref type="bibr" target="#b3">[4]</ref> approach and their proposed representation independently as input of a neural network with a two-stream paradigm. The CNN used was a small seven-layer network consisting of three convolution layers and four fullyconnected layers.</p><p>Yang et al. <ref type="bibr" target="#b33">[34]</ref> claim that the concatenation process of chaining all joints with a fixed order turn into lack of semantic meaning and leads to loss in skeleton structural informa-tion. To that end, Yang et al. <ref type="bibr" target="#b33">[34]</ref> proposed a representation named Tree Structure Skeleton Image (TSSI) to preserve spatial relations. Their method is created by traversing a skeleton tree with a depth-first order algorithm with the premise that the fewer edges there are, the more relevant the joint pair is. The generated representation are then quantified into an image and resized before being sent to a ResNet-50 <ref type="bibr" target="#b6">[7]</ref> CNN architecture.</p><p>As it can be inferred from the reviewed methods, most of them are improved versions of Du et al. <ref type="bibr" target="#b3">[4]</ref> skeleton image focusing on spatial structural of joint axes while the temporal dynamics of the sequence is encoded as variations in columns, or encode motion information in a naive manner (difference of motion joints on consecutive frames). Despite the aforementioned methods produce promising results, they do not explicit encode rich motion information. In view of that, to capture more motion information, our approach directly encodes it by using orientation and magnitude to provide information regarding the velocity of the movement in different temporal scales In view of that, our SkeleMotion approach differs from the literature methods by capturing the temporal dynamics explicitly provided by magnitude and orientation motion information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>In this section, we introduce our proposed skeleton image representation based on magnitude and orientation motion information, named SkeleMotion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">SkeleMotion</head><p>As reviewed in Section 2, the majority of works that encode skeleton data as image representations are based on spatial structure encoding of the skeleton joints. According to Li et al. <ref type="bibr" target="#b13">[14]</ref>, temporal movements of joints can also be used as crucial cues for action recognition and although the temporal dynamics of the sequence can be implicitly learned by using a CNN, an explicit modeling can produce better recognition accuracies.</p><p>Recently, a new temporal stream for the two-stream networks called Magnitude-Orientation Stream (MOS) <ref type="bibr" target="#b0">[1]</ref> was developed. The method is based on non-linear transformations and claim that motion information on a video sequence can be described by the spatial relationship contained on the local neighborhood of magnitude and orientation extracted from the optical flow and has shown excellent results on the 2D action recognition problem. Motivated by such results, in this paper we propose a novel skeleton image representation (named SkeleMotion), based on magnitude and orientation of the joints to explore the temporal dynamics. Our approach expresses the displacement information by using orientation encoding (direction of joints) and magnitude to provide information regarding the velocity of the move-ment. Furthermore, due to the successful results achieved by the skeleton image representations, our approach follows the same fundamentals by representing the skeleton sequences as a matrix. First, we apply the depth-first tree traversal order <ref type="bibr" target="#b33">[34]</ref> to the skeleton joints to generate a predefined chain order C that best preserves the spatial relations between joints in original skeleton structures 2 . Afterwards, we compute matrix S that corresponds to a chain of concatenated skeleton joint coordinates from the frame t. In view of that, each column of the matrix corresponds to the temporal evolution of the arranged chain joint c. At this point, the size of matrix S is C × T × 3, where C is the number of joints of the chain, T is the total frame number of the video sequence and 3 is the number joint coordinate axes (x, y, z). Then, we create the motion structure D as</p><formula xml:id="formula_0">D c,t = S c,t+d − S c ,<label>(1)</label></formula><p>where each matrix cell is composed by the temporal difference computation of each joint between two frames of d distance, resulting in a C × T − d × 3 matrix. By using the proposed motion structure D, we build two different representations: one based on the magnitudes of joint motions and another one based the orientations of the joint motion. We compute both representations using</p><formula xml:id="formula_1">M c,t = (D x c,t ) 2 + (D y c,t ) 2 + (D z c,t ) 2<label>(2)</label></formula><p>and</p><formula xml:id="formula_2">θ c,t = stack(θ xy c,t , θ yz c,t , θ zx c,t ) θ xy c,t = tan −1 D y c,t D x c,t , θ yz c,t = tan −1 D z c,t D y c,t , θ zx c,t = tan −1 D x c,t D z c,t ,<label>(3)</label></formula><p>where M is the magnitude skeleton representation of size J ×T −d×1 and θ is the orientation skeleton representation of size J × T − d × 3 (composed by 3 stacked channels).</p><p>Since the orientation values are estimated for every joint, it might generate noisy values for joints without any movement. Therefore, we perform a filtering on θ based on the values of M as</p><formula xml:id="formula_3">θ c,t = 0, if M c,t &lt; m θ c,t , otherwise ,<label>(4)</label></formula><p>where m is a magnitude threshold value. Finally, the generated matrices are normalized into [0, 1] and empirically resized into a fixed size of C × 100, since number of frames may vary depending on the skeleton sequence of each video. <ref type="figure" target="#fig_0">Figure 1</ref> gives an overview of our method for building the SkeleMotion representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Temporal Scale Aggregation (TSA)</head><p>Skeleton image representations in the literature basically encodes joint coordinates as channels. In view of that, it may cause a problem that the co-occurrence features are aggregated locally, being not able to capture long-range joint interactions involved in actions <ref type="bibr" target="#b13">[14]</ref>. Moreover, one drawback of encoding motion values of joints is the noisy values that can be introduced to the representation due to small distance d between two frames. For instance, if the computation is performed considering two consecutive frames, it could add to the representation unnecessary motion of joints that are irrelevant to predict a specific action (e.g., motion of the head joint on a handshake action).</p><p>To overcome the aforementioned problems, we also propose a variation of our SkeleMotion representation by precomputing the motion structure D considering different d distances. For each of the motion structures D, we compute its respective magnitude skeleton representation M and then stack them all into one single representation. The same is applied to compute the orientation skeleton representation θ, however a weighting scheme is applied during the filtering process explained before, as</p><formula xml:id="formula_4">θ c,t = 0, if M c,t &lt; m × d θ c,t , otherwise .<label>(5)</label></formula><p>Such technique adds more temporal dynamics to the representation by explicitly showing temporal scales to the network. In this way, the network can learn which movements are really important for the action learning and also being able to capture long-range joint interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section we present the experimental results obtained with the proposed skeleton image representation for the 3D action recognition problem. We compare it to other skeleton representations in the literature. Besides the classical skeleton image representation of Du et al. <ref type="bibr" target="#b3">[4]</ref>, we compare with other representations used by state-ofthe-art approaches <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34]</ref> as baselines on NTU RGB+D 60 <ref type="bibr" target="#b24">[25]</ref>. We also compare our approach to sate-of-the-art methods on the NTU RGB+D 120 <ref type="bibr" target="#b14">[15]</ref>. To isolate only the contribution brought by SkeleMotion to the action recognition problem, all other representations were tested on the same datasets with the same split of training and testing data and using the same CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>The NTU RGB+D 60 <ref type="bibr" target="#b24">[25]</ref> is publicly available 3D action recognition dataset. It consists of 56,880 videos from 60 action categories which are performed by 40 distinct subjects. The videos were collected by three Microsoft Kinect sensors. The dataset provides four different data information: (i) RGB frames; (ii) depth maps; (iii) 395 infrared sequences; and (iv) skeleton joints. There are two different evaluation protocols: cross-subject, which split the 40 subjects into training and testing; and cross-view, which uses samples from one camera for testing and the other two for training. The performance is evaluated by computing the average recognition across all classes.</p><p>The NTU RGB+D 120 <ref type="bibr" target="#b14">[15]</ref> is a large-scale 3D action recognition dataset captured under various environmental conditions. It consists of 114,480 RGB+D video samples captured using the Microsoft Kinect sensor. As in NTU RGB+D 60 <ref type="bibr" target="#b24">[25]</ref>, the dataset provides RGB frames, depth maps, infrared sequences and skeleton joints. It is com-...  posed by 120 action categories performed by 106 distinct subjects in a wide range of age distribution. There are two different evaluation protocols: cross-subject, which split the 106 subjects into training and testing; and cross-setup, which divides samples with even setup IDs for training (16 setups) and odd setup IDs for testing (16 setups). The performance is evaluated by computing the average recognition across all classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>To isolate only the contribution brought by the proposed representation to the action recognition problem, all compared skeleton image representations were implemented and tested on the same datasets and using the same network architecture. In view of that, we applied the same split of training and testing data, and employ the evaluation protocols and metrics proposed by the creators of the datasets.</p><p>The network architecture employed is a modified version of the CNN proposed by Li et al. <ref type="bibr" target="#b12">[13]</ref>. They designed a small convolutional neural network which consists of three convolution layers and four fully-connected (FC) layers. However, here we modified it to a tiny version, employing the convolutional layers and only two FC layers. All convolutions have a kernel size of 3 × 3, the first and second convolutional layers with a stride of 1 and the third one with a stride of 2. Max pooling and ReLU neuron are adopted and the dropout regularization ratio is set to 0.5. The learning rate is set to 0.001 and batch size is set to 1000. The training is stopped after 200 epochs. The loss function employed was the categorical cross-entropy. We opted for using such architecture since it demonstrated good performance and, according to the authors, it can be easily trained from scratch without any pre-training and is superior on its compact model size and fast inference speed as well. <ref type="figure" target="#fig_2">Figure 2</ref> presents an overview of the employed architecture.</p><p>To cope with actions involving multi-person interaction (e.g., shaking hands), we apply a common choice in the literature which is to stack skeleton image representations of different people as the network input.</p><p>To obtain the orientation skeleton image representation θ we empirically set the parameter m = 0.004, as described in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation</head><p>In this section, we present experiments for parameters optimization and report a comparison of our proposed skeleton representation. We used a subset of NTU RGB+D 60 <ref type="bibr" target="#b24">[25]</ref> training set (considering cross-view protocol) to perform parameter setting and then used such parameter on the remaining experiments. We focused on the optimization of the number of temporal scales used on temporal scale aggregation (TSA). To set the number of temporal scales of our SkeleMotion approach, we empirically varied it from two to four temporal scales considering 20 frames in total. <ref type="table" target="#tab_0">Table 1</ref> shows the results obtained by such variation. We can see that the best result is obtained by using three temporal scales for both magnitude <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15)</ref> and orientation <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20)</ref>. Moreover, we noticed that the performance tends to saturate or drop when considering four temporal scales. SkeleMotion + Yang et al. <ref type="bibr" target="#b33">[34]</ref> Action Number <ref type="figure">Figure 3</ref>. The complementary between SkeleMotion (Magnitude-Orientation TSA) and Yang et al. <ref type="bibr" target="#b33">[34]</ref> TSSI representation on NTU RGB+D 60 <ref type="bibr" target="#b24">[25]</ref> dataset for cross-view protocol. Best viewed in color. <ref type="table" target="#tab_1">Table 2</ref> presents a comparison of our approach with skeleton image representations of the literature. For the methods that have more than one "image" per representation ( <ref type="bibr" target="#b31">[32]</ref> and <ref type="bibr" target="#b9">[10]</ref>), we stacked them to be used as input to the network. The same was performed for our SkeleMotion approach considering magnitude and orientation. Regarding the cross-subject protocol, the best result was obtained by Reference Joints representation from Ke et al., <ref type="bibr" target="#b9">[10]</ref> achieving 70.8% of accuracy while our best result (SkeleMotion Magnitude (TSA)) achieves a competitive accuracy of 69.6%. It is worth noting that there is a considerable improvement of 12.8 (p.p.) obtained by Skele-Motion Magnitude (TSA) when compared to Li et al., <ref type="bibr" target="#b12">[13]</ref> baseline, which also explicitly encode motion information. On the other side, the best result on cross-view protocol was obtained by our SkeleMotion Magnitude (TSA) approach achieving 80.1% of accuracy. There is an improvement of 4.5 percentage points (p.p.) when compared to the Tree Structure Skeleton Image (TSSI) from Yang et al., <ref type="bibr" target="#b33">[34]</ref>, which was the best baseline result. Again, there is a considerable improvement of 18.8 (p.p.) when compared to Li et al., <ref type="bibr" target="#b12">[13]</ref> baseline. To exploit a possible complementarity of the tempo-ral (our SkeleMotion) and spatial (Yang et al. <ref type="bibr" target="#b33">[34]</ref>) skeleton image representations, we combined the different approaches by employing early and late fusion techniques. For the early fusion, we simply stacked the representations to be used as input to the network. On the other hand, the late fusion technique applied was a non-weighted linear combination of the prediction scores of each method. According to the results showed in <ref type="table" target="#tab_2">Table 3</ref>, any type of combination performed with our SkeleMotion provides better results than their solo versions. Regarding cross-subject protocol, our best results achieves 73.5% of accuracy with early fusion technique against 76.5% of the late fusion approach. Furthermore, on cross-view protocol, our best results achieves 82.4% of accuracy with early fusion technique against 84.7% of the late fusion approach. Detailed improvements are shown in <ref type="figure">Figure 3</ref>. Finally, <ref type="table">Table 4</ref> presents the experiments of our proposed skeleton image representation on the recent available NTU RGB+D 120 <ref type="bibr" target="#b14">[15]</ref> dataset. Due to the results obtained on Table 3, here we employed the late fusion scheme for methods combination.</p><p>We obtained good results with our SkeleMotion representation outperforming many skeleton based methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23]</ref>. When combining our representation with Yang et al. <ref type="bibr" target="#b33">[34]</ref> we achieve state-of-theart results, outperforming the best reported method (Body Pose Evolution Map <ref type="bibr" target="#b22">[23]</ref>) by up to 3.1 p.p. on cross-subject protocol and achieve competitive results on cross-setup protocol.</p><p>In comparison with LSTM approaches, we outperform the best reported method (Two-Stream Attention LSTM) by 1.7 p.p. using our skeleton image representation and 6.5 p.p. when combining it with Yang et al. <ref type="bibr" target="#b33">[34]</ref> method on crosssubject protocol. Regarding the cross-setup protocol, we outperform them by 3.6 p.p. using our skeleton image representation fused with Yang et al. <ref type="bibr" target="#b33">[34]</ref>. This indicates that our skeleton image representation approach used as input for CNNs leads to a better learning of temporal dynamics than the approaches that employs LSTM.  <ref type="bibr" target="#b33">[34]</ref> 75.4 83.2 Orientation (TSA) + Yang et al. <ref type="bibr" target="#b33">[34]</ref> 73.6 80.6 Magnitude-Orientation (TSA) + Yang et al. <ref type="bibr" target="#b33">[34]</ref> 76.5 84.7 <ref type="table">Table 4</ref>. Action recognition accuracy (%) results on NTU RGB+D 120 <ref type="bibr" target="#b14">[15]</ref> dataset. Results for literature methods were obtained from <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-subject Cross-setup Approach</head><p>Acc. (%) Acc. (%)</p><p>Part-Aware LSTM <ref type="bibr" target="#b24">[25]</ref> 25.5 26.3 Soft RNN <ref type="bibr" target="#b8">[9]</ref> 36.3 44.9 Dynamic Skeleton <ref type="bibr" target="#b7">[8]</ref> 50.8 54.7 Spatio-Temporal LSTM <ref type="bibr" target="#b17">[18]</ref> 55.7 57.9 Internal Feature Fusion <ref type="bibr" target="#b16">[17]</ref> 58.2 60.9 Literature GCA-LSTM <ref type="bibr" target="#b19">[20]</ref> 58.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="59.2">results</head><p>Multi-Task Learning Network <ref type="bibr" target="#b9">[10]</ref> 58.4 57.9 FSNet <ref type="bibr" target="#b15">[16]</ref> 59.9 62.4 Skeleton Visualization (Single Stream) <ref type="bibr" target="#b21">[22]</ref> 60.3 63.2 Two-Stream Attention LSTM <ref type="bibr" target="#b18">[19]</ref> 61.2 63.3 Multi-Task CNN with RotClips <ref type="bibr" target="#b10">[11]</ref> 62.2 61.8 Body Pose Evolution Map <ref type="bibr" target="#b22">[23]</ref> 64.6 66.9</p><p>Orientation (TSA) 52.2 54.1 SkeleMotion Magnitude (TSA) 57.6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="60.4">results</head><p>Magnitude-Orientation (TSA) 62.9 63.0 Magnitude-Orientation (TSA) + Yang et al. <ref type="bibr" target="#b33">[34]</ref> 67.7 66.9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Works</head><p>In this work, we proposed a novel skeleton image representation to be used as input of CNNs, named SkeleMotion. The method is based on temporal dynamics encoding by explicitly using motion information (magnitude and orientation) of the skeleton joints. We further propose a variation of the magnitude skeleton representation considering different temporal scales in order to filter noisy motion values as well as aggregating more temporal dynamics to the representation. Experimental results on two publicly available datasets demonstrated the excellent performance of the proposed approach. Another interesting finding is that the combination of our representation with methods of the literature improves the 3D action recognition outperforming the state-of-the-art on NTU RGB+D 120 dataset.</p><p>Directions to future works include the evaluation of SkeleMotion with other distinct architectures. Moreover, we intend to evaluate its behavior on 2D action datasets with skeletons estimated by methods of the literature.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>SkeleMotion representation. (a) Skeleton data sequence of T frames. (b) Computation of the magnitude and orientation from the joint movement. (c) θ and M arrays: each row encodes the spatial information (relation between joint movements) while each column describes the temporal information for each joint movement. (d) Skeleton image after resizing and stacking of each axes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Network architecture employed for 3D action recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Action recognition accuracy (%) results on a subset of NTU RGB+D 60<ref type="bibr" target="#b24">[25]</ref> dataset by applying temporal scale aggregation (TSA) on our SkeleMotion representation.</figDesc><table><row><cell></cell><cell>Temporal</cell><cell cols="2">Magnitude Orientation</cell></row><row><cell></cell><cell>distances</cell><cell>Acc. (%)</cell><cell>Acc. (%)</cell></row><row><cell></cell><cell>1, 5</cell><cell>64.9</cell><cell>62.4</cell></row><row><cell>Two Temporal</cell><cell>1, 10</cell><cell>67.4</cell><cell>62.9</cell></row><row><cell>Scales</cell><cell>1, 15</cell><cell>66.0</cell><cell>64.1</cell></row><row><cell></cell><cell>1, 20</cell><cell>66.1</cell><cell>63.5</cell></row><row><cell>Three Temporal Scales</cell><cell>1, 5, 10 1, 10, 20 5, 10, 15</cell><cell>68.6 69.0 70.1</cell><cell>64.6 65.4 65.1</cell></row><row><cell>Four Temporal</cell><cell>1, 5, 10, 15</cell><cell>69.6</cell><cell>65.2</cell></row><row><cell>Scales</cell><cell>5, 10, 15, 20</cell><cell>67.9</cell><cell>64.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Action recognition accuracy (%) results on NTU RGB+D 60<ref type="bibr" target="#b24">[25]</ref> dataset. Results for the baselines were obtained running each method implementation.</figDesc><table><row><cell></cell><cell></cell><cell>Cross-</cell><cell>Cross-</cell></row><row><cell></cell><cell></cell><cell>subject</cell><cell>view</cell></row><row><cell></cell><cell>Approach</cell><cell cols="2">Acc. (%) Acc. (%)</cell></row><row><cell></cell><cell>Du et al. [4]</cell><cell>68.7</cell><cell>73.0</cell></row><row><cell></cell><cell>Wang et al. [32]</cell><cell>39.1</cell><cell>35.9</cell></row><row><cell>Baselines</cell><cell>Ke et al. [10]</cell><cell>70.8</cell><cell>75.5</cell></row><row><cell></cell><cell>Li et al. [14]</cell><cell>56.8</cell><cell>61.3</cell></row><row><cell></cell><cell>Yang et al. [34]</cell><cell>69.5</cell><cell>75.6</cell></row><row><cell></cell><cell>Orientation</cell><cell>60.6</cell><cell>65.6</cell></row><row><cell cols="2">SkeleMotion Magnitude</cell><cell>58.4</cell><cell>64.2</cell></row><row><cell>results</cell><cell>Orientation (TSA)</cell><cell>65.3</cell><cell>73.2</cell></row><row><cell></cell><cell>Magnitude (TSA)</cell><cell>69.6</cell><cell>80.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison between late and early fusion techniques on NTU RGB+D 60<ref type="bibr" target="#b24">[25]</ref> dataset. Fusion Magnitude (TSA) + Yang et al.</figDesc><table><row><cell>Cross-subject Cross-view</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Chain C considering 25 Kinect joints:<ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2]</ref>, as defined in<ref type="bibr" target="#b33">[34]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the National Council for Scientific and Technological Development -CNPq (Grants 311053/2016-5, 204952/2017-4 and 438629/2018-3), the Minas Gerais Research Foundation -FAPEMIG (Grants APQ-00567-14 and PPM-00540-17) and the Coordination for the Improvement of Higher Education Personnel -CAPES (DeepEyes Project).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Activity recognition based on a magnitudeorientation stream network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">H C D</forename><surname>Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIBGRAPI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Potion: Pose motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3-d human action recognition by shape analysis of motion trajectories on riemannian manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histogram of oriented displacements (hod): Describing trajectories of human joints for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Saban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Space-time representation of people based on 3d skeletal data. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Reily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for rgb-d activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Early action prediction by soft regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning clip representations for skeleton-based 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Skeleton-based action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICMEW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ntu rgb+d 120: A large-scale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Skeleton-based online action prediction using scale selection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kot Chichung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition using spatio-temporal lstm network with trust gates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Skeleton-based human action recognition with global context-aware attention lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Abdiyeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Global context-aware attention lstm networks for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d action recognition using data visualization and convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Enhanced skeleton visualization for view invariant human action recognition. Pattern Recogn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An end-toend spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Differential recurrent neural networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Action recognition based on joint trajectory maps with convolutional neural networks. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mining mid-level features for action recognition based on effective skeleton representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DICTA</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Action recognition based on joint trajectory maps using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia (MM)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Eigenjoints-based action recognition using nave-bayes-nearest-neighbor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Action recognition with spatio-temporal visual attention on skeleton image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The moving pose: An efficient 3d kinematics descriptor for low-latency action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
