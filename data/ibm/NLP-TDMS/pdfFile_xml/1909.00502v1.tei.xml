<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Empirical Study of Incorporating Pseudo Data into Grammatical Error Correction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Kiyono</surname></persName>
							<email>shun.kiyono@riken.jp</email>
							<affiliation key="aff0">
								<orgName type="department">RIKEN Center for Advanced Intelligence Project</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tohoku University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
							<email>jun.suzuki@ecei.tohoku.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">RIKEN Center for Advanced Intelligence Project</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tohoku University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Mita</surname></persName>
							<email>masato.mita@riken.jp</email>
							<affiliation key="aff0">
								<orgName type="department">RIKEN Center for Advanced Intelligence Project</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tohoku University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
							<email>tomoya.mizumoto@riken.jp</email>
							<affiliation key="aff0">
								<orgName type="department">RIKEN Center for Advanced Intelligence Project</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tohoku University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
							<email>inui@ecei.tohoku.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">RIKEN Center for Advanced Intelligence Project</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tohoku University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">An Empirical Study of Incorporating Pseudo Data into Grammatical Error Correction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The incorporation of pseudo data in the training of grammatical error correction models has been one of the main factors in improving the performance of such models. However, consensus is lacking on experimental configurations, namely, choosing how the pseudo data should be generated or used. In this study, these choices are investigated through extensive experiments, and state-of-the-art performance is achieved on the CoNLL-2014 test set (F 0.5 = 65.0) and the official test set of the BEA-2019 shared task (F 0.5 = 70.2) without making any modifications to the model architecture.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>To date, many studies have tackled grammatical error correction (GEC) as a machine translation (MT) task, in which ungrammatical sentences are regarded as the source language and grammatical sentences are regarded as the target language. This approach allows cutting-edge neural MT models to be adopted. For example, the encoder-decoder (EncDec) model <ref type="bibr" target="#b28">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>, which was originally proposed for MT, has been applied widely to GEC and has achieved remarkable results in the GEC research field <ref type="bibr" target="#b12">(Ji et al., 2017;</ref><ref type="bibr" target="#b3">Chollampatt and Ng, 2018;</ref>.</p><p>However, a challenge in applying EncDec to GEC is that EncDec requires a large amount of training data <ref type="bibr" target="#b15">(Koehn and Knowles, 2017)</ref>, but the largest set of publicly available parallel data  in GEC has only two million sentence pairs <ref type="bibr" target="#b17">(Mizumoto et al., 2011)</ref>. Consequently, the method of augmenting the data by incorporating pseudo training data has been studied intensively <ref type="bibr" target="#b32">(Xie et al., 2018;</ref><ref type="bibr" target="#b8">Ge et al., 2018;</ref><ref type="bibr" target="#b16">Lichtarge et al., 2019;</ref><ref type="bibr" target="#b35">Zhao et al., 2019)</ref>. * Current affiliation: Future Corporation When incorporating pseudo data, several decisions must be made about the experimental configurations, namely, (i) the method of generating the pseudo data, (ii) the seed corpus for the pseudo data, and (iii) the optimization setting (Section 2). However, consensus on these decisions in the GEC research field is yet to be formulated. For example, <ref type="bibr" target="#b32">Xie et al. (2018)</ref> found that a variant of the backtranslation <ref type="bibr" target="#b25">(Sennrich et al., 2016b)</ref> method (BACKTRANS (NOISY)) outperforms the generation of pseudo data from raw grammatical sentences (DIRECTNOISE). By contrast, the current state of the art model <ref type="bibr" target="#b35">(Zhao et al., 2019)</ref> uses the DIRECTNOISE method.</p><p>In this study, we investigate these decisions regarding pseudo data, our goal being to provide the research community with an improved understanding of the incorporation of pseudo data. Through extensive experiments, we determine suitable settings for GEC. We justify the reliability of the proposed settings by demonstrating their strong performance on benchmark datasets. Specifically, without any task-specific techniques or architecture, our model outperforms not only all previous single-model results but also all ensemble results except for the ensemble result by <ref type="bibr" target="#b11">Grundkiewicz et al. (2019)</ref> 1 . By applying task-specific techniques, we further improve the performance and achieve state-of-the-art performance on the CoNLL-2014 test set and the official test set of the BEA-2019 shared task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Formulation and Notation</head><p>In this section, we formally define the GEC task discussed in this paper. Let D be the GEC training data that comprise pairs of an ungrammatical source sentence X and grammatical target sentence Y , i.e., D = {(X n , Y n )} n . Here, |D| denotes the number of sentence pairs in the dataset D.</p><p>Let Θ represent all trainable parameters of the model. Our objective is to find the optimal parameter set Θ that minimizes the following objective function L(D, Θ) for the given training data D:</p><formula xml:id="formula_0">L(D, Θ) = − 1 |D| (X,Y )∈D log(p(Y |X, Θ)),<label>(1)</label></formula><p>where p(Y |X, Θ) denotes the conditional probability of Y given X.</p><p>In the standard supervised learning setting, the parallel data D comprise only "genuine" parallel data D g (i.e., D = D g ). However, in GEC, incorporating pseudo data D p that are generated from grammatical sentences Y ∈ T , where T represents seed corpus (i.e., a set of grammatical sentences), is common <ref type="bibr" target="#b32">(Xie et al., 2018;</ref><ref type="bibr" target="#b35">Zhao et al., 2019;</ref><ref type="bibr" target="#b11">Grundkiewicz et al., 2019)</ref>.</p><p>Our interest lies in the following three nontrivial aspects of Equation 1. Aspect (i): multiple methods for generating pseudo data D p are available (Section 3). Aspect (ii): options for the seed corpus T are numerous. To the best of our knowledge, how the seed corpus domain affects the model performance is yet to be shown. We compare three corpora, namely, Wikipedia, Simple Wikipedia (SimpleWiki) and English Gigaword, as a first trial. Wikipedia and SimpleWiki have similar domains, but different grammatical complexities. Therefore, we can investigate how grammatical complexity affects model performance by comparing these two corpora. We assume that Gigaword contains the smallest amount of noise among the three corpora. We can therefore use Gigaword to investigate whether clean text improves model performance. Aspect (iii): at least two major settings for incorporating D p into the optimization of Equation 1 are available. One is to use the two datasets jointly by concatenating them as D = D g ∪ D p , which hereinafter we refer to as JOINT. The other is to use D p for pretraining, namely, minimizing L(D p , Θ) to acquire Θ , and then fine-tuning the model by minimizing L(D g , Θ ); hereinafter, we refer to this setting as PRETRAIN. We investigate these aspects through our extensive experiments (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods for Generating Pseudo Data</head><p>In this section, we describe three methods for generating pseudo data. In Section 4, we experimentally compare these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BACKTRANS (NOISY) and BACKTRANS (SAM-PLE)</head><p>Backtranslation for the EncDec model was proposed originally by <ref type="bibr" target="#b25">Sennrich et al. (2016b)</ref>. In backtranslation, a reverse model, which generates an ungrammatical sentence from a given grammatical sentence, is trained. The output of the reverse model is paired with the input and then used as pseudo data.</p><p>BACKTRANS (NOISY) is a variant of backtranslation that was proposed by <ref type="bibr" target="#b32">Xie et al. (2018)</ref> 2 . This method adds rβ random to the score of each hypothesis in the beam for every time step. Here, noise r is sampled uniformly from the interval [0, 1], and β random ∈ R ≥0 is a hyper-parameter that controls the noise scale. If we set β random = 0, then BACK-TRANS (NOISY) is identical to standard backtranslation.</p><p>BACKTRANS (SAMPLE) is another variant of backtranslation, which was proposed by <ref type="bibr" target="#b6">Edunov et al. (2018)</ref> for MT. In BACKTRANS (SAMPLE), sentences are decoded by sampling from the distribution of the reverse model. DIRECTNOISE Whereas BACKTRANS (NOISY) and BACKTRANS (SAMPLE) generate ungrammatical sentences with a reverse model, DIRECT-NOISE injects noise "directly" into grammatical sentences <ref type="bibr" target="#b6">(Edunov et al., 2018;</ref><ref type="bibr" target="#b35">Zhao et al., 2019)</ref>. Specifically, for each token in the given sentence, this method probabilistically chooses one of the following operations: (i) masking with a placeholder token mask , (ii) deletion, (iii) insertion of a random token, and (iv) keeping the original 3 . For each token, the choice is made based on the categorical distribution (µ mask , µ deletion , µ insertion , µ keep ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The goal of our experiments is to investigate aspect (i)-(iii) introduced in Section 2. To ensure that the experimental findings are applicable to GEC in general, we design our experiments by using the following two strategies: (i) we use an off-the-shelf EncDec model without any task-specific architecture or techniques; (ii) we conduct hyper-parameter tuning, evaluation and comparison of each method or setting on the validation set. At the end of experiments (Section 4.5), we summarize our findings and propose suitable settings. We then perform a single-shot evaluation of their performance on the test set.  <ref type="table">Table 1</ref>: Summary of datasets used in our experiments. Dataset marked with "*" is a seed corpus T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Configurations Dataset</head><p>The BEA-2019 workshop official dataset 4 is the origin of the training and validation data of our experiments. Hereinafter, we refer to the training data as BEA-train. We create validation data (BEA-valid) by randomly sampling sentence pairs from the official validation split 5 .</p><p>As a seed corpus T , we use SimpleWiki 6 , Wikipedia 7 or Gigaword 8 . We apply the noizing methods described in Section 3 to each corpus and generate pseudo data D p . The characteristics of each dataset are summarized in <ref type="table">Table 1</ref>. Evaluation We report results on BEA-valid, the official test set of the BEA-2019 shared task (BEA-test), the CoNLL-2014 test set (CoNLL-2014) <ref type="bibr" target="#b21">(Ng et al., 2014)</ref>, and the JFLEG test set (JFLEG) <ref type="bibr" target="#b20">(Napoles et al., 2017)</ref>. All reported results (except ensemble) are the average of five distinct trials using five different random seeds. We report the scores measured by ERRANT <ref type="bibr" target="#b2">(Bryant et al., 2017;</ref><ref type="bibr" target="#b7">Felice et al., 2016)</ref> for BEA-valid, BEA-test, and CoNLL-2014. As the reference sentences of BEAtest are publicly unavailable, we evaluate the model outputs on CodaLab 9 for BEA-test. We also report results measured by the M 2 scorer <ref type="bibr" target="#b4">(Dahlmeier and Ng, 2012)</ref> on CoNLL-2014 to compare them with those of previous studies. We use the GLEU metric <ref type="bibr" target="#b18">(Napoles et al., 2015</ref><ref type="bibr" target="#b19">(Napoles et al., , 2016</ref> for JFLEG. Model We adopt the Transformer EncDec model <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref> using the fairseq toolkit <ref type="bibr" target="#b22">(Ott et al., 2019)</ref> and use the "Transformer (big)" settings of <ref type="bibr" target="#b31">Vaswani et al. (2017)</ref>. Optimization For the JOINT setting, we opti-  mize the model with Adam <ref type="bibr" target="#b14">(Kingma and Ba, 2015)</ref>. For the PRETRAIN setting, we pretrain the model with Adam and then fine-tune it on BEA-train using Adafactor <ref type="bibr" target="#b27">(Shazeer and Stern, 2018)</ref>  <ref type="bibr">10</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Aspect (i): Pseudo Data Generation</head><p>We compare the effectiveness of the BACK-TRANS (NOISY), BACKTRANS (SAMPLE), and DIRECTNOISE methods for generating pseudo data.</p><p>In DIRECTNOISE, we set (µ mask , µ deletion , µ insertion , µ keep ) = (0.5, 0.15, 0.15, 0.2) 11 . We use β random = 6 for BACKTRANS (NOISY) 12 . In addition, we use (i) the JOINT setting and (ii) all of SimpleWiki as the seed corpus T throughout this section.</p><p>The results are summarized in <ref type="table" target="#tab_2">Table 2</ref>. BACK-TRANS (NOISY) and BACKTRANS (SAMPLE) show competitive values of F 0.5 . Given this result, we exclusively use BACKTRANS (NOISY) and discard BACKTRANS (SAMPLE) for the rest of the experiments. The advantage of BACKTRANS (NOISY) is that its effectiveness in GEC has already been demonstrated by <ref type="bibr" target="#b32">Xie et al. (2018)</ref>. In addition, in our preliminary experiment, BACKTRANS (NOISY) decoded ungrammatical sentence 1.2 times faster than BACKTRANS (SAMPLE) did. We also use DI-RECTNOISE because it achieved the best value of F 0.5 among all the methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Aspect (ii): Seed Corpus T</head><p>We investigate the effectiveness of the seed corpus T for generating pseudo data D p . The three corpora (Wikipedia, SimpleWiki and Gigaword) are compared in <ref type="table" target="#tab_4">Table 3</ref>. We set |D p | = 1.4M. The difference in F 0.5 is small, which implies that the seed corpus T has only a minor effect on the model performance. Nevertheless, Gigaword consistently outperforms the other two corpora. In particular,  DIRECTNOISE with Gigaword achieves the best value of F 0.5 among all the configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Aspect (iii): Optimization Setting</head><p>We compare the JOINT and PRETRAIN optimization settings. We are interested in how each setting performs when the scale of the pseudo data D p compared with that of the genuine parallel data D g is (i) approximately the same (|D p | = 1.4M) and (ii) substantially bigger (|D p | = 14M). Here, we use Wikipedia as the seed corpus T instead of SimpleWiki or Gigaword for two reasons. First, SimpleWiki is too small for the experiment (b) (see <ref type="table">Table 1</ref>). Second, the fact that Gigaword is not freely available makes it difficult for other researchers to replicate our results.</p><p>(a) Joint Training or Pretraining   F 0.5 = 45.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison with Current Top Models</head><p>The present experimental results show that the following configurations are effective for improving the model performance: (i) the combination of JOINT and Gigaword (Section 4.3), (ii) the amount of pseudo data D p not being too large in JOINT (Section 4.4(a)), and (iii) PRETRAIN with BACK-TRANS (NOISY) using large pseudo data D p (Section 4.4(b)). We summarize these findings and attempt to combine PRETRAIN and JOINT. Specifically, we pretrain the model using 70M pseudo data of BACKTRANS (NOISY). We then fine-tune the model by combining BEA-train and relatively small DIRECTNOISE pseudo data generated from Gigaword (we set |D p | = 250K). However, the performance does not improve on BEA-valid. Therefore, the best approach available is simply to pretrain the model with large (70M) BACKTRANS (NOISY) pseudo data and then fine-tune using BEAtrain, which hereinafter we refer to as PRETLARGE. We use Gigaword for the seed corpus T because it has the best performance in <ref type="table" target="#tab_4">Table 3</ref>. We evaluate the performance of PRETLARGE on test sets and compare the scores with the current top models. <ref type="table" target="#tab_8">Table 5</ref> shows a remarkable result, that is,   <ref type="bibr" target="#b11">Grundkiewicz et al. (2019)</ref>.</p><p>To further improve the performance, we incorporate the following techniques that are widely used in shared tasks such as BEA-2019 and WMT 13 : Synthetic Spelling Error (SSE) <ref type="bibr" target="#b16">Lichtarge et al. (2019)</ref> proposed the method of probabilistically injecting character-level noise into the source sentence of pseudo data D p . Specifically, one of the following operations is applied randomly at a rate of 0.003 per character: deletion, insertion, replacement, or transposition of adjacent characters. Right-to-left Re-ranking (R2L) Following <ref type="bibr" target="#b24">Sennrich et al. (2016a</ref><ref type="bibr" target="#b23">Sennrich et al. ( , 2017</ref>; <ref type="bibr" target="#b11">Grundkiewicz et al. (2019)</ref>, we train four right-to-left models. The ensemble of four left-to-right models generate n-best candidates and their corresponding scores (i.e., conditional probabilities). We then pass each candidate to the ensemble of the four right-to-left models and compute the score. Finally, we re-rank the n-best candidates based on the sum of the two scores. Sentence-level Error Detection (SED) SED classifies whether a given sentence contains a grammatical error. <ref type="bibr" target="#b0">Asano et al. (2019)</ref> proposed incorporating SED into the evaluation pipeline and reported improved precision. Here, the GEC model is applied only if SED detects a grammatical error in the given source sentence. The motivation is that SED could potentially reduce the number of false-positive errors of the GEC model. We use the re-implementation of the BERT-based SED model <ref type="bibr" target="#b0">(Asano et al., 2019)</ref>. <ref type="table" target="#tab_8">Table 5</ref> presents the results of applying SSE, 13 http://www.statmt.org/wmt19/ R2L, and SED. It is noteworthy that PRET-LARGE+SSE+R2L achieves state-of-the-art performance on both CoNLL-2014 (F 0.5 = 65.0) and BEA-test (F 0.5 = 69.8), which are better than those of the best system of the BEA-2019 shared task <ref type="bibr" target="#b11">(Grundkiewicz et al., 2019)</ref>. In addition, PRET-LARGE+SSE+R2L+SED can further improve the performance on BEA-test (F 0.5 = 70.2). However, unfortunately, incorporating SED decreased the performance on CoNLL-2014 and JFLEG. This fact implies that SED is sensitive to the domain of the test set since the SED model is fine-tuned with the official validation split of BEA dataset. We leave this sensitivity issue as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this study, we investigated several aspects of incorporating pseudo data for GEC. Through extensive experiments, we found the following to be effective: (i) utilizing Gigaword as the seed corpus, and (ii) pretraining the model with BACKTRANS (NOISY) data. Based on these findings, we proposed suitable settings for GEC. We demonstrated the effectiveness of our proposal by achieving stateof-the-art performance on the CoNLL-2014 test set and the BEA-2019 test set.</p><formula xml:id="formula_1">Dp = Dp ∪ {(X, Y )} B BEA-2019 Workshop Official Dataset</formula><p>The BEA-2019 Workshop official dataset consists of following corpora: the First Certificate in English corpus <ref type="bibr" target="#b34">(Yannakoudakis et al., 2011)</ref>, Lang-8 Corpus of Learner English (Lang-8) <ref type="bibr" target="#b17">(Mizumoto et al., 2011;</ref><ref type="bibr" target="#b30">Tajiri et al., 2012)</ref>, the National University of Singapore Corpus of Learner English (NUCLE) <ref type="bibr" target="#b5">(Dahlmeier et al., 2013)</ref>, and W&amp;I+LOCNESS <ref type="bibr" target="#b33">(Yannakoudakis et al., 2018;</ref><ref type="bibr" target="#b9">Granger, 1998)</ref>. The data is publicly available at https://www.cl.cam.ac.uk/research/nl/bea2019st/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Data Preparation Process</head><p>The training data (BEA-train) is tokenized using spaCy tokenizer 14 .</p><p>We used en core web sm-2.1.0 model 15 . We remove sentence pairs that have identical source and target sentences from the training set, following <ref type="bibr" target="#b3">(Chollampatt and Ng, 2018)</ref>. Then we acquire subwords from target sentence through byte-pair-encoding (BPE) <ref type="bibr" target="#b26">(Sennrich et al., 2016c)</ref> algorithm. We used subword-nmt implementation 16 . We apply BPE splitting to both source and target text. The number of merge operation is set to 8,000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Hyper-parameter Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Configurations Values</head><p>Model Architecture Transformer <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref> ("big" setting) Optimizer</p><p>Adam <ref type="bibr" target="#b14">(Kingma and Ba, 2015)</ref> (β1 = 0.9, β2 = 0.98, = 1 × 10 −8 ) Learning Rate Schedule Same as described in Section 5.3 of <ref type="bibr" target="#b31">Vaswani et al. (2017)</ref>    In this paper, we exclusively focused on the effectiveness of µ mask , and therefore we deliberately fixed µ keep = 0.2, and used µ insertion = µ deletion = (1 − µ keep − µ mask )/2 We investigated the effectiveness of changing mask probability µ mask of BACKTRANS (NOISY) by evaluating the model performance on BEA-valid. We used entire SimpleWiki as the seed corpus T . The result is summarized in <ref type="figure" target="#fig_1">Figure 2</ref>. Here, increasing µ mask within the range of 0.1 &lt; µ mask &lt; 0.5 slightly improved the performance. Thus, used µ mask = 0.5 in the experiment (Section 4). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Noise Strength of BACKTRANS (NOISY)</head><p>We investigated the effectiveness of varying β random hyper-parameter of BACKTRANS (NOISY) by evaluating its performance on BEA-valid ( <ref type="figure" target="#fig_2">Figure 3)</ref>. We used entire SimpleWiki as the seed corpus T . The figure shows that the performance of backtranslation without noise (β random = 0) is worse than the baseline. We believe that when there is no noise, reverse-model becomes too conservative to generate grammatical error, as discussed by <ref type="bibr" target="#b32">Xie et al. (2018)</ref>. Thus, the generated pseudo data cannot provide useful teaching signal for the model.</p><p>In terms of the scale of the noise, β random = 6 is the best value for BACKTRANS (NOISY). Thus, we used this value in the experiment (Section 4).  <ref type="figure">Figure 4</ref> shows examples of noisy sentences that are generated by BACKTRANS (NOISY) and DIRECT-NOISE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Examples of Noisy Sentences</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original:</head><p>He died there , but the death date is not clear . BACKTRANS (NOISY): He died at there , but death date is not clear . DIRECTNOISE: mask mask mask , 2 but mask mask mask is not mask mask</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original:</head><p>On seeing her his joy knew no bounds . BACKTRANS (NOISY): On seeing her joyful knew no bounds .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DIRECTNOISE:</head><p>mask mask her crahis mask mask mask bke mask .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original:</head><p>Gre@@ en@@ space Information for G@@ rea@@ ter London . BACKTRANS (NOISY): The information for Gre@@ en@@ space information about G@@ rea@@ ter London .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DIRECTNOISE:</head><p>mask mask mask for mask mask mask mask</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original:</head><p>The cli@@ p is mixed with images of Toronto streets during power failure . BACKTRANS (NOISY): The cli@@ p is mix with images of Toronto streets during power failure .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DIRECTNOISE:</head><p>The mask is mixed mask images si@@ of The mask streets large mask power R@@ failure place mask</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original:</head><p>At the in@@ stitute , she introduced tis@@ sue culture methods that she had learned in the U.@@ S. BACKTRANS (NOISY): At in@@ stitute , She introduced tis@@ sue culture method that she learned in U.@@ S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DIRECTNOISE:</head><p>mask the the mask mask mask mask tis@@ culture R@@ methods , she P mask the s U.@@ mask <ref type="figure">Figure 4</ref>: Examples of sentences generated by BACKTRANS (NOISY) and DIRECTNOISE methods. <ref type="figure">Figure 5</ref> shows examples generated by DIRECTNOISE, when changing the mask probability (µ mask ). H Performance of the Model without Fine-tuning PRETRAIN setting undergoes two optimization steps, namely, pretraining with pseudo data D p and fine-tuning with genuine parallel data D g . We report the performance of models with pretraining only ( <ref type="figure" target="#fig_4">Figure 6</ref>). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Performance on BEA-valid for different amounts of pseudo data (|D p |). The seed corpus T is Wikipedia.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Performance of the model on BEA-valid as parameter of DIRECTNOISE (µ mask ) is varied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Performance of the model on BEA-valid as parameter of BACKTRANS (NOISY) (β random ) is varied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>He ale threw , ch his ne@@ wife dar@@ mask 0.3 mask mask mask mask ch at ament his Research . 0.5 He o threw the sand@@ ch mask his mask . 0.7 mask mask sand@@ mask mask mask mask wife mask Figure 5: Examples generated when varying µ mask . N/A denotes original text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Performance on BEA-valid when varying the amount of pseudo data (|D p |)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance of models on BEA-valid: a value in bold indicates the best result within the column. The seed corpus T is SimpleWiki.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>MethodSeed Corpus T Prec. Rec. F0.5</figDesc><table><row><cell>Baseline</cell><cell>N/A</cell><cell>46.6 23.1 38.8</cell></row><row><cell>BACKTRANS (NOISY)</cell><cell>Wikipedia</cell><cell>43.8 30.8 40.4</cell></row><row><cell>BACKTRANS (NOISY)</cell><cell>SimpleWiki</cell><cell>42.5 31.3 39.7</cell></row><row><cell>BACKTRANS (NOISY)</cell><cell>Gigaword</cell><cell>43.1 33.1 40.6</cell></row><row><cell>DIRECTNOISE</cell><cell>Wikipedia</cell><cell>48.3 25.5 41.0</cell></row><row><cell>DIRECTNOISE</cell><cell>SimpleWiki</cell><cell>48.9 25.7 41.4</cell></row><row><cell>DIRECTNOISE</cell><cell>Gigaword</cell><cell>48.3 26.9 41.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance on BEA-valid when changing the seed corpus T used for generating pseudo data (|D p | = 1.4M).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>An intuitive explanation for this case is that when pseudo data D p are substantially more than genuine data D g , the teaching signal from D p becomes dominant in JOINT. PRETRAIN alleviates this problem because the model is trained with only D g during fine-tuning. We therefore suppose that PRETRAIN is crucial for utilizing extensive pseudo data.</figDesc><table><row><cell>(b) Amount of Pseudo Data We investigate how</cell></row><row><cell>increasing the amount of pseudo data affects the</cell></row><row><cell>PRETRAIN setting. We pretrain the model with</cell></row><row><cell>different amounts of pseudo data {1.4M, 7M, 14M,</cell></row><row><cell>30M, 70M}. The results in Figure 1 show that</cell></row><row><cell>BACKTRANS (NOISY) has superior sample effi-</cell></row><row><cell>ciency to DIRECTNOISE. The best model (pre-</cell></row><row><cell>trained with 70M BACKTRANS (NOISY)) achieves</cell></row></table><note>presents the results. The most notable result here is that PRETRAIN demonstrates the properties of more pseudo data and better performance, whereas JOINT does not. For example, in BACKTRANS (NOISY), increasing |D p | (1.4M → 14M) improves F 0.5 on PRETRAIN (41.1 → 44.5). By contrast, F 0.5 does not improve on JOINT (40.4 → 40.3).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Performance of the model with different optimization settings on BEA-valid. The seed corpus T is Wikipedia.</figDesc><table><row><cell></cell><cell>46</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>F 0.5 score</cell><cell>42 44</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Backtrans (noisy)</cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DirectNoise</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Baseline</cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell><cell>70</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Amount of Pseudo Data |D p | (M)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Comparison of our best model and current top models: a bold value indicates the best result within the column.</figDesc><table><row><cell>our PRETLARGE achieves F 0.5 = 61.3 on CoNLL-</cell></row><row><cell>2014. This result outperforms not only all previous</cell></row><row><cell>single-model results but also all ensemble results</cell></row><row><cell>except for that by</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Hyper-parameter for JOINT optimization</figDesc><table><row><cell>Configurations</cell><cell>Values</cell></row><row><cell></cell><cell>Pretraining</cell></row><row><cell>Model Architecture</cell><cell>Transformer (Vaswani et al., 2017) ("big" setting)</cell></row><row><cell>Optimizer</cell><cell>Adam (Kingma and Ba, 2015) (β1 = 0.9, β2 = 0.98, = 1 × 10 −8 )</cell></row><row><cell cols="2">Learning Rate Schedule Same as described in Section 5.3 of Vaswani et al. (2017)</cell></row><row><cell>Number of Epochs</cell><cell>10</cell></row><row><cell>Dropout</cell><cell>0.3</cell></row><row><cell>Gradient Clipping</cell><cell>1.0</cell></row><row><cell>Loss Function</cell><cell>Label smoothed cross entropy (smoothing value: ls = 0.1) (Szegedy et al., 2016)</cell></row><row><cell></cell><cell>Fine-tuning</cell></row><row><cell>Model Architecture</cell><cell>Transformer (Vaswani et al., 2017) ("big" setting)</cell></row><row><cell>Optimizer</cell><cell>Adafactor (Shazeer and Stern, 2018)</cell></row><row><cell cols="2">Learning Rate Schedule Constant learning rate of 3 × 10 −5</cell></row><row><cell>Number of Epochs</cell><cell>30</cell></row><row><cell>Dropout</cell><cell>0.3</cell></row><row><cell>Stopping Criterion</cell><cell>Use the model with the best validation perplexity on BEA-valid</cell></row><row><cell>Gradient Clipping</cell><cell>1.0</cell></row><row><cell>Loss Function</cell><cell>Label smoothed cross entropy (smoothing value: ls = 0.1) (Szegedy et al., 2016)</cell></row><row><cell>Beam Search</cell><cell>Beam size 5 with length-normalization</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Hyper-parameter for PRETRAIN optimization</figDesc><table><row><cell>E Mask Probability of DIRECTNOISE</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The paper<ref type="bibr" target="#b11">(Grundkiewicz et al. 2019)</ref> has not been published yet at the time of submission.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">referred as "random noising" in<ref type="bibr" target="#b32">Xie et al. (2018)</ref> 3 The detailed algorithm is described in Appendix A.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Details of the dataset is in Appendix B. 5 The detailed data preparation process is in Appendix C. 6 https://simple.wikipedia.org 7 We used 2019-02-25 dump file at https://dumps. wikimedia.org/other/cirrussearch/.8  We used the English Gigaword Fifth Edition (LDC Catalog No.: LDC2011T07). 9 https://competitions.codalab.org/ competitions/20228</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">The detailed hyper-parameters are listed in Appendix D. 11 These values are derived from preliminary experiments (Appendix E). 12 β random = 6 achieved the best F0.5 in our preliminary experiments (Appendix F).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">https://spacy.io/ 15 https://github.com/explosion/spacy-models/releases/tag/en_core_web_sm-2.1.0 16 https://github.com/rsennrich/subword-nmt</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the three anonymous reviewers for their insightful comments. We are deeply grateful to Takumi Ito and Tatsuki Kuribayashi for kindly sharing the re-implementation of BACKTRANS (NOISY). The work of Jun Suzuki was supported in part by JSPS KAKENHI Grant Number JP19104418 and AIRPF Grant Number 30AI036-8.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DIRECTNOISE Algorithm</head><p>The DIRECTNOISE algorithm is described in Algorithm 1 Here, X consists of sequence of I tokens, namely, X = (x 1 , . . . , x I ) where x i denotes i-th token of X. Similarly, Y consists of sequence of J tokens, namely, Y = (y 1 , . . . , y J ) where y j denotes j-th token of Y . </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The AIP-Tohoku System at the BEA-2019 Shared Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2019)</title>
		<meeting>the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2019)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="176" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR 2015</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="793" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Multilayer Convolutional Encoder-Decoder Neural Network for Grammatical Error Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shamil</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI 2018)</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI 2018)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5755" to="5762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Better Evaluation for Grammatical Error Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2012)</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2012)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="568" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Building a Large Annotated Corpus of Learner English: The NUS Corpus of Learner English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siew Mei</forename><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Workshop on Building Educational Applications Using NLP (BEA 2013)</title>
		<meeting>the 8th Workshop on Building Educational Applications Using NLP (BEA 2013)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding Back-Translation at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic Extraction of Learner Errors in ESL Sentences Using Linguistically Enhanced Alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="825" to="835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fluency Boost Learning and Inference for Neural Grammatical Error Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018)</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1055" to="1065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The computer learner corpus: A versatile new source of data for SLA research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylviane</forename><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learner English on Computer</title>
		<editor>Sylviane Granger</editor>
		<imprint>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="1998" />
			<publisher>Addison Wesley Longman</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Near Human-Level Performance in Grammatical Error Correction with Hybrid Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2018)</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2018)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="284" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural grammatical error correction systems with unsupervised pre-training on synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2019)</title>
		<meeting>the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2019)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Nested Attention Neural Hybrid Model for Grammatical Error Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="753" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Approaching Neural Grammatical Error Correction as a Low-Resource Machine Translation Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubha</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2018)</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2018)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="595" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Six Challenges for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Neural Machine Translation</title>
		<meeting>the First Workshop on Neural Machine Translation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Corpora Generation for Grammatical Error Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Lichtarge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mining Revision Log of Language Learning SNS for Automated Japanese Error Correction of Second Language Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Joint Conference on Natural Language Processing</title>
		<meeting>the 5th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
	<note>IJCNLP 2011</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ground Truth for Grammatical Error Correction Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL &amp; IJCNLP 2015)</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL &amp; IJCNLP 2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="588" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02592</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">GLEU Without Tuning. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="229" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The CoNLL-2014 Shared Task on Grammatical Error Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">Hendy</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">fairseq: A Fast, Extensible Toolkit for Sequence Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The university of Edinburgh&apos;s neural MT systems for WMT17</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Currey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="389" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Edinburgh neural machine translation systems for WMT 16</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="371" to="376" />
		</imprint>
	</monogr>
	<note>Shared Task Papers (WMT 2016)</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving Neural Machine Translation Models with Monolingual Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adafactor: Adaptive Learning Rates with Sublinear Memory Cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (ICML 2018)</title>
		<meeting>the 35th International Conference on Machine Learning (ICML 2018)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4603" to="4611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28 (NIPS 2014)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2016)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Tense and Aspect Error Correction for ESL Learners Using Global Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshikazu</forename><surname>Tajiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012)</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="198" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Noising and Denoising Natural Language: Diverse Backtranslation for Grammar Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Genthial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2018)</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2018)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="619" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Developing an Automated Writing Placement system for ESL Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Øistein</forename><forename type="middle">E</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ardeshir</forename><surname>Geranpayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Nicholls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Measurement in Education</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="267" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A New Dataset and Method for Automatically Grading ESOL Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Medlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving Grammatical Error Correction via Pre-Training a Copy-Augmented Architecture with Unlabeled Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
