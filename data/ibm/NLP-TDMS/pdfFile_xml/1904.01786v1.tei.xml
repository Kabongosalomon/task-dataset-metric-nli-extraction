<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Soft Rasterizer: A Differentiable Renderer for Image-based 3D Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichen</forename><surname>Liu</surname></persName>
							<email>lshichen@ict.usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">USC Institute for Creative Technologies</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<settlement>California 3 Pinscreen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianye</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">USC Institute for Creative Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Chen</surname></persName>
							<email>wechen@ict.usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">USC Institute for Creative Technologies</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<settlement>California 3 Pinscreen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">USC Institute for Creative Technologies</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<settlement>California 3 Pinscreen</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Soft Rasterizer: A Differentiable Renderer for Image-based 3D Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Rendering bridges the gap between 2D vision and 3D scenes by simulating the physical process of image formation. By inverting such renderer, one can think of a learning approach to infer 3D information from 2D images. However, standard graphics renderers involve a fundamental discretization step called rasterization, which prevents the rendering process to be differentiable, hence able to be learned. Unlike the state-of-the-art differentiable renderers <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b18">19]</ref>, which only approximate the rendering gradient in the back propagation, we propose a truly differentiable rendering framework that is able to (1) directly render colorized mesh using differentiable functions and (2) back-propagate efficient supervision signals to mesh vertices and their attributes from various forms of image representations, including silhouette, shading and color images.</p><p>The key to our framework is a novel formulation that views rendering as an aggregation function that fuses the probabilistic contributions of all mesh triangles with respect to the rendered pixels. Such formulation enables our framework to flow gradients to the occluded and far-range vertices, which cannot be achieved by the previous state-of-thearts. We show that by using the proposed renderer, one can achieve significant improvement in 3D unsupervised singleview reconstruction both qualitatively and quantitatively. Experiments also demonstrate that our approach is able to handle the challenging tasks in image-based shape fitting, which remain nontrivial to existing differentiable renderers. Code is available at https://github.com/ ShichenLiu/SoftRas.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Understanding and reconstructing 3D scenes and structures from 2D images has been one of the fundamental goals in computer vision. The key to image-based 3D reasoning is to find sufficient supervisions flowing from the pixels to the 3D properties. To obtain image-to-3D correlations, prior approaches mainly rely on the matching losses based on 2D  <ref type="figure">Figure 1</ref>: We propose Soft Rasterizer R (upper), a truly differentiable renderer, which formulates rendering as a differentiable aggregating process A(·) that fuses per-triangle contributions {D i } in a "soft" probabilistic manner. Our approach attacks the core problem of differentiating the standard rasterizer, which cannot flow gradients from pixels to geometry due to the discrete sampling operation <ref type="bibr">(below)</ref>.</p><p>key points/contours <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32]</ref> or shape/appearance priors <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b47">48]</ref>. However, the above approaches are either limited to task-specific domains or can only provide weak supervision due to the sparsity of the 2D features. In contrast, as the process of producing 2D images from 3D assets, rendering relates each pixel with the 3D parameters by simulating the physical mechanism of image formulation. Hence, by inverting a renderer, one can obtain dense pixellevel supervision for general-purpose 3D reasoning tasks, which cannot be achieved by conventional approaches. However, the rendering process is not differentiable in conventional graphics pipelines. In particular, standard mesh renderer involves a discrete sampling operation, called rasterization, which prevents the gradient to be flowed into the mesh vertices. Since the forward rendering function is highly non-linear and complex, to achieve differentiable rendering, recent advances <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b18">19]</ref> only approximate the backward gradient with hand-crafted functions while directly employing a standard graphics renderer in the forward pass. While promising results have been shown in the task of image-based 3D reconstruction, the inconsistency between the forward and backward propagations may lead to uncontrolled optimization behaviors and limited generalization capability to other 3D reasoning tasks. We show in Section 5.2 that such mechanism would cause problematic situations in image-based shape fitting where the 3D parameters cannot be efficiently optimized.</p><p>In this paper, instead of studying a better form of rendering gradient, we attack the key problem of differentiating the forward rendering function. Specifically, we propose a truly differentiable rendering framework that is able to render a colorized mesh in the forward pass ( <ref type="figure">Figure 1</ref>). In addition, our framework can consider a variety of 3D properties, including mesh geometry, vertex attributes (color, normal etc.), camera parameters and illuminations and is able to flow efficient gradients from pixels to mesh vertices and their attributes. While being a universal module, our renderer can be plugged into either a neural network or a nonlearning optimization framework without parameter tuning.</p><p>The key to our approach is the novel formulation that views rendering as a "soft" probabilistic process. Unlike the standard rasterizer, which only selects the color of the closest triangle in the viewing direction <ref type="figure">(Figure 1</ref> below), we propose that all triangles have probabilistic contributions to each rendered pixel, which can be modeled as probability maps on the screen space. While conventional rendering pipelines merge shaded fragments in a one-hot manner, we propose a differentiable aggregation function that fuses the per-triangle color maps based on the probability maps and the triangles' relative depths to obtain the final rendering result <ref type="figure">(Figure 1</ref> upper). The novel aggregating mechanism enables our renderer to flow gradients to all mesh triangles, including the occluded ones. In addition, our framework can propagate supervision signals from pixels to far-range triangles because of its probabilistic formulation. We call our framework Soft Rasterizer (SoftRas) as it "softens" the discrete rasterization to enable differentiability.</p><p>Thanks to the consistent forward and backward propagations, SoftRas is able to provide high-quality gradient flows that supervise a variety of tasks on image-based 3D reasoning. To evaluate the performance of SoftRas, we show applications in 3D unsupervised single-view mesh reconstruction and image-based shape fitting ( <ref type="figure" target="#fig_0">Figure 2</ref>, Section 5.1 and 5.2). In particular, as SoftRas provides strong error signals to the mesh generator simply based on the rendering loss, one can achieve mesh reconstruction from a single image without any 3D supervision. To faithfully texture the mesh, we further propose a novel approach that extracts representative colors from input image and formulates the color regression as a classification problem. Regarding the task of image-based shape fitting, we show that our approach is able to (1) handle occlusions using the aggregating mechanism that considers the probabilistic contributions of all triangles; and (2) provide much smoother energy landscape, compared to other differentiable renderers, that avoids local minima by using the smooth rendering (Figure 2 left). Experimental results demonstrate that our approach significantly outperforms the state-of-the-arts both quantitatively and qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Differentiable Rendering. To relate the changes in the observed image with that in the 3D shape manipulation, a number of existing techniques have utilized the derivatives of rendering <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30]</ref>. Recently, Loper and Black <ref type="bibr" target="#b28">[29]</ref> introduce an approximate differentiable renderer which generates derivatives from projected pixels to the 3D parameters. Kato et al. <ref type="bibr" target="#b18">[19]</ref> propose to approximate the backward gradient of rasterization with a hand-crafted function to achieve differentiable rendering. More recently, Li et al. <ref type="bibr" target="#b23">[24]</ref> introduce a differentiable ray tracer to realize the differentiability of secondary rendering effects. Recent advances in 3D face reconstruction <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b8">9]</ref>, material inference <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b6">7]</ref> and other 3D reconstruction tasks <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">34]</ref> have leveraged some other forms of differentiable rendering layers to obtain gradient flows in the neural networks. However, these rendering layers are usually designed for special purpose and thus cannot be generalized to other applications. In this paper, we  focus on a general-purpose differentiable rendering framework that is able to directly render a given mesh using differentiable functions instead of only approximating the backward derivatives.</p><p>Image-based 3D Reasoning. 2D images are widely used as the media for reasoning 3D properties. In particular, image-based reconstruction has received the most attentions. Conventional approaches mainly leverage the stereo correspondence based on the multi-view geometry <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b7">8]</ref> but is restricted to the coverage provided by the multiple views. With the availability of large-scale 3D shape dataset <ref type="bibr" target="#b4">[5]</ref>, learning-based approaches <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15]</ref> are able to consider single or few images thanks to the shape prior learned from the data. To simplify the learning problem, recent works reconstruct 3D shape via predicting intermediate 2.5D representations, such as depth map <ref type="bibr" target="#b24">[25]</ref>, image collections <ref type="bibr" target="#b17">[18]</ref>, displacement map <ref type="bibr" target="#b15">[16]</ref> or normal map <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b43">44]</ref>. Pose estimation is another key task to understanding the visual environment. For 3D rigid pose estimation, while early approaches attempt to cast it as classification problem <ref type="bibr" target="#b41">[42]</ref>, recent approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b45">46]</ref> can directly regress the 6D pose by using deep neural networks. Estimating the pose of non-rigid objects, e.g. human face or body, is more challenging. By detecting the 2D key points, great progress has been made to estimate the 2D poses <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>To obtain 3D pose, shape priors <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28]</ref> have been incorporated to minimize the shape fitting errors in recent approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b1">2]</ref>. Our proposed differentiable renderer can provide dense rendering supervision to 3D properties, benefitting a variety of image-based 3D reasoning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Soft Rasterizer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Differentiable Rendering Pipeline</head><p>As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, we consider both extrinsic variables (camera P and lighting conditions L) that define the environmental settings, and intrinsic properties (triangle meshes M and per-vertex appearance A, including color, material etc.) that describe the model-specific properties. Following the standard rendering pipeline, one can obtain the mesh normal N, image-space coordinate U and viewdependent depths Z by transforming input geometry M based on camera P. With specific assumptions of illumination and material models (e.g. Phong model), we can compute color C given {A, N, L}. These two modules are naturally differentiable. However, the subsequent operations including the rasterization and z-buffering in the standard graphics pipeline <ref type="figure" target="#fig_1">(Figure 3</ref> red blocks) are not differentiable with respect to U and Z due to the discrete sampling operations.</p><p>Our differentiable formulation. We take a different perspective that the rasterization can be viewed as binary masking that is determined by the relative positions between the pixels and triangles, while z-buffering merges the rasterization results F in a pixel-wise one-hot manner based on the relative depths of triangles. The problem is then formulated as modeling the discrete binary masks and the one-hot merging operation in a soft and differentiable manner. To achieve this, we propose two major components, namely probability maps {D j } that model the probability of each pixel staying inside a specific triangle f j and aggregate function A(·) that fuses per-triangle color maps based on {D j } and the relative depths among triangles.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Probability Map Computation</head><p>We model the influence of triangle f j on image plane by probability map D j . To estimate the probability of D j at pixel p i , the function is required to take into account both the relative position and the distance between p i and D j . To this end, we define D j at pixel p i as follows:</p><formula xml:id="formula_0">D i j = sigmoid(δ i j · d 2 (i, j) σ ),<label>(1)</label></formula><p>where σ is a positive scalar that controls the sharpness of the probability distribution while δ i j is a sign indicator</p><formula xml:id="formula_1">δ i j = {+1, if p i ∈ f j ; −1, otherwise}.</formula><p>We set σ as 1×10 −4 unless otherwise specified. d(i, j) is the closest distance from p i to f j 's edges. A natural choice for d(i, j) is the Euclidean distance. However, other metrics, such as barycentric or l 1 distance, can be used in our approach.</p><p>Intuitively, by using the sigmoid function, Equation 1 normalizes the output to (0, 1), which is a faithful continuous approximation of binary mask with boundary landed on 0.5. In addition, the sign indicator maps pixels inside and outside f j to the range of (0.5, 1) and (0, 0.5) respectively. <ref type="figure" target="#fig_3">Figure 4</ref> shows D j of a triangle with varying σ using Euclidean distance. Smaller σ leads to sharper probability distribution while larger σ tends to blur the outcome. This design allows controllable influence for triangles on image plane. As σ → 0, the resulting probability map converges to the exact shape of the triangle, enabling our probability map computation to be a generalized form of traditional rasterization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Aggregate Function</head><p>For each mesh triangle f j , we define its color map C j at pixel p i on the image plane by interpolating vertex color using barycentric coordinates. We clip and normalize the barycentric coordinates to [0, 1] for p i outside of f j . We then propose to use an aggregate function A(·) to merge color maps {C j } to obtain rendering output I based on {D j } and the relative depths {z j }. Inspired by the softmax operator, we define an aggregate function A S as follows:</p><formula xml:id="formula_2">I i = A S ({C j }) = j w i j C i j + w i b C b ,<label>(2)</label></formula><p>where C b is the background color; the weights {w j } satisfy j w i j + w i b = 1 and are defined as:</p><formula xml:id="formula_3">w i j = D i j exp(z i j /γ) k D i k exp(z i k /γ) + exp( /γ) ,<label>(3)</label></formula><p>where z i j denotes the normalized inverse depth of the 3D point on f i whose 2D projection is p i ; is small constant that enables the background color while γ (set as 1 × 10 −4 unless otherwise specified) controls the sharpness of the aggregate function. Note that w j is a function of two major  <ref type="figure">Figure 5</ref>: Comparisons with prior differentiable renderers in terms of gradient flow.</p><p>variables: D j and z j . Specifically, w j assigns higher weight to closer triangles that have larger z j . As γ → 0, the color aggregation function only outputs the color of nearest triangle, which exactly matches the behavior of z-buffering. In addition, w j is robust to z-axis translations. D j modulates the w j along the x, y directions such that the triangles closer to p i on screen space will receive higher weight. Equation 2 also works for shading images when the intrinsic vertex colors are set to constant ones. We further explore the aggregate function for silhouettes. Note that the silhouette of object is independent from its color and depth map. Hence, we propose a dedicated aggregation function A O for the silhouette based on the binary occupancy:</p><formula xml:id="formula_4">I i s = A O ({D j }) = 1 − j (1 − D i j ).<label>(4)</label></formula><p>Intuitively, Equation 4 models silhouette as the probability of having at least one triangle cover the pixel p i . Note that there might exist other forms of aggregate functions. One alternative option may be using a universal aggregate function A N that is implemented as a neural network. We provide an ablation study on this regard in Section 5.1.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Comparisons with Prior Works</head><p>In this section, we compare our approach with the state-of-the-art rasterization-based differential renderers: OpenDR <ref type="bibr" target="#b28">[29]</ref> and NMR <ref type="bibr" target="#b18">[19]</ref>, in terms of gradient flows as shown in <ref type="figure">Figure 5</ref>. We provide detailed analysis on gradient computation in Appendix A.</p><p>Gradient from pixels to triangles. Since both OpenDR and NMR utilize standard graphics renderer in the forward pass, they have no control over the intermediate rendering process and thus cannot flow gradient into the triangles that are occluded in the final rendered image ( <ref type="figure">Figure 5</ref> our approach has full control on the internal variables and is able to flow gradients to invisible triangles and the z coordinates of all triangles through the aggregation function ( <ref type="figure">Figure 5</ref>(a) right).</p><p>Screen-space gradient from pixels to vertices. Thanks to our continuous probabilistic formulation, in our approach, the gradient from pixel p j in screen space can flow gradient to all distant vertices ( <ref type="figure">Figure 5</ref>(b) right). However, for OpenDR, a vertex can only receive gradients from neighboring pixels within a close distance due to the local filtering operation ( <ref type="figure">Figure 5</ref>(b) left). Regarding NMR, there is no gradient defined from the pixels inside the white regions with respect to the triangle vertices (( <ref type="figure">Figure 5(b)</ref> middle). In contrast, our approach does not have such issue thanks to our orientation-invariant formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Image-based 3D Reasoning</head><p>With direct gradient flow from image to 3D properties, our differentiable rendering framework enables a variety of tasks on 3D reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Single-view Mesh Reconstruction</head><p>To demonstrate the effectiveness of soft rasterizer, we fix the extrinsic variables and evaluate its performance on single-view 3D reconstruction by incorporating it with a mesh generator. The direct gradient from image pixels to shape and color generators enables us to achieve 3D unsupervised mesh reconstruction. Our framework is demonstrated in <ref type="figure" target="#fig_4">Figure 6</ref>. Given an input image, our shape and color generators generate a triangle mesh M and its corresponding colors C, which are then fed into the soft rasterizer. The SoftRas layer renders both the silhouette I s and color image I c and provide rendering-based error signal by comparing with the ground truths. Inspired by the latest advances in mesh learning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b42">43]</ref>, we leverage a similar idea of synthesizing 3D model by deforming a template mesh. To validate the performance of soft rasterizer, the shape generator employ an encoder-decoder architecture identical to that of <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b46">47]</ref>. The details of the shape and generators are described in Appendix C.  Losses. The reconstruction networks are supervised by three losses: silhouette loss L s , color loss L c and geometry loss L g . LetÎ s and I s denote the predicted and the ground-truth silhouette respectively. The silhouette loss is defined as L s = 1− ||Îs⊗Is||1 ||Îs⊕Is−Îs⊗Is||1 , where ⊗ and ⊕ are the element-wise product and sum operators respectively. The color loss is measured as the l 1 norm between the rendered and input image: L c = ||Î c − I c || 1 . To achieve appealing visual quality, we further impose a geometry loss L g that regularizes the Laplacian of both shape and color predictions. The final loss is a weighted sum of the three losses:</p><formula xml:id="formula_5">L = L s + λL c + µL g .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Color Reconstruction</head><p>Instead of directly regressing the color value, our color generator formulates color reconstruction as a classification problem that learns to reuse the pixel colors in the input image for each sampling point. Let N c denote the number of sampling points on M and H, W be the height and width of the input image respectively. However, the computational cost of a naive color selection approach is prohibitive, i.e. O(HW N c ). To address this challenge, we propose a novel approach to colorize mesh using a color palette, as shown in <ref type="figure" target="#fig_5">Figure 7</ref>. Specifically, after passing input image to a neural network, the extracted features are fed into (1) a sampling network that samples the representative colors for building the palette; and (2) a selection network that combines colors from the palette for texturing the sampling points. The color prediction is obtained by multiplying the color selections with the learned color palette. Our approach reduces the computation complexity to O(N d (HW + N c )), where N p is the size of color palette. With a proper setting of N p , one can significantly reduce the computational cost while achieving sharp and accurate color recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image-based Shape Fitting</head><p>Image-based shape fitting has a fundamental impact in various tasks, such as pose estimation, shape alignment, model-based reconstruction, etc. Yet without direct correlation between image and 3D parameters, conventional approaches have to rely on coarse correspondences, e.g. 2D joints <ref type="bibr" target="#b2">[3]</ref> or feature points <ref type="bibr" target="#b34">[35]</ref>, to obtain supervision signals for optimization. In contrast, SoftRas can directly Input SoftRas (3D unsupervised) NMR (3D unsupervised) Pixel2Mesh (supervised) Ground truth <ref type="figure">Figure 8</ref>: 3D mesh reconstruction from a single image. From left to right, we show input image, ground truth, the results of our method (SoftRas), Neural Mesh Renderer <ref type="bibr" target="#b18">[19]</ref> and Pixel2mesh [43] -all visualized from 2 different views. Along with the results, we also visualize mesh-to-scan distances measured from reconstructed mesh to ground truth. back-propagate pixel-level errors to 3D properties, enabling dense image-to-3D correspondence for high-quality shape fitting. However, a differentiable renderer has to resolve two challenges in order to be readily applicable. (1) occlusion awareness: the occluded portion of 3D model should be able to receive gradients in order to handle large pose changes. (2) far-range impact: the loss at a pixel should have influence on distant mesh vertices, which is critical to dealing with local minima during optimization. While prior differentiable renderers <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29]</ref> fail to satisfy these two criteria, our approach handles these challenges simultaneously. (1) Our aggregate function fuses the probability maps from all triangles, enabling the gradients to be flowed to all vertices including the occluded ones. (2) Our soft approximation based on probability distribution allows the gradient to be propagated to the far end while the size of receptive field can be well controlled <ref type="figure" target="#fig_3">(Figure 4</ref>). To this end, our approach can faithfully solve the image-based shape fitting problem by minimizing the following energy objective:</p><formula xml:id="formula_6">argmin ρ,θ,t ||R(M (ρ, θ, t)) − I t || 2 ,<label>(6)</label></formula><p>where R(·) is the rendering function that generates a rendered image I from mesh M , which is parametrized by its pose θ, translation t and non-rigid deformation parameters ρ. The difference between I and the target image I t provides strong supervision to solve the unknowns {ρ, θ, t}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we perform extensive evaluations on our framework. We also include more visual evaluations in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Single-view Mesh Reconstruction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Experimental Setup</head><p>Datasets and Evaluation Metrics. We use the dataset provided by <ref type="bibr" target="#b18">[19]</ref>, which contains 13 categories of objects from ShapeNet <ref type="bibr" target="#b4">[5]</ref>. Each object is rendered in 24 different views with image resolution of 64 × 64. For fair comparison, we employ the same train/validate/test split on the same dataset as in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b46">47]</ref>. For quantitative evaluation, we adopt the standard reconstruction metric, 3D intersection over union (IoU), to compare with baseline methods. Implementation Details. We use the same structure as <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b46">47]</ref> for mesh generation. Our network is optimized using Adam <ref type="bibr" target="#b20">[21]</ref> with α = 1 × 10 −4 , β 1 = 0.9 and β 2 = 0.999. Specifically, we set λ = 1 and µ = 1 × 10 −3 across all experiments unless otherwise specified. We train the network with multi-view images of batch size 64 and implement it using PyTorch.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Qualitative Results</head><p>Single-view Mesh Reconstruction. We compare the qualitative results of our approach with that of the stateof-the-art supervised <ref type="bibr" target="#b42">[43]</ref> and 3D unsupervised <ref type="bibr" target="#b18">[19]</ref> mesh reconstruction approaches in <ref type="figure">Figure 8</ref>. Though NMR <ref type="bibr" target="#b18">[19]</ref> is able to recover the rough shape, the mesh surface is discontinuous and suffers from a considerable amount of self intersections. In contrast, our method can faithfully reconstruct fine details of the object, such as the tail of the airplane and the barrel of the rifle, while ensuring smoothness of the surface. Though trained without 3D supervision, our approach achieves results on par with the supervised method Pixel2Mesh <ref type="bibr" target="#b42">[43]</ref>. In some cases, our approach can generate even more appealing details than that of <ref type="bibr" target="#b42">[43]</ref>, e.g. the bench legs, the airplane engine and the side of the car. Mesh-toscan distance visualization also shows our results achieve much higher accuracy than <ref type="bibr" target="#b18">[19]</ref> and comparable accuracy with that of <ref type="bibr" target="#b42">[43]</ref>.</p><p>Color Reconstruction. Our method is able to faithfully recover the mesh color based on the input image. <ref type="figure" target="#fig_6">Figure 9</ref> presents the colorized reconstruction from a single image and the learned color palettes. Though the resolution of the input image is rather low (64 × 64), our approach is still able to achieve sharp color recovery and accurately restore the fine details, e.g. the subtle color transition on the body of airplane and the shadow on the phone screen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Quantitative Evaluations</head><p>We show the comparisons on 3D IoU score with the stateof-the-art approaches in <ref type="table" target="#tab_5">Table 1</ref>. We test our approach under two settings: one trained with silhouette loss only (sil.) and the other with both silhouette and shading supervisions (full). Our approach has significantly outperformed all the other unsupervised methods on all categories. In addition, the mean score of our best setting has surpassed the stateof-the-art NMR <ref type="bibr" target="#b18">[19]</ref> by more than 4.5 points. As we use the identical mesh generator and same training settings with <ref type="bibr" target="#b18">[19]</ref>, it indicates that it is the proposed SoftRas renderer that leads to the superior performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SoftRas settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Ablation Study</head><p>In this section, we conduct controlled experiments to validate the importance of different components.</p><p>Loss Terms and Alternative Functions. In gradient is not compatible with the regularizer. It is optional to have color supervision on the mesh generation. However, we show that adding a color loss can significantly improve the performance (64.6 v.s. 62.4) as more information is leveraged for reducing the ambiguity of using silhouette loss only. In addition, we also show that Euclidean metric usually outperforms the barycentric distance while the aggregate function based on neural network A N performs slightly better than the non-parametric counterpart A O at the cost of more computations. Rigid Pose Fitting. We compare our approach with NMR in the task of rigid pose fitting. In particular, given a colorized cube and a target image, the pose of the cube needs to be optimized so that its rendered result matches the target image. Despite the simple geometry, the discontinuity of face colors, the non-linearity of rotation and the large occlusions make it particularly difficult to optimize. As shown in <ref type="figure">Figure 10</ref>, NMR is stuck in a local minimum while our approach succeeds to obtain the correct pose. The key is that our method produces smooth and partially transparent <ref type="bibr" target="#b0">1</ref> The expectation of uniform-sampled SO3 rotation angle is π/2 + 2/π  <ref type="figure">Figure 11</ref>: Results for optimizing human pose given single image target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Image-based Shape Fitting</head><p>renderings which "soften" the loss landscape. Such smoothness can be controlled by σ and γ, which allows us to avoid the local minimum. Further, we evaluate the rotation estimation accuracy on synthetic data given 100 randomly sampled initializations and targets. We compare methods w/ and w/o scheduling schemes, and summarize mean relative angle error in <ref type="table">Table 3</ref>. Without optimization scheduling, our method outperforms the baseline (random estimation) and NMR by 43.68°and 10.60°respectively, demonstrating the effectiveness of the gradient flows provided by our method. Scheduling is a commonly used technique for solving nonlinear optimization problems. For NMR, we solve with multi-resolution images in 5 levels; while for our method, we set schedule to decay σ and γ in 5 steps. While scheduling improves both methods, our approach still achieves better accuracy than NMR by 17.37°, indicating our consistent superiority regardless of using the scheduling strategy.</p><p>Non-rigid Shape Fitting. In <ref type="figure">Figure 11</ref>, we show that Sof-tRas can provide stronger supervision for non-rigid shape fitting even in the presence of part occlusions. We optimize the human body parametrized by SMPL model <ref type="bibr" target="#b27">[28]</ref>. As the right hand (textured as red) is completely occluded in the initial view, it is extremely challenging to fit the body pose to the target image. To obtain correct parameters, the optimization should be able to (1) consider the impact of the occluded part on the rendered image and (2) back-propagate the error signals to the occluded vertices. NMR <ref type="bibr" target="#b18">[19]</ref> fails to move the hand to the right position due to its incapability to handle occlusions. In comparison, our approach can faithfully complete the task as our novel probabilistic formulation and aggregating mechanism can take all triangles into account while being able to optimize the z coordinates (depth) of the mesh vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we have presented a truly differentiable rendering framework (SoftRas) that is able to directly render a given mesh in a fully differentiable manner. Soft-Ras can consider both extrinsic and intrinsic variables in a unified rendering framework and generate efficient gradients flowing from pixels to mesh vertices and their attributes (color, normal, etc.). We achieve this goal by reformulating the conventional discrete operations including rasterization and z-buffering as differentiable probabilistic processes. Such novel formulation enables our renderer to provide more efficient supervision signals, flow gradients to unseen vertices and optimize the z coordinates of mesh triangles, leading to the significant improvements in the tasks of single-view mesh reconstruction and image-based shape fitting. As a general framework, it would be an interesting future avenue to investigate other possibilities of distance and aggregate functions that might lead to even superior performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More blurry</head><p>More transparent <ref type="figure">Figure A1</ref>: Different rendering effects achieved by our proposed SoftRas renderer. We show how a colorized cube can be rendered in various ways by tuning the parameters of SoftRas. In particular, by increasing γ, SoftRas can render the object with more tranparency while more blurry renderings can be achieved via increasing σ. As γ → 0 and σ → 0, one can achieve rendering effect closer to standard rendering.</p><p>C b and w i b denote the color and weight of background respectively where</p><formula xml:id="formula_7">w i b = exp ( /γ) k D i k exp z i k /γ + exp ( /γ) ;<label>(A10)</label></formula><p>z i j is the clipped normalized depth. Note that we normalize the depth so that the closer triangle receives a larger z i j by</p><formula xml:id="formula_8">z i j = Z f ar − Z i j Z f ar − Z near ,<label>(A11)</label></formula><p>where Z i j denotes the actual clipped depth of f j at p i , while Z near and Z f ar denote the far and near cut-off distances of the viewing frustum.</p><p>Specifically, the aggregate function A S (·) satisfies the following three properties: (1) as γ → 0 and σ → 0, w i converges to an one-hot vector where only the closest triangle contains the projection of p i is one, which shows the consistency between A S (·) and z-buffering; (2) w i b is close to one only when there is no triangle that covers p i ; (3) {w i j } is robust to z-axis translation. In addition, γ is a positive scalar that could balance out the scale change on z-axis.</p><p>The gradient ∂I ∂D i j and ∂I ∂z i j can be obtained as follows:</p><formula xml:id="formula_9">∂I i ∂D i j = k ∂I i ∂w i k ∂w i k ∂D i j + ∂I i ∂w i b ∂w i b ∂D i j = k =j −C i k w i j w i k D i j + C i j ( w i j D i j − w i j w i j D i j ) − C i b w i j w i b D i j = w i j D i j (C i j − I i ) (A12) ∂I i ∂z i j = k ∂I i ∂w i k ∂w i k ∂z i j + ∂I i ∂w i b ∂w i b ∂z i j = k =j −C i k w i j w i k γ + C i j ( w i j γ − w i j w i j γ ) − C i b w i j w i b γ = w i j γ (C i j − I i ) (A13)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3.2 Occupancy Aggregate Function</head><p>Independent from color and illumination, the silhouette of the object can be simply described by an occupancy aggregate function A O (·) as follows:</p><formula xml:id="formula_10">I i sil = A O ({D i j }) = 1 − j (1 − D i j ).<label>(A14)</label></formula><p>Hence, the partial gradient lows:</p><formula xml:id="formula_11">∂I i sil ∂D i j = 1 − I i sil 1 − D i j . (A15)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Forward Rendering Results</head><p>As demonstrated in <ref type="figure">Figure A1</ref>, our framework is able to directly render a given mesh, which cannot be achieved by any existing rasterization-based differentiable renderers <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29]</ref>. In addition, compared to standard graphics renderer, SoftRas can achieve different rendering effects in a continuous manner thanks to its probabilistic formulation. Specifically, by increasing σ, the key parameter that controls the sharpness of the screen-space probability distribution, we are able to generate more blurry rendering re-sults. Furthermore, with increased γ, one can assign more weights to the triangles on the far end, naturally achieving more transparency in the rendered image. As discussed in Section 5.2 of the main paper, the blurring and transparent effects are the key for reshaping the energy landscape in order to avoid local minima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Structure</head><p>We provide detailed structures for all neural networks that were mentioned in the main paper. <ref type="figure">Figure C1</ref> shows the structure of A N (Section 3.3 and 5.1.4), an alternative color aggregate function that is implemented as a neural network. In particular, input SoftRas features are first passed to four consecutive convolutional layers and then fed into a sigmoid layer to model non-linearity. We train A N with the output of a standard rendering pipeline as ground truth to achieve a parametric differentiable renderer.</p><p>We employ an encoder-decoder architecture for our single-view mesh reconstruction. The encoder is used as a feature extractor, whose network structure is shown in <ref type="figure" target="#fig_0">Figure C2</ref>. The detailed network structure of the color and shape generators are illustrated in <ref type="figure" target="#fig_1">Figure C3</ref>(a) and (b) respectively. Both networks ( <ref type="figure" target="#fig_4">Figure 6)</ref> share the same feature extractor. The shape generators consists of three fully connected layers and outputs a per-vertex displacement vector that deforms a template mesh into a target model. The color generator contains two fully connected streams: one for sampling the input image to build the color palette and the other one for selecting colors from the color palette to texture the sampling points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Results on Image-based 3D Reasoning</head><p>We show more results on single-view mesh reconstruction and image-base shape fitting.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D1. Single-view Mesh Reconstruction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D1.1 Intermediate Mesh Deformation</head><p>In <ref type="figure">Figure D1</ref>, we visualize the intermediate process of how an input mesh is deformed to a target shape after the supervision provided by SoftRas. As shown in the first row, the mesh generator gradually deforms a sphere template to a desired car shape which matches the input image. We then change the target image to an airplane ( <ref type="figure">Figure D1</ref> second row). The network further deforms the generated car model to faithfully reconstruct the airplane. In both examples, the mesh deformation can quickly converge to a high-fidelity reconstruction within 200 iterations, demonstrating the effectiveness of our SoftRas renderer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D1.2 Single-view Reconstruction from Real Images</head><p>We further evaluate our approach on real images. As demonstrated in <ref type="figure" target="#fig_0">Figure D2</ref>, though only trained on synthetic data, our model generalizes well to real images and novel views with faithful reconstructions and fine-scale details, e.g. the tail fins of the fighter aircraft and thin structures in the rifle and table legs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D1.3 More Reconstruction Results from ShapeNet</head><p>We provide more reconstruction results in <ref type="figure">Figure A1</ref>. For each input image, we show its reconstructed geometry (middle) as well as the colored reconstruction (right). <ref type="figure" target="#fig_1">Figure D3</ref>: Intermediate process of fitting a color cube (second row) to a target pose shown in the input image (first row). The smoothened rendering (third row) that is used to escape local minimum, as well as the colorized fitting errors (fourth row), are also demonstrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D2. Fitting Process for Rigid Pose Estimation</head><p>We demonstrate the intermediate process of how the proposed SoftRas renderer managed to fit the color cube to the target image in <ref type="figure" target="#fig_1">Figure D3</ref>. Since the cube is largely occluded, directly leveraging a standard rendering is likely to lead to local minima ( <ref type="figure">Figure 10</ref>) that causes non-trivial challenges for any gradient-based optimizer. By rendering the cube with stronger blurring at the earlier stage, our approach is able to avoid local minima, and gradually reduce the rendering loss until an accurate pose can be fitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D3. Visualization of Non-rigid Body Fitting</head><p>In <ref type="figure" target="#fig_3">Figure D4</ref>, we compare the intermediate processes of NMR <ref type="bibr" target="#b18">[19]</ref> and SoftRas during the task of fitting the SMPL model to the target pose. As the right hand of subject is completely occluded in the initial image, NMR fails to complete the task due to its incapability of flowing gradient to the occluded vertices. In contrast, our approach is able to obtain the correct pose within 320 iterations thanks to the occlusion-aware technique. <ref type="figure" target="#fig_3">Figure D4</ref>: Comparisons of body shape fitting using NMR <ref type="bibr" target="#b18">[19]</ref> and our approach. Intermediate fitting processes of both methods are visualized.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " E b d N M m z A a U 4 k 5 H Q t i o 0 y z S 1 a v 5 c = " &gt; A A A B 7 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o N 4 C X j x G c J N A s o T Z y W w y Z h 7 L z K w Q l v y D F w 8 q X v 0 g b / 6 N k 2 Q P m l j Q U F R 1 0 9 0 V p 5 w Z 6 / v f X m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j M o 0 o S F R X O l O j A 3 l T N L Q M s t p J 9 U U i 5 j T d j y + n f n t J 6 o N U / L B T l I a C T y U L G E E W y e 1 e k M s B O 5 X a 3 7 d n w O t k q A g N S j Q 7 F e / e g N F M k G l J R w b 0 w 3 8 1 E Y 5 1 p Y R T q e V X m Z o i s k Y D 2 n X U Y k F N V E + v 3 a K z p w y Q I n S r q R F c / X 3 R I 6 F M R M R u 0 6 B 7 c g s e z P x P 6 + b 2 e Q 6 y p l M M 0 s l W S x K M o 6 s Q r P X 0 Y B p S i y f O I K J Z u 5 W R E Z Y Y 2 J d Q B U X Q r D 8 8 i o J L + o 3 9 e D + s t a 4 L N I o w w m c w j k E c A U N u I M m h E D g E Z 7 h F d 4 8 5 b 1 4 7 9 7 H o r X k F T P H 8 A f e 5 w / v I I 7 b &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E b d N M m z A a U 4 k 5 H Q t i o 0 y z S 1 a v 5 c = " &gt; A A A B 7 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o N 4 C X j x G c J N A s o T Z y W w y Z h 7 L z K w Q l v y D F w 8 q X v 0 g b / 6 N k 2 Q P m l j Q U F R 1 0 9 0 V p 5 w Z 6 / v f X m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j M o 0 o S F R X O l O j A 3 l T N L Q M s t p J 9 U U i 5 j T d j y + n f n t J 6 o N U / L B T l I a C T y U L G E E W y e 1 e k M s B O 5 X a 3 7 d n w O t k q A g N S j Q 7 F e / e g N F M k G l J R w b 0 w 3 8 1 E Y 5 1 p Y R T q e V X m Z o i s k Y D 2 n X U Y k F N V E + v 3 a K z p w y Q I n S r q R F c / X 3 R I 6 F M R M R u 0 6 B 7 c g s e z P x P 6 + b 2 e Q 6 y p l M M 0 s l W S x K M o 6 s Q r P X 0 Y B p S i y f O I K J Z u 5 W R E Z Y Y 2 J d Q B U X Q r D 8 8 i o J L + o 3 9 e D + s t a 4 L N I o w w m c w j k E c A U N u I M m h E D g E Z 7 h F d 4 8 5 b 1 4 7 9 7 H o r X k F T P H 8 A f e 5 w / v I I 7 b &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E b d N M m z A a U 4 k 5 H Q t i o 0 y z S 1 a v 5 c = " &gt; A A A B 7 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o N 4 C X j x G c J N A s o T Z y W w y Z h 7 L z K w Q l v y D F w 8 q X v 0 g b / 6 N k 2 Q P m l j Q U F R 1 0 9 0 V p 5 w Z 6 / v f X m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j M o 0 o S F R X O l O j A 3 l T N L Q M s t p J 9 U U i 5 j T d j y + n f n t J 6 o N U / L B T l I a C T y U L G E E W y e 1 e k M s B O 5 X a 3 7 d n w O t k q A g N S j Q 7 F e / e g N F M k G l J R w b 0 w 3 8 1 E Y 5 1 p Y R T q e V X m Z o i s k Y D 2 n X U Y k F N V E + v 3 a K z p w y Q I n S r q R F c / X 3 R I 6 F M R M R u 0 6 B 7 c g s e z P x P 6 + b 2 e Q 6 y p l M M 0 s l W S x K M o 6 s Q r P X 0 Y B p S i y f O I K J Z u 5 W R E Z Y Y 2 J d Q B U X Q r D 8 8 i o J L + o 3 9 e D + s t a 4 L N I o w w m c w j k E c A U N u I M m h E D g E Z 7 h F d 4 8 5 b 1 4 7 9 7 H o r X k F T P H 8 A f e 5 w / v I I 7 b &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E b d N M m z A a U 4 k 5 H Q t i o 0 y z S 1 a v 5 c = " &gt; A A A B 7 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o N 4 C X j x G c J N A s o T Z y W w y Z h 7 L z K w Q l v y D F w 8 q X v 0 g b / 6 N k 2 Q P m l j Q U F R 1 0 9 0 V p 5 w Z 6 / v f X m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j M o 0 o S F R X O l O j A 3 l T N L Q M s t p J 9 U U i 5 j T d j y + n f n t J 6 o N U / L B T l I a C T y U L G E E W y e 1 e k M s B O 5 X a 3 7 d n w O t k q A g N S j Q 7 F e / e g N F M k G l J R w b 0 w 3 8 1 E Y 5 1 p Y R T q e V X m Z o i s k Y D 2 n X U Y k F N V E + v 3 a K z p w y Q I n S r q R F c / X 3 R I 6 F M R M R u 0 6 B 7 c g s e z P x P 6 + b 2 e Q 6 y p l M M 0 s l W S x K M o 6 s Q r P X 0 Y B p S i y f O I K J Z u 5 W R E Z Y Y 2 J d Q B U X Q r D 8 8 i o J L + o 3 9 e D + s t a 4 L N I o w w m c w j k E c A U N u I M m h E D g E Z 7 h F d 4 8 5 b 1 4 7 9 7 H o r X k F T P H 8 A f e 5 w / v I I 7 b &lt; / l a t e x i t &gt; Rendered w/ larger and &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I S i r q r x B H h t u f H E l Q o f 2 / O W b s + w = " &gt; A A A B 7 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o N 4 C X j x G c J N A s o T Z y W w y Z h 7 L z K w Q l v y D F w 8 q X v 0 g b / 6 N k 2 Q P m l j Q U F R 1 0 9 0 V p 5 w Z 6 / v f X m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j M o 0 o S F R X O l O j A 3 l T N L Q M s t p J 9 U U i 5 j T d j y + n f n t J 6 o N U / L B T l I a C T y U L G E E W y e 1 e o Y N B e 5 X a 3 7 d n w O t k q A g N S j Q 7 F e / e g N F M k G l J R w b 0 w 3 8 1 E Y 5 1 p Y R T q e V X m Z o i s k Y D 2 n X U Y k F N V E + v 3 a K z p w y Q I n S r q R F c / X 3 R I 6 F M R M R u 0 6 B 7 c g s e z P x P 6 + b 2 e Q 6 y p l M M 0 s l W S x K M o 6 s Q r P X 0 Y B p S i y f O I K J Z u 5 W R E Z Y Y 2 J d Q B U X Q r D 8 8 i o J L + o 3 9 e D + s t a 4 L N I o w w m c w j k E c A U N u I M m h E D g E Z 7 h F d 4 8 5 b 1 4 7 9 7 H o r X k F T P H 8 A f e 5 w 8 E l Y 7 p &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I S i r q r x B H h t u f H E l Q o f 2 / O W b s + w = " &gt; A A A B 7 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o N 4 C X j x G c J N A s o T Z y W w y Z h 7 L z K w Q l v y D F w 8 q X v 0 g b / 6 N k 2 Q P m l j Q U F R 1 0 9 0 V p 5 w Z 6 / v f X m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j M o 0 o S F R X O l O j A 3 l T N L Q M s t p J 9 U U i 5 j T d j y + n f n t J 6 o N U / L B T l I a C T y U L G E E W y e 1 e o Y N B e 5 X a 3 7 d n w O t k q A g N S j Q 7 F e / e g N F M k G l J R w b 0 w 3 8 1 E Y 5 1 p Y R T q e V X m Z o i s k Y D 2 n X U Y k F N V E + v 3 a K z p w y Q I n S r q R F c / X 3 R I 6 F M R M R u 0 6 B 7 c g s e z P x P 6 + b 2 e Q 6 y p l M M 0 s l W S x K M o 6 s Q r P X 0 Y B p S i y f O I K J Z u 5 W R E Z Y Y 2 J d Q B U X Q r D 8 8 i o J L + o 3 9 e D + s t a 4 L N I o w w m c w j k E c A U N u I M m h E D g E Z 7 h F d 4 8 5 b 1 4 7 9 7 H o r X k F T P H 8 A f e 5 w 8 E l Y 7 p &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I S i r q r x B H h t u f H E l Q o f 2 / O W b s + w = " &gt; A A A B 7 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o N 4 C X j x G c J N A s o T Z y W w y Z h 7 L z K w Q l v y D F w 8 q X v 0 g b / 6 N k 2 Q P m l j Q U F R 1 0 9 0 V p 5 w Z 6 / v f X m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j M o 0 o S F R X O l O j A 3 l T N L Q M s t p J 9 U U i 5 j T d j y + n f n t J 6 o N U / L B T l I a C T y U L G E E W y e 1 e o Y N B e 5 X a 3 7 d n w O t k q A g N S j Q 7 F e / e g N F M k G l J R w b 0 w 3 8 1 E Y 5 1 p Y R T q e V X m Z o i s k Y D 2 n X U Y k F N V E + v 3 a K z p w y Q I n S r q R F c / X 3 R I 6 F M R M R u 0 6 B 7 c g s e z P x P 6 + b 2 e Q 6 y p l M M 0 s l W S x K M o 6 s Q r P X 0 Y B p S i y f O I K J Z u 5 W R E Z Y Y 2 J d Q B U X Q r D 8 8 i o J L + o 3 9 e D + s t a 4 L N I o w w m c w j k E c A U N u I M m h E D g E Z 7 h F d 4 8 5 b 1 4 7 9 7 H o r X k F T P H 8 A f e 5 w 8 E l Y 7 p &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I S i r q r x B H h t u f H E l Q o f 2 / O W b s + w = " &gt; A A A B 7 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o N 4 C X j x G c J N A s o T Z y W w y Z h 7 L z K w Q l v y D F w 8 q X v 0 g b / 6 N k 2 Q P m l j Q U F R 1 0 9 0 V p 5 w Z 6 / v f X m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j M o 0 o S F R X O l O j A 3 l T N L Q M s t p J 9 U U i 5 j T d j y + n f n t J 6 o N U / L B T l I a C T y U L G E E W y e 1 e o Y N B e 5 X a 3 7 d n w O t k q A g N S j Q 7 F e / e g N F M k G l J R w b 0 w 3 8 1 E Y 5 1 p Y R T q e V X m Z o i s k Y D 2 n X U Y k F N V E + v 3 a K z p w y Q I n S r q R F c / X 3 R I 6 F M R M R u 0 6 B 7 c g s e z P x P 6 + b 2 e Q 6 y p l M M 0 s l W S x K M o 6 s Q r P X 0 Y B p S i y f O I K J Z u 5 W R E Z Y Y 2 J d Q B U X Q r D 8 8 i o J L + o 3 9 e D + s t a 4 L N I o w w m c w j k E c A U N u I M m h E D g E Z 7 h F d 4 8 5 b 1 4 7 9 7 H o r X k F T P H 8 A f e 5 w 8 E l Y 7 p &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E b d N M m z A a U 4 k 5 H Q t i o 0 y z S 1 a v 5 c = " &gt; A A A B 7 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o N 4 C X j x G c J N A s o T Z y W w y Z h 7 L z K w Q l v y D F w 8 q X v 0 g b / 6 N k 2 Q P m l j Q U F R 1 0 9 0 V p 5 w Z 6 / v f X m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j M o 0 o S F R X O l O j A 3 l T N L Q M s t p J 9 U U i 5 j T d j y + n f n t J 6 o N U / L B T l I a C T y U L G E E W y e 1 e k M s B O 5 X a 3 7 d n w O t k q A g N S j Q 7 F e / e g N F M k G l J R w b 0 w 3 8 1 E Y 5 1 p Y R T q e V X m Z o i s k Y D 2 n X U Y k F N V E + v 3 a K z p w y Q I n S r q R F c / X 3 R I 6 F M R M R u 0 6 B 7 c g s e z P x P 6 + b 2 e Q 6 y p l M M 0 s l W S x K M o 6 s Q r P X 0 Y B p S i y f O I K J Z u 5 W R E Z Y Y 2 J d Q B U X Q r D 8 8 i o J L + o 3 9 e D + s t a 4 L N I o w w m c w j k E c A U N u I M m h E D g E Z 7 h F d 4 8 5 b 1 4 7 9 7 H o r X k F T P H 8 A f e 5 w / v I I 7 b &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E b d N M m z A a U 4 k 5 H Q t i o 0 y z S 1 a v 5 c = " &gt; A A A B 7 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o N 4 C X j x G c J N A s o T Z y W w y Z h 7 L z K w Q l v y D F w 8 q X v 0 g b / 6 N k 2 Q P m l j Q U F R 1 0 9 0 V p 5 w Z 6 / v f X m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j M o 0 o S F R X O l O j A 3 l T N L Q M s t p J 9 U U i 5 j T d j y + n f n t J 6 o N U / L B T l I a C T y U L G E E W y e 1 e k M s B O 5 X a 3 7 d n w O t k q A g N S j Q 7 F e / e g N F M k G l J R w b 0 w 3 8 1 E Y 5 1 p Y R T q e V X m Z o i s k Y D 2 n X U Y k F N V E + v 3 a K z p w y Q I n S r q R F c / X 3 R I 6 F M R M R u 0 6 B 7 c g s e z P x P 6 + b 2 e Q 6 y p l M M 0 s l W S x K M o 6 s Q r P X 0 Y B p S i y f O I K J Z u 5 W R E Z Y Y 2 J d Q B U X Q r D 8 8 i o J L + o 3 9 e D + s t a 4 L N I o w w m c w j k E c A U N u I M m h E D g E Z 7 h F d 4 8 5 b 1 4 7 9 7 H o r X k F T P H 8 A f e 5 w / v I I 7 b &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E b d N M m z A a U 4 k 5 H Q t i o 0 y z S 1 a v 5 c = " &gt; A A A B 7 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o N 4 C X j x G c J N A s o T Z y W w y Z h 7 L z K w Q l v y D F w 8 q X v 0 g b / 6 N k 2 Q P m l j Q U F R 1 0 9 0 V p 5 w Z 6 / v f X m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j M o 0 o S F R X O l O j A 3 l T N L Q M s t p J 9 U U i 5 j T d j y + n f n t J 6 o N U / L B T l I a C T y U L G E E W y e 1 e k M s B O 5 X a 3 7 d n w O t k q A g N S j Q 7 F e / e g N F M k G l J R w b 0 w 3 8 1 E Y 5 1 p Y R T q e V X m Z o i s k Y D 2 n X U Y k F N V E + v 3 a K z p w y Q I n S r q R F c / X 3 R I 6 F M R M R u 0 6 B 7 c g s e z P x P 6 + b 2 e Q 6 y p l M M 0 s l W S x K M o 6 s Q r P X 0 Y B p S i y f O I K J Z u 5 W R E Z Y Y 2 J d Q B U X Q r D 8 8 i o J L + o 3 9 e D + s t a 4 L N I o w w m c w j k E c A U N u I M m h E D g E Z 7 h F d 4 8 5 b 1 4 7 9 7 H o r X k F T P H 8 A f e 5 w / v I I 7 b &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E b d N M m z A a U 4 k 5 H Q t i o 0 y z S 1 a v 5 c = " &gt; A A A B 7 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o N 4 C X j x G c J N A s o T Z y W w y Z h 7 L z K w Q l v y D F w 8 q X v 0 g b / 6 N k 2 Q P m l j Q U F R 1 0 9 0 V p 5 w Z 6 / v f X m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 j M o 0 o S F R X O l O j A 3 l T N L Q M s t p J 9 U U i 5 j T d j y + n f n t J 6 o N U / L B T l I a C T y U L G E E W y e 1 e k M s B O 5 X a 3 7 d n w O t k q A g N S j Q 7 F e / e g N F M k G l J R w b 0 w 3 8 1 E Y 5 1 p Y R T q e V X m Z o i s k Y D 2 n X U Y k F N V E + v 3 a K z p w y Q I n S r q R F c / X 3 R I 6 F M R M R u 0 6 B 7 c g s e z P x P 6 + b 2 e Q 6 y p l M M 0 s l W S x K M o 6 s Q r P X 0 Y B p S i y f O I K J Z u 5 W R E Z Y Y 2 J d Q B U X Q r D 8 8 i o J L + o 3 9 e D + s t a 4 L N I o w w m c w j k E c A U N u I M m h E D g E Z 7 h F d 4 8 5 b 1 4 7 9 7 H o r X k F T P H 8 A f e 5 w / v I I 7 b &lt; / l a t e x i t &gt; Forward rendering: various rendering effects generated by SoftRas (left). Different degrees of transparency and blurriness can be achieved by tuning γ and σ respectively. Applications based on the backward gradients provided by SoftRas: (1) 3D unsupervised mesh reconstruction from a single input image (middle) and (2) 3D pose fitting to the target image by flowing gradient to the occluded triangles (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Comparisons between the standard rendering pipeline (upper branch) and our rendering framework (lower branch).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>ground truth (b) σ = 0.003 (c) σ = 0.01 (d) σ = 0.03</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Probability maps of a triangle under Euclidean metric. (a) definition of pixel-to-triangle distance; (b)-(d) probability maps generated with different σ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>(a) left and middle). In addition, as their gradients only operate on the image plane, both OpenDR and NMR are not able to optimize the depth value z of the triangles. In contrast, The proposed framework for single-view mesh reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Network structure for color reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Results of colorized mesh reconstruction. The learned principal colors and their usage histogram are visualize on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>1 &lt; 3 &lt;Figure 10 :</head><label>1310</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " 7 s d r n e M K V O 3 P a + d / 2 p 1 x W T B R N 6 s = " &gt; A A A C A 3 i c b V D L S s N A F J 3 U V 4 2 v q M t u B o v i q i R F U B d C w Y 3 L C s Y W m l A m 0 0 k 6 d G Y S Z i Z C C V 2 4 8 V f c u F B x 6 0 + 4 8 2 + c t l l o 6 4 E L Z 8 6 5 l 7 n 3 R B m j S r v u t 1 V Z W V 1 b 3 6 h u 2 l v b O 7 t 7 z v 7 B v U p z i Y m P U 5 b K b o Q U Y V Q Q X 1 P N S D e T B P G I k U 4 0 u p 7 6 n Q c i F U 3 F n R 5 n J O Q o E T S m G G k j 9 Z 1 a o G j C 0 c m V 2 3 A 9 G A R 2 k C A + f 3 t 9 p 2 7 U G e A y 8 U p S B y X a f e c r G K Q 4 5 0 R o z J B S P c / N d F g g q S l m Z G I H u S I Z w i O U k J 6 h A n G i w m J 2 x A Q e G 2 U A 4 1 S a E h r O 1 N 8 T B e J K j X l k O j n S Q 7 X o T c X / v F 6 u 4 4 u w o C L L N R F 4 / l G c M 6 h T O E 0 E D q g k W L O x I Q h L a n a F e I g k w t r k Z p s Q v M W T l 4 n f b F w 2 v N u z e q t Z p l E F N X A E T o E H z k E L 3 I A 2 8 A E G j + A Z v I I 3 6 8 l 6 s d 6 t j 3 l r x S p n D s E f W J 8 / + 6 + V P w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 s d r n e M K V O 3 P a + d / 2 p 1 x W T B R N 6 s = " &gt; A A A C A 3 i c b V D L S s N A F J 3 U V 4 2 v q M t u B o v i q i R F U B d C w Y 3 L C s Y W m l A m 0 0 k 6 d G Y S Z i Z C C V 2 4 8 V f c u F B x 6 0 + 4 8 2 + c t l l o 6 4 E L Z 8 6 5 l 7 n 3 R B m j S r v u t 1 V Z W V 1 b 3 6 h u 2 l v b O 7 t 7 z v 7 B v U p z i Y m P U 5 b K b o Q U Y V Q Q X 1 P N S D e T B P G I k U 4 0 u p 7 6 n Q c i F U 3 F n R 5 n J O Q o E T S m G G k j 9 Z 1 a o G j C 0 c m V 2 3 A 9 G A R 2 k C A + f 3 t 9 p 2 7 U G e A y 8 U p S B y X a f e c r G K Q 4 5 0 R o z J B S P c / N d F g g q S l m Z G I H u S I Z w i O U k J 6 h A n G i w m J 2 x A Q e G 2 U A 4 1 S a E h r O 1 N 8 T B e J K j X l k O j n S Q 7 X o T c X / v F 6 u 4 4 u w o C L L N R F 4 / l G c M 6 h T O E 0 E D q g k W L O x I Q h L a n a F e I g k w t r k Z p s Q v M W T l 4 n f b F w 2 v N u z e q t Z p l E F N X A E T o E H z k E L 3 I A 2 8 A E G j + A Z v I I 3 6 8 l 6 s d 6 t j 3 l r x S p n D s E f W J 8 / + 6 + V P w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 s d r n e M K V O 3 P a + d / 2 p 1 x W T B R N 6 s = " &gt; A A A C A 3 i c b V D L S s N A F J 3 U V 4 2 v q M t u B o v i q i R F U B d C w Y 3 L C s Y W m l A m 0 0 k 6 d G Y S Z i Z C C V 2 4 8 V f c u F B x 6 0 + 4 8 2 + c t l l o 6 4 E L Z 8 6 5 l 7 n 3 R B m j S r v u t 1 V Z W V 1 b 3 6 h u 2 l v b O 7 t 7 z v 7 B v U p z i Y m P U 5 b K b o Q U Y V Q Q X 1 P N S D e T B P G I k U 4 0 u p 7 6 n Q c i F U 3 F n R 5 n J O Q o E T S m G G k j 9 Z 1 a o G j C 0 c m V 2 3 A 9 G A R 2 k C A + f 3 t 9 p 2 7 U G e A y 8 U p S B y X a f e c r G K Q 4 5 0 R o z J B S P c / N d F g g q S l m Z G I H u S I Z w i O U k J 6 h A n G i w m J 2 x A Q e G 2 U A 4 1 S a E h r O 1 N 8 T B e J K j X l k O j n S Q 7 X o T c X / v F 6 u 4 4 u w o C L L N R F 4 / l G c M 6 h T O E 0 E D q g k W L O x I Q h L a n a F e I g k w t r k Z p s Q v M W T l 4 n f b F w 2 v N u z e q t Z p l E F N X A E T o E H z k E L 3 I A 2 8 A E G j + A Z v I I 3 6 8 l 6 s d 6 t j 3 l r x S p n D s E f W J 8 / + 6 + V P w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 s d r n e M K V O 3 P a + d / 2 p 1 x W T B R N 6 s = " &gt; A A A C A 3 i c b V D L S s N A F J 3 U V 4 2 v q M t u B o v i q i R F U B d C w Y 3 L C s Y W m l A m 0 0 k 6 d G Y S Z i Z C C V 2 4 8 V f c u F B x 6 0 + 4 8 2 + c t l l o 6 4 E L Z 8 6 5 l 7 n 3 R B m j S r v u t 1 V Z W V 1 b 3 6 h u 2 l v b O 7 t 7 z v 7 B v U p z i Y m P U 5 b K b o Q U Y V Q Q X 1 P N S D e T B P G I k U 4 0 u p 7 6 n Q c i F U 3 F n R 5 n J O Q o E T S m G G k j 9 Z 1 a o G j C 0 c m V 2 3 A 9 G A R 2 k C A + f 3 t 9 p 2 7 U G e A y 8 U p S B y X a f e c r G K Q 4 5 0 R o z J B S P c / N d F g g q S l m Z G I H u S I Z w i O U k J 6 h A n G i w m J 2 x A Q e G 2 U A 4 1 S a E h r O 1 N 8 T B e J K j X l k O j n S Q 7 X o T c X / v F 6 u 4 4 u w o C L L N R F 4 / l G c M 6 h T O E 0 E D q g k W L O x I Q h L a n a F e I g k w t r k Z p s Q v M W T l 4 n f b F w 2 v N u z e q t Z p l E F N X A E T o E H z k E L 3 I A 2 8 A E G j + A Z v I I 3 6 8 l 6 s d 6 t j 3 l r x S p n D s E f W J 8 / + 6 + V P w = = &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = " T r I i Q 4 2 7 y F 7 r s q g W G C X N c m s l g i k = " &gt; A A A C A 3 i c b V C 7 T s M w F H X K q 5 R X g L G L R Q V i q p I W C R i Q K r E w F o n Q S k 1 U O a 6 T W r W d y H a Q q q g D C 7 / C w g C I l Z 9 g 4 2 9 w 2 w z Q c q Q r H Z 9 z r 3 z v C V N G l X a c b 6 u 0 s r q 2 v l H e r G x t 7 + z u 2 f s H 9 y r J J C Y e T l g i u y F S h F F B P E 0 1 I 9 1 U E s R D R j r h 6 H r q d x 6 I V D Q R d 3 q c k o C j W N C I Y q S N 1 L e r v q I x R y d X T t 1 p Q t + v + D H i 8 3 e z b 9 e M O g N c J m 5 B a q B A u 2 9 / + Y M E Z 5 w I j R l S q u c 6 q Q 5 y J D X F j E w q f q Z I i v A I x a R n q E C c q C C f H T G B x 0 Y Z w C i R p o S G M / X 3 R I 6 4 U m M e m k 6 O 9 F A t e l P x P 6 + X 6 e g i y K l I M 0 0 E n n 8 U Z Q z q B E 4 T g Q M q C d Z s b A j C k p p d I R 4 i i b A 2 u V V M C O 7 i y c v E a 9 Q v 6 + 7 t W a 3 V K N I o g y o 4 A q f A B e e g B W 5 A G 3 g A g 0 f w D F 7 B m / V k v V j v 1 s e 8 t W Q V M 4 f g D 6 z P H w H o l U M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T r I i Q 4 2 7 y F 7 r s q g W G C X N c m s l g i k = " &gt; A A A C A 3 i c b V C 7 T s M w F H X K q 5 R X g L G L R Q V i q p I W C R i Q K r E w F o n Q S k 1 U O a 6 T W r W d y H a Q q q g D C 7 / C w g C I l Z 9 g 4 2 9 w 2 w z Q c q Q r H Z 9 z r 3 z v C V N G l X a c b 6 u 0 s r q 2 v l H e r G x t 7 + z u 2 f s H 9 y r J J C Y e T l g i u y F S h F F B P E 0 1 I 9 1 U E s R D R j r h 6 H r q d x 6 I V D Q R d 3 q c k o C j W N C I Y q S N 1 L e r v q I x R y d X T t 1 p Q t + v + D H i 8 3 e z b 9 e M O g N c J m 5 B a q B A u 2 9 / + Y M E Z 5 w I j R l S q u c 6 q Q 5 y J D X F j E w q f q Z I i v A I x a R n q E C c q C C f H T G B x 0 Y Z w C i R p o S G M / X 3 R I 6 4 U m M e m k 6 O 9 F A t e l P x P 6 + X 6 e g i y K l I M 0 0 E n n 8 U Z Q z q B E 4 T g Q M q C d Z s b A j C k p p d I R 4 i i b A 2 u V V M C O 7 i y c v E a 9 Q v 6 + 7 t W a 3 V K N I o g y o 4 A q f A B e e g B W 5 A G 3 g A g 0 f w D F 7 B m / V k v V j v 1 s e 8 t W Q V M 4 f g D 6 z P H w H o l U M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T r I i Q 4 2 7 y F 7 r s q g W G C X N c m s l g i k = " &gt; A A A C A 3 i c b V C 7 T s M w F H X K q 5 R X g L G L R Q V i q p I W C R i Q K r E w F o n Q S k 1 U O a 6 T W r W d y H a Q q q g D C 7 / C w g C I l Z 9 g 4 2 9 w 2 w z Q c q Q r H Z 9 z r 3 z v C V N G l X a c b 6 u 0 s r q 2 v l H e r G x t 7 + z u 2 f s H 9 y r J J C Y e T l g i u y F S h F F B P E 0 1 I 9 1 U E s R D R j r h 6 H r q d x 6 I V D Q R d 3 q c k o C j W N C I Y q S N 1 L e r v q I x R y d X T t 1 p Q t + v + D H i 8 3 e z b 9 e M O g N c J m 5 B a q B A u 2 9 / + Y M E Z 5 w I j R l S q u c 6 q Q 5 y J D X F j E w q f q Z I i v A I x a R n q E C c q C C f H T G B x 0 Y Z w C i R p o S G M / X 3 R I 6 4 U m M e m k 6 O 9 F A t e l P x P 6 + X 6 e g i y K l I M 0 0 E n n 8 U Z Q z q B E 4 T g Q M q C d Z s b A j C k p p d I R 4 i i b A 2 u V V M C O 7 i y c v E a 9 Q v 6 + 7 t W a 3 V K N I o g y o 4 A q f A B e e g B W 5 A G 3 g A g 0 f w D F 7 B m / V k v V j v 1 s e 8 t W Q V M 4 f g D 6 z P H w H o l U M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T r I i Q 4 2 7 y F 7 r s q g W G C X N c m s l g i k = " &gt; A A A C A 3 i c b V C 7 T s M w F H X K q 5 R X g L G L R Q V i q p I W C R i Q K r E w F o n Q S k 1 U O a 6 T W r W d y H a Q q q g D C 7 / C w g C I l Z 9 g 4 2 9 w 2 w z Q c q Q r H Z 9 z r 3 z v C V N G l X a c b 6 u 0 s r q 2 v l H e r G x t 7 + z u 2 f s H 9 y r J J C Y e T l g i u y F S h F F B P E 0 1 I 9 1 U E s R D R j r h 6 H r q d x 6 I V D Q R d 3 q c k o C j W N C I Y q S N 1 L e r v q I x R y d X T t 1 p Q t + v + D H i 8 3 e z b 9 e M O g N c J m 5 B a q B A u 2 9 / + Y M E Z 5 w I j R l S q u c 6 q Q 5 y J D X F j E w q f q Z I i v A I x a R n q E C c q C C f H T G B x 0 Y Z w C i R p o S G M / X 3 R I 6 4 U m M e m k 6 O 9 F A t e l P x P 6 + X 6 e g i y K l I M 0 0 E n n 8 U Z Q z q B E 4 T g Q M q C d Z s b A j C k p p d I R 4 i i b A 2 u V V M C O 7 i y c v E a 9 Q v 6 + 7 t W a 3 V K N I o g y o 4 A q f A B e e g B W 5 A G 3 g A g 0 f w D F 7 B m / V k v V j v 1 s e 8 t W Q V M 4 f g D 6 z P H w H o l U M = &lt; / l a t e x i t &gt; Visualization of loss function landscapes of NMR and SoftRas for pose optimization given target image (a)and initialization (f). SoftRas achieves global minimum (b) with loss landscape (g). NMR is stuck in local minimum (c) with loss landscape (h). At this local minimum, Soft-Ras produces the smooth and partially transparent rendering (d)(e), which smoothens the loss landscape (i)(j) with larger σ and γ, and consequently leads to better minimum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>2 = 1 ⇥ 10 5 = 3 ⇥ 10 5 = 5 ⇥ 10 5 = 1 ⇥ 10 4 = 3 ⇥ 10 4 = 5 ⇥ 10 4 = 1 ⇥ 10 3 = 3 ⇥ 10 3 = 5 ⇥ 10 3 = 1</head><label>21535551434541333531</label><figDesc>⇥ 10 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>computed as fol-Figure A1: More single-view reconstruction results. Left: input image; middle: reconstructed geometry; right: colorized reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure C1 :Figure C2 :</head><label>C1C2</label><figDesc>Network Architecture of A N , an alternative color aggregate function that is implemented as a neural networks. Network architecture of the feature extractor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>FeatureFigure C3 :Figure D1 :</head><label>C3D1</label><figDesc>Network architectures of the shape and color generator. Visualization of intermediate mesh deformation during training. First row: the network deforms the input sphereto a desired car model that corresponds to the target image. Second row: the generated car model is further deformed to reconstruct the airplane.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure D2 :</head><label>D2</label><figDesc>Single-view reconstruction results on real images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Rendered Image Transform Color Computation Intrinsic Properties Extrinsic Variables Rendering Pipeline</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Traditional Rasterizer</cell></row><row><cell></cell><cell></cell><cell>Rasterization</cell><cell>Z-buffering</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Soft Rasterizer</cell></row><row><cell></cell><cell></cell><cell>Probability</cell><cell>Aggregate</cell></row><row><cell></cell><cell></cell><cell>Computation</cell><cell>Functions</cell></row><row><cell>Differentiable function</cell><cell>Non-differentiable function</cell><cell>Differentiable forward</cell><cell>Non-differentiable forward</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>.7116 0.7697 0.5270 0.6156 0.4628 0.6654 0.6811 0.6878 0.4487 0.7895 0.5953 0.6234 Ours (full) 0.6670 0.5429 0.7382 0.7876 0.5470 0.6298 0.4580 0.6807 0.6702 0.7220 0.5325 0.8127 0.6145 0.6464</figDesc><table><row><cell>Category</cell><cell cols="2">Airplane Bench Dresser</cell><cell>Car</cell><cell>Chair Display Lamp Speaker</cell><cell>Rifle</cell><cell>Sofa</cell><cell>Table</cell><cell>Phone Vessel</cell><cell>Mean</cell></row><row><cell>retrieval [47]</cell><cell>0.5564</cell><cell cols="8">0.4875 0.5713 0.6519 0.3512 0.3958 0.2905 0.4600 0.5133 0.5314 0.3097 0.6696 0.4078 0.4766</cell></row><row><cell>voxel [47]</cell><cell>0.5556</cell><cell cols="8">0.4924 0.6823 0.7123 0.4494 0.5395 0.4223 0.5868 0.5987 0.6221 0.4938 0.7504 0.5507 0.5736</cell></row><row><cell>NMR [19]</cell><cell>0.6172</cell><cell cols="8">0.4998 0.7143 0.7095 0.4990 0.5831 0.4126 0.6536 0.6322 0.6735 0.4829 0.7777 0.5645 0.6015</cell></row><row><cell>Ours (sil.)</cell><cell>0.6419</cell><cell>0.5080 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Comparison of mean IoU with other 3D unsupervised reconstruction methods on 13 categories of ShapeNet datasets.</figDesc><table><row><cell>Input</cell><cell>Reconstructed Results</cell><cell>Learned Color Palettes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of the regularizer and various forms of distance and aggregate functions. A N stands for the aggregation function implemented as a neural network. A S and A O refer to the aggregation functions defined in Equation 2 and 4 respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 ,</head><label>2</label><figDesc>we investigate the impact of Laplacian regularizer and various forms of the distance function (Section 3.2) and the aggregate function. As the RGB color channel and the α channel (silhouette) have different candidate aggregate functions, we separate their lists inTable 2. First, by adding Lapla-</figDesc><table><row><cell>-hoc</cell></row></table><note>cian constraint, our performance is increased by 0.4 point (62.4 v.s. 62.0). In contrast, NMR [19] has reported a nega- tive effect of geometry regularizer on its quantitative results. The performance drop may be due to the fact that the ad</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Hao Li is affiliated with the University of Southern California, the USC Institute for Creative Technologies, and Pinscreen. This research was conducted at USC and was funded by in part by the ONR YIP grant N00014-17-S-FO14, the CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA, the Andrew and Erna Viterbi Early Career Chair, the U.S. Army Research Laboratory (ARL) under contract number W911NF-14-D-0005, Adobe, and Sony. This project was not funded by Pinscreen, nor has it been conducted at Pinscreen or by anyone else affiliated with Pinscreen. The content of the information does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Gradient Computation</head><p>In this section, we provide more analysis on the variants of the probability representation (Section 3.2) and aggregate function (Section 3.3), in terms of the mathematical formulation and the resulting impact on the backward gradient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1. Overview</head><p>According to the computation graph in <ref type="figure">Figure 3</ref>, our gradient from rendered image I to vertices in mesh M is obtained by</p><p>While ∂U ∂M , ∂Z ∂M , ∂I ∂N and ∂N ∂M can be easily obtained by inverting the projection matrix and the illumination models, ∂I ∂U and ∂I ∂Z do not exist in conventional rendering pipelines. Our framework introduces an intermediate representation, probability map D, that factorizes the gradient ∂I ∂U to ∂I ∂D ∂D ∂U , enabling the differentiability of ∂I ∂U . Further, we obtain ∂I ∂Z via the proposed aggregate function. In the following context, we will first address the gradient ∂D ∂U in Section A2 and gradient ∂I ∂D and ∂I ∂Z in Section A3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2. Probability Map Computation</head><p>The probability maps {D i j } based on the relative position between a given triangle f j and pixel p i are obtained via sigmoid function with temperature σ and distance metric D(i, j):</p><p>where the metric D essentially satisfies:</p><p>if p i lies exactly on the boundary of f j . The positive scalar σ controls the sharpness of the probability, where D j converges to a binary mask as σ → 0. We introduce two candidate metrics, namely signed Euclidean distance and barycentric metric. We represent p i using barycentric coordinate b i j ∈ R 3 defined by f j :</p><p>where</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.1 Euclidean Distance</head><p>Let t i j ∈ R 3 be the barycentric coordinate of the point on the edge of f j that is closest to p i . The signed Euclidean distance D E (i, j) from p i to the edges of f j can be computed as:</p><p>where δ i j is a sign indicator defined as</p><p>Then the partial gradient ∂D E (i,j) ∂Uj can be obtained via:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.2 Barycentric Metric</head><p>We define the barycentric metric D B (i, j) as the minimum of barycentric coordinate:</p><p>, then the gradient from D B (i, j) to U j can be obtained through:</p><p>where k and l are the indices of U j 's element.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3. Aggregate function A3.1 Softmax-based Aggregate Function</head><p>According to A S (·), the output color is:</p><p>where the weight {w j } is obtained based on the relative depth {z j } and the screen-space position of triangle f j and pixel p i as indicated in the following equation:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3d faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 26th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face recognition based on fitting a 3d morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1063" to="1074" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Openpose: Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08008</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Single-image svbrdf capture with a renderingaware deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Deschaintre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Drettakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bousseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">128</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accurate, dense, and robust multiview stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised training for 3d morphable model regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8377" to="8386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An evaluation of computational imaging techniques for heterogeneous inverse scattering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gkioulekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zickler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="685" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inverse volume rendering with material dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gkioulekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zickler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">162</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Atlasnet: A papier-mch approach to learning 3d surface generation. computer vision and pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to generate and reconstruct 3d meshes with only 2d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep volumetric video from very sparse multi-view performance capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Legendre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="351" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mesoscopic facial geometry inference using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8407" to="8416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Endto-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning category-specific mesh reconstruction from image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07549</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural 3d mesh renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Posenet: A convolutional network for real-time 6-dof camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2938" to="2946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d-rcnn: Instance-level 3d object reconstruction via render-and-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3559" to="3568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image-based reconstruction of spatial appearance and geometric detail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="234" to="257" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Differentiable monte carlo ray tracing through edge sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno>222:1- 222:11</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH Asia)</title>
		<meeting>SIGGRAPH Asia)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint face alignment and 3d face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="545" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Material editing using a physically based rendering network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Lien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2280" to="2288" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">248</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Opendr: An approximate differentiable renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Approximate bayesian image interpretation using generative probabilistic graphics programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Mansinghka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Perov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pose-aware face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rawls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4838" to="4846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image-based visual hulls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcmillan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 27th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="369" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep shading: convolutional neural networks for screen space shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nalbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arabadzhiyska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="65" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen-Phuoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Balaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06575</idno>
		<title level="m">Rendernet: A deep convolutional network for differentiable rendering from 3d shapes</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Geonet: Geometric neural network for joint depth and surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="283" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised learning of 3d structure from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4996" to="5004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning detailed face reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5553" to="5562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Self-supervised multi-level face model learning for monocular reconstruction at over 250 hz</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2549" to="2559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mofa: Model-based deep convolutional face autoencoder for unsupervised monocular reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Nonlinear 3d face morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7346" to="7355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Viewpoints and keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1510" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pixel2mesh: Generating 3d mesh models from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="539" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Perspective transformer nets: Learning single-view 3d object reconstruction without 3d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Shape-fromshading: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Cryer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="690" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Real-time height map fusion using differentiable rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zienkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4280" to="4287" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
