<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Event-Based Motion Deblurring</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Sichuan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqing</forename><surname>Zou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiancheng</forename><surname>Lv</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Sichuan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Event-Based Motion Deblurring</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recovering sharp video sequence from a motion-blurred image is highly ill-posed due to the significant loss of motion information in the blurring process. For event-based cameras, however, fast motion can be captured as events at high time rate, raising new opportunities to exploring effective solutions. In this paper, we start from a sequential formulation of event-based motion deblurring, then show how its optimization can be unfolded with a novel end-to-end deep architecture. The proposed architecture is a convolutional recurrent neural network that integrates visual and temporal knowledge of both global and local scales in principled manner. To further improve the reconstruction, we propose a differentiable directional event filtering module to effectively extract rich boundary prior from the stream of events. We conduct extensive experiments on the synthetic GoPro dataset and a large newly introduced dataset captured by a DAVIS240C camera. The proposed approach achieves state-of-the-art reconstruction quality, and generalizes better to handling real-world motion blur.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Motion blur happens commonly due to the exposure time required by modern camera sensors, during which scenes are recorded at different time stamps and accumulated into averaged (blurred) signal. The inverse problem called deblurring, which unravels the underlying scene dynamics behind a motion-blurred image and generates a sequence of sharp recovery of the scene, is still challenging in computer vision. While simple motion patterns (e.g. camera shake) have been well modelled <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b1">2]</ref>, formulating more sophisticated motion patterns in real world, however, is much more difficult.</p><p>To model general motion blur, recent deep learning approaches propose to recover a blurred image by observing lots of sharp images and their blurred versions <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b42">43]</ref>. Despite their success in certain scenarios, they * Equal contribution. This work is done when Jiang Zhe is interned with SenseTime Research. † Correspondence should be addressed to: zhangyulb@gmail.com  <ref type="figure">Figure 1</ref>. Motivation of our approach. A severe motion-blurred image (a) is difficult to deblur by observing its ambiguous appearance only even with state-of-the-art deep architecture <ref type="bibr" target="#b49">[50]</ref> (c). Though events (b) provide dense temporal cues, the physical reconstruction approach <ref type="bibr" target="#b30">[31]</ref> still presents unaddressed blur due to the noisiness of events (d). The proposed deep motion deblurring learns to recover plausible details from imperfect image and events (e). may fail reconstructing the scene plausibly for severe motion blur (e.g. <ref type="figure">Fig. 1</ref>), which is common for handheld, vehicle or drone-equipped cameras. In this case, hallucinating the scene details is hardly possible due to the significant loss of temporal order and visual information.</p><p>Instead of purely relying on computational architectures, this work adopts event-based cameras to alleviate this problem at data capture stage. Event cameras are biologically inspired sensors adept at recording the change of pixel intensities (called events) with microsecond accuracy and very low power consumption. The hybrid model of such sensors (e.g. <ref type="bibr" target="#b4">[5]</ref>) allows the events being temporally calibrated with the image. As a result, such data naturally encodes dense temporal information that can facilitate motion deblurring. As shown in <ref type="figure">Fig. 1 (a)</ref> and (b), although the image undergoes significant blur, the accompanying events are temporally dense and reveal clear moving pattern of the scene.</p><p>Despite the high potential of event-based motion deblurring, a critical issue is that events are lossy and noisy signals triggered only if pixel intensity changes up to certain threshold that can vary with the change of scene conditions <ref type="bibr" target="#b34">[35]</ref>. Such discrete and inconsistent sampling makes textures and contrast difficult to restore. As shown in <ref type="figure">Fig. 1 (d)</ref>, state-ofthe-art physical deblurring method <ref type="bibr" target="#b30">[31]</ref> still has difficulty reconstructing the image plausibly. Our solution is to plug deeply learned priors into event-based deblurring process, so as to surpass the imperfectness of data.</p><p>In details, this work starts from a sequential formulation of event-based deblurring. By reinterpreting its optimization with deep networks, we propose a novel recurrent architecture trainable end-to-end. For each time step, coarse reconstructions are obtained from previous reconstruction as well as the local temporal events. Fine details are then supplied by network predictions, guided by appearance and temporal cues at both global and local scales. To further improve the quality of reconstruction, we propose a differentiable Directional Event Filtering (DEF) module, which effectively aggregates the motion boundaries revealed by events and produces sharp deblurring prior. To evaluate the proposed approach, we compile a large outdoor dataset captured using the DAVIS240C camera <ref type="bibr" target="#b4">[5]</ref>. Extensive experiments on this dataset and the synthetic GoPro dataset <ref type="bibr" target="#b24">[25]</ref> show that the proposed approach outperforms various stateof-the-art methods, either image-based or event-based, and generalizes better to handling real-world motion blur.</p><p>Contributions of this paper are summarized as follows. 1) We propose a novel recurrent deep architecture for eventbased motion deblurring, which achieves state-of-the-art results on two large benchmarks. 2) We propose directional event filtering to generate sharp boundary prior from events for motion deblurring. 3) We compile a new event dataset with real-world motion blur to facilitate future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Blind motion deblurring aims to resolve a blurry image without knowing the blurring kernel. Early works have designed various blurring-aware indicators, such as color channel statistics <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b46">47]</ref>, patch recurrence <ref type="bibr" target="#b21">[22]</ref> and "outlier" image signals <ref type="bibr" target="#b7">[8]</ref>, to define latent image priors. Several works propose to learn motion kernels <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b27">28]</ref>, restoration functions <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b10">11]</ref> and image priors <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b41">42]</ref> from data. More complex motion patterns compounded by different objects were also addressed <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b36">37]</ref>. Richer prior knowledge such as scene geometry was proven useful <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>A recent trend is to approach all the complexities of motion deblurring with deep neural networks. Various kinds of effective network designs are proposed, including enlarging the receptive field <ref type="bibr" target="#b51">[52]</ref>, multi-scale fusion <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref>, feature distangling <ref type="bibr" target="#b25">[26]</ref>, and recurrent refinement <ref type="bibr" target="#b43">[44]</ref>. There was also research on decoding the motion dynamics of a blurred image to a sharp video sequence <ref type="bibr" target="#b14">[15]</ref>. Despite these advances, the considerable combinations of real-world lightings, textures and motions, which are severely missing in a blurred image, are still difficult to be plausibly recovered.</p><p>Event cameras <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b4">5]</ref> are a special kind of sensors that detect intensity changes of the scene at microsecond level with slight power consumption. They find applications in various vision tasks, such as visual tracking <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b22">23]</ref>, stereo vision <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b0">1]</ref> and optical flow estimation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b47">48]</ref>. A related branch is to explore the corrupted event signals to restore high frame rate image sequences <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b39">40]</ref>. Recently, Pan et al. <ref type="bibr" target="#b30">[31]</ref> formulates event-based motion blurring with a double integral model. Yet, the noisy hard sampling mechanism of event cameras often introduces strong accmulated noise and loss of scene details/contrast. This work shares the insight of recent works on eventto-video translation <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36]</ref> that surpasses the imperfect event sampling by learning plausible details from data. While <ref type="bibr" target="#b32">[33]</ref> addresses future frame prediction, <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36]</ref> translate events to plausible intensity images in streaming manner depending on local motion cues. Instead, this work explores both long-term, local appearance/motion cues as well as novel event boundary priors to solve motion deblurring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning Event-Based Motion Deblurring</head><p>Given a motion-blurred imageĪ, our objective is to recover a sharp video sequence with T frames, I = {I i } T i=1 . We assume that a set of events E 1∼T are also captured by hybrid image-event sensors during the exposure, where the tilde denotes the time interval. Each event E ∈ E 1∼T has the form E x,y,t , meaning that it is triggered at image coordinate (x, y) and time point t ∈ [1, T ]. Note here t does not need to be an integer, but can be fractional due to the high temporal resolution (i.e. microsecond-level) of event camera. A polarity p x,y,t is recorded for E x,y,t indicating the change of local intensity. Formally, it is defined as <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b4">5]</ref>  </p><p>Eqn. <ref type="bibr" target="#b0">(1)</ref> shows that, events are triggered if the instant image at time point t, namely I t , has pixel intensity changed up to a threshold ±τ in a small time period ∆t. Without loss of generality, we assume that p x,y,t takes zero in case that</p><formula xml:id="formula_1">log It(x,y) It−∆t(x,y) is in [−τ, τ ].</formula><p>For adjacent latent images I i and I i−1 , the following relationship can be derived:</p><formula xml:id="formula_2">I i (x, y) ≈ I i−1 (x, y)·exp τ i t=i−1 p x,y,t 1 (E x,y,t ) dt ,<label>(2)</label></formula><p>The indicator function 1 (·) equals 1 if the event E x,y,t exists, or 0 otherwise.</p><p>One should note that the approximation error of (2) is getting lower when ∆t, τ → 0, which implies denser events according to <ref type="bibr" target="#b0">(1)</ref>. However, with inconsistent τ affected by various kinds of noise, the approximation is mostly insufficient in practice, leading to loss of contrast and details. To address this issue, we propose a joint framework that learns to reconstruct clean images from data, by reinterpreting a sequential deblurring process.</p><p>Deep sequential deblurring. Event-assisted deblurring can be formulated under Maximum-a-Posteriori:</p><formula xml:id="formula_3">I * = arg max I P I|Ī, E 1∼T .<label>(3)</label></formula><p>To solve the combinatorial problem (3) we make the following simplifications. For the joint posterior P I|Ī, E 1∼T , we make use of the temporal relations between adjacent latent images <ref type="formula" target="#formula_2">(2)</ref>, and assume a Markov chain model:</p><formula xml:id="formula_4">P I|Ī, E 1∼T ≈P I T |Ī, E 1∼T × T −1 i=1 P I i |I i+1 ,Ī, E 1∼T ,<label>(4)</label></formula><formula xml:id="formula_5">in which P I i |I i+1 ,Ī, E 1∼T = P I i |I i+1 ,Ī, E i∼i+1</formula><p>with Markov assumption. Note that this simplified model first estimates I T , then perform sequential reconstruction in backward order. According to Bayesian rule, the maximizer of a backward reconstruction step equals to:</p><formula xml:id="formula_6">I * i = arg max Ii P I i+1 ,Ī, E i∼i+1 |I i P (I i ) . (5)</formula><p>Here, the prior term P (I i ) imposes desired distributions of the latent image, e.g. 1 gradient <ref type="bibr" target="#b2">[3]</ref> or manifold smoothness <ref type="bibr" target="#b23">[24]</ref> in recent event-based image reconstruction. To model the likelihood term, we assume that there is an initial estimate from previous reconstruction, via <ref type="bibr" target="#b1">(2)</ref>:</p><formula xml:id="formula_7">I i = I i+1 exp −τ S i i+1 ,<label>(6)</label></formula><p>where ∀x, y, S i i+1 (x, y) = i+1 t=i p x,y,t 1 (E x,y,t ) dt, and denotes Hadamard product. Since the time interval is small, we assume constant τ which introduces only small drift and provides good initialization. To solve I * i , several works assume simple distributions centered aroundÎ * i to define the likelihood term in (5), e.g. in <ref type="bibr" target="#b23">[24]</ref> a Poisson distribution is used. In this manner, Eqn. (5) can be treated as a wellstudied denoising problem.</p><p>Instead of using simple image prior, we borrow from recent research on learning deep denoising prior <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b49">50]</ref>. In particular, we plug a deep network N as a learned denoiser,</p><formula xml:id="formula_8">I * i = N Î i , I i+1 , ,Ī, E i∼i+1 .<label>(7)</label></formula><p>As such, prior of latent image P (I i ) is not explicitly defined but implicitly learned from training data. To reduce parameter size and prevent overfitting, we use the same network governed by the same set of parameters for each deblurring step of (5), leading to a recurrent architecture. The remaining problem of solving (4) is how to get the initial latent image, i.e. I T . We use the fact that the blurred imageĪ roughly equals the average of the instant images in the exposure process. Combining this fact with (6), we havē <ref type="bibr" target="#b5">(6)</ref>. It provides an initial estimation of I T , namelyÎ T , using the blurred imageĪ and events. Thus, we also treat solving I T a denoising problem, centered aroundÎ T , and use a network to approximate it. We note, however, the accumulative operator in <ref type="bibr" target="#b7">(8)</ref> introduces more drift unlike the sequential deblurring steps. We thus correctÎ T via a separate and more powerful network:</p><formula xml:id="formula_9">I ≈ 1 T T i=1 I i = I T 1 T 1 + T t=2 t−1 i=1 B T −i T −i+1 , (8) where B i i+1 = exp −τ S i i+1 and S i i+1 is defined in</formula><formula xml:id="formula_10">I * T = N 0 Î T ,Ī, E 1∼T .</formula><p>The full deblurring process is summarized in Alg. 1. Note that by design <ref type="formula" target="#formula_8">(7)</ref>, the latent image is conditioned on both local and long-term cues from the image and events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Event-assisted Deep Motion Deblurring</head><p>Require: the blurred imageĪ, events E 1∼T 1: Get initial estimateÎ T by solving (8) 2: Deblurring:</p><formula xml:id="formula_11">I * T = N 0 Î T ,Ī, E 1∼T 3: Initialize counter: i = T − 1 4: while i ≥ 1 do 5:</formula><p>Get initial estimateÎ i by solving <ref type="formula" target="#formula_2">(2)</ref> 6:</p><p>Deblurring: <ref type="figure" target="#fig_2">. 2</ref> shows the proposed event-based motion deblurring architecture, which contains: a read network that traverses over the events and generates a single representation of the global scene motion, an initialize network that couples appearance and motion to generate the initial latent image, and the recurrent process network sequentially deblurring all the latent images 1 . The read and initialize networks instantiates N 0 while the process network implements N in Alg. 1.</p><formula xml:id="formula_12">I * i = N Î i , I i+1 , E i∼i+1 ,Ī, E 1∼T 7: i ← i − 1 8: end while 9: return Deblurred sequence I * = {I i } T i=1 4. Network Architecture Fig</formula><p>The read network reads all the event data and generate a joint representation that accounts for the global event motion. To accomplish that, events during the exposure are first binned into equal-length time intervals (3 intervals in <ref type="figure" target="#fig_2">Fig. 2</ref>). In each time interval, events are represented with stacked event frames <ref type="bibr" target="#b16">[17]</ref>, through further dividing an interval into 8 equal-size chunks, summing over the polarities of events falling into each chunk, and stacking the results along channel dimension. The read network is a recurrent  The initialize network decodes the appearance from the blurred image and couples it with the global motion to solve the latent image I * T . It takes as input both the blurred im-ageĪ and the initial estimateÎ T (via solving Eqn. <ref type="bibr" target="#b7">(8)</ref>) and processes them with a convolutional encoder, concatenates the encodings with the accumulated global motion features from the read network, and feeds the joint features into a decoder to get the result.</p><p>Given the initial result, the process network then sequentially deblurs the remaining latent images. In the ith step, it consumes both image and event-based observations. The image part include: 1) the initial estimateÎ i as obtained by Eqn. (6) using the previous reconstruction I i+1 , 2) the local historical image by transforming the previous result I i+1 with the Motion Compensation ("MC" in <ref type="figure" target="#fig_2">Fig. 2</ref>) module, and 3) the boundary guidance map given by the Directional Event Viltering ("DEF" in <ref type="figure" target="#fig_2">Fig. 2</ref>) module. These two modules will be explained further shortly after. Input images are processed by convolutional layers and concatenated with the per-step event features extracted from the read network via latent fusion. The fused features are processed and fed to another convolutional LSTM to propagate temporal knowledge along time. Finally, a decoder takes the joint features and generates the deblurred image.</p><p>Motion compensation. We use a motion compensation module to warp previous deblurring result I i+1 and generate an initialization of the ith time step. Although Eqn. <ref type="bibr" target="#b5">(6)</ref> achieves this by event integration, we find it more effective to predict a flow field from which we directly warp the clean result I i+1 as additional guidance. Motion compensation for events have already been discussed in <ref type="bibr" target="#b9">[10]</ref>. For efficiency, we adopt a FlowNetS architecture <ref type="bibr" target="#b8">[9]</ref> to take events E i∼i+1 as input and directly regress forward flows from i to i + 1. Warping is implemented with a differentiable spatial transformer layer <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Directional event filtering. The initial estimatesÎ i may suffer unaddressed blur due to the naive blurring model <ref type="bibr" target="#b7">(8)</ref> and the noisiness of events. We alleviate this issue with the aid of sharp boundary prior, a widely explored image prior for blind deblurring <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b45">46]</ref>, extracted from events E i∼i+1 .</p><p>Events indicate local change of scene illuminace and reveal physical boundaries. However, as scene boundaries are moving, at a specific time they are only spatially aligned with the latest events triggered at their positions. As a toy example, <ref type="figure" target="#fig_3">Fig. (3)</ref> shows after the imaging the top and bottom lines correspond to events at two different time points. It gives that one can generate scene boundary prior by sampling events at proper space-time positions. Note that due to variation of scene depth, different scene parts may have distinct motion, and position-adaptive sampling is essential.</p><p>Besides, as events are sparse, noisy, and non-uniformly distributed signals, a robust sampling process should decide both where (i.e. center) and how many (i.e. scale) to sample. We learn this task from data via differentiable sampling and filtering. For each image position p, a temporal center c (p) and a set of 2k + 1 filtering coefficients {α i } k i=−k , where k is the support of filtering kernel, are predicted with a small network from the events, satisfying ∀i, α i ≥ 0 and where λ defines sampling stride (we use k = 2, λ = 1), s (·, ·) denotes a sampling function in space-time domain. For the stacked event frame representation of events E i+1 i , one can apply the trilinear kernel for continuous sampling <ref type="bibr" target="#b20">[21]</ref>. Note that the velocity d should follow the direction of local motion of events at space-time point (p, c (p)) to filter along the density surface of events but not across it.</p><p>To get local velocity, we reuse the flow vectors predicted by motion compensation module. We assume object velocity stays constant, which is roughly true in this context as there is just a fraction of time duration (i.e. only 1/ (T − 1) the exposure). Motion compensation gives the velocities of all the positions p 0 ∈ P at time i, d (p 0 , i). At time c (p), a pixel p 0 would be shifted by the flows to a new position:</p><formula xml:id="formula_13">n (p 0 ) = p 0 + (c (p) − i) d (p 0 , i) .<label>(10)</label></formula><p>Note that n (p 0 ) inherits the velocity of p 0 under the local constancy assumption: d (n (p 0 ) , c (p)) = d (p 0 , i). However, the intersected positions at time plane c (p), namely {n (p 0 ) |p 0 ∈ P}, does not ensure complete sampling of the image space. Thus, we resample the velocity at a given target p with a Nadaraya-Watson estimator <ref type="bibr" target="#b3">[4]</ref>:</p><formula xml:id="formula_14">d (p, c (p)) = p 0 ∈P κ (n (p 0 ) − p) d (n (p 0 ) , c (p)) p 0 ∈P κ (n (p 0 ) − p) ,<label>(11)</label></formula><p>where the kernel κ is simply defined with a standard Gaussian. This in spirit shares similarity with the "gather" approach in computer graphics for surface rendering <ref type="bibr" target="#b48">[49]</ref>.</p><p>Eqn. (11) uses all p 0 s to estimate each position p, which is inefficient. In practice we only use samples located within a local L × L window centered around p. The window size L should account for the maximal spatial displacement of pixels, which we find L = 20 sufficient. All of the proposed steps are differentiable, and can be plugged into the network for end-to-end training.</p><p>Loss Function. We use the following joint loss function: <ref type="bibr" target="#b11">(12)</ref> Here, L content is the photometric 1 loss 1</p><formula xml:id="formula_15">L total = L content + λ a L adv + L f low + λ t L tv ,</formula><formula xml:id="formula_16">T T i=1 I * i − I g i , where I g i is the groundtruth clean image.</formula><p>To improve sharpness of the result, we also incorporate an adversarial loss L adv . We use the same PatchGAN discriminator <ref type="bibr" target="#b12">[13]</ref> and follow its original loss definitions strictly.</p><p>The flow network introduces two other loss terms. The first L f low is the photometric reconstruction loss:</p><formula xml:id="formula_17">L f low = 1 T − 1 T −1 i=1 ω I * i+1 , F i→i+1 − I g i ,<label>(13)</label></formula><p>where ω (·, ·) is a backward warping function using forward flows F i→i+1 , and L tv = 1</p><formula xml:id="formula_18">T −1 T −1 i=1 ∇F i→i+1</formula><p>is the total variation loss for flow field smoothing. For these terms, we follow the same definitions of <ref type="bibr" target="#b13">[14]</ref>. The weights λ a and λ t are set to 0.01 and 0.05, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Settings</head><p>Dataset preparation. We use two datasets for evaluation. First, we evaluate on the GoPro <ref type="bibr" target="#b24">[25]</ref> dataset which is widely adopted for image motion deblurring and recently used by <ref type="bibr" target="#b30">[31]</ref> to benchmark event-based deblurring. To synthesize events reliably, we use the open ESIM event simulator <ref type="bibr" target="#b34">[35]</ref>. We follow the suggested training and testing split. The blurred image is also provided officially by averaging nearby (the number varies from 7 to 13) frames.</p><p>As there lacks a large-scale dataset for evaluating eventbased motion deblurring in real-world scenarios, we capture a novel dataset of urban environment, called Blur-DVS, with a DAVIS240C camera. It hybrids a high speed event sensor with a low frame-rate Active Pixel Sensor (APS) recording intensity images at 180 × 240 . Thus, APS may suffer motion blur in fast moving. We collect two subsets for evaluation. The slow subset consists of 15246 images captured with slow and stable camera movement of relatively static scenes, thus motion blur rarely happens. We synthesize motion blurs by averaging nearby 7 frames, resulting into 2178 pairs with blurred image and sharp sequence. In this manner, we can conduct quantitative benchmarkings. We select 1782 pairs for training, and 396 for testing. The fast subset consists of additional 8 sequences with 740 frames in total, captured under fast camera movement of fast moving scenes to investigate how the proposed approach generalizes to real motion blur. However, there is no groundtruth data available on this subset.</p><p>Method comparison. We conduct extensive comparisons with recent motion deblurring methods with available   results and/or codes. They include image-based methods: DCP <ref type="bibr" target="#b28">[29]</ref>, MBR <ref type="bibr" target="#b41">[42]</ref>, FLO <ref type="bibr" target="#b10">[11]</ref>, DMS <ref type="bibr" target="#b24">[25]</ref>, EVS <ref type="bibr" target="#b14">[15]</ref>, SRN <ref type="bibr" target="#b42">[43]</ref>, SVR <ref type="bibr" target="#b51">[52]</ref> and MPN <ref type="bibr" target="#b49">[50]</ref>, and the state-of-the-art event-based motion deblurring method BHA <ref type="bibr" target="#b30">[31]</ref>. We also compare with three event-based video reconstruction methods, including CIE <ref type="bibr" target="#b37">[38]</ref>, MRL <ref type="bibr" target="#b23">[24]</ref> and the state-of-the-art learning-based approach ETV <ref type="bibr" target="#b35">[36]</ref>. PSNR and SSIM metrics are used for quantitative evaluation. Implementation details. For both datasets, our training adopts a batch size of 2 training pairs and Adam optimizer. The network is trained for 400 epochs, with a learning rate 10 −4 at the beginning and linearly decayed to zero starting from the 200th epoch. All the components of the network are jointly trained from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparisons with State-of-the-Art Models</head><p>On the GoPro dataset, we report the results on both single image deblurring (i.e. only recovering the middle frame) and video reconstruction (i.e. recover all the sharp frames) in <ref type="table" target="#tab_1">Table 1</ref> and 2, respectively. Numbers of other approaches are directly taken from papers. Our approach achieves the top place in both tasks, demonstrating the advantages of event-assisted deblurring than purely relying on images, and the superiority of the proposed framework over physical reconstruction model. We show visual comparisons on two fast moving scenes in <ref type="figure" target="#fig_4">Fig. 4</ref>: while image-based method MPN cannot well address such blur, BHA is sensitive to the noise of events especially along object edges. Our approach generates cleaner and sharper results.</p><p>Note that GoPro dataset mainly presents small to moderate motion blur, thus the blurred input is of good quality and improvement from events is marginal. Thus recent powerful architectures SRN and MPN get very promising results though they do not see events. For this reason, we compare our approach with state-of-the-art methods on the proposed Blur-DVS dataset, in which severe motion blur are more universal. Again, we report results on single image deblurring <ref type="table" target="#tab_3">(Table 3</ref>) and video reconstruction <ref type="table" target="#tab_4">(Table 4)</ref> tasks. Note that for fair comparisons, The learning-based methods SRN, MPN and ETV are finetuned on the training set of Blur-DVS. We also compare with their enhanced versions that see both image and events: for image-based methods SRN and MPN, we concatenate the input blurred image with all the 48 (8 binned frames in each time interval and (7 − 1) intervals) event frames. For the event-based method ETV, we also feed the blurred image along with the events to each of its recurrent reconstruction step. We denote these variants as SRN+, MPN+ and ETV+, respectively.</p><p>In <ref type="table" target="#tab_3">Table 3</ref> and 4, the proposed approach achieves the best results. It also outperforms all the enhanced variants, demonstrating the effectiveness of the proposed framework. <ref type="figure" target="#fig_5">Fig. 5</ref> illustrates that: 1) in case of fast motion, imagebased cues alone are not sufficient, limiting performance of MPN; 2) the physical model BHA is prone to noise and presents unaddressed blur due to the lossy sampling mechanism of events; 3) event-based reconstruction methods CIE, MRL and ETV do not restore scene contrast correctly due to the lack of image guidance and/or the simplified physical model. Our approach does not suffer the mentioned issues, and presents sharper results even than the enhanced image+event variants equipped with powerful architectures.</p><p>Finally, we analyse the generalization behavior to realworld motion blur. As shown in <ref type="figure" target="#fig_6">Fig. 6</ref>, the proposed ap-   proach achieves the best visual quality. We suspect that the explicit modeling of motion deblurring and introduction of strong deblurring priors may alleviate the learning difficulty and avoid potential overfitting in more black-box architectures. In practice we find such improvement consistent on real data, as demonstrated by more results on the fast subset provided in our supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Performance Analysis</head><p>Analysing different components. We isolate the important algorithm components to see ther contributions to the final performance, and summarize the results in <ref type="table">Table 5</ref> and <ref type="figure" target="#fig_7">Fig. 7</ref>. As it shows, each component is necessary to improve the PSNR and SSIM of the results. Using image appearance only without events (App.) cannot deblur the image well. Using events only, on the other hand, recovers plenty of details but intensity contrast is not well recovered (see <ref type="figure" target="#fig_7">Fig. 7</ref> Table 5. Component analysis on the Blur-DVS dataset. "App." and "event" denotes using the blurred image appearance and event data as input, respectively. "MC" and "DEF" refer to the motion compensation and directional event filtering modules, respectively.  <ref type="figure" target="#fig_7">Fig. 7 (c)</ref>). Further incorporating motion compensation (+MC) helps in these aspects as it imposes temporal smoothness. Finally, further introducing the directional event filtering module (+DEF), sharper results and richer details can be generated thanks to the learned boundary guidance. Justification of the DEF module. In <ref type="table" target="#tab_6">Table 6</ref>, we justify the necessity of the proposed directional event filtering module. Here, "w/o guid." does not include boundary guidance in the whole pipeline. On the contrary, "guid   only." discards event features in each sequential deblurring step while using boundary guidance only as additional cue. We further design a variant "+param.", which does not incorporate DEF but has additional convolution layers in the encoder of process network which exceeds the current parameter size. Results show that the learned boundary guidance greatly improves the estimation (from 0.786 to 0.827 in SSIM), and itself without other cues can already leads to promising results. Simply enlarging the network size, however, does not observe meaningful improvement. In <ref type="figure" target="#fig_9">Fig. 8</ref>, we visualize the impact of learned boundary guidance. Note how the network learns to select different time centers according to the scene's motion ( <ref type="figure" target="#fig_9">Fig. 8 (c)</ref>). Boundary guidance improves the sharpness of the scene significantly and recovers missing details ( <ref type="figure" target="#fig_9">Fig. 8 (e</ref>) and (f)).</p><p>Low-light photography. A potential application of the proposed approach is low-light photography, as shown in <ref type="figure">Fig 9.</ref> The short-exposure (13ms) image is light-starved. The long-exposure (104ms) one, however, may suffer severe motion blur. Leveraging event cues, our approach generates natural results without such blur.    <ref type="figure">Figure 9</ref>. Low-light photography using our approach. Images and events are captured with DAVIS240C camera in an indoor scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we propose to extract a video from a severe motion-blurred image under the assistance of events. To this end, a novel deep learning architecture is proposed to effectively fuse appearance and motion cues at both global and local granularity. Furthermore, sharp event boundary guidance is extracted to improve reconstructed details with a novel directional event filtering module. Extensive evaluations show that the proposed approach achieves superior performance than various existing image and event-based methods, on both synthetic and real-world datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>log It(x,y) It−∆t(x,y) &gt; τ, −1, if log It(x,y) It−∆t(x,y) &lt; −τ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The proposed learning framework for event-based motion deblurring. For better visualization, we only assume 4 sharp frames are recovered from the blurred image. Detailed layer and parameter configurations are referred to the supplementary material. Note that the Motion Compensation (MC) module is not illustrated due to the lack of space. See text for detailed description of the architecture. encoder consisting of convolutional blocks and a convolutional LSTM [41] on top to accumulate features along time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>k i=−k α k = 1. The filtered result is obtained by G (p) = k i=−k α k s (p + λkd (p, c (p)) , c (p) + λk) , (9) Motivation of adaptive event sampling. (a) A toy scene where the top line moves down first, after which the bottom line moves up. Events with positive and negative polarities are shown as red and green dots, respectively. (b) The projected image of the scene after the imaging process. Scene boundaries correspond to the latest triggered events, which may vary for different positions, as indicated by arrows. (c) The accumulation map of events.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Visual comparisons on the GoPro dataset. From left to right, we show two examples with the blurred image, results of MPN [50], BHA<ref type="bibr" target="#b30">[31]</ref> and our approach, as well as groundtruth sharp image, respectively. Zoom in for better view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Representative results of two examples generated by different approaches on the slow subset of Blur-DVS dataset. More results can be found in our supplementary material. Zoom in for better view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Representative results generated by different approaches on the fast subset (real-world motion blur) of Blur-DVS dataset. More results can be found in our supplementary material. Zoom in for better view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Visually analysing the contributions of different components on the DVS-Blur dataset. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Visualizing learned boundary guidance. Note how motion boundaries from different time stamps are selected in the attention map (c) (red for large value and blue for small values).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Conv. Encoder Conv. Decoder Feature tensor Conv. LSTM DEF: Directional Event Filtering MC: Motion Compensation Positive/negative events Motion features Image features +</head><label></label><figDesc></figDesc><table><row><cell>Blurred</cell><cell>Eqn. (8)</cell><cell>Event frames 4→3</cell><cell>Event frames 3→2</cell><cell>Event frames 2→1</cell></row><row><cell cols="2">Concat</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Eq. (6)</cell><cell>Eq. (6)</cell><cell>Eq. (6)</cell><cell></cell></row><row><cell>Result I 4</cell><cell></cell><cell>ProcessNet</cell><cell>ProcessNet</cell><cell>ProcessNet</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Directional Event Filtering (DEF)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Single-image motion deblurring performance on the GoPro dataset. Models DCP [29] MBR [42] FLO [11] EVS [15] SRN [43] SVR [52] DMS [25] MPN [50] BHA [31] Ours</figDesc><table><row><cell>PSNR</cell><cell>23.50</cell><cell>25.30</cell><cell>26.05</cell><cell>26.98</cell><cell>30.26</cell><cell>29.18</cell><cell>29.08</cell><cell>31.50</cell><cell>29.06</cell><cell>31.79</cell></row><row><cell>SSIM</cell><cell>0.834</cell><cell>0.851</cell><cell>0.863</cell><cell>0.892</cell><cell>0.934</cell><cell>0.931</cell><cell>0.914</cell><cell>0.948</cell><cell>0.943</cell><cell>0.949</cell></row><row><cell>Input</cell><cell></cell><cell>MPN</cell><cell></cell><cell>BHA</cell><cell></cell><cell>Ours</cell><cell></cell><cell>GT</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Video reconstruction performance on the GoPro dataset.</figDesc><table><row><cell cols="6">Models CIE [38] CIE+SRN  *  EVS [15] BHA [31] Ours</cell></row><row><cell>PSNR</cell><cell>25.84</cell><cell>26.34</cell><cell>25.62</cell><cell>28.49</cell><cell>29.67</cell></row><row><cell>SSIM</cell><cell>0.790</cell><cell>0.809</cell><cell>0.856</cell><cell>0.920</cell><cell>0.927</cell></row></table><note>*A hybird baseline that adopts CIE to reconstruct images first, then SRN to deblur each image. See [31] for details.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Single-image deblurring performance on the Blur-DVS dataset. Models DMS [25] SRN [43] SRN+ * MPN [50] MPN+ * CIE [38] MRL [24] ETV [36] ETV+ SRN+, MPN+ and ETV+ denote enhanced versions of SRN, MPN, ETV respectively. See text for details.</figDesc><table><row><cell>BHA [31] Ours</cell></row></table><note>**</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Video reconstruction performance on Blur-DVS dataset.</figDesc><table><row><cell cols="5">Models CIE [38] MRL [24] ETV [36] ETV+ BHA [31] Ours</cell></row><row><cell>PSNR 18.94</cell><cell>10.57</cell><cell>16.60</cell><cell>24.10 22.06</cell><cell>25.33</cell></row><row><cell>SSIM 0.473</cell><cell>0.194</cell><cell>0.587</cell><cell>0.777 0.699</cell><cell>0.827</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Analysing the directional event filtering module on the DVS-Blur dataset. See text for details.</figDesc><table><row><cell cols="3">Models guid. only w/o guid.</cell><cell>full</cell><cell>+param.</cell></row><row><cell>PSNR</cell><cell>25.16</cell><cell>24.71</cell><cell>25.33</cell><cell>24.64</cell></row><row><cell>SSIM</cell><cell>0.816</cell><cell>0.786</cell><cell>0.827</cell><cell>0.788</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Due to space limit we briefly describe the component design and refer the detailed layer/parameter configurations to our supplementary material.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank the reviewers for their valuable feedback. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A low power, high throughput, fully event-based stereo system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andreopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Flickner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7532" to="7542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Non-uniform blind deblurring by reblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bahat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3306" to="3314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simultaneous optical flow and intensity estimation from an event camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bardow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="884" to="892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Nadaraya-Watson Kernel regression function estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Bierens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Serie Research Memoranda</title>
		<imprint>
			<biblScope unit="issue">0058</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A 240 × 180 130 db 3 µs latency global shutter spatiotemporal vision sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brandli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbrück</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2333" to="2341" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A neural approach to blind motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="221" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">145</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Blind image deblurring with outlier handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2497" to="2505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A unifying contrast maximization framework for event cameras, with applications to motion, depth, and optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3867" to="3876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">From motion blur to motion flow: A deep learning solution for removing heterogeneous motion blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3806" to="3815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast removal of non-uniform camera shake</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="463" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image-toimage translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Super slomo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9000" to="9008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to extract a video sequence from a single motion-blurred image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meishvili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6334" to="6342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3160" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Eventbased High Dynamic Range Image and Very High Frame Rate Video Generation using Conditional Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mostafavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning blind video temporal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="179" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A 128×128 120 db 15 µs latency asynchronous temporal contrast vision sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lichtsteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbrück</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="566" to="576" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adaptive time-slice block-matching optical flow algorithm for dynamic vision sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbrück</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">88</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4473" to="4481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Blind deblurring using internal patch recurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="783" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Event-based moving object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mitrokhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fermüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Parameshwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Real-time intensityimage reconstruction for event cameras using manifold regularisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Munda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reinbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1381" to="1393" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="257" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Blurinvariant deep learning for blind-deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Nimisha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4762" to="4770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Motion deblurring in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chandramouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition (GCPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="65" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning discriminative data fitting functions for blind image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1077" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Blind image deblurring using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1628" to="1636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Simultaneous stereo video deblurring and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6987" to="6996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bringing a blurry frame alive at high frame-rate with an event camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scheerlinck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint estimation of camera pose, depth, deblurring, and super-resolution from a blurred image sequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4623" to="4631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learn to See by Events: RGB Frame Synthesis from Event Cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Borghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02041[cs.CV]</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprints</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Long-term object tracking with a moving event camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">241</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ESIM: an open event camera simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robotics Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Eventsto-video: Bringing modern computer vision to event cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Video deblurring via semantic segmentation and pixel-wise non-linear kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1086" to="1094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Continuous-time intensity estimation using event cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scheerlinck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahony</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Confence on Compututer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Discriminative non-blind deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jancsary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="604" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Photorealistic Image Reconstruction from Hybrid Intensity and Event based Sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Shedligeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mitra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.0614[cs.CV]</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning a convolutional neural network for non-uniform motion blur removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="769" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scale-recurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8174" to="8182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning blind motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wieschollek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P A</forename><surname>Lensch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning high-order filters for efficient blind deconvolution of document photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="734" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Two-phase kernel estimation for robust motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="157" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Image deblurring via extreme channels prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6978" to="6986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Unsupervised Learning of Dense Optical Flow, Depth and Egomotion from Sparse Event Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mitrokhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fermüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Yorke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08625</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprints</note>
	<note>cs.CV]</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Reconstructing surfaces of particlebased fluids using anisotropic kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Turk</surname></persName>
		</author>
		<idno>5:1-5:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep stacked hierarchical multi-patch network for image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5978" to="5986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Non-uniform camera shake removal using a spatially-adaptive sparse penalty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1556" to="1564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring using spatially variant recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2521" to="2529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning deep CNN denoiser prior for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2808" to="2817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Realtime time synchronized event-based stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="438" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Discriminative learning of iteration-wise priors for blind deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3232" to="3240" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
