<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEARNING SPATIO-TEMPORAL REPRESENTATIONS WITH TEMPORAL SQUEEZE POOLING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxi</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<postCode>YO10 5GH</postCode>
									<settlement>York</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<postCode>YO10 5GH</postCode>
									<settlement>York</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LEARNING SPATIO-TEMPORAL REPRESENTATIONS WITH TEMPORAL SQUEEZE POOLING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Convolution Neural Networks (CNN)</term>
					<term>Temporal Squeeze pooling</term>
					<term>video representation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a new video representation learning method, named Temporal Squeeze (TS) pooling, which can extract the essential movement information from a long sequence of video frames and map it into a set of few images, named Squeezed Images. By embedding the Temporal Squeeze pooling as a layer into off-the-shelf Convolution Neural Networks (CNN), we design a new video classification model, named Temporal Squeeze Network (TeSNet). The resulting Squeezed Images contain the essential movement information from the video frames, corresponding to the optimization of the video classification task. We evaluate our architecture on two video classification benchmarks, and the results achieved are compared to the state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Video classification attracts increasing research interest, given its numerous applications. Extracting features characteristic to the movement in the scene is essential in tasks which are required in applications from video surveillance to video summarization, or when attributing the movement of an actor in a movie. We can identify two categories of approaches: using hand-crafted features, and based on deep learning.</p><p>Hand-crafted features, such as 3D histograms of gradients <ref type="bibr" target="#b0">[1]</ref>, scale-invariant spatio-temporal interest points <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> dense trajectories <ref type="bibr" target="#b3">[4]</ref> as well as the dynamics of change in movement and location <ref type="bibr" target="#b4">[5]</ref>, have been used in various video tasks. Bilen et al. <ref type="bibr" target="#b5">[6]</ref> proposed dynamic images that summarize spatio-temporal information of a video clip into a single image which preserves the semantics of the scene in a compact format.</p><p>Convolution Neural Networks (CNN) have been used for learning visual representations in many applications and recently they have been employed successfully for video processing as well. The two-stream video classification model <ref type="bibr" target="#b6">[7]</ref> contains both spatial and temporal processing pipelines. CNNs models containing 3D convolution kernels (3D CNN), such as C3D <ref type="bibr" target="#b7">[8]</ref> and I3D <ref type="bibr" target="#b8">[9]</ref>, represent a promising way for *This paper is accepted to ICASSP 2020. spatio-temporal representation learning. However, 3D CNNs are prone to overfitting when trained on small datasets from scratch. Meanwhile, the training of 3D CNN on large datasets requires very large computational demands, and the model size is quadratic when compared to 2D CNNs used for video processing. Long short-term memory (LSTM) networks <ref type="bibr" target="#b9">[10]</ref>, represent a category of recursive neural networks (RNN) that can learn the long term dependency of time series data and can compensate for the shortcomings of 2D CNNs to some extent. 2D CNN+LSTM <ref type="bibr" target="#b10">[11]</ref> was proposed to capture the spatio-temporal information from videos. However, 2D CNN+LSTM was shown to have lower performance than the two-stream model in action recognition benchmarks, <ref type="bibr" target="#b8">[9]</ref>.</p><p>In this study, a new Temporal Squeeze (TS) pooling methodology, which can be embedded into CNNs, is proposed. The proposed TS pooling approach aggregates the temporal video information into a reduced spatial dimension by means of an optimization approach which preserves the video information characteristics. In this study, TS pooling is optimized for the video classification task. TS pooling can compensate for the shortcomings of the dynamic images <ref type="bibr" target="#b5">[6]</ref>, by controlling the pooling size. In this research study, we demonstrate that the proposed TS pooling mechanism can summarize the visual representation of up to 64 video frames while dynamic images would only process 10 frames. By embedding the temporal squeeze pooling as a layer into the off-the-shelf CNNs, we design a new video classification model named Temporal Squeeze Network (TeSNet). The proposed methodology for representing video information is presented in Section 2. The experimental results are provided in Section 3 and the conclusion is drawn in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">VIDEO INFORMATION REPRESENTATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Temporal Squeeze Pooling</head><p>The proposed approach relies on the observation that consecutive video frames usually contain repeating information, especially either the background for a still camera, or the foreground, when the camera follows a target. A temporal squeeze pooling aims to compress the dynamic information of a video clip with K frames into D frames (D &lt;&lt; K), such that essential information is preserved. Consequently, repeating information is filtered out while pre-arXiv:2002.04685v1 [cs.CV] 11 Feb 2020 serving the essential, usually specific movement patterns. Let X = [x 1 , x 2 , . . . , x K ] denote K video frames where x i ∈ R H×W ×C , i = 1, . . . , K and H, W , C represent the height, width and the number of channels (colour), respectively. The TS layer aims to find out the optimal hyperplane A ∈ R K×D , and map every pixel of X from the vector space of R K onto a much smaller information defining space R D . The aim is to preserve the relevant dynamic information across the temporal direction into the compressed space.</p><p>In the following, the squeeze and excitation operations proposed in <ref type="bibr" target="#b11">[12]</ref> are adopted for the TS pooling. The frame sequence X is initially processed by the squeeze operation, producing a frame descriptor. The squeeze operation is implemented by using global average pooling along the spatial dimensions H, W and C. Then, the squeeze operation is followed by the excitation operation, which is made up of two consecutive fully connected (FC) layers. The output of the excitation operation is reshaped to become the column space of A, which defines a hyperplane. In the squeeze operation, the k-th element of a frame-wise z ∈ R K is calculated by:</p><formula xml:id="formula_0">z k = F sq (x k ) = 1 HW C H i=1 W j=1 C l=1 x k (i, j, l).<label>(1)</label></formula><p>In the excitation operation, the input-specific hyperplane is calculated by:</p><formula xml:id="formula_1">F ex (z, W) = δ 2 (W 2 δ 1 (W 1 z)),<label>(2)</label></formula><p>where δ 1 and δ 2 refer to the activation functions and W 1 ∈ R K×K , W 2 ∈ R KD×K refer to the weights of the FC layers. Then, the output of equation <ref type="formula" target="#formula_1">(2)</ref> is reshaped into a matrix A ∈ R K×D . The input-specific hyperplane for the projection is given by</p><formula xml:id="formula_2">A = Φ(A ),<label>(3)</label></formula><p>where Φ is a function that guarantees A is column independent. We flatten X along its H, W and C dimensions into a vector X = [x 1 ,x 2 , ...,x HW C ] wherex i ∈ R K , and then project it onto the hyperplane A, resulting in a vector X. The i-th element of the projection, x is calculated by:</p><formula xml:id="formula_3">y i = (A T A) −1 A Tx i , x i = Ay i , i = 1, . . . , HW C<label>(4)</label></formula><p>where y i ∈ R D represent the mapping coefficients. We re-</p><formula xml:id="formula_4">shape the vector Y = [y 1 , y 2 , . . . , y HW C ] into a new image sequence Y of D frames of size H × W × C.</formula><p>The squeezed sequence of D frames can be used as a simplified, yet an information comprehensive representation, that summarizes the dynamics taking place in the given set of K video frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Optimization</head><p>In this study, the TS pooling is optimized with respect to the video classification task. In order to ensure that the projection X retains as much meaningful spatio-temporal information as possible from the original video sequence X, X should be close to the original video data X. This relies onto finding the optimal hyperplane A fitting X aiming to minimize the residuals of projections. Let us denote the mean absolute error (MAE) on projections by l proj , calculated as:</p><formula xml:id="formula_5">l proj = 1 HW C HW C i x i − x i ,<label>(5)</label></formula><p>where · represents the standard L 2 norm in the K-dimensional Euclidean space R K .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Temporal Squeeze Network</head><p>The Temporal Squeeze pooling can process not just video frames but also the outputs of convolution layers of a CNN. When it is plugged into off-the-shelf CNNs, it forms a new architecture, named Temporal Squeeze Network (TeSNet). We choose the Inception-ResNet-V2 <ref type="bibr" target="#b12">[13]</ref> as the backbone CNN embedding the TS pooling block. In order to form an endto-end training, we add the loss term l proj from <ref type="formula" target="#formula_5">(5)</ref> to the classification loss used in the original network, resulting in the following loss function:</p><formula xml:id="formula_6">l f inal = l classif + β M i=1 l i proj + λl L2 ,<label>(6)</label></formula><p>where l classif is the cross-entropy loss of the classification <ref type="bibr" target="#b12">[13]</ref>, l L2 is the L2 normalization term of all the trainable weights in the architecture, λ is the weight decay, β is the weight for the TS loss component l proj , where the projection residuals are summed up for all M TS layers. TS layers can be embedded in different sections of the backbone CNN. We design our model by following the principle of decreasing the number of mapped frames D when embedding into a deeper network layer position. In this case, the model represents a pyramidal video processing scheme. The first TS layer should be configured with a relatively larger D generating more frames, and therefore the loss of temporal information caused by successive pyramidal projections would be reduced. We adopt the two-stream architecture <ref type="bibr" target="#b6">[7]</ref>, including an RGB image frame stream and an Optical Flow (OF) stream. For the OF stream we use the TV-L1 optical flow algorithm <ref type="bibr" target="#b13">[14]</ref> and its output is stored as JPEG images, where the colour encodes the optical flow vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset and Implementation Details</head><p>We conduct experiments on two human activity classification benchmarks, UCF101 <ref type="bibr" target="#b14">[15]</ref> and HMDB51 <ref type="bibr" target="#b15">[16]</ref>. UCF101 contains 13,320 real video sequences labelled int 101 classes, collected from YouTube <ref type="bibr" target="#b14">[15]</ref>, while HMDB51 contains 7,000 video clips distributed in 51 action classes <ref type="bibr" target="#b15">[16]</ref>. During the experiments we follow the "three train/test splits" rule, and report the final performance by averaging the top-1 accuracy over the three splits. Our model is pre-trained on ImageNet <ref type="bibr" target="#b16">[17]</ref>. To evaluate our model, we reimplement the Temporal (a) Selection of 10 consecutive frames from video sequence.</p><p>(b) Output of TS <ref type="figure">Fig. 1</ref>. Visualizing the input and the corresponding output of the TS layer with K = 10, D = 2.</p><p>Segment Network (TSN) <ref type="bibr" target="#b17">[18]</ref> with our backbone network. We set the dropout as 0.5 to prevent overfitting and adopt the same data augmentation techniques as in <ref type="bibr" target="#b17">[18]</ref> for network training. The size of the input frames is set to 299 × 299, which is randomly cropped from the resized images, and K consecutive frames are randomly selected from each video sequence. We use Stochastic Gradient Descent for optimizing the network parameters in which the batch size is set to 32, momentum of 0.9, weight decay λ = 4e −5 , β = 10. The initial learning rate is set to 0.001 for the image stream and at 0.005 for the Optical Flow stream. We train the model for 30 epochs, with a ten times reduction for the learning rate when the validation accuracy saturates.</p><p>During testing, we uniformly sample 20 clips from each video clip and perform spatially fully convolutional inference for all clips, and the video-level score is obtained by averaging all the clip prediction scores of a video. For the proposed TeSNet, we set Φ(·) = I in (3), resulting in A = A, while the column independent A is properly initialized. We consider LeakyReLU for δ 2 (·) and the Sigmoid activation function for δ 1 (·) in equation <ref type="bibr" target="#b1">(2)</ref> and these choices are crucial for the performance of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Visualization Analysis</head><p>We explore how the temporal squeeze pooling represents the spatio-temporal information within the video clips by visualizing its outputs. <ref type="figure">In Fig 1,</ref> we show the output of the TS layer with K = 10, D = 2 resulting in 2 squeezed images. The clip, shown on the first row in <ref type="figure">Fig 1a display a clear</ref> salient movement, and we can observe that its corresponding output of TS summarizes the spatio-temporal information, as shown on the first row in <ref type="figure">Fig 1b.</ref> The other clip, shown on the second row, does not contain any obvious movement. When there is no movement present in a video clip, the TS layer captures the characteristic static information about the scene, as shown in the last two images from the second row of <ref type="figure" target="#fig_1">Fig 1b.  Fig 2 depicts</ref> the outputs of the TS layer with K = 10, D = 2. The output of the TS layer with RGB frames is shown in <ref type="figure" target="#fig_1">Fig. 2b</ref>, and the output of the TS layer of optical flow images is shown in <ref type="figure" target="#fig_1">Fig. 2d</ref>. We observe that the output of the TS layer tends to preserve the still information and the motion information separately. This indicates that by considering a single image we may not be able to represent the underlying spatio-temporal information from the video. More-over, when considering D = 3, the classification accuracy is higher than for D = 1, according to the results from <ref type="table" target="#tab_0">Table  1</ref>. This result further demonstrates that summarizing the dynamics of a long video clip into a single image would lose essential spatio-temporal information. A dynamic image <ref type="bibr" target="#b5">[6]</ref> attempts to summarize the entire information from a video clip into a single image, which can explain why they fail to properly represent long video clips.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Embedding the TS layer into the network</head><p>In the following, we explore where and how to embed TS layers into the CNN. The results are shown in <ref type="table" target="#tab_0">Table 1</ref>, where the second column indicates the location for inserting a TS layer with the corresponding D indicated in the third column. A single TS layer, M = 1 is embedded in the settings No. 1 and 2, while M = 2 for the settings No. 3, 4 and 5. The model from No. 1 setting, which embeds a TS layer directly after the inputs of the network, achieves the best result in all settings. However, the model with No. 5 setting which embeds two TS layers into the backbone network requires less computation and has almost the same performance as the No. 1 setting. When a lower level of computation complexity is required, then No. 5 setting is preferable to be used. When inserting the TS layers into the middle section of the backbone network it leads to worse performance. One possible explanation is that the network was initially pretrained on Im-ageNet and then the inserted TS layers did not fit well with the settings of these pretrained kernels and resulted in poor performance. To avoid this problem, the model including its TS layers has to be pretrained on a large video dataset. We also explore how the length of the input video clip affects the performance of our model. we consider a rather small batch size of 8 and a maximum clip length of 64 because of the GPU memory limitation. For a clip length of 64, we adopt the setting No. 5 from <ref type="table" target="#tab_0">Table 1</ref>   <ref type="table">Table 3</ref>. Performance of different architectures with twostream on the split 1 of UCF101 database.</p><p>We also evaluate TeSNet by comparing with the baseline and TSN <ref type="bibr" target="#b17">[18]</ref> whose the backbone network is Inception-ResNet-v2. The results provided by different architectures and streams are shown in <ref type="table">Table 3</ref>. We can observe that the fusion of the RGB and OF streams with TeSNet successfully boosts up the top-1 accuracy from 92.5% to 95.2% on the split 1 of UCF101, and outperforms that of TSN (Inception-ResNet-v2) by 2.3% which demonstrates the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>UCF101 HMDB51 iDT+Fisher vector <ref type="bibr" target="#b18">[19]</ref> 84.8 57.2 iDT+HSV <ref type="bibr" target="#b19">[20]</ref> 87.9 61.1 C3D+iDT+SVM <ref type="bibr" target="#b7">[8]</ref> 90.4 -Two-Stream (fusion by SVM) <ref type="bibr" target="#b6">[7]</ref> 88.0 59.4 Two-Stream Fusion+iDT <ref type="bibr" target="#b20">[21]</ref> 93.5 69.2 TSN (BN-Inception) <ref type="bibr" target="#b17">[18]</ref> 94.2 69.4 Two-Stream I3D <ref type="bibr" target="#b8">[9]</ref> 93.4 66.4 TDD+iDT <ref type="bibr" target="#b21">[22]</ref> 91.5 65.9 Dynamic Image Network <ref type="bibr" target="#b5">[6]</ref> 95.5 72.5 Temporal Squeeze Network 95.2 71.5 <ref type="table">Table 4</ref>. Temporal squeeze network compared with other methods on UCF101 and HMDB51, in terms of top-1 accuracy, averaged over three splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Comparison with the state-of-the-art</head><p>For fare comparisons, we only consider those models that are pre-trained on ImageNet <ref type="bibr" target="#b16">[17]</ref>. The results are provided in <ref type="table">Table 4</ref>. The proposed TeSNet achieves 95.2% top-1 accuracy on UCF101 and 71.5% on HMDB51, which outperforms TSN (BN-Inception) by 1% and 2.1% on UCF101 and HMDB51, respectively. As the dynamic image network fuses the prediction scores of four streams using a better backbone network architecture, while the proposed model only uses two-streams, and therefore the results are not directly comparable. Nevertheless, the advantage of our proposed method is that we can control the number of frames for the output of the TS layer, while the dynamic image method <ref type="bibr" target="#b5">[6]</ref> can only summarize a part of the spatio-temporal information into a single image. The proposed TeSNet method can represent the information through TS pooling from as many as 64 frames, unlike in <ref type="bibr" target="#b5">[6]</ref>, where the dynamic image method would show performance degradation when processing more than 20 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>In this paper, we propose a new video representation scheme, while aiming to improve video classification tasks, called Temporal Squeeze (TS) pooling. By embedding the TS layer into off-the-shelf CNNs, the network learns spatio-temporal features, characteristic to discriminating classes of video sequences. We have investigated various locations in the structure of the CNN network for embedding the TS layers. Experiments have been performed on both UCF101 and HMDB51 datasets and the results indicate that the temporal squeezed representations are compact and meaningful for improving the video classification performance. The proposed temporal squeeze layers can be embedded into a wide range of CNN networks, leading to a video summarization which is optimized with respect to the video classification task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Given input video frames, flow images and the corresponding outputs for the TS layers K = 10, D = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Evaluating the accuracy when embedding the TS layer at different depths of the CNN.</figDesc><table><row><cell cols="2">No. Location of</cell><cell cols="2">Number of squeezed Top-1</cell></row><row><cell></cell><cell>TS layer</cell><cell>frames (D i )</cell><cell>(%)</cell></row><row><cell>1</cell><cell>Input</cell><cell>D 1 = 3</cell><cell>85.4</cell></row><row><cell>2</cell><cell>Input</cell><cell>D 1 = 1</cell><cell>83.1</cell></row><row><cell>3</cell><cell>Conv2d 1a 3x3 Conv2d 4a 3x3</cell><cell>D 1 = 3 D 2 = 1</cell><cell>81.7</cell></row><row><cell>4</cell><cell>Conv2d 1a 3x3 Block A</cell><cell>D 1 = 3 D 2 = 1</cell><cell>84.9</cell></row><row><cell>5</cell><cell>Conv2d 1a 3x3 Block B</cell><cell>D 1 = 3 D 2 = 1</cell><cell>85.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>but consider D 1 = 16 and D 2 = 4. When considering clip lengths of 10 or 16, we use the first setting fromTable 1. The results are shown inTable 2. It can be observed that when increasing the length of the video clip, the performance is improving as well. Comparing the effect of various clip length of videos on RGB stream on the split 1 of UCF101 database.</figDesc><table><row><cell></cell><cell cols="2">clip length Classif. (%)</cell><cell></cell></row><row><cell></cell><cell>10</cell><cell>85.3</cell><cell></cell></row><row><cell></cell><cell>16</cell><cell>86.2</cell><cell></cell></row><row><cell></cell><cell>64</cell><cell>87.8</cell><cell></cell></row><row><cell cols="4">Architecture length RGB OF RGB + OF</cell></row><row><cell>Baseline</cell><cell>1</cell><cell>83.5 85.4</cell><cell>92.5</cell></row><row><cell>TSN</cell><cell>3</cell><cell>85.0 85.1</cell><cell>92.9</cell></row><row><cell>TeSNet</cell><cell>64</cell><cell>87.8 88.2</cell><cell>95.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A spatiotemporal descriptor based on 3d-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the British Machine Vision Conf. (BMVC)</title>
		<meeting>of the British Machine Vision Conf. (BMVC)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="995" to="1004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Comp. Vision</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An efficient dense and scale-invariant spatio-temporal interest point detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. on Comp. Vision (ECCV)</title>
		<meeting>Eur. Conf. on Comp. Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">5303</biblScope>
			<biblScope unit="page" from="650" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int. conf. on Comp. vision</title>
		<meeting>of IEEE Int. conf. on Comp. vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modelling of interactions for the recognition of activities in groups of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stephens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="34" to="46" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Action recognition with dynamic image networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2799" to="2813" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Proc. Systems (NIPS)</title>
		<meeting>Advances in Neural Inf. . Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Int. Conf. on Comp. Vision (ICCV)</title>
		<meeting>IEEE Int. Conf. on Comp. Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Comp. Vision and Pattern Recognition (CVPR</title>
		<meeting>of IEEE Comp. Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Comp. Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Comp. Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Comp. Vision and Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conf. on Comp. Vision and Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. on Artificial Intelligence</title>
		<meeting>AAAI Conf. on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime TV-L1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Joint Pattern Recog. Symp</title>
		<meeting>Joint Pattern Recog. Symp</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">4713</biblScope>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Comp. Vision (ICCV)</title>
		<meeting>IEEE Int. Conf. on Comp. Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comp. Vision and Pattern Recognition</title>
		<meeting>IEEE Comp. Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9912</biblScope>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Action recognition with stacked fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="581" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page" from="109" to="125" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Comp. Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Comp. Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Comp vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE Conf. on Comp vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
