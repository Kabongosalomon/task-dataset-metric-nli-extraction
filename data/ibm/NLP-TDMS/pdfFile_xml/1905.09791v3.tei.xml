<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-relational Poincaré Graph Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balažević</surname></persName>
							<email>ivana.balazevic@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
							<email>carl.allen@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
							<email>t.hospedales@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Samsung AI Centre</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-relational Poincaré Graph Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hyperbolic embeddings have recently gained attention in machine learning due to their ability to represent hierarchical data more accurately and succinctly than their Euclidean analogues. However, multi-relational knowledge graphs often exhibit multiple simultaneous hierarchies, which current hyperbolic models do not capture. To address this, we propose a model that embeds multi-relational graph data in the Poincaré ball model of hyperbolic space. Our Multi-Relational Poincaré model (MuRP) learns relation-specific parameters to transform entity embeddings by Möbius matrix-vector multiplication and Möbius addition. Experiments on the hierarchical WN18RR knowledge graph show that our Poincaré embeddings outperform their Euclidean counterpart and existing embedding methods on the link prediction task, particularly at lower dimensionality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hyperbolic space can be thought of as a continuous analogue of discrete trees, making it suitable for modelling hierarchical data <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b9">10]</ref>. Various types of hierarchical data have recently been embedded in hyperbolic space <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32]</ref>, requiring relatively few dimensions and achieving promising results on downstream tasks. This demonstrates the advantage of modelling tree-like structures in spaces with constant negative curvature (hyperbolic) over zero-curvature spaces (Euclidean).</p><p>Certain data structures, such as knowledge graphs, often exhibit multiple hierarchies simultaneously. For example, lion is near the top of the animal food chain but near the bottom in a tree of taxonomic mammal types <ref type="bibr" target="#b21">[22]</ref>. Despite the widespread use of hyperbolic geometry in representation learning, the only existing approach to embedding hierarchical multi-relational graph data in hyperbolic space <ref type="bibr" target="#b30">[31]</ref> does not outperform Euclidean models. The difficulty with representing multi-relational data in hyperbolic space lies in finding a way to represent entities (nodes), shared across relations, such that they form a different hierarchy under different relations, e.g. nodes near the root of the tree under one relation may be leaf nodes under another. Further, many state-of-the-art approaches to modelling multi-relational data, such as DistMult <ref type="bibr" target="#b36">[37]</ref>, ComplEx <ref type="bibr" target="#b33">[34]</ref>, and TuckER <ref type="bibr" target="#b1">[2]</ref> (i.e. bilinear models), rely on inner product as a similarity measure and there is no clear correspondence to the Euclidean inner product in hyperbolic space <ref type="bibr" target="#b31">[32]</ref> by which these models can be converted. Existing translational approaches that use Euclidean distance to measure similarity, such as TransE <ref type="bibr" target="#b5">[6]</ref> and STransE <ref type="bibr" target="#b22">[23]</ref>, can be converted to the hyperbolic domain, but do not currently compete with the bilinear models in terms of predictive performance. However, it has recently been shown in the closely related field of word embeddings <ref type="bibr" target="#b0">[1]</ref> that the difference (i.e. relation) between word pairs that form analogies manifests as a vector offset, suggesting a translational approach to modelling relations.</p><p>In this paper, we propose MuRP, a theoretically inspired method to embed hierarchical multi-relational data in the Poincaré ball model of hyperbolic space. By considering the surface area of a hypersphere of increasing radius centered at a particular point, Euclidean space can be seen to "grow" polynomially, whereas in hyperbolic space the equivalent growth is exponential <ref type="bibr" target="#b9">[10]</ref>. Therefore, moving outwards from the root of a tree, there is more "room" to separate leaf nodes in hyperbolic space than in Euclidean. MuRP learns relation-specific parameters that transform entity embeddings by Möbius matrix-vector multiplication and Möbius addition <ref type="bibr" target="#b34">[35]</ref>. The model outperforms not only its Euclidean counterpart, but also current state-of-the-art models on the link prediction task on the hierarchical WN18RR dataset. We also show that our Poincaré embeddings require far fewer dimensions than Euclidean embeddings to achieve comparable performance. We visualize the learned embeddings and analyze the properties of the Poincaré model compared to its Euclidean analogue, such as convergence rate, performance per relation, and influence of embedding dimensionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and preliminaries</head><p>Multi-relational link prediction A knowledge graph is a multi-relational graph representation of a collection F of facts in triple form (e s , r, e o ) ∈ E ×R×E, where E is the set of entities (nodes) and R is the set of binary relations (typed directed edges) between them. If (e s , r, e o ) ∈ F, then subject entity e s is related to object entity e o by relation r. Knowledge graphs are often incomplete, so the aim of link prediction is to infer other true facts. Typically, a score function φ : E ×R×E → R is learned, that assigns a score s = φ(e s , r, e o ) to each triple, indicating the strength of prediction that a particular triple corresponds to a true fact. A non-linearity, such as the logistic sigmoid function, is often used to convert the score to a predicted probability p = σ(s) ∈ [0, 1] of the triple being true.</p><p>Knowledge graph relations exhibit multiple properties, such as symmetry, asymmetry, and transitivity. Certain knowledge graph relations, such as hypernym and has_part, induce a hierarchical structure over entities, suggesting that embedding them in hyperbolic rather than Euclidean space may lead to improved representations <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32]</ref>. Based on this intuition, we focus on embedding multi-relational knowledge graph data in hyperbolic space.  The model predicts the triple (e s , r, e o ) as true and (e s , r, e o ) as false. (c) Each entity embedding has a sphere of influence, whose radius is determined by the entity-specific bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperbolic geometry of the Poincaré ball The Poincaré ball</head><formula xml:id="formula_0">(B d c , g B ) of radius 1/ √ c, c &gt; 0 is a d-dimensional manifold B d c = {x ∈ R d : c x 2 &lt; 1} equipped with the Riemannian metric g B which is conformal to the Euclidean metric g E = I d with the conformal factor λ c x = 2/(1 − c x 2 ), i.e. g B = (λ c x ) 2 g E .</formula><p>The distance between two points x, y ∈ B d c is measured along a geodesic (i.e. shortest path between the points, see <ref type="figure" target="#fig_1">Figure 1a</ref>) and is given by:</p><formula xml:id="formula_1">d B (x, y) = 2 √ c tanh −1 ( √ c − x ⊕ c y ),<label>(1)</label></formula><p>where · denotes the Euclidean norm and ⊕ c represents Möbius addition <ref type="bibr" target="#b34">[35]</ref>:</p><formula xml:id="formula_2">x ⊕ c y = (1 + 2c x, y + c y 2 )x + (1 − c x 2 )y 1 + 2c x, y + c 2 x 2 y 2 ,<label>(2)</label></formula><p>with ·, · being the Euclidean inner product. Ganea et al. <ref type="bibr" target="#b12">[13]</ref> show that Möbius matrix-vector multiplication can be obtained by projecting a point x ∈ B d c onto the tangent space at 0 ∈ B d c with the logarithmic map log c 0 (x), performing matrix multiplication by M ∈ R d×k in the Euclidean tangent space, and projecting back to B d c via the exponential map at 0, i.e.:</p><formula xml:id="formula_3">M ⊗ c x = exp c 0 (Mlog c 0 (x)).<label>(3)</label></formula><p>For the definitions of exponential and logarithmic maps, see Appendix A.</p><p>3 Related work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hyperbolic geometry</head><p>Embedding hierarchical data in hyperbolic space has recently gained popularity in representation learning. Nickel and Kiela <ref type="bibr" target="#b24">[25]</ref> first embedded the transitive closure 1 of the WordNet noun hierarchy, in the Poincaré ball, showing that low-dimensional hyperbolic embeddings can significantly outperform higher-dimensional Euclidean embeddings in terms of both representation capacity and generalization ability. The same authors subsequently embedded hierarchical data in the Lorentz model of hyperbolic geometry <ref type="bibr" target="#b25">[26]</ref>.</p><p>Ganea et al. <ref type="bibr" target="#b12">[13]</ref> introduced Hyperbolic Neural Networks, connecting hyperbolic geometry with deep learning. They build on the definitions for Möbius addition, Möbius scalar multiplication, exponential and logarithmic maps of Ungar <ref type="bibr" target="#b34">[35]</ref> to derive expressions for linear layers, bias translation and application of non-linearity in the Poincaré ball. Hyperbolic analogues of several other algorithms have been developed since, such as Poincaré GloVe <ref type="bibr" target="#b31">[32]</ref> and Hyperbolic Attention Networks <ref type="bibr" target="#b15">[16]</ref>. More recently, Gu et al. <ref type="bibr" target="#b14">[15]</ref> note that data can be non-uniformly hierarchical and learn embeddings on a product manifold with components of different curvature: spherical, hyperbolic and Euclidean. To our knowledge, only Riemannian TransE <ref type="bibr" target="#b30">[31]</ref> seeks to embed multi-relational data in hyperbolic space, but the Riemannian translation method fails to outperform Euclidean baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Link prediction for knowledge graphs</head><p>Bilinear models typically represent relations as linear transformations acting on entity vectors. An early model, RESCAL <ref type="bibr" target="#b23">[24]</ref>, optimizes a score function φ(e s , r, e o ) = e s M r e o , containing the bilinear product between the subject entity embedding e s , a full rank relation matrix M r and the object entity embedding e o . RESCAL is prone to overfitting due to the number of parameters per relation being quadratic relative to the number per entity. DistMult <ref type="bibr" target="#b36">[37]</ref> is a special case of RESCAL with diagonal relation matrices, reducing parameters per relation and controlling overfitting. However, due to its symmetry, DistMult cannot model asymmetric relations. ComplEx <ref type="bibr" target="#b33">[34]</ref> extends DistMult to the complex domain, enabling asymmetry to be modelled. TuckER <ref type="bibr" target="#b1">[2]</ref> performs a Tucker decomposition of the tensor of triples, which enables multi-task learning between different relations via the core tensor. The authors show each of the linear models above to be a special case of TuckER.</p><p>Translational models regard a relation as a translation (or vector offset) from the subject to the object entity embeddings. These models include TransE <ref type="bibr" target="#b5">[6]</ref> and its many successors, e.g. FTransE <ref type="bibr" target="#b11">[12]</ref>, STransE <ref type="bibr" target="#b22">[23]</ref>. The score function for translational models typically considers Euclidean distance between the translated subject entity embedding and the object entity embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multi-relational Poincaré embeddings</head><p>A set of entities can form different hierarchies under different relations. In the WordNet knowledge graph <ref type="bibr" target="#b21">[22]</ref>, the hypernym, has_part and member_meronym relations each induce different hierarchies over the same set of entities. For example, the noun chair is a parent node to different chair types (e.g. folding_chair, armchair) under the relation hypernym and both chair and its types are parent nodes to parts of a typical chair (e.g. backrest, leg) under the relation has_part. An ideal embedding model should capture all hierarchies simultaneously.</p><p>Score function As mentioned above, bilinear models measure similarity between the subject entity embedding (after relation-specific transformation) and an object entity embedding using the Euclidean inner product <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b1">2]</ref>. However, a clear correspondence to the Euclidean inner product does not exist in hyperbolic space <ref type="bibr" target="#b31">[32]</ref>. The Euclidean inner product can be expressed as a function of Euclidean distance and norms, i.e. x, y = 1</p><formula xml:id="formula_4">2 (−d E (x, y) 2 + x 2 + y 2 ), d E (x, y) = x − y .</formula><p>Noting this, in Poincaré GloVe, Tifrea et al. <ref type="bibr" target="#b31">[32]</ref> absorb squared norms into biases b x , b y and replace the Euclidean with the Poincaré distance d B (x, y) to obtain the hyperbolic version of GloVe <ref type="bibr" target="#b26">[27]</ref>.</p><p>Separately, it has recently been shown in the closely related field of word embeddings that statistics pertaining to analogies naturally contain linear structures <ref type="bibr" target="#b0">[1]</ref>, explaining why similar linear structure appears amongst word embeddings of word2vec <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b18">19]</ref>. Analogies are word relationships of the form "w a is to w * a as w b is to w * b ", such as "man is to woman as king is to queen", and are in principle not restricted to two pairs (e.g. "...as brother is to sister"). It can be seen that analogies have much in common with relations in multi-relational graphs, as a difference between pairs of words (or entities) common to all pairs, e.g. if (e s , r, e o ) and (e s , r, e o ) hold, then we could say "e s is to e o as e s is to e o ". Of particular relevance is the demonstration that the common difference, i.e. relation, between the word pairs (e.g. (man, woman) and (king, queen)) manifests as a common vector offset <ref type="bibr" target="#b0">[1]</ref>, justifying the previously heuristic translational approach to modelling relations.</p><p>Inspired by these two ideas, we define the basis score function for multi-relational graph embedding:</p><formula xml:id="formula_5">φ(e s , r, e o ) = −d(e (r) s , e (r) o ) 2 + b s + b o = −d(Re s , e o + r) 2 + b s + b o ,<label>(4)</label></formula><p>where d : </p><formula xml:id="formula_6">E ×R×E → R + is a distance function, e s , e o ∈ R</formula><formula xml:id="formula_7">φ MuRP (e s , r, e o ) = −d B (h (r) s , h (r) o ) 2 + b s + b o = −d B (exp c 0 (Rlog c 0 (h s )), h o ⊕ c r h ) 2 + b s + b o ,<label>(5)</label></formula><p>where  <ref type="figure" target="#fig_1">Figure 1b</ref>). Since biases are subject and object entity-specific, each subject-object pair induces a different decision boundary. The relation-specific parameters R and r determine the position of the relation-adjusted embeddings, but the radius of the entity-specific decision boundary is independent of the relation. The score function in Equation 4 resembles the score functions of existing translational models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23]</ref>, with the main difference being the entity-specific biases, which can be seen to change the geometry of the model. Rather than considering an entity as a point in space, each bias defines an entity-specific sphere of influence surrounding the center given by the embedding vector (see <ref type="figure" target="#fig_1">Figure 1c</ref>). The overlap between spheres measures relatedness between entities. We can thus think of each relation as moving the spheres of influence in space, so that only the spheres of subject and object entities that are connected under that relation overlap.</p><formula xml:id="formula_8">h s , h o ∈ B d</formula><formula xml:id="formula_9">√ b s + b o (see</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training and Riemannian optimization</head><p>We use the standard data augmentation technique <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b1">2]</ref> of adding reciprocal relations for every triple, i.e. we add (e o , r −1 , e s ) for every (e s , r, e o ). To train both models, we generate k negative samples for each true triple (e s , r, e o ), where we corrupt either the object (e s , r, e o ) or the subject (e o , r −1 , e s ) entity with a randomly chosen entity from the set of all entities E. Both models are trained to minimize the Bernoulli negative log-likelihood loss:</p><formula xml:id="formula_10">L(y, p) = − 1 N N i=1 (y (i) log(p (i) ) + (1 − y (i) )log(1 − p (i) )),<label>(6)</label></formula><p>where p is the predicted probability, y is the binary label indicating whether a sample is positive or negative and N is the number of training samples.</p><p>For fairness of comparison, we optimize the Euclidean model using stochastic gradient descent (SGD) and the hyperbolic model using Riemannian stochastic gradient descent (RSGD) <ref type="bibr" target="#b4">[5]</ref>. We note that the Riemannian equivalent of adaptive optimization methods has recently been developed <ref type="bibr" target="#b2">[3]</ref>, but leave replacing SGD and RSGD with their adaptive equivalent to future work. To compute the Riemannian gradient ∇ R L, the Euclidean gradient ∇ E L is multiplied by the inverse of the Poincaré metric tensor, i.e.</p><formula xml:id="formula_11">∇ R L = 1/(λ c θ ) 2 ∇ E L.</formula><p>Instead of the Euclidean update step θ ← θ − η∇ E L, a first order approximation of the true Riemannian update, we use exp c θ to project the gradient ∇ R L ∈ T θ B d c onto its corresponding geodesic on the Poincaré ball and compute the Riemannian update θ ← exp c θ (−η∇ R L), where η denotes the learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>To evaluate both Poincaré and Euclidean models, we first test their performance on the knowledge graph link prediction task using standard WN18RR and FB15k-237 datasets:</p><p>FB15k-237 <ref type="bibr" target="#b32">[33]</ref> is a subset of Freebase <ref type="bibr" target="#b3">[4]</ref>, a collection of real world facts, created from FB15k <ref type="bibr" target="#b5">[6]</ref> by removing the inverse of many relations from validation and test sets to make the dataset more challenging. FB15k-237 contains 14,541 entities and 237 relations.</p><p>WN18RR <ref type="bibr" target="#b10">[11]</ref> is a subset of WordNet <ref type="bibr" target="#b21">[22]</ref>, a hierarchical collection of relations between words, created in the same way as FB15k-237 from WN18 <ref type="bibr" target="#b5">[6]</ref>, containing 40,943 entities and 11 relations.</p><p>To demonstrate the usefulness of MuRP on hierarchical datasets (given WN18RR is hierarchical and FB15k-237 is not, see Section 5.3), we also perform experiments on NELL-995 <ref type="bibr" target="#b35">[36]</ref>, containing 75,492 entities and 200 relations, ∼ 22% of which hierarchical. We create several subsets of the original dataset by varying the proportion of non-hierarchical relations, as described in Appendix B.</p><p>We evaluate each triple from the test set by generating n e (where n e denotes number of entities in the dataset) evaluation triples, which are created by combining the test entity-relation pair with all possible entities E. The scores obtained for each evaluation triple are ranked. All true triples are removed from the evaluation triples apart from the current test triple, i.e. the commonly used filtered setting <ref type="bibr" target="#b5">[6]</ref>. We evaluate our models using the evaluation metrics standard across the link prediction literature: mean reciprocal rank (MRR) and hits@k, k ∈ {1, 3, 10}. Mean reciprocal rank is the average of the inverse of a mean rank assigned to the true triple over all n e evaluation triples. Hits@k measures the percentage of times the true triple appears in the top k ranked evaluation triples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation details</head><p>We implement both models in PyTorch and make our code, as well as all the subsets of the NELL-995 dataset, publicly available. <ref type="bibr" target="#b1">2</ref> We choose the learning rate from {1, 5, 10, 20, 50, 100} by MRR on the validation set and find that the best learning rate is 50 for WN18RR and 10 for FB15k-237 for both models. We initialize all embeddings near the origin where distances are small in hyperbolic space, similar to <ref type="bibr" target="#b24">[25]</ref>. We set the batch size to 128 and the number of negative samples to 50. In all experiments, we set the curvature of MuRP to c = 1, since preliminary experiments showed that any material change reduced performance. <ref type="table" target="#tab_0">Table 1</ref> shows the results obtained for both datasets. As expected, MuRE performs slightly better on the non-hierarchical FB15k-237 dataset, whereas MuRP outperforms on WN18RR which contains  <ref type="bibr" target="#b1">[2]</ref> note that the two models perform comparably), primarily due to multi-task learning across relations. This is highly advantageous on FB15k-237 due to a large number of relations compared to WN18RR and thus relatively little data per relation in some cases. As the first model to successfully represent multiple relations in hyperbolic space, MuRP does not also set out to include multi-task learning, but we hope to address this in future work. Further experiments on NELL-995, which substantiate our claim on the advantage of embedding hierarchical multi-relational data in hyperbolic over Euclidean space, are presented in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Link prediction results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">MuRE vs MuRP</head><p>Effect of dimensionality We compare the MRR achieved by MuRE and MuRP on WN18RR for embeddings of different dimensionalities d ∈ {5, 10, 15, 20, 40, 100, 200}. As expected, the difference is greatest at lower embedding dimensionality (see <ref type="figure" target="#fig_5">Figure 2a</ref>). <ref type="figure" target="#fig_5">Figure 2b</ref> shows the MRR per epoch for MuRE and MuRP on the WN18RR training and validation sets, showing that MuRP also converges faster.  Model architecture ablation study <ref type="table" target="#tab_1">Table 2</ref> shows an ablation study of relation-specific transformations and bias choices. We note that any change to the current model architecture has a negative effect on performance of both MuRE and MuRP. Replacing biases by the (transformed) entity embedding norms leads to a significant reduction in performance of MuRP, in part because norms are constrained to [0, 1), whereas the biases they replace are unbounded. Performance per relation Since not every relation in WN18RR induces a hierarchical structure over the entities, we report the Krackhardt hierarchy score (Khs) <ref type="bibr" target="#b16">[17]</ref> of the entity graph formed by each relation to obtain a measure of the hierarchy induced. The score is defined only for directed networks and measures the proportion of node pairs (x, y) where there exists a directed path x → y, but not y → x (see Appendix D for further details). The score takes a value of one for all directed acyclic graphs, and zero for cycles and cliques. We also report the maximum and average shortest path between any two nodes in the graph for hierarchical relations. To gain insight as to which relations benefit most from embedding entities in hyperbolic space, we compare hits@10 per relation of MuRE and MuRP for entity embeddings of low dimensionality (d = 20). From <ref type="table" target="#tab_2">Table 3</ref> we see that both models achieve comparable performance on non-hierarchical, symmetric relations with the Krackhardt hierarchy score 0, such as verb_group, whereas MuRP generally outperforms MuRE on hierarchical relations. We also see that the difference between the performances of MuRE and MuRP is generally larger for relations that form deeper trees, fitting the hypothesis that hyperbolic space is of most benefit for modelling hierarchical relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convergence rate</head><p>Computing the Krackhardt hierarchy score for FB15k-237, we find that 80% of the relations have Khs = 1, however, the average of maximum path lengths over those relations is 1.14 with only 2.7% relations having paths longer than 2, meaning that the vast majority of relational sub-graphs consist of directed edges between pairs of nodes, rather than trees.  <ref type="figure">Figure 3</ref>. This shows an overall correlation between embedding vector norm and bias (or radius of the sphere of influence) for both MuRE and MuRP. This makes sense intuitively, as the sphere of influence increases to "fill out the space" in regions that are less cluttered, i.e. further from the origin. <ref type="figure">Figure 4</ref> shows a 40-dimensional subject embedding for the word asia and a random subset of 1500 object embeddings for the hierarchical WN18RR relation has_part, projected to 2 dimensions so that distances and angles of object entity embeddings relative to the subject entity embedding are preserved (see Appendix E for details on the projection method). We show subject and object entity embeddings before and after relation-specific transformation. For both MuRE and MuRP, we see that applying the relation-specific transformation separates true object entities from false ones. However, in the Poincaré model, where distances increase further from the origin, embeddings are moved further towards the boundary of the disk, where, loosely speaking, there is more space to separate and therefore distinguish them. Analysis of wrong predictions Here we analyze the false positives and false negatives predicted by both models. MuRP predicts 15 false positives and 0 false negatives, whereas MuRE predicts only 2 false positives and 1 false negative, so seemingly performs better. However, inspecting the alleged false positives predicted by MuRP, we find they are all countries on the Asian continent (e.g. sri_lanka, palestine, malaysia, sakartvelo, thailand), so are actually correct, but missing from the dataset. MuRE's predicted false positives (philippines and singapore) are both also correct but missing, whereas the false negative (bahrain) is indeed falsely predicted. We note that this suggests current evaluation methods may be unreliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial layout</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and future work</head><p>We introduce a novel, theoretically inspired, translational method for embedding multi-relational graph data in the Poincaré ball model of hyperbolic geometry. Our multi-relational Poincaré model MuRP learns relation-specific parameters to transform entity embeddings by Möbius matrix-vector multiplication and Möbius addition. We show that MuRP outperforms its Euclidean counterpart MuRE and existing models on the link prediction task on the hierarchical WN18RR knowledge graph dataset, and requires far lower dimensionality to achieve comparable performance to its Euclidean analogue. We analyze various properties of the Poincaré model compared to its Euclidean analogue and provide insight through a visualization of the learned embeddings.</p><p>Future work may include investigating the impact of recently introduced Riemannian adaptive optimization methods compared to Riemannian SGD. Also, given not all relations in a knowledge graph are hierarchical, we may look into combining the Euclidean and hyperbolic models to produce mixed-curvature embeddings that best fit the curvature of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Poincaré ball model of hyperbolic geometry</head><p>The Poincaré ball model is one of five isometric models of hyperbolic geometry <ref type="bibr" target="#b6">[7]</ref>, each offering different perspectives for performing mathematical operations in hyperbolic space. The isometry means there exists a one-to-one distance-preserving mapping from the metric space of one model (X , d) onto that of another (X , d ), where X , X are sets and d, d distance functions, or metrics, providing a notion of equivalence between the models. For the Poincaré ball, these are defined <ref type="bibr" target="#b12">[13]</ref> as:</p><formula xml:id="formula_12">exp c x (v) = x ⊕ c tanh √ c λ c x v 2 v √ c v<label>(7)</label></formula><p>log c  As expected, the difference between model performances gets smaller as we increase the number of non-hierarchical relations and is the smallest on NELL-995-h25. For d = 200, MuRE starts to perform comparably (or even outperforms on some metrics) to MuRP on NELL-995-h50 and outperforms on NELL-995-h25. These results substantiate our claim on the advantage of embedding hierarchical data in hyperbolic space, particularly in a scenario where low embedding dimensionality is required.    </p><formula xml:id="formula_13">x (y) = 2 √ cλ c x tanh −1 ( √ c − x ⊕ c y ) −x ⊕ c y − x ⊕ c y .<label>(8</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Krackhardt hierarchy score</head><p>Let R ∈ R n×n be the binary reachability matrix of a directed graph G with n nodes, with R i,j = 1 if there exists a directed path from node i to node j and 0 otherwise. The Krackhardt hierarchy score of G [17] is defined as:</p><formula xml:id="formula_14">Khs G = n i=1 n j=1 1(R i,j == 1 ∧ R j,i == 0) n i=1 n j=1 1(R i,j == 1)</formula><p>.</p><p>(9)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Dimensionality reduction method</head><p>To project high-dimensional embeddings to 2 dimensions for visualization purposes, we use the following method to compute dimensions x, y for projection e i of entity e i :</p><p>• e x i = es es e i , i ∈ {s, o 0 , o 1 , ..., o N }, where e s is the original high-dimensional subject entity embedding and N is the number of object entity embeddings. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>(a) Geodesics in the Poincaré disk, indicating the shortest paths between pairs of points. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>d are the embeddings and b s , b o ∈ R scalar biases of the subject and object entities e s and e o respectively. R ∈ R d×d is a diagonal relation matrix and r ∈ R d a translation vector (i.e. vector offset) of relation r. e (r) s = Re s and e (r) o = e o + r represent the subject and object entity embeddings after applying the respective relation-specific transformations, a stretch by R to e s and a translation by r to e o . Hyperbolic model Taking the hyperbolic analogue of Equation 4, we define the score function for our Multi-Relational Poincaré (MuRP) model as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>c are hyperbolic embeddings of the subject and object entities e s and e o respectively, and r h ∈ B d c is a hyperbolic translation vector of relation r. The relation-adjusted subject entity embedding h (r) s ∈ B d c is obtained by Möbius matrix-vector multiplication: the original subject entity embedding h s ∈ B d c is projected to the tangent space of the Poincaré ball at 0 with log c 0 , transformed by the diagonal relation matrix R ∈ R d×d , and then projected back to the Poincaré ball by exp c 0 . The relation-adjusted object entity embedding h (r) o ∈ B d c is obtained by Möbius addition of the relation vector r h ∈ B d c to the object entity embedding h o ∈ B d c . Since the relation matrix R is diagonal, the number of parameters of MuRP increases linearly with the number of entities and relations, making it scalable to large knowledge graphs. To obtain the predicted probability of a fact being true, we apply the logistic sigmoid to the score, i.e. σ(φ MuRP (e s , r, e o )). To directly compare the properties of hyperbolic embeddings with the Euclidean, we implement the Euclidean version of Equation 4 with d(e (r) s , e (r) o ) = d E (e (r) s , e (r) o ). We refer to this model as the Multi-Relational Euclidean (MuRE) model. Geometric intuition We see from Equation 4 that the biases b s , b o determine the radius of a hypersphere decision boundary centered at e (r) s . Entities e s and e o are predicted to be related by r if relation-adjusted e (r) o falls within a hypershpere of radius</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>MRR covergence rate per epoch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>(a) MRR log-log graph for MuRE and MuRP for different embeddings sizes on WN18RR. (b) Comparison of the MRR convergence rate for MuRE and MuRP on the WN18RR training (dashed line) and validation (solid line) sets with embeddings of size d = 40 and learning rate 50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Scatter plot of norms vs biases for MuRP (left) and MuRE (right). Entities with larger embedding vector norms generally have larger biases for both MuRE and MuRP. Learned 40-dimensional MuRP and MuRE embeddings for WN18RR relation has_part, projected to 2 dimensions. indicates the subject entity embedding, indicates true positive object entities predicted by the model, true negatives, false positives and false negatives. Lightly shaded blue and red points indicate object entity embeddings before applying the relation-specific transformation. The line in the left figure indicates the boundary of the Poincaré disk. The supposed false positives predicted by MuRP are actually true facts missing from the dataset (e.g. malaysia).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Each point on the Poincaré ball x ∈ B d c has a tangent space T x B d c , a d-dimensional vector space, that is a local first-order approximation of the manifold B d c around x, which for the Poincaré ball B d c is a d-dimensional Euclidean space, i.e. T x B d c = R d . The exponential map exp c x : T x B d c → B d c allows one to move on the manifold from x in the direction of a vector v ∈ T x B d c , tangential to B d c at x. The inverse is the logarithmic map log c x : B d c → T x B d c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5</head><label>5</label><figDesc>emphasizes the difference in performance (MRR) of MuRP and MuRE (taken from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Difference in performance (MRR) of MuRP and MuRE on the NELL-995-h{100, 75, 50, 25} datasets for d = 40 and d = 200. The difference becomes smaller (turning negative for d = 200) as the number of non-hierarchical relations increases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>• e y i = e i 2 − e x i 2 ,</head><label>2</label><figDesc>i ∈ {s, o 0 , o 1 , ..., o N }. This projects the reference subject entity embedding onto the x-axis (e x s = e s , e y s = 0) and all object entity embeddings are positioned relative to it, according to their e x i component aligned with the subject entity and their "remaining" component e y i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Link prediction results on WN18RR and FB15k-237. Best results in bold and underlined, second best in bold. The RotatE<ref type="bibr" target="#b29">[30]</ref> results are reported without their self-adversarial negative sampling (see Appendix H in the original paper) for fair comparison.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">WN18RR</cell><cell></cell><cell></cell><cell cols="2">FB15k-237</cell><cell></cell></row><row><cell></cell><cell cols="4">MRR Hits@10 Hits@3 Hits@1</cell><cell cols="4">MRR Hits@10 Hits@3 Hits@1</cell></row><row><cell>TransE [6]</cell><cell>.226</cell><cell>.501</cell><cell>−</cell><cell>−</cell><cell>.294</cell><cell>.465</cell><cell>−</cell><cell>−</cell></row><row><cell>DistMult [37]</cell><cell>.430</cell><cell>.490</cell><cell>.440</cell><cell>.390</cell><cell>.241</cell><cell>.419</cell><cell>.263</cell><cell>.155</cell></row><row><cell>ComplEx [34]</cell><cell>.440</cell><cell>.510</cell><cell>.460</cell><cell>.410</cell><cell>.247</cell><cell>.428</cell><cell>.275</cell><cell>.158</cell></row><row><cell>Neural LP [38]</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>.250</cell><cell>.408</cell><cell>−</cell><cell>−</cell></row><row><cell>MINERVA [9]</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>.456</cell><cell>−</cell><cell>−</cell></row><row><cell>ConvE [11]</cell><cell>.430</cell><cell>.520</cell><cell>.440</cell><cell>.400</cell><cell>.325</cell><cell>.501</cell><cell>.356</cell><cell>.237</cell></row><row><cell>M-Walk [29]</cell><cell>.437</cell><cell>−</cell><cell>.445</cell><cell>.414</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell></row><row><cell>TuckER [2]</cell><cell>.470</cell><cell>.526</cell><cell>.482</cell><cell>.443</cell><cell>.358</cell><cell>.544</cell><cell>.394</cell><cell>.266</cell></row><row><cell>RotatE [30]</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>.297</cell><cell>.480</cell><cell>.328</cell><cell>.205</cell></row><row><cell>MuRE d = 40</cell><cell>.459</cell><cell>.528</cell><cell>.474</cell><cell>.429</cell><cell>.315</cell><cell>.493</cell><cell>.346</cell><cell>.227</cell></row><row><cell cols="2">MuRE d = 200 .475</cell><cell>.554</cell><cell>.487</cell><cell>.436</cell><cell>.336</cell><cell>.521</cell><cell>.370</cell><cell>.245</cell></row><row><cell>MuRP d = 40</cell><cell>.477</cell><cell>.555</cell><cell>.489</cell><cell>.438</cell><cell>.324</cell><cell>.506</cell><cell>.356</cell><cell>.235</cell></row><row><cell cols="2">MuRP d = 200 .481</cell><cell>.566</cell><cell>.495</cell><cell>.440</cell><cell>.335</cell><cell>.518</cell><cell>.367</cell><cell>.243</cell></row></table><note>hierarchical relations (as shown in Section 5.3). Both MuRE and MuRP outperform previous state- of-the-art models on WN18RR on all metrics apart from hits@1, where MuRP obtains second best overall result. In fact, even at relatively low embedding dimensionality (d = 40), this is maintained, demonstrating the ability of hyperbolic models to succinctly represent multiple hierarchies. On FB15k-237, MuRE is outperformed only by TuckER [2] (and similarly ComplEx-N3 [18], since Balažević et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of different model architecture choices on WN18RR: relational transformations (left) and biases (right). Current model (top row) outperforms all others.</figDesc><table><row><cell cols="4">(a) Relational transformations.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) Biases.</cell><cell></cell></row><row><cell>Distance function</cell><cell cols="4">MuRE MRR H@1 MRR H@1 MuRP</cell><cell cols="2">Bias choice</cell><cell></cell><cell cols="4">MuRE MRR H@1 MRR H@1 MuRP</cell></row><row><cell>d(Re s , e o + r)</cell><cell cols="4">.459 .429 .477 .438</cell><cell>b s &amp; b o</cell><cell></cell><cell></cell><cell cols="4">.459 .429 .477 .438</cell></row><row><cell>d(e s , e o + r)</cell><cell>.340</cell><cell>.235</cell><cell>.307</cell><cell>.192</cell><cell>b s only</cell><cell></cell><cell></cell><cell>.455</cell><cell>.414</cell><cell>.463</cell><cell>.415</cell></row><row><cell>d(Re s , e o ) d(R s e s , R o e o + r)</cell><cell>.413 .341</cell><cell>.381 .299</cell><cell>.401 .367</cell><cell>.363 .335</cell><cell>b o only b x = e x</cell><cell>2</cell><cell></cell><cell>.453 .414</cell><cell>.412 .393</cell><cell>.460 .414</cell><cell>.409 .352</cell></row><row><cell>d(e s + r, Re o )</cell><cell>.442</cell><cell>.410</cell><cell>.454</cell><cell>.413</cell><cell cols="2">b x = e (r) x</cell><cell>2</cell><cell>.443</cell><cell>.404</cell><cell>.434</cell><cell>.372</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of hits@10 per relation for MuRE and MuRP on WN18RR for d = 20.Biases vs embedding vector norms We plot the norms versus the biases b s for MuRP and MuRE in</figDesc><table><row><cell>Relation Name</cell><cell cols="2">MuRE MuRP</cell><cell cols="4">∆ Khs Max Path Avg Path</cell></row><row><cell>also_see</cell><cell>.634</cell><cell>.705</cell><cell cols="2">.071 0.24</cell><cell>44</cell><cell>15.2</cell></row><row><cell>hypernym</cell><cell>.161</cell><cell>.228</cell><cell cols="2">.067 0.99</cell><cell>18</cell><cell>4.5</cell></row><row><cell>has_part</cell><cell>.215</cell><cell>.282</cell><cell>.067</cell><cell>1</cell><cell>13</cell><cell>2.2</cell></row><row><cell>member_meronym</cell><cell>.272</cell><cell>.346</cell><cell>.074</cell><cell>1</cell><cell>10</cell><cell>3.9</cell></row><row><cell>synset_domain_topic_of</cell><cell>.316</cell><cell>.430</cell><cell cols="2">.114 0.99</cell><cell>3</cell><cell>1.1</cell></row><row><cell>instance_hypernym</cell><cell>.488</cell><cell cols="2">.471 −.017</cell><cell>1</cell><cell>3</cell><cell>1.0</cell></row><row><cell>member_of_domain_region</cell><cell>.308</cell><cell>.347</cell><cell>.039</cell><cell>1</cell><cell>2</cell><cell>1.0</cell></row><row><cell>member_of_domain_usage</cell><cell>.396</cell><cell>.417</cell><cell>.021</cell><cell>1</cell><cell>2</cell><cell>1.0</cell></row><row><cell>derivationally_related_form</cell><cell>.954</cell><cell>.967</cell><cell cols="2">.013 0.04</cell><cell>−</cell><cell>−</cell></row><row><cell>similar_to</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>−</cell><cell>−</cell></row><row><cell>verb_group</cell><cell>.974</cell><cell>.974</cell><cell>0</cell><cell>0</cell><cell>−</cell><cell>−</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>] contains only 12 out of 200 relations present in the training set, none of which are hierarchical. To ensure a fair representation of all training set relations in the validation and test sets, we create new validation and test set splits by combining the initial validation and test sets with the training set and randomly selecting 10,000 triples each from the combined dataset.To evaluate the influence of the proportion of hierarchical relations in a dataset on the difference in performance between MuRE and MuRP, we create four subsets of the newly created NELL-995 dataset split, containing 100%, 75%, 50% and 25% hierarchical relations, named NELL-995-h100, NELL-995-h75, NELL-995-h50 and NELL-995-h25, containing 43 hierarchical relations each and 0, 14, 43 and 129 non-hierarchical relations respectively.</figDesc><table><row><cell>)</cell></row><row><cell>B NELL-995-h{100, 75, 50, 25} dataset splits</cell></row><row><cell>NELL-995 [36] is a subset of the Never-Ending Language Learner (NELL) [8]. The commonly used</cell></row><row><cell>test set of NELL-995 [36C NELL-995-h{100, 75, 50, 25} experiments</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>shows link prediction results on the NELL-995-h{100, 75, 50, 25} datasets for MuRE and MuRP at d = 40 and d = 200. At d = 40, MuRP consistently outperforms MuRE on all four datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Link prediction results on the NELL-995-h{100, 75, 50, 25} datasets for d = 40 and d = 200.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>d = 40</cell><cell></cell><cell></cell><cell></cell><cell>d = 200</cell><cell></cell></row><row><cell>Dataset</cell><cell cols="4">Model MRR Hits@10 Hits@3 Hits@1</cell><cell cols="4">MRR Hits@10 Hits@3 Hits@1</cell></row><row><cell>NELL-995-h100</cell><cell>MuRE .330 MuRP .344</cell><cell>.502 .511</cell><cell>.366 .383</cell><cell>.245 .261</cell><cell>.355 .360</cell><cell>.527 .529</cell><cell>.398 .401</cell><cell>.266 .274</cell></row><row><cell>NELL-995-h75</cell><cell>MuRE .330 MuRP .345</cell><cell>.497 .506</cell><cell>.368 .382</cell><cell>.246 .263</cell><cell>.356 .359</cell><cell>.526 .524</cell><cell>.396 .401</cell><cell>.269 .275</cell></row><row><cell>NELL-995-h50</cell><cell>MuRE .342 MuRP .356</cell><cell>.510 .519</cell><cell>.383 .399</cell><cell>.256 .271</cell><cell>.372 .371</cell><cell>.544 .539</cell><cell>.415 .415</cell><cell>.284 .284</cell></row><row><cell>NELL-995-h25</cell><cell>MuRE .337 MuRP .343</cell><cell>.489 .494</cell><cell>.374 .379</cell><cell>.259 .266</cell><cell>.365 .359</cell><cell>.515 .507</cell><cell>.404 .397</cell><cell>.287 .282</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>). We can see that on the purely hierarchical NELL-995-h100 (43 hierarchical and 0 non-hierarchical relations), MuRP outperforms MuRE both at lower and higher dimensionality. On the other hand, on NELL-995-h25 which is mostly non-hierarchical (43 hierarchical and 129 non-hierarchical relations), MuRP is only slightly better than MuRE at d = 40, while MuRE outperforms at d = 200.</figDesc><table><row><cell></cell><cell>0.015</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.010</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MRRMuRP -MRRMuRE</cell><cell>0.000 0.005</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.005</cell><cell>d=40</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>d=200</cell><cell></cell><cell></cell></row><row><cell></cell><cell>NELL-995-h100 0.010</cell><cell>NELL-995-h75</cell><cell>Dataset</cell><cell>NELL-995-h50</cell><cell>NELL-995-h25</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Each node in a directed graph is connected not only to its children, but to every descendant, i.e. all nodes to which there exists a directed path from the starting node.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/ibalazevic/multirelational-poincare</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Rik Sarkar, Ivan Titov, Jonathan Mallinson, Eryk Kopczyński and the anonymous reviewers for helpful comments. Ivana Balažević and Carl Allen were supported by the Centre for Doctoral Training in Data Science, funded by EPSRC (grant EP/L016427/1) and the University of Edinburgh.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analogies Explained: Towards Understanding Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tensor Factorization for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balažević</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Riemannian Adaptive Optimization Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD International Conference on Management of Data</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stochastic Gradient Descent on Riemannian Manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvere</forename><surname>Bonnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Hyperbolic Geometry. Flavors of Geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James W Cannon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Floyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kenyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="59" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Toward an Architecture for Never-ending Language Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Estevam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom M</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Go for a Walk and Arrive at the Answer: Reasoning over Paths in Knowledge Bases Using Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shehzaad</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Representation Tradeoffs for Hyperbolic Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Christopher De Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional 2D Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding by Flexible Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Principles of Knowledge Representation and Reasoning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hyperbolic Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavian</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hyperbolic Entailment Cones for Learning Hierarchical Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Octavian-Eugen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning Mixed-Curvature Representations in Product Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beliz</forename><surname>Gunel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hyperbolic Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graph Theoretical Dimensions of Informal Organizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krackhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Organization Theory</title>
		<imprint>
			<publisher>Psychology Press</publisher>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Canonical Tensor Decomposition for Knowledge Base Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Linguistic Regularities in Sparse and Explicit Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Linguistic Regularities in Continuous Space Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">WordNet: a Lexical Database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">STransE: a Novel Embedding Model of Entities and Relationships in Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kairit</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Sirts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Three-Way Model for Collective Learning on Multi-Relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Poincaré Embeddings For Learning Hierarchical Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximillian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximillian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Low Distortion Delaunay Embedding of Trees in Hyperbolic Plane</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Graph Drawing</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to Walk over Graphs using Monte Carlo Tree Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. M-Walk</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Riemannian TransE: Multi-relational Graph Embedding in Non-Euclidean Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosuke</forename><surname>Enokida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Yamanishi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1xRW3A9YX" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Poincaré GloVe: Hyperbolic Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Tifrea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Representing Text for Joint Embedding of Text and Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Complex Embeddings for Simple Link Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hyperbolic Trigonometry and its Application in the Poincaré Ball Model of Hyperbolic Geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Mathematics with Applications</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="135" to="147" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thien</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Differentiable Learning of Logical Rules for Knowledge Base Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
