<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast and Simple Mixture of Softmaxes with BPE and Hybrid-LightRNN for Language Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Kong</surname></persName>
							<email>xiangk@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
							<email>qizhex@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>hovy@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fast and Simple Mixture of Softmaxes with BPE and Hybrid-LightRNN for Language Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mixture of Softmaxes (MoS) has been shown to be effective at addressing the expressiveness limitation of Softmax-based models. Despite the known advantage, MoS is practically sealed by its large consumption of memory and computational time due to the need of computing multiple Softmaxes. In this work, we set out to unleash the power of MoS in practical applications by investigating improved word coding schemes, which could effectively reduce the vocabulary size and hence relieve the memory and computation burden. We show both BPE and our proposed Hybrid-LightRNN lead to improved encoding mechanisms that can halve the time and memory consumption of MoS without performance losses. With MoS, we achieve an improvement of 1.5 BLEU scores on IWSLT 2014 German-to-English corpus and an improvement of 0.76 CIDEr score on image captioning. Moreover, on the larger WMT 2014 machine translation dataset, our MoSboosted Transformer yields 29.6 BLEU score for English-to-German and 42.1 BLEU score for English-to-French, outperforming the single-Softmax Transformer by 0.9 and 0.4 BLEU scores respectively and achieving the state-of-the-art result on WMT 2014 English-to-German task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background: Mixture of Softmaxes</head><p>Mixture of Softmaxes (MoS) <ref type="bibr" target="#b34">(Yang et al. 2018</ref>) is introduced to address the expressiveness limitations of Softmaxbased models. In this section, we briefly review the motiva-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Sequence-to-Sequence model (seq2seq) <ref type="bibr" target="#b30">(Sutskever, Vinyals, and Le 2014;</ref><ref type="bibr" target="#b1">Bahdanau, Cho, and Bengio 2014)</ref> has led to significant research progress on language generation over the last few years. A typical seq2seq model employs an autoregressive factorization of the joint distribution and outputs the conditional probability of each token given the previous tokens. A standard approach to calculate the conditional probability is to apply the Softmax function over the logits.</p><p>Though seq2seq models with a standard Softmax output function are largely effective, <ref type="bibr" target="#b34">Yang et al. (2018)</ref> show that the standard Softmax formulation limits the expressiveness of the generation model and results in the Softmax bottleneck. They propose Mixture of Softmaxes (MoS) to address this issue and demonstrate improved performances on language modeling. However, MoS poses a non-negligible burden on the computation time and the memory consumption. Specifically, MoS outputs a weighted average of K Softmax components, where computing each Softmax involves Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. a huge dot-product between the hidden state and the embedding matrix, costing a considerable amount of time and memory.</p><p>To address the aforementioned drawbacks, a natural idea is to improve the time-and memory-efficiency of computing each Softmax. On a high level, we aim at an encoding mechanism of the vocabulary so that each word can be represented as a code sequence. Then, computing a single Softmax reduces to the product of a sequence of conditional code distributions. Given a code dictionary size, the number of possible words that can be represented increases exponentially w.r.t. the code sequence length, while the computation and memory cost only increases linearly. Hence, such an encoding scheme can theoretically reduce the time and memory consumption exponentially. Clearly, some encoding schemes must have better statistical properties than others and thus lead to better empirical performances. Ideally, the encoding could be learned directly from the data.</p><p>In this work, we investigate two algorithms for these purposes: The first one is called Hybrid-LightRNN, which learns a encoding mechanism from the data based on the language modeling objective. The other one is Byte Pair Encoding (BPE) <ref type="bibr" target="#b8">(Gage 1994;</ref><ref type="bibr" target="#b27">Sennrich, Haddow, and Birch 2016)</ref>, which was originally proposed to help with translating rare words. When evaluated on machine translation (MT) and image captioning, both of these approaches can effectively reduce the time and memory consumption of MoS with no performance losses. Specifically, utilizing MoS brings a performance gain of up to 1.5 BLEU scores on IWSLT 2014 German to English and 0.76 CIDEr scores on image captioning. On WMT 2014 machine translation benchmarks, we achieve a BLEU score of 29.6 on Englishto-German and 42.1 on English-to-French, leading to a stateof-the-art result on the WMT 2014 English-to-German task.</p><p>Our contribution is two-fold. Firstly, we propose to use Hybrid-LightRNN and BPE to make MoS time-and memory-efficient. Secondly, we demonstrate the empirical effectiveness of MoS on sentence generation by improved results on machine translation and image captioning. tion and the formulation of MoS.</p><p>With the autoregressive factorization, a generation model estimates the distribution of the next token x given the context c. In language modeling, the context is composed of previous words of x. In conditional generation tasks such as MT or image captioning, the context also contains the source sentence or the image. Let P * (X | c i ) denote the groundtruth distribution of the next token given context c. Then the standard Softmax function computes the probability distribution P θ (x | c) as</p><formula xml:id="formula_0">P θ (x | c) = exp h c w x x exp h c w x<label>where</label></formula><p>h c is the context vector or the RNN hidden state and w x is the word embedding. <ref type="bibr" target="#b34">Yang et al. (2018)</ref> show the expressiveness limitation of the Softmax function from a matrix factorization perspective. Specifically, suppose that the number of valid contexts is finite. We list all contexts as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Softmax Bottleneck</head><formula xml:id="formula_1">c 1 , c 2 , · · · , c N . Let A ∈ R N ×V , W ∈ R V ×d , H ∈ R N ×d</formula><p>denote the log probability of the ground-truth distribution, the word embedding matrix and the context representation matrix respectively, where N is the number of contexts, V is the vocabulary size and d is the dimensionality of the embedding vector and the context vector. In other words,</p><formula xml:id="formula_2">A i,j = log P * (x j | c i ), W j = w xj , H i = h ci .</formula><p>Let F (A) denote all matrices obtained by applying rowwise shifting to A. Since all matrices in F (A) result in the same probability distribution due to the normalization term in the Softmax, the Softmax function can output the groundtruth distribution P * if and only if the factorization HW approximate any matrix in F (A).</p><p>However, in language generation tasks, matrices in F (A) cannot be approximated by HW because of the differences in their matrix ranks. More specifically, the rank of HW is limited by the embedding vector dimensionality d. In comparison, as shown in <ref type="bibr" target="#b34">Yang et al. (2018)</ref>, A and any other matrices within F (A) have similar high ranks since different contexts result in highly different probability distributions of the next token. Consequently, the ground-truth distribution P * cannot be approximated by the Softmax distribution P θ , which results in the Softmax Bottleneck.</p><p>MoS To tackle the Softmax bottleneck problem, MoS formulate the distribution as the weighted average of K Softmax components:</p><formula xml:id="formula_3">P θ (x | c) = K k=1 π c,k exp h c,k w x x exp h c,k w x<label>(1)</label></formula><p>where π c,k is the mixture weight of the k-th Softmax component and h c,k is the k-th context vector. On language modeling, it has been shown empirically that such a formulation leads to a high rank matrix. Note that since all Softmaxes share the same word embedding matrix, the number of parameters do not increase rapidly with more mixtures, preventing overfitting.</p><p>The mixture weight and the context vectors are computed as</p><formula xml:id="formula_4">π c,k = exp g w (π) k K k =1 exp g w (π) k h c,k = tanh(W (h) k g) (2)</formula><p>where g denotes a vector representation of the context c. w (π) and W (h) denote the parameters of the mixture weight and the parameters of the context vector with a slight abuse of notation.</p><p>In our machine translation experiments, the attention model <ref type="bibr" target="#b1">(Bahdanau, Cho, and Bengio 2014)</ref> is employed to obtain an context vector of the source sentence. g is obtained by passing the concatenation of the context vector and the RNN hidden state through an MLP. In the captioning case, the decoder is a vanilla RNN and the vector representation g is the decoder's hidden state.</p><p>Time and Memory Cost As shown in Eqn. 1, MoS computes K Softmaxes and output the weighted average of the K probability distributions. Though MoS effectively increases the expressiveness of a generation model, it also incurs a large time and memory cost since it needs to perform K Softmax operations on the whole vocabulary. The time and memory costs not only hinder rapid algorithm developments but also limit the mixture number when resources are limited, restricting the power of MoS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoding Words for Efficient MoS</head><p>In this section, we introduce two word encoding algorithms to reduce the memory and time consumptions of MoS. We aim to obtain an encoding mechanism of each word where the number of potential codes is much smaller than the vocabulary size. In theory, given a code dictionary, the number of possible words that can be represented increases exponentially w.r.t. the code sequence length, while the computation cost only increases linearly. Then a generation model is trained to output a code sequence to generate a sentence. By decomposing words into shared codewords, the Softmax in the generation model only needs to be computed over the code dictionary. However, the encoding function need to be optimized to reflect semantic correlations between words since the semantic representations of words are shared through the embeddings of the codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background: Learning Encoding Mechanisms Using Optimal Transport</head><p>We first provide an optimal transport (OT) (Peyré and Cuturi 2017) perspective of learning the encoding mechanism. Broadly speaking, optimal transport is the assignment problem between probability distributions. In the case of learning encoding mechanisms, the probability distributions are simply the delta distribution for each word and each code sequence. We define the following Wasserstein distance between the word space and the code sequence space.</p><formula xml:id="formula_5">min T 1≤i≤|V |, s∈S C i,s T i,s s.t. T i,s ∈ {0, 1} i T i,s = 1, j T i,s = 1 (3)</formula><p>where i enumerates words in the vocabulary V and S is the set of all code sequences. T i,s is an indicator function of whether wordx i is assigned to code sequence s. The constrains over T ensures that each word is only mapped to a code sequence and that each code sequence is only mapped to a word. Hence a valid T would naturally result in a desired bijection mapping. C i,s is the cost of assigning word x i to code sequence s and 1≤i≤V,s∈S C i,s T i,s is the overall cost and the optimization objective. For simplicity, we assume that the number of possible code sequences |S| is equal to the vocabulary size, since we can always add unused tokens to the vocabulary and assign them to redundant code sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hybrid-LightRNN</head><p>As mentioned earlier, the encoding function should be learned so that words can effectively share semantics through common codes. More importantly, since the encoding function is used in language generation tasks, it is desirable to have encoded sequences that are easy to model by current RNN-based models. As language modeling can be used to measure the difficulty of modeling the code sequences, we propose Hybrid-LightRNN that optimizes the encoding function according to the language modeling objective.</p><p>To compute the probability of a sentence under the current encoding function, we replace each sentence x by its code sequence and compute the log probability of its corresponding code sequence. Formally, let g(x) = s = (s 1 , s 2 , · · · , s M ) be the encoding function which maps a wordx into a code sequence s. The log probability of a sentence is as follows when an encoding function is employed:</p><formula xml:id="formula_6">log P Θ (x) = k log P Θ (x k | x 1 , · · · , x k−1 ) = k j log P Θ (g(x i ) j | Z k,j ) (4) where Z k,j = g(x 1 ) 1 , · · · , g(x 1 ) M , · · · , g(x k ) 1 , · · · , g(x k ) j−1</formula><p>is the concatenation of code sequences of the context and Θ is the parameters of a neural language model. Then the optimal encoding function is defined as:</p><formula xml:id="formula_7">argmin g min Θ x∈X − log P Θ (x)<label>(5)</label></formula><p>where X is the training corpus.</p><p>Optimization Ideally, for each encoding function g(·), we would like to find the optimal language modeling cost. However, it is too computationally heavy to enumerate the combinatorial possibilities of encoding function and evaluate the language modeling performance. Instead, we would like to jointly optimize the encoding function g(·) and the language model parameters Θ. However, the encoding function g(·) is represented by discrete parameters, hence we resort to an approximated algorithm. The high-level idea of the approximated algorithm is to iteratively optimize one of the language model parameters Θ and the encoding function g(·)</p><p>while keeping the other one fixed. Since all language model parameters in Θ are fully differentiable, we can simply utilize SGD to optimize them. Then, the core difficulty lies in the step of optimizing the discrete parameters of g(·), during which, ideally, we want the following two properties to hold</p><p>• The encoding function remains valid. In other words, the mapping between words and code sequences remains bijections.</p><p>• The language modeling objective function is decreased.</p><p>At first glance, this optimization problem seems intractable since there are combinatorially many possible g(·). However, since finding the optimal mapping is naturally an assignment problem, we can rely on existing algorithms of optimal transport if we can approximate the language modeling loss function by the Wasserstein distance defined in Eqn. 3. The key idea here is to decompose the corpus level likelihood to the encoding decisions of each word. More specifically, in the language modeling objective, for each word, we are measuring the likelihood of its current code sequence for each occurrence in the training data. Naturally, we can define the cost of assigning the word to the corresponding code sequences by the likelihood of other code sequences.</p><p>Formally, the cost of assigning wordx i to code sequence s can be defined as</p><formula xml:id="formula_8">x∈X k I(x i = x k ) log −P (s | Z k,1 ) where I(·)</formula><p>is the indicator function. Here, since the context is encoded by the original encoding function, we implicitly assume the independence between the costs of different words' mapping. We further assume the independence between codes and approximate log −P (s | Z k,1 ) as j log −P (s j | Z k,j ) to avoid evaluating the language model for |S| times. Finally, we obtain the cost function as follows:</p><formula xml:id="formula_9">C i,s = x∈X k I(x i = x k ) j log −P (s j | Z k,j )<label>(6)</label></formula><p>Note that, when we use the original encoding function, i.e., T i,s = I(g(x i ) = s), the optimal transport objective equals to the current language modeling likelihood, i.e., i,s C i,s T i,s = x∈X − log P (x). Hence the Wasserstein distance will always be lower than the current language modeling cost. However, because of the independence assumptions, the language modeling loss is not guaranteed to decrease after optimizing the encoding function.</p><p>A canonical solution to the optimal transport problem is the minimum cost maximum flow (MCMF) algorithm <ref type="bibr" target="#b0">(Ahuja et al. 1993</ref>). However, the computation complexity of the MCMF is O(|V | 3 ). Following LightRNN <ref type="bibr" target="#b17">(Li et al. 2016)</ref>, we adopt an 1 2 -approximation algorithm <ref type="bibr" target="#b26">(Preis 1999</ref>), which has a complexity of O(|V | 2 ). With the approximation algorithm, the time consumption of solving the optimal transport problem only constitutes a small proportion of the whole LightRNN algorithm when taking the time of training the neural language model into account.</p><p>Increasing the Capacity for Frequent Words Although the algorithm does not require the maximum code sequence length M to be small, in our experiments, we set M to 2 since the dictionary size can already be reduced to O( |V |) if the first code and the second code can take |V | values respectively. However, if we only uses O( |V |) number of codes to model all words in the vocabulary, though the efficiency is improved greatly, the capacity of the model is hurt significantly, since each word is forced to share embeddings with 2 × |V | − 1 words which share the first code or the second code with it. As a result, the encoding function should assign exclusive codes to important words. Since frequent words have a large impact on the overall performance, we set the encoding function so that the codes of the most frequent K words are not shared with other words. Specifically, for word x i (i &lt; K), we manually specify their code sequences to be a length 1 sequence (i). For all other words, their code sequences do not contain code i and are learned using the optimal transport objective.</p><p>Since the code sequence has maximum length two, we can use the following table to represent the encoding function, where the row and column denotes the first and the second respectively:</p><formula xml:id="formula_10">A = D UNK UNK L</formula><p>where the matrix D is a sparse diagonal matrix to which frequent words are assigned. L ∈ Z d1×d2 is a dense matrix learned through optimal transport. To fit V words into the table, the dimensions of D and L should satisfy K + d 1 × d 2 ≥ |V |. LightRNN <ref type="bibr" target="#b17">(Li et al. 2016</ref>) is a special case of Hybrid-LightRNN where they do not model frequent words separately. We will show in the experiments that it is very important to model frequent words separately. Furthermore, LightRNN defines the dimension of the first code to be equal to the dimension of the second code d 1 = d 2 = √ V for best efficiency. However, when one dimension is larger, the model can have more embedding vectors and has a larger capacity, which also results in an encoding mechanism similar to the hierarchical Softmax <ref type="bibr" target="#b22">(Morin and Bengio 2005)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Byte Pair Encoding (BPE)</head><p>Byte Pair Encoding (BPE) <ref type="bibr" target="#b8">(Gage 1994;</ref><ref type="bibr" target="#b27">Sennrich, Haddow, and Birch 2016)</ref> was introduced to address the difficulties of translating rare words and out-of-vocabulary words in machine translation. BPE is of interest here since it can reduce the vocabulary size effectively and can speedup the computation of Softmaxes.</p><p>In the encoding learned by the BPE, each code is a subword. Formally, BPE learns the code dictionary S as fol-lows: We initialize the code dictionary as the set of all possible characters and break all words into sequences of codes. Then we iteratively run the following steps to add new codes to the dictionary: 1. Count the frequency of all code pairs within training data.</p><p>Find out the most frequent pair/bigram of codes A and B.</p><p>2. Add the new code AB to the dictionary. Replace all occurrence of pair <ref type="figure">(A, B)</ref> with AB.</p><p>3. End the iteration if the dictionary size reaches a threshold. Otherwise go to step 1.</p><p>BPE is an algorithm based on heuristics. However, the strong inductive bias of BPE always gives more capacity to frequent words when it comes to the tradeoff between efficiency and capacity if we vary the subword unit dictionary size, since the more frequent words will be segmented into fewer parts, which will lead to more exclusive embeddings instead of shared embeddings.</p><p>When we use a larger code dictionary, more frequent words and subwords are added to the dictionary and their semantics are modeled by separate embedding vectors, leading to a larger model capacity. On the other hand, the model efficiency is improved with a smaller subword dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Apart from the previously mentioned related works, mixture of Softmaxes is closely related to works that mix representation vectors <ref type="bibr" target="#b7">(Eigen, Ranzato, and Sutskever 2013;</ref><ref type="bibr" target="#b29">Shazeer et al. 2017</ref>). <ref type="bibr" target="#b34">Yang et al. (2018)</ref> show that this approach does not solve the softmax bottleneck problem.</p><p>Hierarchical Softmax <ref type="bibr" target="#b22">(Morin and Bengio 2005)</ref> is an extensively studied technique to improve the efficiency of Softmaxes. <ref type="bibr" target="#b22">Morin and Bengio (2005)</ref> uses the synsets in the WordNet to build the hierarchical tree. <ref type="bibr" target="#b20">Mnih and Hinton (2009)</ref> propose to learn the hierarchical tree with a clustering algorithm. The idea of separately modeling frequent words is also explored in Adaptive Softmax <ref type="bibr" target="#b10">(Grave et al. 2016)</ref>. Although hierarchical Softmax can reduce the time and memory consumptions during training, it still requires computing the Softmax over the whole vocabulary during testing. Noise Contrastive Estimation <ref type="bibr" target="#b11">(Gutmann and Hyvärinen 2012;</ref><ref type="bibr" target="#b21">Mnih and Teh 2012)</ref> and Negative Sampling <ref type="bibr" target="#b19">(Mikolov et al. 2013)</ref> can also speed up Softmax during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we describe our experiments on machine translation and image captioning and study our models quantitatively and qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Settings</head><p>Machine Translation We first evaluate our models on the IWSLT 2014 German to English (DE-EN) dataset <ref type="bibr" target="#b2">(Cettolo et al. 2014)</ref>.</p><p>We employ an LSTM <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber 1997)</ref>    <ref type="bibr" target="#b29">Shazeer et al. (2017)</ref> 26.03 40.56 <ref type="bibr" target="#b9">Gehring et al. (2017)</ref> 26.43 41.62 <ref type="bibr" target="#b31">Vaswani et al. (2017)</ref> 28.4 41.8 <ref type="bibr" target="#b5">Dehghani et al. (2018)</ref> 28.9 N/A <ref type="bibr" target="#b28">Shaw, Uszkoreit, and Vaswani (2018)</ref> 29.2 41.5 <ref type="bibr" target="#b23">Ott et al. (2018)</ref> 29 <ref type="formula">.</ref>  2015) as the baseline. We build the baseline using the Py-Torch code from <ref type="bibr" target="#b4">Dai, Xie, and Hovy (2018)</ref>. For Hybrid-LightRNN, we set K to 9, 652 and set d 1 and d 2 to 174 to represent a total of 30K words. As model performances exhibit small variances on IWSLT, we run each experiment for five times with different random seeds and report the average performance and the standard deviation.</p><p>We also test our best model on the standard WMT 2014 English-to-German (EN-DE) and English-to-French (EN-FR) benchmarks, consisting of 4.5M and 36M sentence pairs respectively. We follow the preprocessing steps of ConvS2S <ref type="bibr" target="#b9">(Gehring et al. 2017)</ref> for the EN-FR task. We employ BPE with 32K merge operations for both tasks. The Transformer model <ref type="bibr" target="#b31">(Vaswani et al. 2017</ref>) is employed as our baseline. Our configuration largely follows the configuration of <ref type="bibr" target="#b31">Vaswani et al. (2017)</ref>, except that we multiply the original learning rate by 0.8 for the Transformer equipped with MoS. Specifically, we test both the Base configuration and Big configuration, which respectively have embeddings of dimension 512 and 1024, the dimension of the inner layer 2048 and 4096 and the number of attention heads 8 and 16. We used the Adam optimizer (Kingma and Ba 2014) with β 1 = 0.9, β 2 = 0.98, and = 10 −9 . We set the mixture number to 9. We use the corpus-level BLEU score <ref type="bibr" target="#b24">(Papineni et al. 2002)</ref> as the evaluation metric. Our Transformer training and evaluation code is based on an open source toolkit THUMT <ref type="bibr" target="#b35">(Zhang et al. 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Captioning</head><p>We conduct experiments on the MSCOCO dataset <ref type="bibr" target="#b18">(Lin et al. 2014</ref>) and follow the same preprocessing procedure and the train/validation/test split as used in <ref type="bibr" target="#b14">Karpathy and Fei-Fei (2015)</ref>. We use the Neural Image Caption (NIC) model <ref type="bibr" target="#b32">(Vinyals et al. 2015)</ref> as the baseline model. Following <ref type="bibr" target="#b4">Dai, Xie, and Hovy (2018)</ref> and <ref type="bibr" target="#b16">Kong et al. (2018)</ref>, we employ a pretrained 101-layer ResNet <ref type="bibr" target="#b12">(He et al. 2016)</ref> instead of a GoogLeNet to extract a feature vector from an input image. We employ an LSTM of size 512 as the decoder. We report BLEU-4, METEOR and CIDERr scores using the scripts provided by <ref type="bibr" target="#b3">Chen et al. (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Details</head><p>For the BPE and Hybrid-LightRNN, we set the code dictionary sizes to 10K for IWSLT and 3K for MSCOCO. We measure the speed and memory usage on a Titan X with PyTorch version v0.3.1 and CUDA 9.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head><p>In our experiments, we denote Hybrid-LightRNN-MoS and BPE-MoS as the seq2seq models with MoS which employ Hybrid-LightRNN and BPE respectively. The baseline seq2seq model without MoS is denoted as Baseline. This experiment shows that MoS can effectively improve the expressiveness of generation models by learning a highrank log probability matrix. As expected, the improvement is larger on MT than on image captioning, which can be explained by the differences of language complexities used in these two tasks. Specifically, on image captioning, the captions largely share similar patterns, resulting in a lower-rank probability matrix and a smaller improvement space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall Performances on IWSLT and MSCOCO</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performances on WMT 14 EN-DE and EN-FR</head><p>Since BPE is better than Hybrid-LightRNN with a small margin on IWSLT, we only test BPE-MoS on WMT. As shown in Tab. 2, we achieve 29.6 and 42.1 BLEU scores respectively on WMT 14 EN-DE and EN-FR, improving the Transformer model by 0.9 and 0.4 BLEU scores. We achieve the state-of-the-art result that does not employ data augmentation on WMT 14 EN-DE. Note that data augmentation can also effectively improve the machine translation performance    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>In this section, we perform extensive studies to better understand our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Softmaxes</head><p>Since a larger mixture number would likely to lead to a higher rank log probability matrix, we verify whether a larger mixture number leads to a better performance. We vary the number of mixture in the BPE-MoS model and compare their performances on MT. As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, more Softmax components clearly lead to better performances. However, the improvement margin exhibits a diminishing return effect, which means that several Softmaxes are enough to learn a high-rank matrix.      The comparison is shown in Tab. 8. Removing the generated OOV words do lead to a performance decrease of 0.05 BLEU score. However, when both Hybrid-LightRNN and BPE are disabled from translating OOV, BPE is still better than Hybrid-LightRNN by a gap of 0.07 BLEU score. This result indicates that the encoding function learned by BPE better captures the data statistics than the encoding learned by Hybrid-LightRNN, showing that Hybrid-LightRNN has a lot of potentials for improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mapping Table Qualitative</head><p>Study In Hybrid-LightRNN, words in the same column/row share the same column/row embedding vector. Intuitively, it is important to group semantically-similar or syntactically-similar words into the same column/row. We examine whether the learned table have this property in Tab. 6. We find that most words within the same row are either semantically-similar or syntactically-similar to each other.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>We show the comparison between a standard LSTM seq2seq model with the Hybrid-LightRNN-MoS and BPE-MoS in Tab. 1. Hybrid-LightRNN-MoS and BPE-MoS both outperform the baseline on both tasks. Specifically, on machine translation, BPE-MoS can outperform the baseline by a BLEU score of 1.5. On image captioning, BPE-MoS outperforms the baseline by 0.42, 0.4 and 0.76 in terms of BLEU-4, METEOR and CIDEr respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>BPE-MoS's average validation performance over multiple runs on IWSLT with various numbers of mixture components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>± 0.20 23.60 ± 0.12 88.50 ± 0.47 Hybrid-LightRNN-MoS 9 28.79 ± 0.23 30.02 ± 0.16 23.87 ± 0.18 88.96 ± 0.21 BPE-MoS 9 28.91 ± 0.06 30.06 ± 0.10 24.00 ± 0.24 89.26 ± 0.11</figDesc><table><row><cell></cell><cell></cell><cell>Machine Translation (IWSLT)</cell><cell cols="3">Image Captioning (MSCOCO)</cell></row><row><cell>Model</cell><cell># Softmaxes</cell><cell>BLEU</cell><cell>BLEU-4</cell><cell>METEOR</cell><cell>CIDEr</cell></row><row><cell>Baseline</cell><cell>1</cell><cell>27.41 ± 0.15</cell><cell>29.64</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">seq2seq model with the dot-product attention (Bah-</cell></row><row><cell></cell><cell></cell><cell cols="4">danau, Cho, and Bengio 2014; Luong, Pham, and Manning</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Overall performance comparisons on IWSLT and MSCOCO</figDesc><table><row><cell>Model</cell><cell cols="2">EN-DE EN-FR</cell></row><row><cell>Wu et al. (2016)</cell><cell>26.30</cell><cell>41.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Experiment results on the WMT 2014 English-</cell></row><row><cell>German (EN-DE) and English-French (EN-FR) where</cell></row><row><cell>Transformer-MoS denotes the Transformer model with</cell></row><row><cell>MoS.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Memory and time efficiency comparisons on MT and image captioning when using the same number of Softmaxes. Bold faces highlight the best in the corresponding category. The shown memory is in GB and the speed is in ms/batch.</figDesc><table><row><cell cols="2">Memory (GB) Speed (ms/batch)</cell><cell>Model</cell><cell># Mixtures</cell><cell>BLEU</cell></row><row><cell></cell><cell></cell><cell>Baseline</cell><cell>1 × 30k</cell><cell>27.41 ± 0.15</cell></row><row><cell>5.3</cell><cell>45.5</cell><cell>Hybrid-LightRNN-MoS</cell><cell>3 × 10k</cell><cell>28.36 ± 0.11</cell></row><row><cell></cell><cell></cell><cell>BPE-MoS</cell><cell>3 × 10k</cell><cell>28.47 ± 0.16</cell></row><row><cell></cell><cell></cell><cell>MoS</cell><cell>3 × 30k</cell><cell>28.42 ± 0.14</cell></row><row><cell>10.4</cell><cell>90.4</cell><cell>Hybrid-LightRNN-MoS</cell><cell>9 × 10k</cell><cell>28.79 ± 0.23</cell></row><row><cell></cell><cell></cell><cell>BPE-MoS</cell><cell>9 × 10k</cell><cell>28.91 ± 0.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparisons on IWSLT under the same memory and time budget. The Softmaxes size is number of Softmaxes × Softmax dictionary size. Bold faces highlight the best in the corresponding category</figDesc><table><row><cell>Memory and Time Efficiency We study the memory</cell></row><row><cell>consumption and efficiency of Hybrid-LightRNN-MoS and</cell></row><row><cell>BPE-MoS. As shown in Tab. 3, when applying on the Base-</cell></row><row><cell>line model and the MoS model, BPE and Hybrid-LightRNN</cell></row><row><cell>can reduce the time and memory usage with no performance</cell></row><row><cell>losses. In addition, on MT where the vocabulary is large,</cell></row><row><cell>they can halve the time and memory consumption when ap-</cell></row><row><cell>plied on MoS with 3 mixtures. When there are more mix-</cell></row><row><cell>utres, the improvements will continue to grow since com-</cell></row><row><cell>puting Softmaxes take a larger proportion of time.</cell></row><row><cell>Comparisons under the Same Computation Budget</cell></row><row><cell>When computational resources are limited, BPE and</cell></row><row><cell>LightRNN enable the use of more Softmaxes, leading to</cell></row><row><cell>potentially higher rank probability matrices. Hence, we</cell></row><row><cell>study the performances of BPE-MoS, Hybrid-LightRNN-</cell></row><row><cell>MoS and MoS given the same computation budget. As</cell></row><row><cell>shown in Tab. 4. BPE-MoS and Hybrid-LightRNN-MoS</cell></row><row><cell>consistently outperform the baseline and the MoS model.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Hybrid-LightRNN Ablation StudyWe further study the importance of the learned table and the importance of the model's capacity in Hybrid-LightRNN. Firstly, we vary the dictionary size to investigate whether it is necessary to give enough capacity to frequent words.As shown in Tab. 7, larger dictionary sizes consistently lead to better performances. Secondly, when compared with LightRNN, Hybrid-LightRNN achieves an improvement of 2.68 BLEU score, which shows that it is necessary to employ extra capacities for frequent words. Thirdly, as a sanity check of whether thetable learning is necessary, we compare the table learned by LightRNN with the table obtained by simply sorting words based on their frequency and the table with random word allocations. The table learned by ± 0.20 23.60 ± 0.12 88.50 ± 0.47 Hybrid-LightRNN-MoS 3 × 3k 29.93 ± 0.14 23.74 ± 0.29 88.52 ± 0.18 BPE-MoS 3 × 3k 29.96 ± 0.08 23.67 ± 0.34 88.61 ± 0.27 ± 0.14 23.69 ± 0.20 88.84 ±0.66 Hybrid-LightRNN-MoS 9 × 3k 30.02 ± 0.16 23.87 ± 0.18 88.96 ± 0.21 BPE-MoS 9 × 3k 30.06 ± 0.10 24.00 ± 0.24 89.26 ± 0.11</figDesc><table><row><cell>Memory (GB) Speed (ms/batch)</cell><cell>Model</cell><cell>Softmaxes Size</cell><cell>BLEU-4</cell><cell>METEOR</cell><cell>CIDEr</cell></row><row><cell cols="4">1.0 29.64 4.3 14.6 MoS 1 × 10k 26.4 MoS 3 × 10k 29.91</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Comparisons on image captioning under the same memory and time budget. The Softmaxes size is number of Softmaxes × Softmax dictionary size. Bold faces highlight the best ones in each category.</figDesc><table><row><cell>row</cell><cell></cell><cell></cell><cell></cell><cell>words</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>45</cell><cell>700</cell><cell>3.3</cell><cell>28</cell><cell>19</cell><cell>7</cell><cell>86</cell><cell>35</cell><cell>...</cell></row><row><cell>48</cell><cell>around</cell><cell>between</cell><cell>by</cell><cell>into</cell><cell>down</cell><cell>for</cell><cell>off</cell><cell>...</cell></row><row><cell>54</cell><cell>mined</cell><cell>imaged</cell><cell>advised</cell><cell>pickled</cell><cell>outfitted</cell><cell>filled</cell><cell>withheld</cell><cell>...</cell></row><row><cell>91</cell><cell>bristol</cell><cell>chinatown</cell><cell>rochester</cell><cell cols="5">kingston guangdong guangzhou chongqing ...</cell></row><row><cell>93</cell><cell>pursuing</cell><cell>posing</cell><cell cols="2">proposing reacting</cell><cell>replacing</cell><cell>blogging</cell><cell>pointing</cell><cell>...</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Example mapping table where the row denotes the first code and the column denotes the second code. Numbers and places are grouped together in row 45, 91. Syntactically similar words are also grouped together in row 48, 54 and 93.</figDesc><table><row><cell>Mapping Table</cell><cell cols="3">Table Size Learned Table BLEU</cell></row><row><cell>Hybrid-LightRNN</cell><cell>10k</cell><cell></cell><cell>30.07</cell></row><row><cell>Hybrid-LightRNN</cell><cell>5k</cell><cell>Yes</cell><cell>29.69</cell></row><row><cell>Hybrid-LightRNN</cell><cell>1k</cell><cell></cell><cell>28.73</cell></row><row><cell>LightRNN</cell><cell>0.2k</cell><cell>Yes</cell><cell>27.39</cell></row><row><cell>Frequency table Random table</cell><cell>0.2k 0.2k</cell><cell>No</cell><cell>25.84 24.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Average validation BLEU on IWSLT of Hybrid-LightRNN-MoS using different mapping tables.</figDesc><table><row><cell>Model</cell><cell cols="2">OOV Translation BLEU</cell></row><row><cell>BPE-MoS</cell><cell>Yes</cell><cell>30.19</cell></row><row><cell>BPE-MoS</cell><cell>No</cell><cell>30.14</cell></row><row><cell>Hybrid-LightRNN-MoS</cell><cell>No</cell><cell>30.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Ablation study on the importance of translating OOV words. The BLEU score are evaluated on the validation set of IWSLT.LightRNN outperforms models with the random table or the frequency-based table by BLEU scores of 2.41 and 1.55 respectively, which means that optimizing a language modeling objective learns an effective encoding function.Is BPE-MoS better because of modeling OOVs? As indicated in Tab. 1, BPE-MoS is slightly better than Hybrid-Light-MoS on MT and image captioning. In principle, both Hybrid-LightRNN and BPE can model the semantics of all frequent words and rare words in the training set by sharing embeddings with other words. One exclusive advantage of BPE is the ability to generate out-of-vocabulary (OOV) words. A natural question to ask is "how much performance difference would OOVs cause?" To investigate the importance of modeling OOVs, We take the best BPE-MoS model, replace all generated OOV words with UNK and test its performance.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions and Discussions</head><p>In this work, we investigate two algorithms, i.e., Byte Pair Encoding and Hybrid-LightRNN, to reduce the vocabulary size so as to improve the memory-and time-efficiency of MoS. We evaluate these two methods on machine translation and image captioning and show improved performances over the baseline system without MoS. Further, both of these methods effectively speed up the training process and reduce the memory consumption of MoS with no performance losses. We demonstrate the effectiveness of our models by improved performances on machine translation and image captioning.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Network flows: theory, algorithms, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Magnanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Orlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Prentice hall</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Report on the 11th iwslt evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IWSLT</title>
		<meeting>of IWSLT</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m">Microsoft coco captions: Data collection and evaluation server</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10974</idno>
		<title level="m">From credit assignment to entropy regularization: Two new algorithms for neural sequence prediction</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03819</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Universal transformers. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Understanding back-translation at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09381</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4314</idno>
		<title level="m">Learning factored representations in a deep mixture of experts</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A new algorithm for data compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The C Users Journal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="23" to="38" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03122</idno>
		<title level="m">Convolutional sequence to sequence learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04309</idno>
		<title level="m">Efficient softmax approximation for gpus</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">U</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="307" to="361" />
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08541</idno>
		<title level="m">Neural machine translation with adequacy-oriented learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lightrnn: Memory and computation-efficient recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4385" to="4393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
	</analytic>
	<monogr>
		<title level="m">Effective approaches to attention-based neural machine translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ECCV. Springer. Luong, M</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6426</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aistats</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00187</idno>
		<title level="m">Scaling neural machine translation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Computational optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Linear time 1/2-approximation algorithm for maximum weighted matching in general graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Preis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STACS</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="259" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<title level="m">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Breaking the softmax bottleneck: a high-rank rnn language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06415</idno>
		<title level="m">Thumt: An open source toolkit for neural machine translation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
