<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Social Ways: Learning Multi-Modal Distributions of Pedestrian Trajectories with GANs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javad</forename><surname>Amirian</surname></persName>
							<email>javad.amirian@inria.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bernard</forename><surname>Hayet</surname></persName>
							<email>jbhayet@cimat.mx</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Cimat</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>México</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Pettré</surname></persName>
							<email>julien.pettre@inria.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<region>Inria, IRISA</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<region>Inria, IRISA</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Social Ways: Learning Multi-Modal Distributions of Pedestrian Trajectories with GANs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a novel approach for predicting the motion of pedestrians interacting with others. It uses a Generative Adversarial Network (GAN) to sample plausible predictions for any agent in the scene. As GANs are very susceptible to mode collapsing and dropping, we show that the recently proposed Info-GAN allows dramatic improvements in multi-modal pedestrian trajectory prediction to avoid these issues. We also left out L2-loss in training the generator, unlike some previous works, because it causes serious mode collapsing though faster convergence.</p><p>We show through experiments on real and synthetic data that the proposed method leads to generate more diverse samples and to preserve the modes of the predictive distribution. In particular, to prove this claim, we have designed a toy example dataset of trajectories that can be used to assess the performance of different methods in preserving the predictive distribution modes. * The research is supported by the CrowdBot H2020 EU Project http: //crowdbot.org/ † J.B. Hayet is partially funded by the Intel Probabilistic Computing initiative.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many end-user applications make an intensive use of data analytics about pedestrians motion: urban safety, city planning, marketing, autonomous driving, to name a few ones. Typically, this implies the recollection and the offline analysis of these data, for understanding the pedestrians behaviors and taking decisions about the environment. In some contexts, however, one needs to go further and anticipate, in an online way, what will be the next pedestrian moves and infer their short or mid-term intentions. This allows to trigger early alarms or to take preventive actions when monitoring systems with critical real-time decisiontaking processes. In the case of autonomous driving, for ex- <ref type="figure">Figure 1</ref>. Illustration of the trajectory prediction problem. Having the observed trajectories of a pedestrian of interest, here shown with a star, and the ones of other pedestrians in the environment, the system should be able to build a predictive distribution of possible trajectories (here with two modes in dashed yellow lines). ample, inferring the intention of the pedestrians surrounding the car is of paramount importance in avoiding collisions.</p><p>Nevertheless, this inference problem is extremely complicated to solve. First, because there are many variables which are strongly relevant for the trajectories of single pedestrians: The nature of the surrounding obstacles and their spatial distribution, the nature of the ground, the longterm goal of the pedestrian, his age, his mental state, etc. Then, to make things even more difficult, the motions of a whole set of agents sharing a common space are dependent, through a whole range of interactions that can go from avoidance to meeting intention or person following. A number of interesting studies from neuroscience and biomechanics have isolated single factors or optimization principles governing the human motion in very specific contexts (one-to-one interactions, well-stated goals. . . ). However, in more general cases, one may rapidly attain the limits of hand-tailored mathematical models. This has motivated the pursuit of more flexible, data-driven statistical approaches that can automatically select the most relevant features for explaining pedestrians walks, and that can benefit from the great efficiency of machine learning techniques. Our work belongs to the aforementioned category of data-driven methods for predicting the motion of pedestrians in the horizon of a few seconds, given a set of observations of their own past motion and of those of the pedestrians sharing the same space, as illustrated in <ref type="figure">Fig. 1</ref>. It relies on a Generative Adversarial Network (GAN)-based trajectory sampler to propose plausible future trajectories. It naturally encompasses the uncertainty and the potential multi-modality of the pedestrian steering decision, which is of critical importance when using this predictive distribution as a belief in higher level decision-making processes.</p><p>The main contributions of this work are the following:</p><p>• An efficient, unsupervised process to train a trajectory prediction GAN architecture based on Info-GAN <ref type="bibr" target="#b2">[3]</ref>, without L2 loss, which gives better results than previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref> in preserving the multi-modal nature of the predictive distribution.</p><p>• The definition of an attention-based pooling scheme that relies on a few hand-designed interaction features inspired from the neuroscience/bio-mechanics literature, as a form of prior; the best way to combine them to assess the interaction is learned by our system.</p><p>• The design of a synthetic dataset specifically oriented to the evaluation of the preservation of multi-modality in trajectories predictive distributions.</p><p>Our architecture is described in <ref type="figure" target="#fig_0">Fig. 2</ref>. It adopts a new strategy to produce plausible samples for an agent from the joint predictive distribution of the set of agents. Our Sampler ( <ref type="figure" target="#fig_0">Fig. 2</ref> and Section 3.2) is trained to generate plausible predictions for a single agent, given past observations of trajectories for the whole set of the agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Closed-form mathematical models. Many closed-form mathematical models explaining human motion have been introduced in the simulation, graphics and crowd animation areas. Computational geometry-based approaches <ref type="bibr" target="#b18">[19]</ref> produce optimal motions typically at the limits of collision and not human-like. Optimization-based methods <ref type="bibr" target="#b22">[23]</ref> optimize on-the-fly the parameters of an objective function hand-designed to cover relevant aspects of the motion.</p><p>In multiple-target tracking, Bayesian techniques typically require prediction processes with simple motion models (random walk or constant velocity) or with parameterized modelling of the social interactions, the goal, etc. <ref type="bibr" target="#b12">[13]</ref>.</p><p>Data-driven statistical models. Because of the complexity of pedestrians motion, hand-tailored deterministic models fail to adapt to a wide range of contexts, whereas machine-learning based techniques benefit from large human motion datasets. In <ref type="bibr" target="#b8">[9]</ref>, for tracking pedestrians from a vehicle, interaction features useful for avoidance are learned from optical flow data. In <ref type="bibr" target="#b6">[7]</ref>, pedestrian path prediction, in the same context of mobile sensing, is done in a lowdimensional latent space through Gaussian process dynamical models with augmented features extracted from the video optical flow. In <ref type="bibr" target="#b17">[18]</ref>, interacting mixtures of Gaussian Processes (GPs) are used for predicting the whereabouts of goal-driven social agents in crowds, where the parameters are learned from training data.</p><p>NN-based data-driven models. With the advent of NNbased machine learning, the sequential nature of motion has motivated the use of Recurrent Neural Networks or more efficient variants, such as LSTMs <ref type="bibr" target="#b4">[5]</ref>, for the prediction task. The Social-LSTM architecture <ref type="bibr" target="#b0">[1]</ref> associates each agent to a LSTM network and a social pooling aggregates the hidden states of the neighboring agents, to form an interaction feature. Then, each agent interaction feature is combined with its own hidden state to generate the predicted positions for the future frames, with another LSTM network.</p><p>In <ref type="bibr" target="#b19">[20]</ref>, groups of agents are modeled as spatio-temporal graphs where edges (temporal and spatial) are associated to RNNs. Temporal edges capture the evolution of single humans while spatial edges capture the evolution of agentto-neighbors relationships. These hidden features are combined linearly to produce an influence score feeding the temporal network. The prediction output takes the form of a bivariate Gaussian distribution.</p><p>In <ref type="bibr" target="#b21">[22]</ref>, a Crowd Interaction Deep Neural Network uses four modules: A trajectory encoding module encodes individual trajectories using LSTMs units; A location encoding module maps the locations of the pedestrians and the influence they have on each other; An interaction module forms linear combinations of other agents trajectory encodings, weighted by their influence; Finally, the predicted trajectory is determined by sending this linear combination through a fully connected layer. The reported results look promising, however we were not able to reproduce them entirely.</p><p>In <ref type="bibr" target="#b13">[14]</ref>, LSTMs capture the evolution of single trajectories, while the interaction history is handled through a LSTM fed with histograms of closest distances over an angular discretization of the surrounding, while the local obstacles are embedded in an occupancy grid.</p><p>Handling the multimodal nature of predictions with generative NNs. In many situations, the predictive distribution of a pedestrian motion is inherently multi-modal, e.g., at a crossroads. Without a proper modeling of this multi-modality, RNN-based methods, given observed trajectories with multiple possible outcomes, may simply be condemned to average all the possible outputs. The DE-SIRE architecture <ref type="bibr" target="#b9">[10]</ref> handles this multi-modality. A Sample Generation Module based on variational auto-encoders generates samples of potential outcome trajectories and the Ranking and Refinement Module evaluates a learned longterm score associated to the sampled trajectories and refines these trajectories, in an inverse optimal control scheme.</p><p>In <ref type="bibr" target="#b16">[17]</ref>, a social-aware LSTM, similar to <ref type="bibr" target="#b0">[1]</ref>, embeds the prior from the training data as hidden feature. Motion variability is taken into account by using layered Gaussian processes acting on the hidden features of the LSTMs. Finally, following the success of Generative Adversarial Networks (GAN) <ref type="bibr" target="#b3">[4]</ref> in other areas to learn data distributions and produce new samples <ref type="bibr" target="#b1">[2]</ref>, Gupta et al. have proposed a trajectory sampler that handles the interactions between all the observed pedestrians by pooling the GAN input random vector with a vector combining the hidden representations of the other pedestrians trajectories <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem statement and system overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notations and problem formulation</head><p>In the following, we use indices i, j ∈ {1, ..., N } to refer to pedestrians, where N is the total number of pedestrians; a single observation of pedestrian i in the scene at time t is denoted by the 4 × 1 vector x i t , which itself contains the position p i t and velocity v i t of the pedestrian:</p><formula xml:id="formula_0">x i t = ∆ ((p i t ) T , (v i t ) T ) T .</formula><p>We assume that we have access to τ + 1 consecutive observed samples x i −τ :0 of the pedestrians trajectory for each i ∈ {1, ..., N }. We also handle the set of observed samples of all pedestrians except i with</p><formula xml:id="formula_1">X ¬i −τ :0 = ∆ {x j −τ :0 |j ∈ {1, ..., N }, j = i}.</formula><p>The problem is then to predict the trajectories of each pedestrian for the next T time steps, i.e. x i 1:T . The rationale behind our approach is the following: When deciding his steering actions, a pedestrian anticipates likely scenarios about the evolution of his surrounding in the near future. Now, this anticipation may not be always very easy, because of the uncertainties in the neighbors future motion and intentions. In most recent NN-based motion prediction systems <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b13">14]</ref>, the input is taken as the set of most recent observations of the surrounding pedestrians. Hence, the mappings from observations to predicted trajectories built through the networks do not consider explicitly the uncertain and multimodal nature of the neighbors future trajectories, and, in a way, the network is expected to learn it too, which may be too much to expect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">GAN-based Individual Trajectory Sampler</head><p>Our Social Ways GAN generates independent random trajectory samples that mimic the distribution of trajectories among our training data, conditioned on observed initial tracklets of duration τ for all the agents in the scene. This system is depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>. It takes as an input the observed trajectories of N pedestrians, X −τ :0 and a random vector z sampled from a fixed distribution p z . It samples a plausible trajectoryx i,k 1:T for agent i for the next T time steps, where k identifies one generated sample. The network should learn the whereabouts of an agent altogether with the impact a surrounding crowd has on its trajectory.</p><p>A GAN contains two components that act in opposition to each other during the training phase <ref type="bibr" target="#b3">[4]</ref>. The Discriminator D is trained to detect fake samples from real ones, while the Generator G should produce new samples that fool the Discriminator and confuse its predictions. In a conditional version, both the Generator and the Discriminator are conditioned on some given data. Here, our GAN is conditioned on recent observations x i −τ :0 , for agent i, and X ¬i −τ :0 , for the other agents, and the Generator uses a noise vector z to complete x i −τ :0 into a full trajectory G(z|x i −τ :0 , X ¬i −τ :0 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Description of the Generator network</head><p>Our system shares a number of characteristics with existing trajectory generation systems <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref> but it also includes critical novelties. The Generator network uses one LSTM layer (denoted as LSTM-E) to learn the temporal features along trajectories. The encoding of past trajectories x i −τ :0</p><p>for an agent is similar to <ref type="bibr" target="#b5">[6]</ref>. The LSTM-E cell encodes the history of the agent i through the recursive application of:</p><formula xml:id="formula_2">h i t = λ e (h i t−1 , µ(x i t ; W µ ); W λ e )<label>(1)</label></formula><p>with t ∈ [−τ, 0], µ a linear embedding of the agent state and λ e the cell of LSTM-E. h i t is the hidden state vector in LSTM-E at time t. It is depicted at the left part of <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>For the decoding process and the generation of samples, we apply a similar process through another LSTM layer (denoted as LSTM-D) with hidden state k i</p><formula xml:id="formula_3">t k i t = λ d (k i t−1 , o i t−1 ; W λ d )<label>(2)</label></formula><p>with t ∈ [1, T ] and λ d the decoding LSTM-D layer. The input vector is:</p><formula xml:id="formula_4">o i t = [(h i t ) T , ( j =i a ij h j t ) T , (z) T ] T<label>(3)</label></formula><p>It stacks information from the encoded history of observations of agent i up to t, h i t , from the noise vector z, and from the impact of future trajectories of the neighboring agents j, j =i a ij h j t . The construction of this term is described hereafter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Social Ways: Attention pooling</head><p>The influence of the other agents on agent i is evaluated by encoding the vector X ¬i 1:T , through LSTM-E, and by applying an attention weighting process that produces weights a i = ∆ [a i1 , .., a ij , ..., a iN ] T for agent i. They are defined as in <ref type="bibr" target="#b15">[16]</ref>, for j = i, based on pre-defined geometric features δ ij ∈ R 3 stacking (1) the Euclidean distance between agents i and j, (2) the bearing angle of agent j from agent i (i.e. the angle between the velocity vector of agent i and the vector joining agents i and j), and (3) the distance of closest approach (i.e. the smallest distance two agents would reach in the future if both maintain their current velocity) <ref type="bibr" target="#b7">[8]</ref>.</p><p>An interaction feature vector between agents i and j is defined as an embedding in R dσ of the social features δ ij , through a FC layer f ij = φ(δ ij ; W φ ). Finally, the attention weights are obtained with the following scalar products and softmax operations between the hidden history vectors h k and the interaction feature vectors f ik</p><formula xml:id="formula_5">σ(f ik , h k ) = N − 1 √ dσ &lt; f ik , Wσh k &gt;,<label>(4)</label></formula><formula xml:id="formula_6">a ij = exp(σ(f ij , h j )) k =i exp(σ(f ik , h k ))<label>(5)</label></formula><p>where d σ is the common number of rows of the embedded features f and of the linear mapping W σ applied on the hidden features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Discriminator</head><p>The Discriminator is described on the right part of <ref type="figure" target="#fig_0">Fig. 2</ref>. It contains two encoding LSTM layers, one (applied τ + 1 times) for observations, and one (applied T times) for predictions, and 2 FC layers to predict the samples labels. It takes as an input either a composite candidate trajectories for agent i, [x i −τ :0 ,x i,k 1:T ], or a ground truth trajectory, [x i −τ :T ], and outputs a probability for any of them to have been taken as a sample from the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Training the GAN</head><p>GAN training is known to be hard, as it may not converge, exhibit vanishing gradients when there is imbalance between the Generator and the Discriminator, or may be subject to mode collapsing, i.e. sampling of synthetic data without diversity. When predicting pedestrian motion, it is critical to avoid mode collapsing, as it could result in catastrophic decisions, i.e. for an autonomous driving agent.</p><p>Here, we have introduced two major changes in the GAN training. First, we do not use, as in other stochastic prediction methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref>, an L2 loss term G(z|x i −τ :0 , X ¬i −τ :0 )− x i −τ :T 2 enforcing the generated samples to be close to the true data, because we have observed negative impact of this term in the diversity of the generated samples.</p><p>Also, we have implemented an Info-GAN <ref type="bibr" target="#b2">[3]</ref> architecture, which, as we will see in the experimental results section, has a very positive impact on avoiding the mode collapsing problem with respect to other versions of GANs. Info-GAN learns disentangled representations of the sources of variation among the data, and does so by introducing a new coding variable c as an input (see <ref type="figure" target="#fig_0">Fig. 2</ref>). The training is performed by adding another term to maximize a lower bound of the mutual information between the distribution of c and the distribution of the generated outputs, which requires training another sub-network Q(c|x 1:T ) (with parameters θ Q ) which serves as a surrogate to evaluate the likelihoods p(c|x 1:T ) over the generated data x 1:T . The training optimization problem is written as:</p><formula xml:id="formula_7">min θ G ,θ Q max θ D V (θG, θQ, θD) = E p data (x i −τ :T ) [log D(x i 1:T |x i −τ :0 ; θD)]+ E pz (z) [log(1 − D(G(z|x i −τ :0 , X ¬i −τ :0 ; θG); θD))]− λE p(c),pz (z) [log Q(c|G(z|x i −τ :0 , X ¬i −τ :0 ; θG); θQ)]<label>(6)</label></formula><p>where z is the noise input and c the new latent code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>We implemented our system using PyTorch framework. First, note that all the internal FC layers of both the Generator and the Discriminator are associated to LeakyReLU activation functions, with slope 0.1.</p><p>Generator In each dataset, we train the GAN network with the following hyper-parameters setting: mini-batch size 64, learning rate 0.001 for Generator and 0.0001 for Discriminator, momentum 0.9. The GAN is trained for 20000 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head><p>For the evaluation of our approach, we use two publicly available datasets: ETH <ref type="bibr" target="#b12">[13]</ref> and UCY <ref type="bibr" target="#b10">[11]</ref>. These datasets consist of real-world human trajectories. They are labeled manually at a rate of 2.5 fps. The ETH dataset contains 2 experiments (coined as ETH and Hotel) and the UCY dataset contains 3 experiments (ZARA01, ZARA02 and Univ). In order to evaluate the prediction algorithm, each dataset is split into 5 subsets, where we train and validate our model on 4 sets and test on the remaining set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Baseline Predictors and Accuracy Metrics</head><p>We consider two sets of baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Deterministic prediction models, that generate one trajectory for each observation:</head><p>• Linear: This is a simple constant velocity predictor.</p><p>• S-Force: It uses an energy function based on Social Forces to optimize the next agent action. The function penalizes jerky movements, high minimum distance to other agents and so on. We use the version by Yamaguchi et al. <ref type="bibr" target="#b22">[23]</ref>, in which a term enforces the agent to stay close to the group it belongs to.</p><p>• S-LSTM <ref type="bibr" target="#b0">[1]</ref>: It associates each pedestrian to one LSTM unit (the Social-LSTM) and gathers the hidden states of neighboring pedestrians with a so-called social-pooling mechanism to perform the prediction.</p><p>2. Stochastic prediction models, that generate a set of samples from a surrogate of the predictive distribution:</p><p>• Social-GAN: A GAN-based prediction <ref type="bibr" target="#b5">[6]</ref>. We consider the variants S-GAN-P and S-GAN, with and without a pooling mechanism, respectively.</p><p>• SoPhie <ref type="bibr" target="#b15">[16]</ref> which implements Social and Physical attention mechanism in a GAN predictor.</p><p>Similarly to previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>, we use the following metrics to evaluate the proposed system over the prediction on one testing data x i −τ :T : 1. Average Displacement Error (ADE), averaging Euclidean distances between ground truth and predicted positions over all time steps:</p><formula xml:id="formula_8">ADE(x i −τ :T ) = 1 T T t=1 x i t −x i t (x i −τ,0 , X ¬i −τ,0 ) .<label>(7)</label></formula><p>2. Final Displacement Error (FDE), i.e. Euclidean distance between the ground truth and predicted final position:</p><formula xml:id="formula_9">FDE(x i −τ :T ) = x i T −x i T (x i −τ,0 ) .<label>(8)</label></formula><p>Then, we evaluate the expectations of these errors over all the samples in our testing datasets. We observe τ = 8 frames (2.8 seconds) and predict the next T = 12 frames (4.8 seconds).</p><p>To evaluate stochastic models (that generate a set of samples), we use the methodology proposed in <ref type="bibr" target="#b5">[6]</ref>. We generate K samples and take the closest one to Ground truth for evaluation. Hereafter, we consider K = 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation of Prediction Errors</head><p>The average prediction errors for both ADE and FDE metrics are shown in <ref type="table" target="#tab_0">Table 1</ref>. As it can be seen, the use of our approach leads to significantly lower prediction errors for the ETH and Hotel experiments, but not on the ZARA experiments. We attribute this behavior in that, in the ZARA experiments, the width of the waypath for pedestrians is significantly smaller than in the Hotel and ETH scenes. Hence, there is less variance in the trajectories. Our proposed system intrinsically tends to generate various samples that result in good performance with more complex scenes and non-linear trajectories.</p><p>Among the deterministic models, though Social-LSTM model uses a much more complex system than its counterparts, it fails to outperform the other baselines and as the authors in <ref type="bibr" target="#b5">[6]</ref> mention it, it needs a synthetic dataset as a second source of training to improve the system accuracy. In <ref type="figure" target="#fig_2">Figure 3</ref>, we give qualitative examples of the outputs and intermediate elements in our approach. We generated 128 samples with our method and the predictive distribution are shown with magenta points. In most of the scenarios (including non-linear actions, collision avoidance and group behaviors), the distribution has a good coverage of the ground truth trajectories and also generates what seems to be plausible alternative trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Quality of the Predictive Distributions</head><p>As commented in Section 3.2, our architecture and its training process are designed to preserve the modes of the predictive trajectory distribution. However, in all the datasets that we have tested, there are very few examples of clearly multi-modal predictive trajectory distributions. Hence, we have created a toy example dataset to study the mode collapsing problem with stochastic predictors.</p><p>This toy example is depicted in <ref type="figure" target="#fig_3">Fig. 4</ref>: Given an observed sub-trajectory (blue lines), the Generator should predict the rest of the trajectory (red lines). Each of the 6 groups represents one separate condition to the system (x i −τ :0 ), and each of the 3 sub-groups represents a different mode in the conditional distribution p(x i 1:T |x i −τ :0 ). Note that the interactions between agents are not considered here.</p><p>In order to compare our approach with other GAN-based techniques, we implemented several baselines. In all of them, the prediction architecture is the one we proposed without the attention-pooling; the GAN subsystem changes.</p><p>• Vanilla-GAN: This is simplest baseline, where the Generator is just trained with the adversarial loss.</p><p>• L2-GAN In addition to adversarial loss, a L2 loss is added to the Generator optimizer.</p><p>• S-GAN-V20: The Variety loss proposed in Social-GAN method <ref type="bibr" target="#b5">[6]</ref> is added to the adversarial loss. This L2loss only penalizes the closest prediction to ground truth among V = 20 predictions and gives more freedom to choose prediction samples.</p><p>• Unrolled10: Vanilla-GAN with the unrolling mechanism proposed in <ref type="bibr" target="#b11">[12]</ref>. The number of unrolling steps is 10.</p><p>For each of the 6 possible observations, we generate 128 samples, which are depicted in <ref type="figure" target="#fig_4">Fig. 5</ref>. The Info-GAN together with Unrolled-GAN performs the best, with a slight advantage for Info-GAN, since almost all of the modes are preserved successfully after 90,000 iterations. At the same time, Vanilla-GAN, L2-GAN and S-GAN-V20 could not preserve the multi-modality of the predictions. One can see that using L2 loss, the model is converging faster than Vanil-laGAN and S-GAN-V20.</p><p>For a more quantitative evaluation of generative models, we have used the following two metrics to assess the set of fake trajectories versus the set of real samples <ref type="bibr" target="#b20">[21]</ref>. Given two sets of samples S r = {x i r } and S g = {x j g } with |S r | = |S g | and x i r ∼ P r and and x j g ∼ P g :</p><p>1. A 1-Nearest Neighbor classifier, used in two-sample tests to assess whether two distributions are identical. We compute the leave-one-out accuracy of a 1-NN classifier trained on S r and S g with positive labels for S r and negative labels for S g . The classification accuracy for data from an ideal GAN should be close to 50% when |S r | = |S g | is large enough. Values close to 100% mean that the generated samples are not close to real samples enough. Values close to 0% mean that the generated samples are exact copies of real samples, and that there is a lack of innovation in such system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The Earth</head><p>Mover's Distance (EMD) between the two distributions. It is computed as in Eq. 9:</p><formula xml:id="formula_10">EM D(Pr, Pg) = min w∈R n×m n i=1 m j=1 w ij d(x i r , x j g ) s.t. ∀i, j w ij ≥ 0, m k=1 w ik = 1 n , n k=1 w kj = 1 m .<label>(9)</label></formula><p>where d() is called the ground distance. In our case we use the ADE of Eq. 7, between the future parts of the two trajectories.</p><p>We computed both 1-NN and EMD metrics on our toy dataset with |S r | = |S g | = 20, for each of the 6 observed trajectories. The results for different baselines are shown in Figures 6. We added evaluations for a few combinations of the aforementioned baselines (e.g., Info-GAN+unrolling  steps or Unrolled+L2). The lower 1-NN accuracy of our approach using Info-GAN shows its higher performance for matching the target distribution, compared to Vanilla-GAN and other baselines. It is worth noting that the fluctuations in the accuracies are related to the small size of the set of samples. As it can be seen, Unrolled10 and Info+Unrolled5 have also better performances, while it is obvious that by adding L2 loss, the results are getting worse. The results of the EMD test also proves that both Info-GAN and Un-rolled10 offer more stable predictors with lower distances between the fake and real samples. There is no evidence that the Variety loss offers better results than a Vanilla-GAN.</p><p>Moreover, on real trajectories, we have tested our algorithm on the Stanford Drone Dataset (SDD) <ref type="bibr" target="#b14">[15]</ref>. In fact, we have used subsets of trajectories from two scenes (Hyang-6 and Gates-2). As you see in <ref type="figure">Fig</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Works</head><p>We have presented a novel approach for the prediction of pedestrians trajectories among crowds. It uses an Info-GAN to produce samples from the predictive distribution of individual trajectories, and integrates a few hand-designed interaction features inspired from the neuroscience/bio-  mechanics literature, as a form of prior over the attention pooling process. We have shown through extensive evaluations on commonly used datasets that this approach partly improves the prediction accuracy of state-of-the-art methods on the datasets where the predictive distributions have the largest variances. We have also proposed a specifically designed dataset and an evaluation benchmark to show that Info-GANs achieve the best results in preserving multimodality, compared with other variants. Finally, we are aware that is still room for improving the current generative models in pedestrian motion prediction and, above all, for exploiting these models in decision making.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Block Diagram of the Social Ways prediction system. The yellow ellipses represent loss calculations. The dashed arrows show the backpropagation directions. The bold arrows carry ground truth data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>:</head><label></label><figDesc>comprises a first FC linear embedding µ of size 4 × 128, over positions and velocities. The Encoder block in Generator contains one layer of 128 LSTM units (LSTM-E). Using 2 continuous latent code, noise vector with length of 62, and pooling vectors of size 64, which totally gives a 256-d vector, the Decoder LSTM (LSTM-D's) then uses 128 LSTM units in one layer and 3 FC layers with size of 64, 32, 2 to decode the predictions. Weights are shared among LSTM layers with the same function.Discriminator: uses two LSTM blocks (LSTM-OE and LSTM-PE) with hidden layers of size 128 to process both the observed trajectories (size 4 × τ + 4) and the predicted/"future" trajectories (size 4 × T ); these outputs are processed in parallel with two 64 × 64 FC layers. Then they are concatenated in fed to two separate FC blocks: softclassifier (D) [64 × 1] and latent-code reconstructor [64 × 2] (Q). Finally, τ and T are set to 7 and 12 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>In this figure, we illustrate our sample outputs (in magenta color). The observed trajectories are shown in blue and ground truth prediction and constant-velocity predictions are shown in cyan and orange lines, respectively. [Best viewed in color.]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Toy trajectory dataset. There are six groups of trajectories, all starting from one specific point located along a circle (blue dots). When approaching the circle center, they split into 3 subgroups. Their endpoints are the green dots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>. 7, with our system (left column), separate modes of the predictions appear clearly where the intuition would set them, while the Vanilla-GAN (right column) could not produce various paths. Results of learning baselines on Toy Example, for different numbers of iterations. [Best viewed in color.]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Statistics for different GAN implementations over training iteration. Upper row: 1-NN accuracy metric (closer to %50 is better). Lower row: Earth Mover's Distance between generated and ground truth samples (the lower, the better). Multi-modal trajectory predictive distributions on the SDD dataset: Social-Ways vs. Vanila-GAN. [Best viewed in color.]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>/ 1.22 0.67 / 1.52 1.09 / 2.35 0.68 / 1.26 0.77 / 1.38 0.70 / 1.43 0.39 / 0.64 Hotel 0.36 / 0.64 0.52 / 1.03 0.79 / 1.76 0.47 / 1.01 0.44 / 0.89 0.76 / 1.67 0.39 / 0.66 Univ 0.82 / 1.68 0.74 / 1.12 0.67 / 1.40 0.56 / 1.18 0.75 / 1.50 0.54 / 1.24 0.55 / 1.31 ZARA01 0.44 / 0.98 0.40 / 0.60 0.47 / 1.00 0.34 / 0.69 0.35 / 0.69 0.30 / 0.63 Comparison of prediction error of our proposed method (S-Ways) vs baselines. The ADE and FDE values are separated by slash.</figDesc><table><row><cell></cell><cell cols="3">Deterministic Models</cell><cell></cell><cell cols="2">Stochastic Models</cell><cell></cell></row><row><cell>Dataset</cell><cell>Linear</cell><cell>S-Force</cell><cell>S-LSTM</cell><cell>S-GAN</cell><cell>S-GAN-P</cell><cell>SoPhie</cell><cell>S-Ways</cell></row><row><cell>ETH</cell><cell cols="7">0.59 0.44 / 0.64</cell></row><row><cell cols="7">ZARA02 0.43 / 0.95 0.40 / 0.68 0.56 / 1.17 0.31 / 0.64 0.36 / 0.72 0.38 / 0.78</cell><cell>0.51 / 0.92</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Began: Boundary equilibrium generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<idno>abs/1703.10717</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<idno>II-1764-II- 1772. JMLR.org</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Machine Learning (ICML)</title>
		<meeting>of the Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Social gan: Socially acceptable trajectories with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Will the pedestrian cross? a study on pedestrian path prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="494" to="506" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Context-based pedestrian path prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F P</forename><surname>Kooij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Flohr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning an image-based motion context for multiple people tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fenzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="3542" to="3549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Desire: Distant future prediction in dynamic scenes with interacting agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of the Int. Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Crowds by example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chrysanthou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="655" to="664" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno>abs/1611.02163</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">You&apos;ll never walk alone: Modeling social behavior for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE Int. Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2009-09" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A data-driven model for interactionaware pedestrian motion prediction in object cluttered environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Paolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Robotics and Automation (ICRA)</title>
		<meeting>of the IEEE Int. Conf. on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning social etiquette: Human trajectory understanding in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Sophie: An attentive gan for predicting paths compliant to social and physical constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01482</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Forecast the plausible paths in crowd scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>of the Int. Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2772" to="2778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robot navigation in dense human crowds: Statistical models and experimental studies of human-robot cooperation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Trautman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="335" to="356" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reciprocal n-body collision avoidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics Research</title>
		<editor>C. Pradalier, R. Siegwart, and G. Hirzinger</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Social attention: Modeling attention in human crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vemula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muelling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Robotics and Automation (ICRA</title>
		<meeting>of the IEEE Int. Conf. on Robotics and Automation (ICRA</meeting>
		<imprint>
			<date type="published" when="2018-05" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07755</idno>
		<title level="m">An empirical study on evaluation metrics of generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Encoding crowd interaction with deep neural network for pedestrian trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Who are you with and where are you going?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1345" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
