<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distributional Robustness Loss for Long-tail Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dvir</forename><surname>Samuel</surname></persName>
							<email>dvirsamuel@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Gal Chechik Bar</orgName>
								<orgName type="institution">Ilan University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">NVIDIA Research</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Distributional Robustness Loss for Long-tail Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-world data is often unbalanced and long-tailed, but deep models struggle to recognize rare classes in the presence of frequent classes. To address unbalanced data, most studies try balancing the data, the loss, or the classifier to reduce classification bias towards head classes. Far less attention has been given to the latent representations learned with unbalanced data. We show that the feature extractor part of deep networks suffers greatly from this bias. We propose a new loss based on robustness theory, which encourages the model to learn high-quality representations for both head and tail classes. While the general form of the robustness loss may be hard to compute, we further derive an easy-to-compute upper bound that can be minimized efficiently. This procedure reduces representation bias towards head classes in the feature space and achieves new SOTA results on CIFAR100-LT, ImageNet-LT, and iNaturalist long-tail benchmarks. We find that training with robustness increases recognition accuracy of tail classes while largely maintaining the accuracy of head classes. The new robustness loss can be combined with various classifier balancing techniques and can be applied to representations at several layers of the deep model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Real-world data typically has a long-tailed distribution over semantic classes: few classes are highly frequent, while many classes are only rarely encountered. When trained with such unbalanced data, deep models tend to produce predictions that are biased and over-confident towards head classes and fail to recognize tail classes.</p><p>Early approaches for handling unbalanced data used resampling <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref> or loss reweighing <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27]</ref> aiming to re-balance the training process. Other approaches address unbalanced data by transferring information from head to tail classes <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b28">29]</ref>, or by applying an adaptive loss <ref type="bibr" target="#b5">[6]</ref> or regularization of classifiers <ref type="bibr" target="#b20">[21]</ref>. The main focus of these techniques is on balancing the multi-class classifier. Here, the empirical centroid µ 2 (framed pink triangle) is estimated based only on few samples (four pink triangles) and as a result it deviates far from the true centroid (µ 2 ). Our loss pulls the same-class samples (green arrow), and pushes away other-class samples (red arrows). The loss takes into account the estimation error by pushing and pulling towards a worst-case possible distribution within an uncertainty area around the estimated centroid (dashed red line). Uncertainty areas are typically larger for tail classes, compared with head classes that have many samples (blue dashed line around µ 1 ).</p><p>Far less attention has been given to the latent representations learned with unbalanced data. Intuitively, head classes are encountered more often during training and are expected to dominate the latent representation at the top layer of a deep model. Counter to this intuition, <ref type="bibr" target="#b20">[21]</ref> compared a series of techniques for rebalancing representations and concluded that "data imbalance might not be an issue in learning high-quality representations", and that strong long-tailed recognition can be achieved by only adjusting the classifier. However, the effect of unbalanced data on the learned representations is far from being understood, and the extent to which it may hurt classification accuracy is unknown. While existing rebalancing methods do not improve representa-tions, the question remains if better representations could substantially improve recognition with unbalanced data.</p><p>The current paper focuses on improving the representation layer of deep models trained with unbalanced data. We show that large gains in accuracy can be achieved simply by balancing the representation at the last layer. The key insight is that the distribution of training samples of tail classes does not represent well the true distribution of the data. This yields representations that hinder the classifier applied to that representation.</p><p>To address this problem, we introduce ideas from robust optimization theory. We design a loss that is more robust to the uncertainty and variability of tail representations. Standard training follows Empirical Risk Minimization (ERM), which is designed to learn models that perform well on the training distribution. However, ERM assumes that the test distribution is the same as the training distribution, and this assumption often breaks with tail classes. In comparison, Distributionally Robust Optimization (DRO) <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b3">4]</ref> is designed to be robust against likely shifts of the test distribution. It learns classifiers that can handle the worst-case distribution within a neighborhood of the training distribution. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates this idea.</p><p>In the general case, computing a loss based on a worstcase distribution may be computationally hard. Here, we show how the worst-case loss can be bounded from above, with a bound that has an intuitive form and can be easily minimized. The resulting bound allows us to minimize the DRO loss, which only affects the representation, and to combine it with a standard classification loss, which tunes a classifier on top of the representation.</p><p>The main contributions of this paper are:</p><p>1. We formulate learning with unbalanced data as a problem of robust optimization and highlight the role of high variance in hindering tail accuracy.</p><p>2. We develop a new loss, DRO-LT Loss, based on distributional robustness optimization for learning a balanced feature extractor. Training with DRO-LT Loss yield representations that describe well both head classes and tail classes.</p><p>3. We derive an upper bound of the robustness loss that can be computed and optimized efficiently. We further show how to learn the robustness safety margin for each class, jointly with the task, avoiding additional hyper-parameter tuning.</p><p>4. We evaluate our approach on three long-tailed visual recognition benchmarks: CIFAR100-LT, ImageNet-LT, and iNaturalist. Our proposed method consistently achieve superior performance over previous models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Long-tail recognition</head><p>Real-world data usually follows a long-tailed distribution, which causes models to favor head classes and overfit tail classes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35]</ref>. Previous efforts to address this effect can be divided into four main approaches: Data-manipulation approaches, Loss-manipulation approaches, Two-stage finetuning, and ensemble methods.</p><p>Data-manipulation (re-sampling): Data-manipulation approaches aim to balance long-tail data. There are three popular techniques of resampling strategies: (1) Over-sampling minority (tail) classes by simply copying samples <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>(2) Under-sampling majority (head) classes by removing samples <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20]</ref>. (3) Generating augmented samples to supplement tail classes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b6">7]</ref>. While simple and intuitive, over-sampling methods suffer from heavy over-fitting on the tail classes, under-sampling methods degrade the generalization of models, and data augmentation methods are expensive to develop.</p><p>Loss-manipulation (re-weighting): Loss reweighting approaches encourage learning of tail classes by setting costs to be non-uniform across classes. For instance, <ref type="bibr" target="#b17">[18]</ref> scaled the loss by inverse class frequency. An alternative strategy down-weighs the loss of well-classified examples, preventing easy negatives from dominating the loss <ref type="bibr" target="#b26">[27]</ref> or dynamically rescale the cross-entropy loss based on the difficulty to classify a sample <ref type="bibr" target="#b33">[34]</ref>. <ref type="bibr" target="#b5">[6]</ref> proposed to encourage larger margins for rare classes. <ref type="bibr" target="#b21">[22]</ref> use class-uncertainty information, using Bayesian uncertainty estimates, to learn robust features, and <ref type="bibr" target="#b9">[10]</ref> reweighs using the effective number of samples instead of proportional frequency.</p><p>Two-stage fine-tuning: Two-stage methods separate the training process into representation learning and classifier learning <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref>. The first stage aims to learn good representations from unmodified long-tailed data, training using conventional cross-entropy without re-sampling or reweighting. The second stage aims to balance the classifier by freezing the backbone and finetuning the last fully connected layers with re-sampling techniques or by learning to unbias the confidence of the classifier. Those methods basically assume that the bias towards head classes is significant only in the classifier (i.e, last fully connected layer), or that tweaking the classifier layer can correct the underlying biases in the representation.</p><p>Ensemble-models: Ensemble models focus on generating a balanced model by assembling and grouping models. Typically, classes are separated into groups, where classes that contain similar training instances are grouped together. Then, individual models focused on each group are assembled to form a multi-expert framework. <ref type="bibr" target="#b45">[46]</ref> learned one branch for head classes and another for tail classes, then combine the branches using a soft-fusion procedure. <ref type="bibr" target="#b42">[43]</ref> distilled a unified model from multiple teacher classifiers. Each classifier focuses on the classification of a small and relatively balanced group of classes from the data. <ref type="bibr" target="#b39">[40]</ref> described a shared architecture for multiple classifiers, a distribution-aware loss, and an expert routing module. The current paper proposes a two-stage approach for training one model with a single classifier. Thus, we compare our method with non-ensemble approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Distributionally robust optimization</head><p>Distributionally Robust Optimization (DRO) <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b3">4]</ref> considers the case where the test distribution is different from the train distribution. It does that by defining an uncertainty set of test distributions around the training distribution and learning a classifier that minimizes the expected risk with respect to the worst-case test distribution. DRO has been shown to be equivalent to standard robust optimization (RO) <ref type="bibr" target="#b43">[44]</ref> and has a strong relationship with regularization methods <ref type="bibr" target="#b35">[36]</ref>, Risk-Aversion methods and game theory <ref type="bibr" target="#b32">[33]</ref>. For more details, see <ref type="bibr" target="#b32">[33]</ref>. As far as we know, DRO theory was not applied for long-tail learning.</p><p>Many studies looked into learning representations that are robust to adversarial attacks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b15">16]</ref> but this is outside the scope of the current paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview of our approach</head><p>We start with an overview of the main idea of our approach and provide the details in subsequent sections.</p><p>Our goal is to learn a representation at the last layer of a deep network, such that the distributions of samples of different classes are well separated. Then, they can later be correctly classified by different linear classifiers. Deep networks can learn such representations efficiently when trained with enough labeled data. However, when a class has only few samples, the distribution of training samples may not represent well the true distribution of the data, and the representations learned by the model hinder the classifier.</p><p>To remedy this shortcoming, we design a loss that is applied to the representation, which takes into account errors due to a small number of samples. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates this idea. Our loss extends standard contrastive losses, which pull samples closer to the centroid of their own class and push away samples that belong to other classes. Our new loss accounts for the fact that the true centroids are unknown, and their estimate is noisy. It, therefore, optimizes against the worst possible centroids within a safety hyper ball around the empirical centroid.</p><p>In the general case, computing such a worst-case loss may be computationally hard. We further derive an upper bound of that loss that can be computed easily and show that using that bound as a surrogate loss, yields significantly better representations. The resulting loss has a simple general form, yielding that the loss for a sample z is</p><formula xml:id="formula_0">L(z) Robust = − log e −d( µc,z)−∆ z e −d( µc,z )−∆ .<label>(1)</label></formula><p>where d( µ c , z) measures the distance in feature space between a sample z and the estimated centroid of its class µ c , ∆ and ∆ are robustness margins that we describe below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Distributional Robust Optimization</head><p>When learning a classifier, we seek a model f that minimizes the expected loss for the data distribution P (x, y).</p><formula xml:id="formula_1">min f E x∼P [l f (x)] = l(f (x), y)dP (x, y).<label>(2)</label></formula><p>Since the data distribution is not known, Empirical Risk Minimization (ERM) <ref type="bibr" target="#b38">[39]</ref> proposes to use the training data for an empirical estimate of the data distribution P δ = 1</p><formula xml:id="formula_2">n δ(x = x i , y = y i ), where δ is the kronecker delta. ERM: min f E (x,y)∼P δ [l f (x)]<label>(3)</label></formula><p>Unfortunately, using P δ to approximate P makes the naive assumption that the test distribution would be close to the empirical train distribution. That assumption may be far from true when the training data is small. In those cases, it is beneficial to choose other estimates of P that are more likely to reduce the loss over the test distribution. One such solution is given by Distributionally robust optimization (DRO) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b3">4]</ref>. It suggests learning a model f that minimizes the loss within a family of possible distributions</p><formula xml:id="formula_3">DRO: min f sup Q∈U E (x,y)∼Q [l f (x)].<label>(4)</label></formula><p>DRO aims to perform well simultaneously for a set of test distributions, each in an uncertainty set U . The set of distributions U is typically chosen as a hyper ball of radius ( -ball) around the empirical training distribution P :</p><formula xml:id="formula_4">U := {Q : D(Q, P ) ≤ },<label>(5)</label></formula><p>where D is a discrepancy measure between distributions, usually chosen to be the KL-divergence or the Wasserstein distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Our approach</head><p>We now formally describe our approach: DRO-LT Loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Preliminaries</head><p>We are given n labeled samples (x i , y i ) i = 1, . . . , n, where y i is one of k classes {c 1 , . . . , c k }. Let f θ be a feature extractor function f with learnable parameters θ, which maps any given input sample x i to z i = f θ (x i ). The set of mapped input Z = z 1 , ...z n resides in some latent vector space.</p><p>Let S c = {z i |y i = c} be the set of feature vectors representing samples whose label is c. We denote by µ c the empirical centroid of that set, namely the mean of all feature vectors z of class c, µ c = 1 |c| |c| i=1 z i . We also denote by µ c the mean of the true data distribution P of samples from class c, µ c = E x∼P |y=c [z].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">A loss for representation learning</head><p>We adopt ideas from metric learning and representation learning <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37]</ref> for designing a representation-learning loss.</p><p>Given a representation z i of a sample x i that has a label y i = c, we wish to design a loss that maps z i close in feature space to other samples from the same class, and far from samples of other classes. We start by developing a contrastive loss and later extend it to a robust one.</p><p>Consider a sample (x i , y i ) from a class y i = c, whose feature representation is z i . We model the samples of class c as if they are distributed around a centroid µ c , and the likelihood of a sample decays exponentially with the distance from the centroid µ c of class c. Such exponential decay has been long used in metric learning and can be viewed as reflecting a random walk over the representation space <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13]</ref>. The normalized likelihood of a sample is, therefore:</p><formula xml:id="formula_5">P (z i |µ c ) = e −d(µc,zi) z ∈Z e −d(µc,z ) ,<label>(6)</label></formula><p>where d is a measure of divergence or distance between the centroid µ c and a feature representation z i . d is typically set to be the Euclidean distance, but can also be modeled with heavier tails as when using a student t-distribution <ref type="bibr" target="#b37">[38]</ref>. Similarly, for a set S c = {z i |y i = c}, the log-likelihood is:</p><formula xml:id="formula_6">log P (S c |µ c ) = zi∈Sc log e −d(µc,zi) z ∈Z e −d(µc,z ) .<label>(7)</label></formula><p>We define a negative log-likelihood loss as a weighted average over per-class losses:</p><formula xml:id="formula_7">L N LL = c∈C w(c) (− log P (S c |µ c )) (8) = − c∈C w(c) zi∈Sc log e −d(µc,zi) z ∈Z e −d(µc,z ) ,</formula><p>where w(c) are class weights. Setting w(c) = 1 |Sc| gives equal weighting to all classes and prevents head classes from dominating the loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">A robust loss</head><p>The log-likelihood loss of Eq. <ref type="formula" target="#formula_6">(7)</ref> operates under the assumption that the centroid of each class µ c is known. In reality, it is not available to us. Following Empirical-risk minimization, we could naively plug in the empirical estimate µ c in Eq. <ref type="formula" target="#formula_6">(7)</ref>, but µ c may be a poor approximation of µ c , and estimation error grows as the number of samples |S c | decreases. As a result, the log-likelihood and the loss would also be badly estimated.</p><p>Instead of approximating the NLL loss (− log P (S c |µ c )), we show that we can bound it with high probability by computing the worst-case loss over a set of candidate centroids. For that purpose, we take an approach based on distributionally robust optimization.</p><p>Let p c be the empirical distribution of samples of class c. We define the uncertainty set of candidate distributions in its neighborhood to be</p><formula xml:id="formula_8">U c := {q|D(q|| p c ) ≤ c },<label>(9)</label></formula><p>where D is a measure of divergence of two distributions. Specifically, we consider here the case where D is the Kullback-Leibler divergence, and the distributions under consideration are same-variance spherical Gaussians. In this case, their divergence equals D KL (q|| p c ) = 1 2σ 2 d(µ q , µ pc ) 2 <ref type="bibr" target="#b8">[9]</ref>, where d is the Euclidean distance. Hence for any q ∈ U c , we have</p><formula xml:id="formula_9">d(µ q , µ pc ) ≤ σ c √ 2 c ≡ ε c ,<label>(10)</label></formula><p>where we define ε c ≡ σ c √ 2 c for convenience. We now derive an upper bound on the NLL loss that we can compute using the estimated centroids µ c . The form of the bound is very intuitive, it can be viewed as a modification of the NLL loss, where the distance between a sample and an empirical centroid, is increased by a factor that depends on the radius of the uncertainty ball. Here, µ q c is the centroid of a distribution q ∈ U c . To bound − log P (z|µ q c ), we use the triangle inequality and write:</p><formula xml:id="formula_10">d(µ q c , z) ≤ d( µ c , z) + d( µ c , µ q c ) (13) d( µ c , z) ≤ d(µ q c , z) + d( µ c , µ q c ) yielding −d(µ q c , z)≥−d( µ c , z) − d( µ c , µ q c )≥−d( µ c , z) − ε c (14) −d(µ q c , z)≤−d( µ c , z) + d( µ c , µ q c )≤−d( µ c , z) + ε c (15)</formula><p>Applying Eq. <ref type="formula" target="#formula_0">(14)</ref> to the numerator of Eq. (6), and applying Eq. (15) to its denominator, we obtain:</p><formula xml:id="formula_11">P (z|µ q c ) = e −d(µ q c ,z) z ∈Z e −d(µ q c ,z ) ≥ e −d( µc,z)−2εc z ∈Z e −d( µc,z )−2εcδ(z ,c)<label>(16)</label></formula><p>where δ(z, c) = 1 when c is the class of z and 0 otherwise. This inequality holds for all distributions q ∈ U c , and also for the true distribution µ c as a special case (with probability p( )). The negative log-likelihood is therefore bounded by</p><formula xml:id="formula_12">−log P (z|µ c ) ≤ − log e −d( µc,z)−2εc z ∈Z e −d( µc,z )−2εcδ(z ,c) .<label>(17)</label></formula><p>This completes the proof.</p><p>Based on the theorem, we define a surrogate robustness loss: Joint loss. In practice, we train the deep network with a combination of two losses. A standard cross-entropy loss applied to the output of the classification layer, and the robust loss is applied to the latent representation of the penultimate layer. We linearly combine these two losses</p><formula xml:id="formula_13">L Robust = −</formula><formula xml:id="formula_14">L = λL CE + (1 − λ)L Robust .<label>(19)</label></formula><p>The trade-off parameter λ can be tuned using a validation set. See implementation details below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Training</head><p>Uncertainty radius: The size of the uncertainty -ball around each class plays an important role. When the uncertainty radius is too small, the probability that the true centroid is within the uncertainty area decreases, together with the probability that the bound holds. When the radius is too large, the bound is more likely to hold, but it becomes less tight. Furthermore, since tail classes have fewer samples, the estimate of class centroids is expected to be noisier and a larger radius is needed. We explored three ways to determine the radius:</p><p>1. Shared ε: All classes share the same uncertainty radius. 2. Sample count ε/ √ n: The class radius scales with 1/ √ n, where n is the number of training samples. This scaling is based on the fact that the standard-error-ofthe-mean decays as √ n, and leads to tail classes having a larger safety radius. 3. Learned ε: We treated the radius as a learnable parameter and tuned its value during training.</p><p>In the first two cases, the radius parameter ε was treated as a hyper-parameter and tuned using a validation set.</p><p>Training process: To compute the robustness loss, an initial feature representation of the data is required for estimating class centroids. As a result, during training, we first train the model with standard cross-entropy loss (λ = 1) to learn initial feature representations and centroids. Then, we add the DRO loss to the training by setting λ &lt; 1. Finally, as in <ref type="bibr" target="#b20">[21]</ref>, we learn a balanced classifier by freezing the feature extractor and fine-tune only the classifier with balanced sampling.</p><p>Estimating centroids: Calculating the centroids for each class is computationally expensive if computed often using the full dataset. One could estimate the centroids within each batch, but with unbalanced data, minority classes would hardly have any samples and their centroid estimates would be very poor. Instead, we compute the features z i for every sample x i at the beginning of every epoch, compute the centroids for each class, and keep them fixed in memory for the duration of the epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Datasets</head><p>We evaluated our proposed method using experiments on three major long-tailed recognition benchmarks.</p><p>(1) CIFAR100-LT [6]: A long-tailed version of CI-FAR100 <ref type="bibr" target="#b24">[25]</ref>. CIFAR100 consists of 60K images from 100 categories. Following <ref type="bibr" target="#b5">[6]</ref>, we control the degree of data imbalance with an imbalance factor β. β = Nmax Nmin , where N max and N min are the number of training samples for the most frequent and the least frequent classes, respectively. We conduct experiments with β = 100, 50, and 10.</p><p>(2) ImageNet-LT <ref type="bibr" target="#b30">[31]</ref>: A long-tailed version of the largescale object classification dataset ImageNet <ref type="bibr" target="#b10">[11]</ref> by sampling a subset following the Pareto distribution with power value  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Compared methods</head><p>We compared our approach with the following methods. (A) Baselines: CE: Naive training with a cross-entropy loss; ReSample: Over-sampling classes to reach a uniform distribution as in <ref type="bibr" target="#b5">[6]</ref>. (B) Loss-manipulation: Reweight: Reweight the loss as in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b45">46]</ref>; Focal Loss <ref type="bibr" target="#b26">[27]</ref> and LDAM Loss <ref type="bibr" target="#b5">[6]</ref>. (C) Two-stage fine-tuning: τ -norm <ref type="bibr" target="#b20">[21]</ref>, cRT <ref type="bibr" target="#b20">[21]</ref> and smDragon <ref type="bibr" target="#b34">[35]</ref>. (D) Representation learning: SSL the method by <ref type="bibr" target="#b22">[23]</ref> and SSP, the method by <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(E) DRO-LT variants:</head><p>We compared four ways to set the radii of U c . Set ε = 0 (ERM); ε value shared across all classes; A shared value divided by the square root of class size (ε/ √ n); and a learned value per class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Evaluation Protocol</head><p>Following <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b20">[21]</ref>, we report the top-1 accuracy over all classes on class balanced test sets. This metric is denoted by "Acc". For CIFAR-100 and ImageNet-LT, we further report accuracy on three splits of the set of classes. "Many": classes with more than 100 train samples; "Med": classes with 20 -100 train samples; and "Few": classes with less than 20 train samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Implementation details</head><p>In all experiments, we use an SGD optimizer with a momentum of 0.9 to optimize the network. For long-tailed CIFAR-100, we follow <ref type="bibr" target="#b5">[6]</ref> and train a ResNet-32 backbone on one GPU with a multistep learning-rate schedule. For ImageNet-LT and iNaturalist, we follow <ref type="bibr" target="#b20">[21]</ref> and use the cosine learning-rate schedule to train a ResNet-50 backbone on 4 GPUs.</p><p>Hyper-parameter tuning: We determined the number of training epochs (early-stopping), and tuned hyperparameters using the validation set. We optimized the following hyper-parameters: (1) Radius parameter ε ∈ {1, 2, 5, 10, 30, 70} for "Shared ε" and "Sample Count ε/ √ n". (2) Trade-off parameter λ ∈ {0, 0.3, 0.5, 0.7, 1}. (3) Learning rate ∈ {10 −4 , 10 −3 , 10 −2 }. we studied the sensitivity of the accuracy to the values of ε and λ, and found that high accuracy is obtained for a wide range of values. We also found accuracy to be quite stable when tuning λ and selected λ = 0.5. See the supplemental for details. <ref type="table" target="#tab_1">Table 1</ref> compares DRO-LT with common long-tail methods on CIFAR100-LT. It shows that all our robust loss variants consistently achieve the best results on all imbalance factors, emphasizing the importance of robust learning in unbalanced data. "Learned ε" outperforms other variants of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR100-LT:</head><p>ImageNet-LT: <ref type="table">Table 2</ref> further evaluates our approach on ImageNet-LT and CIFAR100-LT (imbalance factor=100) reporting accuracy for different test splits. DRO-LT performs well on tail classes ("Few") as well as head classes ("Many"). This contrasts with previous methods that sacrifice head accuracy for better tail classification. This also suggests that DRO-LT learns high-quality features for all classes.</p><p>iNaturalist: <ref type="table" target="#tab_3">Table 3</ref> evaluates our approach on the largescale iNaturalist. DRO-LT slightly imroves the accuracy compared with SoTA baselines.</p><p>Classifier vs feature extractor: Our method focuses  <ref type="table">Table 2</ref>: Top-1 accuracy on long-tailed CIFAR-100 <ref type="bibr" target="#b5">[6]</ref> with imbalance factor 100, and ImageNet-LT <ref type="bibr" target="#b30">[31]</ref>. We also report many-shot ("Many"), medium-shot ("Med") and few-show ("Few") performance separately. Our method performs well on tail classes without sacrificing head accuracy. Asterisks * denote results reproduced using code published by authors.</p><p>iNaturalist CE 61.7 LDAM Loss <ref type="bibr" target="#b5">[6]</ref> 68.0 τ −norm <ref type="bibr" target="#b20">[21]</ref> 69.5 CB LWS <ref type="bibr" target="#b20">[21]</ref> 69.5 smDragon <ref type="bibr" target="#b34">[35]</ref> 69.1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DRO-LT (ours)</head><p>Learned ε 69.7 on improving the learned representation at the penultimate layer. Other methods focused on improving the classifier applied to that representation. Therefore, it is interesting to explore the relation between these two tasks (compare with <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b20">21]</ref>). We, therefore, compared different feature extractors and classifier training methods. For representation learning, we employ plain training with a cross-entropy loss (CE), balanced resampling of the data (RS) and our method (DRO-LT). For classifier learning, we freeze the parameters of the feature extractor and fine-tune the classifier in three ways: cross-entropy loss (CE), re-sampling (RS) following the protocol of <ref type="bibr" target="#b5">[6]</ref>, and balanced classifier (LWS) <ref type="bibr" target="#b20">[21]</ref>. <ref type="table">Table 4</ref> provides the top-1 accuracy of all combinations. It shows that our representation learning approach enables all types of classifiers to reach good performance, compared to other representation learning approaches. This strongly suggests that our method learns: (1) good feature representations for both head and tail classes, and (2) to separate them in a way that discriminative classifiers can easily distinguish between classes.</p><p>Adaptive robustness: <ref type="figure" target="#fig_3">Figure 2</ref> shows the values of uncertainty per-class radii (ε) that were learned by training a ResNet-32 on CIFAR100-LT with an imbalance ratio of 50. The model learned slightly larger radii for tail classes compared with head classes. See the supplemental for details about the effect of the radius on accuracy.</p><p>Robustness: Our loss is expected to improve recogni-  <ref type="table">Table 4</ref>: Top-1 accuracy of different representation learning manners and classifier learning manners, on CIFAR100-LT. CE refers to Cross-Entropy, RS refers resampling and LWS refers to balanced classifier according to <ref type="bibr" target="#b20">[21]</ref>. The results suggest that simple resampling methods achieve good results when the learned features are good for both head and tail.</p><p>tion mostly at tail classes. <ref type="figure">Figure 4</ref> compares train error and test error between a model trained with cross-entropy loss (red) and a model trained with our approach (blue). Using a robustness loss cuts down errors substantially, in tail classes, without hurting head classes. Feature space visualization: To gain additional insight, we look at the t-SNE projection of learned representations and compared vanilla cross-entropy loss with our proposed method. <ref type="figure" target="#fig_5">Figure 5</ref> shows that our learned feature space is more compact with margins around head and tail classes. Tail classes have larger margins since the estimation of their features is less accurate.</p><p>How unbalanced are latent representations? The above analysis focused on correcting the representation at the penultimate layer. But how biased are the representations at lower layers? Intuitively, the low layers represent more "physical" properties of inputs, while higher layers capture properties that correspond more to semantic classes. One would expect that early layers would be quite balanced.</p><p>We tested class imbalance in several layers of a ResNet-32 in the following way. We first trained a ResNet-32 on the unbalanced CIFAR100-LT dataset. Then, for each latent representation, we computed the centroids of each class and Right: The model was trained on CIFAR-100-LT. We report balanced validation accuracy for head classes (blue), medium classes (orange) and tail classes (green). The accuracy gap between head and tail classes is substantial even at all layers. <ref type="figure">Figure 4</ref>: Comparing train and test error between a model trained with vanilla cross-entropy loss and a model trained with our approach. The gap between the train and test error for tail classes is much lower in our approach, while the gap does not change for head classes.</p><p>used them to classify all samples using a nearest-centroid classifier. <ref type="figure" target="#fig_4">Figure 3</ref> shows the accuracy obtained with this classifier when applied to layers 0, 10, 20, and 30 of the ResNet-32. When training with balanced data (left), the accuracy grows when using higher layers, as expected. Surprisingly, when training with unbalanced data (right), there is a substantial difference in accuracy achieved for head and tail classes in every layer that we tested. While accuracy grows when using higher layers, the accuracy differences between head and tail classes are maintained, even in low layers that are believed to represent class agnostic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Discussion</head><p>This paper investigates the feature representations learned by deep models trained on long-tail data. We find that such models suffer greatly from bias towards head classes in their feature extractor (backbone), which hurts recognition. This is in contrast to previous studies suggesting that unbalanced data does not hurt representation learning and re-balancing the classifier layer is sufficient. To learn a balanced representation, we take a robustness approach and develop a novel loss based on Distributionally Robust Optimization (DRO) theory. We further derive an upper bound of that loss that can be minimized efficiently. We show how the robustness safety margin can be learned during training, and do not require additional hyper-parameter tuning.</p><p>Training with a combination of the DRO-LT loss and a standard classifier sets new state-of-the-art results on three long-tail benchmarks: CIFAR100-LT, ImageNet-LT, and iNaturalist. Our method not only improves the performance of tail classes but also maintains high accuracy at the head. These results suggest that proper training of representations for unbalanced data can have a large impact on downstream accuracy. It remains to see how similar ideas can be extended to more tasks and architectures. <ref type="figure">Figure 6</ref>: Accuracy of a nearest-centroid neighbor classifier when applied to convolutional layers 0, 10, 20, and 30 of a ResNet-32. The model was trained on CIFAR-100-LT with an imbalance factor of 100. We compare a model trained with standard cross-entropy loss (solid line) and a model trained with DRO-LT (dashed line) where the loss is applied to the last convolutional layer (conv30). We report balanced validation accuracy for head classes (blue), medium classes (orange), and tail classes (green).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional information about p( )</head><p>We provide a more formal definition of the probability p( ). Consider the empirical distribution p c (z) of n samples z 1 , . . . . , z n from class c. This empirical distributionp can be viewed by itself as a random variable because it receives different values for different instantiations of the random sample.p is distributed over the simplex ∆ d where d is the dimension of the representation of z. As a result, the -ball around µ c is also a random variable, since different random samples yield different centroids and balls.</p><p>For any given , some of these balls may cover the true distribution p and some may not, depending on the instances z i drawn. p( ) denotes the probability that such an -ball covers the true distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DRO-LT Sample count / √ n</head><p>Given a set of samples z i , their mean is known to have a standard deviation of σ/ √ n, where σ is the standard deviation of the sample distribution p(z). In our case, σ is not known.</p><p>The DRO-LT variant that we call Sample count / √ n, can be viewed as assuming that all classes share the same standard deviation of their sample distribution σ, which we tune as a hyperparameter, and the uncertainty about class centroid only varies by the number of samples.   <ref type="figure">Figure 6</ref> shows the accuracy obtained with a nearestcentroid classifier when applied to layers 0, 10, 20, and 30 of a ResNet-32. We compare a model trained with standard cross-entropy loss (solid line) and a model trained with DRO-LT (dashed line) where the loss is applied to the last convolutional layer (conv30). We show that our model narrows the accuracy gap between head classes (blue) and tail classes (green), mostly in the last layer. Th suggests that applying our loss to the rest of the layers might result in a more balanced model overall. <ref type="figure" target="#fig_6">Figure 7</ref> explores the effect of different uncertainty radii (ε) on the validation accuracy of a model trained on CIFAR100-LT with an imbalance factor 100. The radius is set to be equal for all classes in our DRO-LT loss (Shared ε variant). It shows that the accuracy is maintained over a large range of ε values. Setting ε to very small values, nullifies the robustness and reduces accuracy. At the same time, very large values of ε cause the worst-case centroid in the -ball to be too far from µ c making the bound too loose and again reduces the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Balancing latent representations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Robustness vs Performance</head><p>E. Loss trade-off parameter λ <ref type="figure" target="#fig_7">Figure 8</ref> quantifies the effect of the trade-off parameter λ (Eq. 19) on the validation accuracy. The model was trained on CIFAR100-LT with an imbalance factor of 100 and with our DRO-LT loss (Learnable ε variant). It shows that training with DRO-LT alone (λ = 0) is not enough and leads to poor accuracy. Combining the robustness loss with a discriminative loss (cross-entropy) gives the best results and suggests high quality feature representations and discriminative classifier.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our distributional robustness loss is designed for learning a representation where samples are kept close to the centroid of their class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Theorem 1 .</head><label>1</label><figDesc>Let c be the radius of the uncertainty set U c and let σ c be the variance of the distribution of a sample in class c. Let p( ) be the probability that the true distribution whose centroid is µ c is within U c . The per-class negative log-likelihood is bounded by− log P (z|µ c ) ≤ − log e −d( µc,z)−2εc z ∈Z e −d( µc,z )−2εcδ(z ,c) (11)with probability p( ), where δ(z, c) = 1 if z is of the class c and 0 otherwise and ε c = σ c √ 2 c . Proof. Denote the negative log-likelihood of a given sample z and class distribution p with a centroid µ by L(z, p) = − log P (z|µ). With probability p( ), the true distribution p c is within the uncertainty ball p c ∈ U c . Therefore L(z, p c ) ≤ max q∈Uc L(z, q) = max q∈Uc (− log P (z|µ q c )). (12)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>( µc,z)−2εc z e −d( µc,z )−2εcδ(z ,c) (18) This surrogate loss amends the loss of Eq. (8) in a simple and intuitive way. It increases the distance d( µ c , z) between a sample and an empirical centroid in a way that depends on the radius of the uncertainty ball of a class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Learned ε values for each class, of a ResNet-32 trained on CIFAR100-LT with imbalance ratio 50. Blue: Polynomial fit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Accuracy of a nearest-centroid neighbor classifier when applied to convolutional layers 0, 10, 20, and 30 of a ResNet-32. Left: The model was trained on balanced CIFAR-100. The validation accuracy grows when using higher layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>t-SNE visualization of embedding space of CIFAR100-LT obtained using cross-entropy loss and DRO-LT Loss method. The feature embedding of our model is more compact for both head (blue) and tail (green and red) classes and better separated. The number of samples for each class is written in parentheses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Validation accuracy of a model trained on CIFAR100-LT (imbalance factor 100) with different uncertainty radius ε. The radius is set to be equal for all classes in our shared ε variant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Validation accuracy of a model trained on CIFAR100-LT (imbalance factor 100) with different loss trade-off parameter (λ) values between standard crossentropy loss and our DRO-LT loss (Learnable ε variant).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>It is long-tailed by nature, with an extremely unbalanced label distribution. Has 437.5K images from 8,142 categories.</figDesc><table /><note>Top-1 accuracy of ResNet32 on long-tailed CIFAR- 100 [6], comparing our method and SoTA techniques. Aster- isks * denote reproduced results. DRO-LT variants achieve best results on all imbalance ratios.α = 6. Consists of 115.8K images from 1000 categories with 1280 to 5 images per class. (3) iNaturalist [19]: A large-scale dataset for species classification.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Top-1 accuracy</cell></row><row><cell>on long-tailed iNaturalist.</cell></row><row><cell>DRO-LT achieves slightly</cell></row><row><cell>better results compared to</cell></row><row><cell>previous methods.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the robustness of semantic segmentation models to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Synthetic examples improve generalization for rare classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Piavis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05916</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Pawlowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><forename type="middle">Daisy</forename><surname>Zhuo</surname></persName>
		</author>
		<title level="m">Robust classification. INFORMS Journal on Optimization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Datadriven robust optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Kallus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority oversampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Feature space augmentation for long-tailed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaopeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03673</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Elements of information theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">Classbalanced loss based on effective number of samples. CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">C 4 . 5 , class imbalance , and cost sensitivity : Why under-sampling beats oversampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Drummond</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Euclidean embedding of co-occurrence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Amir Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributionally robust optimization and its tractable approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvyn</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations research</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neighbourhood components analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unravelling robustness of deep learning based face recognition against adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nalini</forename><surname>Ratha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richa</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Vatsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Borderline-smote: A new over-sampling method in imbalanced data sets learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIC</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The inaturalist challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to segment the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Striking the right balance with uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Supervised contrastive learning. Nuerips, abs/2004.11362, 2020</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">M2m: Imbalanced classification via major-to-minor translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongheon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Metric learning: A survey. Foundations and trends in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust classification under sample selection bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep representation learning on long-tailed data: A learnable embedding augmentation perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuchu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Factors in finetuning deep model for object detection with long-tail distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wanli Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Distributionally robust optimization: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahimian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Anchor loss: Modulating loss scale based on prediction difficulty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ryou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">From generalized zero-shot learning to long-tail with class descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dvir</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distributionally robust logistic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroosh</forename><surname>Shafieezadeh-Abadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peyman Mohajerin Esfahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visualizing data using (t-sne)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The nature of statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer science &amp; business media</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Long-tailed recognition by routing diverse distribution-aware experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01809</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dynamic curriculum learning for imbalanced data classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence K</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning from multiple experts: Self-paced knowledge distillation for long-tailed classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuyu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A distributional interpretation of robust optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantine</forename><surname>Caramanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rethinking the value of labels for improving class-imbalanced learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao-Min</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
