<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie√üner</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">AutoX 4 Adobe Systems</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Matching local geometric features on real-world depth images is a challenging task due to the noisy, lowresolution, and incomplete nature of 3D scan data. These difficulties limit the performance of current state-of-art methods, which are typically based on histograms over geometric properties. In this paper, we present 3DMatch, a data-driven model that learns a local volumetric patch descriptor for establishing correspondences between partial 3D data. To amass training data for our model, we propose a self-supervised feature learning method that leverages the millions of correspondence labels found in existing RGB-D reconstructions. Experiments show that our descriptor is not only able to match local geometry in new scenes for reconstruction, but also generalize to different tasks and spatial scales (e.g. instance-level object model alignment for the Amazon Picking Challenge, and mesh surface correspondence). Results show that 3DMatch consistently outperforms other state-of-the-art approaches by a significant margin. Code, data, benchmarks, and pre-trained models are available online at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Matching 3D geometry has a long history starting in the early days of computer graphics and vision. With the rise of commodity range sensing technologies, this research has become paramount to many applications including object pose estimation, object retrieval, 3D reconstruction, and camera localization.</p><p>However, matching local geometric features in lowresolution, noisy, and partial 3D data is still a challenging task as shown in <ref type="figure">Fig. 1</ref>. While there is a wide range of low-level hand-crafted geometric feature descriptors that can be used for this task, they are mostly based on signatures derived from histograms over static geometric properties <ref type="bibr" target="#b20">[19,</ref><ref type="bibr" target="#b23">22,</ref><ref type="bibr" target="#b30">28]</ref>. They work well for 3D models with complete surfaces, but are often unstable or inconsistent in real-world partial surfaces from 3D scanning data and difficult to adapt to new datasets. As a result, state-of-the- <ref type="figure">Figure 1</ref>. In this work, we present a data-driven local descriptor 3DMatch that establishes correspondences (green) to match geometric features in noisy and partial 3D scanning data. This figure illustrates an example of bringing two RGB-D scans into alignment using 3DMatch on depth information only. Color images are for visualization only. art 3D reconstruction methods using these descriptors for matching geometry require significant algorithmic effort to handle outliers and establish global correspondences <ref type="bibr" target="#b5">[5]</ref>.</p><p>In response to these difficulties, and inspired by the recent success of neural networks, we formulate a data-driven method to learn a local geometric descriptor for establishing correspondences between partial 3D data. The idea is that by learning from example, data-driven models can sufficiently address the difficulties of establishing correspondences between partial surfaces in 3D scanning data. To this end, we present a 3D convolutional neural network (Con-vNet), called 3DMatch, that takes in the local volumetric region (or 3D patch) around an arbitrary interest point on a 3D surface and computes a feature descriptor for that point, where a smaller distance between two descriptors indicates a higher likelihood of correspondence.</p><p>However, optimizing a 3D ConvNet-based descriptor for this task requires massive amounts of training data (i.e., 1 <ref type="figure">Figure 2</ref>. Learning 3DMatch from reconstructions. From existing RGB-D reconstructions (a), we extract local 3D patches and correspondence labels from scans of different views (b). We collect pairs of matching and non-matching local 3D patches and convert into a volumetric representation (c) to train a 3D ConvNet-based descriptor (d). This geometric descriptor can be used to establish correspondences for matching 3D geometry in various applications (e) such as reconstruction, model alignment, and surface correspondence. ground truth matches between local 3D patches). Obtaining this training data with manual annotations is a challenging endeavor. Unlike 2D image labels, which can be effectively crowd-sourced or parsed from the web, acquiring ground truth correspondences by manually clicking keypoint pairs on 3D partial data is not only time consuming but also prone to errors.</p><p>Our key idea is to amass training data by leveraging correspondence labels found in existing RGB-D scene reconstructions. Due to the importance of 3D reconstructions, there has been much research on designing algorithms and systems that can build high-fidelity reconstructions from RGB-D data <ref type="bibr" target="#b26">[25,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" target="#b8">8]</ref>. Although these reconstructions have been used for high-level reasoning about the environment <ref type="bibr" target="#b41">[39,</ref><ref type="bibr" target="#b42">40]</ref>, it is often overlooked that they can also serve as a massive source of labeled correspondences between surfaces points of aligned frames. By training on correspondences from multiple existing RGB-D reconstruction datasets, each with its own properties of sensor noise, occlusion patterns, variance of geometric structures, and variety of camera viewpoints, we can optimize 3DMatch to generalize and robustly match local geometry in real-world partial 3D data.</p><p>In this paper, we train 3DMatch over 8 million correspondences from a collection of 62 RGB-D scene reconstructions [37, <ref type="bibr" target="#b33">31,</ref><ref type="bibr" target="#b42">40,</ref><ref type="bibr" target="#b22">21,</ref><ref type="bibr" target="#b17">16]</ref> and demonstrate its ability to match 3D data in several applications. Results show that 3DMatch is considerably better than state-ofthe-art methods at matching keypoints, and outperforms other algorithms for geometric registration when combined with standard RANSAC. Furthermore, we demonstrate that 3DMatch can also generalize to different tasks and spatial resolutions. For example, we utilize 3DMatch to obtain instance-level model alignments for 6D object pose esti-mation as well as to find surface correspondences in 3D meshes. To facilitate further research in the area of 3D keypoint matching and geometric registration, we provide a correspondence matching benchmark as well as a surface registration benchmark similar to <ref type="bibr" target="#b5">[5]</ref>, but with real-world scan data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Learning local geometric descriptors for matching 3D data lies at the intersection of computer vision and graphics. We briefly review the related work in both domains.</p><p>Hand-crafted 3D Local Descriptors. Many geometric descriptors have been proposed including Spin Images <ref type="bibr" target="#b20">[19]</ref>, Geometry Histograms <ref type="bibr" target="#b12">[12]</ref>, and Signatures of Histograms <ref type="bibr" target="#b37">[35]</ref>, Feature Histograms <ref type="bibr" target="#b31">[29]</ref>. Many of these descriptors are now available in the Point Cloud Library <ref type="bibr" target="#b2">[3]</ref>. While these methods have made significant progress, they still struggle to handle noisy, low-resolution, and incomplete real-world data from commodity range sensors. Furthermore, since they are manually designed for specific applications or 3D data types, it is often difficult for them to generalize to new data modalities. The goal of our work is to provide a new local 3D descriptor that directly learns from data to provide more robust and accurate geometric feature matching results in a variety of settings.</p><p>Learned 2D Local Descriptors. The recent availability of large-scale labeled image data has opened up new opportunities to use data-driven approaches for designing 2D local image patch descriptors. For instance, various works <ref type="bibr" target="#b35">[33,</ref><ref type="bibr" target="#b34">32,</ref><ref type="bibr" target="#b44">42,</ref><ref type="bibr" target="#b18">17,</ref><ref type="bibr" target="#b45">43,</ref><ref type="bibr" target="#b18">17]</ref> learn non-linear mappings from local image patches to feature descriptors. Many of these prior works are trained on data generated from multi-view stereo datasets <ref type="bibr" target="#b3">[4]</ref>. However, in addition to being limited to 2D correspondences on images, multi-view stereo is difficult to scale up in practice and is prone to error from missing correspondences on textureless or non-Lambertian surfaces, so it is not suitable for learning a 3D surface descriptor. A more recent work <ref type="bibr" target="#b32">[30]</ref> uses RGB-D reconstructions to train a 2D descriptor, while we train a 3D geometric descriptor.</p><p>Learned 3D Global Descriptors. There has also been rapid progress in learning geometric representations on 3D data. 3D ShapeNets <ref type="bibr" target="#b41">[39]</ref> introduced 3D deep learning for modeling 3D shapes, and several recent works <ref type="bibr" target="#b24">[23,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b36">34]</ref> also compute deep features from 3D data for the task of object retrieval and classification. While these works are inspiring, their focus is centered on extracting features from complete 3D object models at a global level. In contrast, our descriptor focuses on learning geometric features for real-world RGB-D scanning data at a local level, to provide more robustness when dealing with partial data suffering from various occlusion patterns and viewpoint differences.</p><p>Learned 3D Local Descriptors. More closely related to this work is Guo et al. <ref type="bibr" target="#b16">[15]</ref>, which uses a 2D ConvNet descriptor to match local geometric features for mesh labeling. However, their approach operates only on synthetic and complete 3D models, while using ConvNets over input patches of concatenated feature vectors that do not have any kind of spatial correlation. In contrast, our work not only tackles the harder problem of matching real-world partial 3D data, but also properly leverages 3D ConvNets on volumetric data in a spatially coherent way.</p><p>Self-supervised Deep Learning. Recently, there has been significant interest in learning powerful deep models using automatically-obtained labels. For example, recent works show that the temporal information from videos can be used as a plentiful source of supervision to learn embeddings that are useful for various tasks <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b29">27]</ref>. Other works show that deep features learned from egomotion supervision perform better than features using class-labels as supervision for many tasks <ref type="bibr" target="#b1">[2]</ref>. Analogous to these recent works in self-supervised learning, our method of extracting training data and correspondence labels from existing RGB-D reconstructions online is fully automatic, and does not require any manual labor or human supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning From Reconstructions</head><p>In this paper, our goal is to create a function œà that maps the local volumetric region (or 3D patch) around a point on a 3D surface to a descriptor vector. Given any two points, an ideal function œà maps their local 3D patches to two descriptors, where a smaller 2 distance between the descriptors indicates a higher likelihood of correspondence. We learn the function œà by making use of data from existing high quality RGB-D scene reconstructions.</p><p>The advantage of this approach is threefold: First, reconstruction datasets can provide large amounts of training correspondences since each reconstruction contains millions of points that are observed from multiple different scanning views. Each observation pair provides a training example for matching local geometry. Between different observations of the same interest point, its local 3D patches can look very different due to sensor noise, viewpoint variance, and occlusion patterns. This helps to provide a large and diverse correspondence training set. Second, reconstructions can leverage domain knowledge such as temporal information and well-engineered global optimization methods, which can facilitate wide baseline registrations (loop closures). We can use the correspondences from these challenging registrations to train a powerful descriptor that can be used for other tasks where the aforementioned domain knowledge is unavailable. Third, by learning from multiple reconstruction datasets, we can optimize 3DMatch to generalize and robustly match local geometry in real-world partial 3D data under a variety of conditions. Specifically, we use a total of over 200K RGB-D images of 62 different scenes collected from Analysis-by-Synthesis [37], 7-Scenes <ref type="bibr" target="#b33">[31]</ref>, SUN3D <ref type="bibr" target="#b42">[40]</ref>, RGB-D Scenes v.2 <ref type="bibr" target="#b22">[21]</ref>, and Halber et al. <ref type="bibr" target="#b17">[16]</ref>. 54 scenes are used for training and 8 scenes for testing. Each of the reconstruction datasets are captured in different environments with different local geometries at varying scales and built with different reconstruction algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generating Training Correspondences</head><p>To obtain training 3D patches and their ground truth correspondence labels (match or non-match), we extract local 3D patches from different scanning views around interest points randomly sampled from reconstructions. To find correspondences for an interest point, we map its 3D position in the reconstruction into all RGB-D frames for which the 3D point lies within the frame's camera view frustum and is not occluded. The locations of the cameras from which the RGB-D frames are taken are enforced to be at least 1m apart, so that the views between observation pairs are sufficiently wide-baselined. We then extract two local 3D patches around the interest point from two of these RGB-D frames, and use them as a matching pair. To obtain nonmatching pairs, we extract local 3D patches from randomly picked depth frames of two interest points (at least 0.1m apart) randomly sampled from the surface of the reconstruction. Each local 3D patch is converted into a volumetric representation as described in Sec. 4.1.</p><p>Due to perturbations from depth sensor noise and imperfections in reconstruction results, the sampled interest points and their surrounding local 3D patches can experience some minor amounts of drift. We see this jitter as an opportunity for our local descriptor to learn small amounts of translation invariance. Since we are learning from RGB-D reconstruction datasets using different sensors and algorithms, the jitter is not consistent, which enables the descriptor to generalize and be more robust to it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning A Local Geometric Descriptor</head><p>We use a 3D ConvNet to learn the mapping from a volumetric 3D patch to an 512-dimensional feature representation that serves as the descriptor for that local region. During training, we optimize this mapping (i.e., updating the weights of the ConvNet) by minimizing the 2 distance between descriptors generated from corresponding interest points (matches), and maximize the 2 distance between descriptors generated from non-corresponding interest points (non-matches). This is equivalent to training a ConvNet with two streams (i.e., Siamese Style ConvNets <ref type="bibr" target="#b6">[6]</ref>) that takes in two local 3D patches and predicts whether or not they correspond to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">3D Data Representation</head><p>For each interest point, we first extract a 3D volumetric representation for the local region surrounding it. Each 3D region is converted from its original representation (surface mesh, point cloud, or depth map) into a volumetric 30 √ó 30 √ó 30 voxel grid of Truncated Distance Function (TDF) values. Analogous to 2D pixel image patches, we refer to these TDF voxel grids as local 3D patches. In our experiments, these local 3D patches spatially span 0.3m 3 , where voxel size is 0.01m 3 . The voxel grid is aligned with respect to the camera view. If camera information is unavailable (i.e. for pre-scanned 3D models), the voxel grid is aligned to the object coordinates. The TDF value of each voxel indicates the distance between the center of that voxel to the nearest 3D surface. These TDF values are truncated, normalized and then flipped to be between 1 (on surface) and 0 (far from surface). This form of 3D representation is cross-compatible with 3D meshes, point-clouds, and depth maps. Analogous to 2D RGB pixel matrices for color images, 3D TDF voxel grids also provide a natural volumetric encoding of 3D space that is suitable as input to a 3D Con-vNet.</p><p>The TDF representation holds several advantages over its signed alternative TSDF <ref type="bibr" target="#b7">[7]</ref>, which encodes occluded space (values near -1) in addition to the surface (values near 0) and free space (values near 1). By removing the sign, the TDF loses the distinction between free space and occluded space, but gains a new property that is crucial to the robustness of our descriptor on partial data: the largest gradients between voxel values are concentrated around the surfaces rather than in the shadow boundaries between free space and occluded space. Furthermore, the TDF representation reduces the ambiguity of determining what is occluded space on 3D data where camera view is unavailable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Network Architecture</head><p>3DMatch is a standard 3D ConvNet, inspired by AlexNet <ref type="bibr" target="#b9">[9]</ref>. Given a 30√ó30√ó30 TDF voxel grid of a local 3D patch around an interest point, we use eight convolutional layers (each with a rectified linear unit activation function for nonlinearity) and a pooling layer to compute a 512-dimensional feature representation, which serves as the feature descriptor. Since the dimensions of the initial input voxel grid are small, we only include one layer of pooling to avoid a substantial loss of information. Convolution parameters are shown in <ref type="figure">Fig. 2</ref> as (kernel size, number of filters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Network Training</head><p>During training, our objective is to optimize the local descriptors generated by the ConvNet such that they are similar for 3D patches corresponding to the same point, and dissimilar otherwise. To this end, we train our ConvNet with two streams in a Siamese fashion where each stream independently computes a descriptor for a different local 3D patch. The first stream takes in the local 3D patch around a surface point p 1 , while the second stream takes in a second local 3D patch around a surface point p 2 . Both streams share the same architecture and underlying weights. We use the 2 norm as a similarity metric between descriptors, modeled during training with the contrastive loss function <ref type="bibr" target="#b6">[6]</ref>. This loss minimizes the 2 distance between descriptors of corresponding 3D point pairs (matches), while pulling apart the 2 distance between descriptors of noncorresponding 3D point pairs. During training, we feed the network with a balanced 1:1 ratio of matches to nonmatches. This strategy of balancing positives and negatives has shown to be effective for efficiently learning discriminative descriptors <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b34">32,</ref><ref type="bibr" target="#b44">42]</ref>. <ref type="figure" target="#fig_0">Fig.3</ref> shows a t-SNE embedding <ref type="bibr" target="#b40">[38]</ref> of local 3D patches based on their 3DMatch descriptors, which demonstrates the ConvNet's ability to clus- ter local 3D patches based on their geometric structure as well as local context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head><p>In this section, we first evaluate how well our learned local 3D descriptor (3DMatch) can match local 3D patches of interest point pairs (Sec. 5.1). We then evaluate its practical use as part of geometric registration for matching 3D data in several applications, such as scene reconstruction (Sec. 5.2) and 6D object pose estimation (Sec. 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Keypoint Matching</head><p>Our first set of experiments measure the quality of a 3D local descriptor by testing its ability to distinguish between matching and non-matching local 3D patches of keypoint pairs. Using the sampling algorithm described in Sec. 3, we construct a correspondence benchmark, similar to the Photo Tourism dataset <ref type="bibr" target="#b3">[4]</ref> but with local 3D patches extracted from depth frames. The benchmark contains a collection of 30, 000 3D patches, with a 1:1 ratio between matches and non-matches. As in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">17]</ref>, our evaluation metric is the false-positive rate (error) at 95% recall, the lower the better.</p><p>Is our descriptor better than others? We compare our descriptor to several other state-of-the-art geometric descriptors on this correspondence benchmark. For Johnson et al. (Spin-Images) <ref type="bibr" target="#b20">[19]</ref> and Rusu et al. (Fast Point Feature Histograms) <ref type="bibr" target="#b30">[28]</ref>, we use the implementation provided in the Point Cloud Library (PCL). While 3DMatch uses local TDF voxel grids computed from only a single depth frame, we run Johnson et al. and Rusu et al. on meshes fused from 50 nearby depth frames to boost their performance on this benchmark, since these algorithms failed to produce reasonable results on single depth frames. Nevertheless, 3DMatch outperforms these methods by a significant margin. What's the benefit of 3D volumes vs. 2D depth patches?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>We use TDF voxel grids to represent 3D data, not only because it is an intermediate representation that can be easily converted from meshes or point clouds, but also because this 3D representation allows reasoning over real-world spatial scale and occluded regions, which cannot be directly encoded in 2D depth patches. To evaluate the advantages of this 3D TDF encoding over 2D depth, we train a variant of our method using a 2D ConvNet on depth patches. The depth patches are extracted from a 0.3m 3 crop and resized to 64x64 patches. For a fair comparison, the architecture of the 2D ConvNet is similar to our 3D ConvNet with two extra convolution layers to achieve a similar number of parameters as the 3D ConvNet. As shown in <ref type="table" target="#tab_0">Table 1</ref>, this 2D ConvNet yields a higher error rate (38.5 vs. <ref type="bibr" target="#b37">35</ref>.3).</p><p>Should we use a metric network? Recent work <ref type="bibr" target="#b18">[17]</ref> proposes the joint learning of a descriptor and similarity metric with ConvNets to optimize matching accuracy. To explore this idea, we replace our contrastive loss layer with three fully connected layers, followed by a Softmax layer for binary classification of "match" vs "non-match". We evaluate the performance of this network on our keypoint matching benchmark, where we see an error of 33.1% (2.2% improvement). However, as noted by Yi et al. <ref type="bibr" target="#b44">[42]</ref>, descriptors that require a learned metric have a limited range of applicability due to the O(n 2 ) comparison behaviour at test time since they cannot be directly combined with metric-based acceleration structures such as KD-trees. To maintain runtime within practical limits, we use the version of 3DMatch trained with an 2 metric in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Geometric Registration</head><p>To evaluate the practical use of our descriptor, we combine 3DMatch with a RANSAC search algorithm for geometric registration, and measure its performance on standard benchmarks. More specifically, given two 3D point clouds from scanning data, we first randomly sample n keypoints from each point cloud. Using the local 3D 30 √ó 30 √ó 30 TDF patches around each keypoint (aligned to the camera axes, which may be different per point cloud), we compute 3DMatch descriptors for all 2n keypoints. We find keypoints whose descriptors are mutually closest to each other in Euclidean space, and use RANSAC over the 3D positions of these keypoint matches to estimate a rigid transformation between the two point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Matching Local Geometry in Scenes</head><p>We evaluate our 3DMatch-based geometric registration algorithm (i.e., 3DMatch + RANSAC) on both real and synthetic datasets. For synthetic, we use the benchmark from Choi et al. <ref type="bibr" target="#b5">[5]</ref> which contains 207 fragments (each fused from 50 depth frames) of four scenes from the ICL-NUIM dataset <ref type="bibr" target="#b19">[18]</ref>. However, the duplicated and over-simplified geometry in this ICL-NUIM dataset is very different from that of real-world scenes. Therefore, we create a separate benchmark with fragments formed from the testing split of our real-world reconstruction datasets. We use the same evaluation scheme introduced by Choi et al. <ref type="bibr" target="#b5">[5]</ref>, measuring the recall and precision of a method based on two factors: (1) how well it finds loop closures, and (2) how well it estimates rigid transformation matrices. Given two nonconsecutive scene fragments (P i , P j ), the predicted relative rigid transformation T ij is a true positive if (1) over 30% of T ij P i overlaps with P j and if (2) T ij is sufficiently close to the ground-truth transformation T * ij . T ij is correct if it brings the RMSE of the ground truth correspondences K * ij between P i and P j below a threshold œÑ = 0.2</p><formula xml:id="formula_0">1 |K * ij | (p * ,q * )‚ààK * ij ||T ij p * ‚àí q * || 2 &lt; œÑ 2<label>(1)</label></formula><p>where p * q * are the ground truth correspondences. Since the benchmark from Choi et al. <ref type="bibr" target="#b5">[5]</ref> uses fragments fused from multiple depth frames, we fine-tune our 3DMatch ConvNet on correspondences over a set of fragments constructed in the same way using the 7-scenes training set. We then run pairwise geometric registration with 3DMatch + RANSAC on every pair of fragments from the benchmarks. We compare the performance of our 3DMatch-based registration approach versus other state-ofthe-art geometric registration methods on the synthetic data benchmark in <ref type="table">Table 2</ref>  <ref type="bibr" target="#b5">[5]</ref>, and the real data benchmark in <ref type="table">Table 3</ref>. We also compare with Rusu et al. <ref type="bibr" target="#b30">[28]</ref> and Johnson et al. <ref type="bibr" target="#b20">[19]</ref> using the same RANSAC-based pipeline. Overall, our descriptor with RANSAC outperforms other methods by a significant margin on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Recall <ref type="formula">(</ref>  <ref type="table">Table 3</ref>. Performance of geometric registration algorithms between fused fragments of real-world scans. <ref type="figure">Figure 6</ref>. 3DMatch for reconstructions. On the left, we show a complete reconstruction of an apartment from SUN3D <ref type="bibr" target="#b42">[40]</ref> using only 3DMatch. On the right, we show two reconstructions using only SIFT to match color features (top), using only 3DMatch to match geometric features (middle), and using both SIFT and 3DMatch (bottom). The red boxes highlight areas with poor reconstruction quality, while the green boxes highlight areas with improved quality. These examples show that 3DMatch provides strong geometric feature correspondences that are complementary to color features and can help to improve the quality of reconstructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Integrate 3DMatch in Reconstruction Pipeline.</head><p>In this section, we show that 3DMatch is not only capable of detecting challenging cases of loop closure, but also can be used in a standard reconstruction pipeline to generate highquality reconstructions of new scenes. We use our 3DMatch descriptor as part of a standard sparse bundle adjustment formulation for scene reconstruction <ref type="bibr" target="#b38">[36,</ref><ref type="bibr" target="#b0">1]</ref>. Traditionally, sparse RGB features, such as SIFT or SURF, are used to establish feature matches between frames. With 3DMatch, we are able to establish keypoint matches from geometric information and add to the bundle adjustment step. With this simple pipeline we are able to generate globally-consistent alignments in challenging scenes using only geometric information as shown in <ref type="figure">Fig. 6</ref>. We also find that color and depth provide complementary information for RGB-D reconstructions. For example, sparse RGB features can provide correspondences where there is insufficient geometric information in the scans, while geometric signals are helpful where there are drastic viewpoint or lighting changes that cause traditional RGB features to fail. <ref type="figure" target="#fig_2">Fig. 5</ref> shows challenging cases of loop closure from the testing split of the SUN3D datasets that are difficult for color-based descriptors to find correspondences due to drastic viewpoint differences. Our 3DMatch-based registration algorithm is capable of matching the local geometry to find correspondences and bring the scans into alignment. In <ref type="figure">Fig. 6</ref>, we show several reconstruction results where combining correspondences from both SIFT (color) and 3DMatch (geometry) improves alignment quality as a whole.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Can 3DMatch generalize to new domains?</head><p>As a final test, we evaluate the ability of our 3DMatch descriptor, which is trained from 3D reconstructions, to generalize to completely different tasks and spatial scales; namely, 6D object pose estimation by model alignment and correspondence labeling for 3D meshes.</p><p>6D Object Pose Estimation by model alignment. In our first experiment, the task is to register pre-scanned object models to RGB-D scanning data for the Shelf &amp; Tote benchmark in the Amazon Picking Challenge (APC) setting <ref type="bibr" target="#b46">[44]</ref>, as illustrated in <ref type="figure" target="#fig_3">Fig. 7</ref>. This scenario is different from scene level reconstruction in the following two aspects: (1) object sizes and their geometric features are much smaller in scale and (2) the alignment here is from full pre-scanned models to partial scan data, instead of partial scans to partial scans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Rotation (%)Translation (%) Baseline <ref type="bibr" target="#b46">[44]</ref> 49.0 67. To account for spatial scale differences, we reduce the size of each voxel to 0.005m 3 within the local 3D patches. The voxel grids from the object models are aligned with respect to the object model's coordinates axes, while the voxel grids from the scans are aligned to the camera axes. We use the 3DMatch network pre-trained on reconstructions, and fine-tune it on a 50% training split of the Shelf &amp; Tote data. Similar to how we align scene fragments to each other in Sec. 5.2, we use a RANSAC based geometric registration approach to align the object models to scans. The predicted rigid transformation is used as object pose. Similar to the baseline approach, we perform model alignment between  object models to segmentation results from <ref type="bibr" target="#b46">[44]</ref>.</p><p>We evaluate on the testing split of the Shelf&amp;Tote dataset using the error metric from <ref type="bibr" target="#b46">[44]</ref>, where we report the percentage of pose predictions with error in orientation smaller than 15 ‚Ä¢ and translations smaller than 5cm. We compare to the baseline approach for the Shelf &amp; Tote benchmark, as well as to other approaches in <ref type="table">Table 4</ref>. Several of our predictions are illustrated in <ref type="figure" target="#fig_4">Fig. 8</ref>. Our descriptor significantly outperforms the baseline approach with over 10% improvement in rotation prediction accuracy and other registration variants. The 3DMatch model without pre-training on reconstructions yields a lower performance, demonstrating the importance of pre-training on reconstruction data.</p><p>Surface Correspondence on 3D meshes. In our final experiment, we test 3DMatch's ability to generalize even further to other modalities. We take a 3DMatch model trained on RGB-D reconstruction data, and directly test it on 3D mesh models without any fine-tuning to see whether 3DMatch is able to find surface correspondences based on local geometry. Given a query point on the surface of a 3D mesh, the goal is to find geometrically similar points on a second mesh (e.g. for transferring annotations about human contact points <ref type="bibr" target="#b21">[20]</ref>). We do this by first encoding the local volumetric regions (with size 0.3m 3 ) of the query point from the first mesh and all surface points from the second mesh into TDF volume aligned to object coordinate, and compute their 3DMatch descriptors. For every surface point on the second mesh, we color it with intensity based on its descriptor's 2 distance to the descriptor of the query point. <ref type="figure" target="#fig_5">Fig. 9</ref> shows results on meshes from the Shape2Pose dataset <ref type="bibr" target="#b21">[20]</ref>. The results demonstrate that without any fine-tuning on the mesh data, 3DMatch can be used as a general 3D shape descriptor to find correspondences with similar local geometry between meshes. Interestingly 3DMatch is also able to find geometric correspondences across different object categories. For example in the third row of <ref type="figure" target="#fig_0">Fig. 9, 3DMatch</ref> is able to match the handles in very different meshes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we presented 3DMatch, a 3D ConvNetbased local geometric descriptor that can be used to match partial 3D data for a variety of applications. We demonstrated that by leveraging the vast amounts of correspondences automatically obtained from RGB-D reconstructions, we can train a powerful descriptor that outperforms existing geometric descriptors by a significant margin. We make all code and pre-trained models available for easy use and integration. To encourage further research, we also provide a correspondence matching benchmark and a surface registration benchmark, both with real-world 3D data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>In this section, we present several statistics of the RGB-D reconstruction datasets used to generate training correspondences for 3DMatch, the implementation details of our network, and run-time statistics relevant to the experiments discussed in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. RGB-D Reconstruction Datasets</head><p>As mentioned in Sec. 3 of the main paper, we use registered depth frames of 62 different real-world scenes collected from Analysis-by-Synthesis [37], 7-Scenes <ref type="bibr" target="#b33">[31]</ref>, SUN3D <ref type="bibr" target="#b42">[40]</ref>, RGB-D Scenes v.2 <ref type="bibr" target="#b22">[21]</ref>, and Halber et al. <ref type="bibr" target="#b17">[16]</ref>, with 54 scenes for training and 8 scenes for testing. For selected scenes of the SUN3D dataset, we use the method from Halber et al. to estimate camera poses. For the precise train/test scene splits, see our project webpage. In <ref type="figure">Fig. 11</ref>, we show top-down views of the completed reconstructions. They are diverse in the environments they capture, with local geometries at varying scales, which provide a diverse surface correspondence training set for 3DMatch. In total, there are 214,569 depth frames over the 54 training scenes, most of which are made up of indoor bedrooms, offices, living rooms, tabletops, and restrooms. The depth sensors used for scanning include the Microsoft Kinect, Structure Sensor, Asus Xtion Pro Live, and Intel RealSense.</p><p>The size of the correspondence training set correlates with the amount of overlap between visible surfaces from different scanning views. In <ref type="figure">Fig. 10</ref>, we show the average distribution of volumetric voxels (size 0.02 3 m) on the surface vs. the number of frames in which the voxels were seen by the depth sensor. We plot this distribution averaged over the 54 training scenes (left) and illustrate a heat map of over two example reconstructions (right), where a warmer region implies that the area has been seen more times. The camera trajectories are plotted with a red line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Implementation Details</head><p>We implement the 3DMatch network architecture in Marvin <ref type="bibr" target="#b43">[41]</ref>, a lightweight GPU deep learning framework that supports 3D convolutional neural networks. Weights of the network are initialized with the Xavier algorithm <ref type="bibr" target="#b14">[13]</ref>, and biases are initialized to 0. We train by SGD with momentum using a fixed learning rate of 10 -3 , a momentum of 0.99, and weight decay of 5 <ref type="bibr" target="#b3">4</ref> . Random sampling matching and non-matching 3D training patches from reconstructions (refer to Sec. 3) is performed on-the-go during network training. We used a batch size of 128, contrastive margin of 1, no batch or patch normalization. Our reference model was trained for approximately eight days on a single NVIDIA Tesla K40c, over 16 million 3D patch pairs, which includes 8 million correspondences and 8 million non-correspondences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Run-time Information</head><p>The following run-times are reported from the implementations used in our experiments. We did not optimize for speed. TDF Conversion. As mentioned in Sec 4.1, a local 3D patch around an interest point is represented as a 30√ó30√ó30 voxel grid of TDF values (in all of our experiments, truncation margin is 5 voxels). Converting a point cloud spanning 0.3m 3 of a depth frame into a TDF voxel grid (using CUDAenabled GPU acceleration) takes anywhere from 3 -20 milliseconds, depending on the density of the point cloud. We observe an average conversion time of 8.9 milliseconds per patch from the keypoint matching benchmark in Sec. 5.1 on an NVIDIA Tesla K40c. 3DMatch Descriptor. Computing a 3DMatch descriptor (e.g. ConvNet forward pass) for a single 30 √ó 30 √ó 30 TDF voxel grid takes an average of 3.2 milliseconds with Marvin. Geometric Registration. To register two surfaces, we randomly sample n keypoints per surface. n = 5000 for scene fragment surface registration (Sec. 5.2.1 -Sec. 5.2.2) and n = 1000 for model alignment in the Amazon Picking Challenge (Sec. 5.3). Finding keypoints with mutually closest 3DMatch descriptors takes 2 seconds or less, while RANSAC for estimating rigid transformation can take anywhere from 2 seconds up to a minute depending on the number of RANSAC iterations and rate of convergence. These numbers are reported using a single CPU thread on an Intel Core i7-3770K clocked at 3.5 GHz. <ref type="figure">Figure 11</ref>. Visualizing several RGB-D reconstructions of the datasets used to train 3DMatch in our experiments. Each dataset contains depth scans of different environments with different local geometries at varying scales, registered together by different reconstruction algorithms. These datasets provide a diverse surface correspondence training set with varying levels of sensor noise, viewpoint variance, and occlusion patterns. Color for visualization only.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>t-SNE embedding of 3DMatch descriptors for local 3D patches from the RedKitchen test scene of 7-Scenes<ref type="bibr" target="#b26">[25]</ref>. This embedding suggests that our 3DMatch ConvNet is able to cluster local 3D patches based on local geometric features such as edges (a,f), planes (e), corners (c,d), and other geometric structures (g, b, h) in the face of noisy and partial data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Which 3D patches are matched by 3DMatch? On the left, we show two fused fragments (A and B) taken at different scan view angles, as well as their registration result using 3DMatch + RANSAC. On the right, each row shows a local 3D patch from fragment A, followed by three nearest neighbor local 3D patches from fragment B found by 3DMatch descriptors. The bounding boxes are color coded to the keypoints illustrated on fragment A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Challenging cases of loop closures from test scenes of SUN3D<ref type="bibr" target="#b42">[40]</ref>. In these instances, color features in the RGB images (top row) are insufficient for registering the scan pairs due to drastic viewpoint differences. While Rusu et al.<ref type="bibr" target="#b30">[28]</ref> fails at aligning the pairs (middle row), 3DMatch is able to successfully align each pair of scans (bottom row) by matching local geometric features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>6D pose estimation in the Amazon Picking Challenge by aligning object models (a) to scanning data (b). (c) is a topdown view of the scanned shelf highlighting the noisy, partial nature of depth data from the RGB-D sensor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Predicted object poses on the Shelf &amp; Tote Benchmark using 3DMatch + RANSAC. Predicted object poses are shown on the images of the scans with 3D bounding boxes of the transformed object models. 3DMatch + RANSAC works well for many cases; however, it may fail when there is insufficient depth information due to occlusion or clutter (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>Surface correspondences on 3D meshes. The first column shows the input mesh and query points (red and blue). The other columns show the respective correspondences found in other meshes of the same object category (top and middle row) and across different object categories (bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Keypoint matching task error (%) at 95% recall.</figDesc><table><row><cell>Error</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work is supported by the NSF/Intel VEC program and Google Faculty Award. Andy Zeng is supported by the Gordon Y.S. Wu Fellowship. Shuran Song is supported by the Facebook Fellowship. Matthias Nie√üner is a member of the Max Planck Center for Visual Computing and Communications (MPC-VCC). We gratefully acknowledge the support of NVIDIA and Intel for hardware donations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Building rome in a day</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="105" to="112" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Point cloud library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aldoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wohlkinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeisl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gedikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
		<idno>1070(9932/12</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics &amp; Automation Magazine</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discriminative learning of local image descriptors. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="57" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">We also illustrate a heat map over the reconstructions of example scenes (right), where a warmer region implies that the surface voxels in area has been seen more times</title>
		<imprint/>
	</monogr>
	<note>02 3 m) on the surface vs. the number of frames in which the voxels were seen by the depth sensor in each scene (left). Regions seen less than 50 times are removed from the illustrations. The camera trajectories are drawn in red</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust reconstruction of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5556" to="5565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A volumetric method for building complex models from range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 23rd annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bundlefusion: Real-time globally consistent 3d reconstruction using on-the-fly surface re-integratio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie√üner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh√∂fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Model globally, match locally: Efficient and robust 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="998" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3d deep shape descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2319" to="2328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recognizing objects in range data using regional point descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kolluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>B√ºlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2004</title>
		<imprint>
			<biblScope unit="page" from="224" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Aistats</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised learning of spatiotemporally coherent metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le-Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4086" to="4093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3d mesh labeling via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Structured global registration of rgb-d scans in indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08539</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Matchnet: Unifying feature and metric learning for patchbased matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A benchmark for RGB-D visual odometry, 3D reconstruction and SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Conf. on Robotics and Automation</title>
		<meeting><address><addrLine>ICRA, Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using spin images for efficient object recognition in cluttered 3d scenes. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shape2pose: human-centric shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">120</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for 3d scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-local affine parts for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The British Machine Vision Association (BMVA)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
	<note>British Machine Vision Conference (BMVC&apos;04)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3D Convolutional Neural Networks for Landing Zone Detection from LiDAR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Super 4pcs fast global pointcloud registration via smart indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mellado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="205" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Kinectfusion: Real-time dense surface mapping and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mixed and augmented reality (ISMAR), 2011 10th IEEE international symposium on</title>
		<imprint>
			<biblScope unit="page">127</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Real-time 3d reconstruction at scale using voxel hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie√üner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh√∂fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">169</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning temporal embeddings for complex video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4471" to="4479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast point feature histograms (fpfh) for 3d registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation, 2009. ICRA&apos;09. IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Aligning point cloud views using persistent feature histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3384" to="3391" />
		</imprint>
	</monogr>
	<note>Intelligent Robots and Systems</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-supervised visual descriptor learning for dense correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="420" to="427" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scene coordinate regression forests for camera relocalization in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discriminative learning of deep convolutional feature point descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning local feature descriptors using convex optimisation. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1573" to="1585" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02300</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unique signatures of histograms for local surface description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="356" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bundle adjustmenta modern synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Mclauchlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision algorithms: theory and practice</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="298" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning to navigate the energy landscape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie√üner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">3D ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">SUN3D: A database of big spaces reconstructed using SfM and object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Marvin: A minimalist GPU-only N-dimensional ConvNet framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<idno>Ac- cessed: 2015-11-10. 9</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09114</idno>
		<title level="m">Lift: Learned invariant feature transform</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>≈Ωbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4326</idno>
		<title level="m">Computing the stereo matching cost with a convolutional neural network</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Multi-view self-supervised deep learning for 6d pose estimation in the amazon picking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Walker</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09475</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fast global registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="766" to="782" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
