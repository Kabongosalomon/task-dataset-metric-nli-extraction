<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Learning Generalisable Omni-Scale Representations for Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Cavallaro</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Learning Generalisable Omni-Scale Representations for Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Person Re-Identification</term>
					<term>Omni-Scale Learning</term>
					<term>Lightweight Network</term>
					<term>Cross-Domain Re-ID</term>
					<term>Neural Architecture Search !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An effective person re-identification (re-ID) model should learn feature representations that are both discriminative, for distinguishing similar-looking people, and generalisable, for deployment across datasets without any adaptation. In this paper, we develop novel CNN architectures to address both challenges. First, we present a re-ID CNN termed omni-scale network (OSNet) to learn features that not only capture different spatial scales but also encapsulate a synergistic combination of multiple scales, namely omni-scale features. The basic building block consists of multiple convolutional streams, each detecting features at a certain scale. For omni-scale feature learning, a unified aggregation gate is introduced to dynamically fuse multi-scale features with channel-wise weights. OSNet is lightweight as its building blocks comprise factorised convolutions. Second, to improve generalisable feature learning, we introduce instance normalisation (IN) layers into OSNet to cope with cross-dataset discrepancies. Further, to determine the optimal placements of these IN layers in the architecture, we formulate an efficient differentiable architecture search algorithm. Extensive experiments show that, in the conventional same-dataset setting, OSNet achieves state-of-the-art performance, despite being much smaller than existing re-ID models. In the more challenging yet practical cross-dataset setting, OSNet beats most recent unsupervised domain adaptation methods without using any target data. Our code and models are released at https://github.com/KaiyangZhou/deep-person-reid.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Person re-identification (re-ID), as a fine-grained instance recognition problem, aims to match people across nonoverlapping camera views. With the development of deep learning technology, recent research in person re-ID has shifted from tedious feature engineering <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[2]</ref> to endto-end feature representation learning with deep neural networks <ref type="bibr" target="#b4">[3]</ref>, <ref type="bibr" target="#b5">[4]</ref>, <ref type="bibr" target="#b6">[5]</ref>, <ref type="bibr" target="#b7">[6]</ref>, especially convolutional neural networks (CNNs).</p><p>Though the re-ID performance has been improved significantly thanks to end-to-end representation learning with CNNs, two problems remain unsolved. They hinder largescale deployment of re-ID models in real-world applications. The first problem is discriminative feature learning. As an instance recognition task, re-identifying people under disjoint camera views needs to overcome both intraclass variations and inter-class ambiguity. For instance, in <ref type="figure" target="#fig_0">Fig. 1(a)</ref> the view change from front to back across cameras brings large appearance changes in the backpack region, making person matching a challenging task. Moreover, from a distance as typical in video surveillance scenes, people can look incredibly similar, as exemplified by the false matches in <ref type="figure" target="#fig_0">Fig. 1</ref>. This requires the re-ID features to capture finegrained details for distinguishing people of similar appearances (e.g., the sun glasses in <ref type="figure" target="#fig_0">Fig. 1(d)</ref>).</p><p>• K. Zhou is with Nanyang Technological University, Singapore. E-mail:</p><p>{kaiyang.zhou}@ntu.edu.sg • Y. <ref type="bibr">Yang</ref>   The second problem is generalisable feature learning. Due to intrinsic domain gaps between re-ID datasets caused by differences in, for example, lighting conditions, background and viewpoint (see <ref type="figure" target="#fig_0">Fig. 1</ref>), directly applying a re-ID model trained on a source dataset to an unseen target dataset will typically lead to large performance drops <ref type="bibr" target="#b8">[7]</ref>, <ref type="bibr" target="#b9">[8]</ref>, <ref type="bibr">[9]</ref>, <ref type="bibr">[10]</ref>. This suggests that the learned re-ID features severely overfit the source domain data and hence are not domain-generalisable. A domain-generalisable re-ID model has great values for real-world large-scale deployment. This is because such a model can work in any unseen scenarios, without the need to go through the tedious processes of data collection, annotation, and model updating/fine-tuning.</p><p>In this paper, we address both problems by designing novel CNN architectures. First, we argue that discriminative re-ID features need to be of omni-scale, defined as the combination of variable homogeneous scales and heteroge-arXiv:1910.06827v5 [cs.CV] 29 Apr 2021 neous scales, each of which is composed of a mixture of multiple scales. The need for omni-scale features is evident from <ref type="figure" target="#fig_0">Fig. 1</ref>. Specifically, to match people and distinguish them from distractors that cause false matches, the features corresponding to small local regions (e.g., shoes and glasses) and global whole body regions are equally important. For instance, given the query image in <ref type="figure" target="#fig_0">Fig. 1(a, left)</ref>, looking at the global-scale features (e.g., young man, a white Tshirt + grey shorts combo) could narrow down the search to the true match (middle) and a distractor (right). Now the local-scale features come into play-the shoe region explains away the fact that the person on the right is a distractor (trainers vs. sandals). However, for more challenging cases, even the features of variable homogeneous scales are not enough; more complicated and richer features that span multiple scales are required. For instance, to eliminate the distractor in <ref type="figure" target="#fig_0">Fig. 1(b, right)</ref>, one needs the features that represent a white T-shirt with a specific logo in the front. Note that the logo is not distinctive on its own-without the white T-shirt as context, it can be confused with many other patterns. Moreover, the white T-shirt is likely everywhere in summer, e.g., <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. It is however the unique combination, captured by heterogeneous features spanning both small (logo size) and medium (upper body size) scales, that makes the features most effective.</p><p>We therefore propose omni-scale network (OSNet), a novel CNN architecture designed specifically for omni-scale feature learning. The underpinning building block of OSNet consists of multiple convolutional streams with different receptive field sizes 1 (see <ref type="figure" target="#fig_1">Fig. 2</ref>). The feature scale that each stream focuses on is determined by exponent, a new dimension factor that linearly increases across streams to ensure that various scales can be captured in each individual block. Critically, the resulting multi-scale feature maps are dynamically fused by channel-wise weights generated by a unified aggregation gate (AG). The AG is a mini-network sharing parameters across all streams with a number of desirable properties for effective model training. Since the AG are trainable, the generated channel-wise weights are inputdependent, realising a dynamic scale fusion. This novel AG design is crucial for learning omni-scale feature representations: conditioning on a specific input image, the gate can focus on a single scale by assigning a dominant weight to a particular stream or scale; alternatively, it can select and mix jointly to produce features with heterogeneous scales.</p><p>Another key characteristic of OSNet is lightweight. A lightweight re-ID model has a couple of benefits: (1) Re-ID datasets are often of moderate size due to difficulties in collecting cross-camera matched person images. A lightweight network with a small number of parameters is thus less prone to overfitting; <ref type="bibr" target="#b3">(2)</ref> In large-scale surveillance applications (e.g., city-wide surveillance with thousands of cameras), the most practical way for re-ID is to perform feature extraction at the camera end and send the extracted features to the central server rather than the raw videos. For on-device processing, small re-ID networks are clearly preferred. To this end, in our building block we factorise standard convolutions with pointwise and depthwise convolutions <ref type="bibr" target="#b12">[11]</ref>, <ref type="bibr" target="#b13">[12]</ref>, making OSNet not only discriminative 1. In this paper, 'scale' and 'receptive field' are used interchangeably. in feature learning but also efficient in implementation and deployment.</p><p>To address the second problem caused by domain gaps across different re-ID datasets, we notice that these gaps are typically reflected by different image styles, such as brightness, colour temperatures and view angles (see <ref type="figure" target="#fig_0">Fig. 1</ref>). These style variations are caused by differences in both lighting condition and camera characteristics/setup in different camera networks. Existing works address this problem using unsupervised domain adaptation (UDA) methods <ref type="bibr" target="#b8">[7]</ref>, <ref type="bibr" target="#b9">[8]</ref>, <ref type="bibr">[9]</ref>, <ref type="bibr">[10]</ref>. These require unlabelled target domain data to be available for model adaptation. In contrast, we treat this as a more general domain generalisation (DG) problem <ref type="bibr" target="#b14">[13]</ref> without using any target domain data. By eliminating the tedious processes of data collection and model updating given a new target domain, our approach enables a re-ID model trained using source datasets to be applied out-ofthe-box for any unseen target dataset.</p><p>Concretely, our solution to domain-generalisable feature learning is to introduce instance normalisation (IN) <ref type="bibr" target="#b15">[14]</ref> to our OSNet architecture. Unlike batch normalisation (BN) <ref type="bibr" target="#b16">[15]</ref> based on mini-batch level statistics, IN calibrates a sample using inner statistics, thus eliminating instancespecific contrast and style that are largely affected by domain-specific environments <ref type="bibr" target="#b17">[16]</ref>, <ref type="bibr" target="#b18">[17]</ref>, <ref type="bibr" target="#b19">[18]</ref>. In this way, IN can naturally address the fundamental style discrepancy problem in cross-domain person re-ID. However, IN has never been exploited for solving such a cross-domain issue in re-ID. It has been noticed that where and how many IN layers to have in a CNN are critical for DG <ref type="bibr" target="#b20">[19]</ref>, <ref type="bibr" target="#b21">[20]</ref>, but there is no clear guidance on how to design the network architecture. We therefore propose to learn the optimal model configuration directly from data via differentiable architecture search. More specifically, we design a novel search space, which contains candidate building blocks with different IN configurations. As the discrete selection variables disable search differentiation, we further leverage the Gumbel-Softmax <ref type="bibr" target="#b22">[21]</ref>, <ref type="bibr" target="#b23">[22]</ref> to create continuous representations for candidate selection, allowing end-to-end optimisation via gradient descent.</p><p>The contributions can be summarised as follows. (1) We introduce, for the first time, the concept of omni-scale feature learning for discriminative person re-ID. This leads to OSNet, a novel CNN architecture capable of simultaneously learning homogeneousand heterogeneous-scale features. By using factorised convolutions, OSNet is lightweight with only 2.2 million parameters-more than one order of magnitude smaller than the common ResNet50-based re-ID models. (2) To improve domain generalisation cross datasets we incorporate instance normalisation (IN) into the OSNet design through differentiable architecture search, which we call OSNet-AIN. To our knowledge, this is the first work that explores both IN and neural architecture search for cross-domain re-ID. (3) We evaluate OSNet by conducting extensive experiments on seven person re-ID datasets. In the same-domain setting, OSNet achieves stateof-the-art performance, outperforming many far larger re-ID models, often by a clear margin. Importantly, in the crossdomain setting, OSNet-AIN exhibits a remarkable generalisation ability: it beats most recent unsupervised domain adaptation (UDA) methods on unseen target domains while maintaining strong source domain performance, requiring neither the target domain data nor per-domain training. Our code and models are publicly available to facilitate future research in re-ID. <ref type="bibr" target="#b3">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN Architectures for Person Re-ID</head><p>Most existing deep re-ID methods <ref type="bibr" target="#b6">[5]</ref>, <ref type="bibr" target="#b7">[6]</ref>, <ref type="bibr" target="#b24">[23]</ref>, <ref type="bibr" target="#b25">[24]</ref>, <ref type="bibr" target="#b26">[25]</ref> adopt CNN architectures that are originally designed for generic object classification problems, especially those ImageNet-winning models <ref type="bibr" target="#b27">[26]</ref>, <ref type="bibr" target="#b28">[27]</ref>, <ref type="bibr" target="#b29">[28]</ref>. These architectures are intrinsically limited for instance recognition in re-ID. Modifications are thus made to tackle problems specific to re-ID, such as misalignment <ref type="bibr" target="#b30">[29]</ref>, <ref type="bibr" target="#b31">[30]</ref> and pose variations <ref type="bibr" target="#b26">[25]</ref>, <ref type="bibr" target="#b32">[31]</ref>. As persons usually stand upright, <ref type="bibr" target="#b24">[23]</ref>, <ref type="bibr" target="#b31">[30]</ref>, <ref type="bibr" target="#b33">[32]</ref> partition feature maps horizontally and inject parallel supervision signals to each stripe in order to enhance the learning of part-level features. Attention mechanisms are designed in <ref type="bibr" target="#b6">[5]</ref>, <ref type="bibr" target="#b25">[24]</ref>, <ref type="bibr" target="#b34">[33]</ref> to focus feature learning on the foreground image regions. In <ref type="bibr" target="#b35">[34]</ref>, <ref type="bibr" target="#b36">[35]</ref>, <ref type="bibr" target="#b37">[36]</ref>, <ref type="bibr" target="#b38">[37]</ref>, <ref type="bibr" target="#b39">[38]</ref>, <ref type="bibr" target="#b40">[39]</ref>, body part specific CNNs are learned by means of off-the-shelf pose detectors. In <ref type="bibr" target="#b41">[40]</ref>, <ref type="bibr" target="#b42">[41]</ref>, <ref type="bibr" target="#b43">[42]</ref>, CNNs are branched to learn representations from global and local image regions. Since low-level visual cues such as colour are relevant for re-ID, <ref type="bibr" target="#b7">[6]</ref>, <ref type="bibr" target="#b44">[43]</ref>, <ref type="bibr" target="#b45">[44]</ref>, <ref type="bibr" target="#b46">[45]</ref> combine multi-level features extracted at different CNN layers. However, none of the existing re-ID networks can learn multi-scale features explicitly at each CNN layer as in our OSNet. Unlike OSNet, they typically rely on external pose models and/or hand-pick some layers for multi-scale learning. Moreover, the ability to learn heterogeneous-scale features representing the mixture of different scales is also missing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Scale and Multi-Stream CNNs</head><p>As far as we know, the concept of omni-scale deep feature learning has never been introduced before. Nonetheless, the importance of multi-scale feature learning has been recognised recently, and the multi-stream building block design has also been adopted in re-ID <ref type="bibr" target="#b47">[46]</ref>. Compared to a small number of re-ID networks that have multi-stream building blocks <ref type="bibr" target="#b7">[6]</ref>, <ref type="bibr" target="#b48">[47]</ref> OSNet is significantly different. Specifically, the layer design in <ref type="bibr" target="#b7">[6]</ref> is based on ResNeXt <ref type="bibr" target="#b49">[48]</ref>, where each stream learns features at the same scale, while the streams in each OSNet block cover different scales. The network in <ref type="bibr" target="#b48">[47]</ref> is built on Inception <ref type="bibr" target="#b28">[27]</ref>, <ref type="bibr" target="#b50">[49]</ref>, where the multi-streams were originally designed for low computational cost with a hand-crafted mixture of convolutional and pooling layers. In contrast, our building block uses a scale-controlling factor to diversify the spatial scales. Moreover, <ref type="bibr" target="#b48">[47]</ref> fuses multi-stream features with learnable but fixed-once-learned stream-wise weights only at the final block. Whereas we fuse multi-scale features within each building block using dynamic (inputdependent) channel-wise weights to learn combinations of multi-scale patterns. Therefore, only our OSNet is capable of learning omni-scale features with each feature channel potentially capturing more discriminative features of either a single scale or a weighted mixture of multiple scales. Our experiments (in Sec. 4.1) show that OSNet significantly outperforms the models in <ref type="bibr" target="#b7">[6]</ref>, <ref type="bibr" target="#b47">[46]</ref>, <ref type="bibr" target="#b48">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lightweight Network Design</head><p>With embedded AI becoming topical, lightweight CNN design has attracted increasing attention. SqueezeNet <ref type="bibr" target="#b51">[50]</ref> compresses feature dimensions using 1×1 convolutions. IGCNet <ref type="bibr" target="#b52">[51]</ref>, ResNeXt <ref type="bibr" target="#b49">[48]</ref> and Con-denseNet <ref type="bibr" target="#b53">[52]</ref> leverage group convolutions. Xception <ref type="bibr" target="#b54">[53]</ref> and the MobileNet series <ref type="bibr" target="#b12">[11]</ref>, <ref type="bibr" target="#b13">[12]</ref> are based on depthwise separable convolutions. Dense 1 × 1 convolutions are grouped with channel shuffling in ShuffleNet <ref type="bibr" target="#b55">[54]</ref>. In terms of lightweight design, our OSNet is similar to MobileNetboth use factorised convolutions-but with a modification in the ordering that empirically works better for omni-scale feature learning (see Sec. 3.1 for the details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Generalisation</head><p>Cross-dataset generalisation has been studied in re-ID <ref type="bibr" target="#b56">[55]</ref>, but no specific designs have ever been introduced to make re-ID models more intrinsically generalisable. Recently, unsupervised domain adaptation (UDA) methods <ref type="bibr" target="#b8">[7]</ref>, <ref type="bibr" target="#b9">[8]</ref>, <ref type="bibr">[9]</ref>, <ref type="bibr">[10]</ref>, <ref type="bibr" target="#b57">[56]</ref> have been extensively studied to adapt a re-ID model from source to target domain. However, UDA methods have to use unlabelled target domain data, so data collection and (per-domain) model update are still required. In contrast, without these steps, our OSNet-AIN is much more efficient in practice. Beyond re-ID, the problem of domain generalisation (DG) has been investigated in deep learning <ref type="bibr" target="#b58">[57]</ref>, <ref type="bibr" target="#b59">[58]</ref>, <ref type="bibr" target="#b60">[59]</ref>, <ref type="bibr" target="#b61">[60]</ref>, <ref type="bibr" target="#b62">[61]</ref>, <ref type="bibr" target="#b63">[62]</ref> (see <ref type="bibr" target="#b14">[13]</ref> for a comprehensive survey in this topic). However, most existing DG methods <ref type="bibr" target="#b59">[58]</ref>, <ref type="bibr" target="#b60">[59]</ref>, <ref type="bibr" target="#b64">[63]</ref> assume that the source and target domains have the same label space, which apparently conflicts with the disjoint label space case in re-ID. Some recent few-shot meta-learning approaches are also re-purposed for DG <ref type="bibr" target="#b65">[64]</ref>. However, they assume a fixed number of classes for the target domain and are trained specifically for that number using source data. Therefore, they cannot be directly applied to re-ID, where the target domain has a different and variable number of identity classes.</p><p>Our DG-oriented re-ID solution is based on instance normalisation (IN) layers <ref type="bibr" target="#b15">[14]</ref>. IN's ability to eliminate instancespecific style discrepancy has been investigated for the style transfer task <ref type="bibr" target="#b17">[16]</ref>, <ref type="bibr" target="#b18">[17]</ref>, <ref type="bibr" target="#b19">[18]</ref>. Recently, several works have attempted to integrate CNNs with IN layers to improve model generalisation. <ref type="bibr" target="#b21">[20]</ref> tackle multi-domain learning by fusing BN and IN with a convex weight. In <ref type="bibr" target="#b20">[19]</ref>, an architecture called IBN-Net is engineered by inserting IN to shallow CNN layers for cross-domain semantic segmentation. The empirical study in <ref type="bibr" target="#b20">[19]</ref> suggests that appearance variations mainly lie in shallow CNN layers, and therefore, inserting IN to shallow layers should be more effective. However, there is no clear definition of what 'shallow' layers are in deep neural networks. Moreover, person re-ID is an instance recognition problem, for which the empirical rule derived from the semantic segmentation task might not work. In this paper, instead of hand-picking layers for inserting IN, we propose to use neural architecture search to optimally explore the capability of IN for improving DG.</p><p>Neural Architecture Search Neural architecture search (NAS) aims to automate the process of network architecture engineering. Early NAS methods are typically based on either reinforcement learning (RL) <ref type="bibr" target="#b66">[65]</ref>, <ref type="bibr" target="#b67">[66]</ref> or evolutionary algorithm (EA) <ref type="bibr" target="#b68">[67]</ref>, <ref type="bibr" target="#b69">[68]</ref>, where hundreds and thousands of models need to be trained from scratch and evaluated on a separate validation set to provide the supervision signal (reward for RL and fitness score for EA). This is computationally extremely expensive, requiring hundreds or even thousands of GPU days to complete the search. The followup research is mainly focused on accelerating the search by, for example, weight sharing <ref type="bibr" target="#b70">[69]</ref>, <ref type="bibr" target="#b71">[70]</ref>. Recently, there has been a growing interest in modelling NAS with directed acyclic graph (DAG) and using continuous representations for end-to-end optimisation. DARTS <ref type="bibr" target="#b72">[71]</ref> uses softmax to relax the discrete one-hot actions over search space. Similarly, SNAS <ref type="bibr" target="#b73">[72]</ref> and GDAS <ref type="bibr" target="#b74">[73]</ref> utilise a discrete gradient estimator <ref type="bibr" target="#b22">[21]</ref>, <ref type="bibr" target="#b23">[22]</ref> to overcome the non-differentiable nature in categorical variables. To address the high GPU memory problem in differentiable NAS, ProxylessNAS <ref type="bibr" target="#b75">[74]</ref> forces gradients to propagate through only one of the candidate paths. Different from these NAS methods, we do not search architecture from scratch. Instead, we base the architecture on OSNet and leverage NAS to find the best way to combine OSNet with IN.</p><p>An earlier and preliminary version of this work was published in ICCV'19 <ref type="bibr" target="#b76">[75]</ref>. Compared with <ref type="bibr" target="#b76">[75]</ref>, which only focuses on discriminative feature learning for same-domain re-ID, this paper brings up the problem of generalisable feature learning for cross-domain re-ID, which has been largely overlooked in existing re-ID research. To that end, this work proposes to combine OSNet with IN by automatically searching for the best model configuration directly from data. Extensive experiments in the cross-domain re-ID setting, together with a comprehensive comparison with state-of-the-art cross-domain re-ID methods, demonstrate that our OSNet achieves strong performance on unseen target datasets even without 1) using any target domain data and 2) undesirable per-domain model adaptation steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OMNI-SCALE NETWORK FOR PERSON RE-ID</head><p>In this section, we detail the design of our omni-scale network (OSNet), which is aimed at learning omni-scale feature representations for person re-ID. We first discuss depthwise separable convolutions, which are used to make OSNet lightweight. Then, we introduce our novel omni-scale residual block for learning discriminative re-ID features. Finally, to enhance generalisation in unseen datasets, we extend OSNet by adding instance normalisation (IN) layers and further present a differentiable architecture search mechanism to automatically infer the optimal IN configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Depthwise Separable Convolutions</head><p>For lightweight network design, we adopt the depthwise separable convolution <ref type="bibr" target="#b12">[11]</ref>, <ref type="bibr" target="#b54">[53]</ref>. The basic idea is to divide a convolution layer,</p><formula xml:id="formula_0">ReLU(w * x), with kernel w ∈ R k×k×c×c , into two separate layers, ReLU((v • u) * x),</formula><p>with depthwise kernel u ∈ R k×k×1×c and pointwise kernel v ∈ R 1×1×c×c , where * denotes convolution, k the kernel size, c the input channel width and c the output channel width. Given an input tensor x ∈ R h×w×c of height h and width w, the computational cost is reduced from</p><formula xml:id="formula_1">h · w · k 2 · c · c to h · w · (k 2 + c) · c , and the parameter size from k 2 · c · c to (k 2 + c) · c . In our implementation, we find that ReLU((u • v) * x) (pointwise → depthwise instead of depthwise → pointwise)</formula><p>turns out to be more effective for omni-scale feature learning. <ref type="bibr" target="#b4">3</ref> We call such layer Lite 3×3 hereafter. The design is depicted in <ref type="figure" target="#fig_3">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Omni-Scale Residual Block</head><p>The building block in OSNet is based on the residual bottleneck <ref type="bibr" target="#b27">[26]</ref>, but equipped with the Lite 3 × 3 layer ( <ref type="figure" target="#fig_4">Fig. 4)</ref>. Given an input x, a residual bottleneck aims to learn a residualx via a mapping function F , i.e.</p><formula xml:id="formula_2">y = x +x, withx = F (x),<label>(1)</label></formula><p>where F denotes a Lite 3 × 3 convolution layer that learns single-scale features (i.e. receptive field = 3 × 3). Note that here the 1×1 convolution layers are ignored in notation as they are used to manipulate feature channels and do not contribute to the aggregation of spatial information <ref type="bibr" target="#b27">[26]</ref>, <ref type="bibr" target="#b49">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Scale Feature Learning</head><p>To achieve multi-scale feature learning, we extend the residual function F by introducing a new dimension, exponent t, to represent the feature scale. For F t , with t &gt; 1, we stack t Lite 3×3 layers, resulting in a receptive field of size (2t + 1) × (2t + 1). Then, the residual to be learned,x, is the sum of incremental scales of representations up to T :</p><formula xml:id="formula_3">x = T t=1 F t (x), T 1.<label>(2)</label></formula><p>When T = 1, Eq. (2) reduces to Eq. (1), i.e. the baseline single-scale bottleneck as shown in <ref type="figure" target="#fig_4">Fig. 4(a)</ref>. Considering the computational cost, we use T = 4 in this paper where the largest receptive field is 9×9. This is depicted in <ref type="figure" target="#fig_4">Fig. 4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic and Unified Aggregation Gate</head><p>So far, each individual stream gives us features of only one specific scale, i.e. scale homogeneous. To learn effective omni-scale features, we propose to combine the outputs of different streams in a dynamic way, i.e. different weights are assigned to different scales according to the input image, rather than being fixed and identical for all the data after training. More specifically, the dynamic scale fusion is achieved by a novel aggregation gate (AG), which is essentially a learnable neural network.</p><formula xml:id="formula_4">Let x t denote F t (x), the omni-scale residualx is then formulated bỹ x = T t=1 G(x t ) x t , with x t F t (x),<label>(3)</label></formula><p>where G(x t ) is a data-conditioned vector with length spanning the entire channel dimension of input x t , and denotes the Hadamard product. G is implemented as a mini-network composed of a non-parametric global average pooling layer <ref type="bibr" target="#b77">[76]</ref> and a multi-layer perceptron (MLP) with one ReLU-activated hidden layer, followed by the sigmoid activation. To reduce parameter overhead, we follow <ref type="bibr" target="#b78">[77]</ref>, <ref type="bibr" target="#b79">[78]</ref> to reduce the MLP's hidden dimension with a reduction ratio, which is set to <ref type="bibr">16.</ref> In our design, the AG is shared across all the feature streams in the same omni-scale residual block (dashed box in <ref type="figure" target="#fig_4">Fig. 4(b)</ref>). In spirit, this is similar to the parameter sharing of convolution filters in CNNs, resulting in a number of advantages. First, the number of parameters is independent of the number of streams T , thus the model becomes more scalable. Second, unifying AG has a nice property when performing gradient backpropagation. Concretely, suppose the network is supervised by a differentiable loss function L and the gradient ∂L ∂x can be computed. The gradient w.r.t G, based on Eq. (3), is</p><formula xml:id="formula_5">∂L ∂G = ∂L ∂x ∂x ∂G = ∂L ∂x ( T t=1 x t ).<label>(4)</label></formula><p>It is clear that the second term in Eq. (4) indicates that supervision signals from all streams are gathered together to guide the learning of G. This desirable property disappears when each stream has its own gate.  During a forward pass, the selected candidate is determined by sampling discrete actions (one-hot) from a categorical distribution parameterised by the architecture parameters. To make the computational graph differentiable, we relax the discrete variables to continuous representations using the Gumbel-Softmax <ref type="bibr" target="#b22">[21]</ref>, <ref type="bibr" target="#b23">[22]</ref>. IN: Instance Normalisation <ref type="bibr" target="#b15">[14]</ref>.</p><formula xml:id="formula_6">⇡ 1 ⇡ 2 ⇡ 3 ⇡ 4 ↵ 1 ↵ 2 ↵ 3 ↵ 4 Architecture parameters Sampled actions (a) (b) (c) (d)</formula><p>We further stress two design considerations. First, in contrast to using a single-scalar gate function that provides a coarse scale fusion, we use channel-wise vector gating, i.e. AG's output G(x t ) is a vector rather a scalar for the t-th stream. This design results in a more fine-grained fusion that tunes each feature channel. Second, the weights are dynamically computed by conditioning on the input data. This is crucial for re-ID as the training and test data describe disjoint identity populations; input adaptive feature-scale fusion is hence more effective and scalable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inserting Instance Normalisation Layers</head><p>Different from batch normalisation (BN) <ref type="bibr" target="#b16">[15]</ref>, which normalises each sample using statistics computed over a minibatch, IN performs normalisation on each sample using its own mean and standard deviation <ref type="bibr" target="#b17">[16]</ref>. As such, IN allows the instance-specific style information to be effectively removed. Therefore inserting IN layers into a re-ID CNN has the potential of eliminating image style differences caused by distinct environments, lighting conditions, camera setups, etc. in each dataset. However, it is unclear how to integrate a re-ID CNN with IN to maximise the gain, e.g., which layers to insert? Inside or outside a residual block?</p><p>Architecture Search Space We propose to learn the optimal way of integrating OSNet with IN by neural architecture search (NAS). To that end, we define a novel search space Ω consisting of candidate omni-scale (OS) blocks in different IN-incorporating designs. Specifically, besides the standard OS block ( <ref type="figure" target="#fig_4">Fig. 4(b)</ref>), we design three other variants (see <ref type="figure" target="#fig_6">Fig. 5</ref> for an illustration of our search space). Following <ref type="bibr" target="#b80">[79]</ref>, we keep the residual learning module unchanged, i.e. only adding IN after the residual. For clarity, we refer to <ref type="figure" target="#fig_6">Fig. 5(b-d)</ref> as OS+IN in block, OS+IN out block, and OS+IN in−out block, respectively.</p><p>Formulation In our NAS formulation, the output y of each OS layer is obtained as a weighted sum of operations</p><formula xml:id="formula_7">in Ω, y = ω∈Ω α ω ω(x),<label>(5)</label></formula><p>where α = [α ω ] ω∈Ω is a |Ω|-dimensional one-hot vector, with the activated element "1" corresponding to the IN design selection. The objective is to minimise the following expectation through jointly optimising the model architecture α and the parameters θ as,</p><formula xml:id="formula_8">E[L(x, θ, α)].<label>(6)</label></formula><p>For an OSNet with m blocks, the search space contains a total of 4 m different architecture design choices. A key challenge lies in the optimisation of the discrete selection that is non-differentiable due to its discontinuous nature. This disables the adoption of strong gradient-based architecture search optimisation. Relaxation and Reparameterisation Trick To solve the non-differentiable problem, we develop a continuous relaxation and reparameterisation strategy. This is achieved by first treating α as a continuous 1 -normalised random variable sampled from a probability distribution P π parameterised by π (i.e. the target architecture parameters) as</p><formula xml:id="formula_9">P (α ω = 1) = exp(π ω ) ω ∈Ω exp(π ω ) ,<label>(7)</label></formula><p>and then reparameterising this sampling process by the Gumbel-Softmax <ref type="bibr" target="#b22">[21]</ref>, <ref type="bibr" target="#b23">[22]</ref> defined as</p><formula xml:id="formula_10">α ω = f π (z ω ) = exp (log π ω + z ω )/λ ω ∈Ω exp (log π ω + z ω )/λ ,<label>(8)</label></formula><p>where λ is the softmax temperature and z ω ∼ Gumbel(0, 1) a Gumbel distribution. Concretely, z ω is obtained by the following transformation of the uniform distribution: z ω = − log(− log(u ω )), where u ω ∼ Uniform(0, 1).</p><p>The objective function is reformulated as:</p><formula xml:id="formula_11">E z∼P (z) [L(x, θ, f π (z))],<label>(9)</label></formula><p>which is fully differentiable w.r.t. both θ and π. The gradients can be approximated by Monte Carlo sampling <ref type="bibr" target="#b81">[80]</ref>,</p><formula xml:id="formula_12">θ E z∼P (z) [L(x, θ, f π (z))] (10) = E z∼P (z) [ θ L(x, θ, f π (z))]<label>(11)</label></formula><formula xml:id="formula_13">1 S S s=1 θ L(x, θ, f π (z s ))],<label>(12)</label></formula><p>where S denotes the number of sampling steps, and similarly,</p><formula xml:id="formula_14">π E z∼P (z) [L(x, θ, f π (z))] (13) = E z∼P (z) [ π L(x, θ, f π (z))]<label>(14)</label></formula><formula xml:id="formula_15">1 S S s=1 π L(x, θ, f π (z s ))].<label>(15)</label></formula><p>In doing so, we transfer the dependency on π from P to f . Importantly, as proved in <ref type="bibr" target="#b23">[22]</ref>, when λ → 0, the relaxed softmax formulation (Eq. (8)) approaches the discrete argmax computation, i.e. an unbiased gradient estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Search Outcome</head><p>At the end of search, we derive a compact network architecture by selecting for each layer the OS block with the largest π, i.e. ω * = arg max ω∈Ω π ω . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Network Architecture</head><p>OSNet is constructed by stacking the proposed lightweight bottleneck (OS block) layer-by-layer. The detailed network architecture is shown in <ref type="table" target="#tab_2">Table 1</ref>. For comparison, the same network architecture with normal convolutions has 6.9 million parameters and 3,384.9 million multadd operations. This is 3× larger than OSNet with the Lite 3 × 3 convolution layer design. The OSNet architecture in <ref type="table" target="#tab_2">Table 1</ref> can be easily scaled up or down in practice, to balance model size, computational cost and performance. To this end, we use a width multiplier <ref type="bibr" target="#b5">4</ref> and an image resolution multiplier, following <ref type="bibr" target="#b12">[11]</ref>, <ref type="bibr" target="#b13">[12]</ref>, <ref type="bibr" target="#b55">[54]</ref>.</p><p>OSNet-AIN denotes the network architecture with automatically searched IN layers. In the experiment, we run the searching algorithm four times with different random seeds and select the one with the best cross-domain performance as our final model, which is a commonly adopted protocol in the NAS literature <ref type="bibr" target="#b72">[71]</ref>, <ref type="bibr" target="#b73">[72]</ref>, <ref type="bibr" target="#b75">[74]</ref>. The found best architecture model is shown in the OSNet-AIN column in <ref type="table" target="#tab_2">Table 1</ref>. The detailed experimental setup for NAS will be covered in Sec. 4.2. As IN only introduces a small number of parameters, the complexity between OSNet-AIN and OSNet is similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation to Prior Architectures</head><p>In terms of the multistream design, OSNet is related to Inception <ref type="bibr" target="#b28">[27]</ref> and ResNeXt <ref type="bibr" target="#b49">[48]</ref>, but has several crucial differences. 1) First, the multi-stream design in OSNet strictly follows the scaleincremental principle dictated by the exponent variable (see Eq. <ref type="formula" target="#formula_3">(2)</ref>). Such a design is more effective for covering a wide range of scales. In contrast, Inception was originally <ref type="bibr" target="#b5">4</ref>. Width multiplier with magnitude smaller than 1 works on all layers in OSNet except the last fc layer whose feature dimension is fixed to 512. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Same-Domain Person Re-Identification</head><p>We first evaluate OSNet in the conventional person re-ID setting where the model is trained and tested on the same dataset (domain).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and Settings</head><p>Seven popular re-ID benchmarks are used, including Market1501 <ref type="bibr" target="#b82">[81]</ref>, CUHK03 <ref type="bibr" target="#b4">[3]</ref>, DukeMTMC-reID (Duke) <ref type="bibr" target="#b83">[82]</ref>, <ref type="bibr" target="#b84">[83]</ref>, MSMT17 <ref type="bibr" target="#b85">[84]</ref>, <ref type="bibr" target="#b6">5</ref> VIPeR <ref type="bibr" target="#b86">[85]</ref>, GRID <ref type="bibr" target="#b87">[86]</ref> and CUHK01 <ref type="bibr" target="#b88">[87]</ref>. The overall dataset statistics are detailed in <ref type="table" target="#tab_3">Table 2</ref>. The first four are typically considered as big re-ID datasets-even though their sizes are fairly moderate (around 30k training images for the largest dataset MSMT17). The rest three datasets are generally too small to train deep models without proper pre-training <ref type="bibr" target="#b35">[34]</ref>, <ref type="bibr" target="#b41">[40]</ref>. For CUHK03, we use the 767/700 split <ref type="bibr" target="#b89">[88]</ref> with the detected images. For VIPeR, GRID and CUHK01 (485/486 split <ref type="bibr" target="#b0">[1]</ref>), we follow <ref type="bibr" target="#b30">[29]</ref>, <ref type="bibr" target="#b35">[34]</ref>, <ref type="bibr" target="#b41">[40]</ref>, <ref type="bibr" target="#b43">[42]</ref>, <ref type="bibr" target="#b45">[44]</ref> to perform pretraining on large re-ID datasets and then fine-tune the model on the target dataset, where the results are averaged over 10 random splits. For evaluation metrics, we use cumulative matching characteristics (CMC) rank accuracy and mean average precision (mAP). The performance is reported in percentage. follows the standard classification paradigm where each person identity is regarded as a unique class. Similar to <ref type="bibr" target="#b6">[5]</ref>, <ref type="bibr" target="#b7">[6]</ref>, the cross-entropy loss with label smoothing <ref type="bibr" target="#b50">[49]</ref> is used for supervision. For fair comparison against existing methods, we implement two versions of OSNet. One is trained from scratch while the other is fine-tuned from ImageNet pre-trained weights. Person matching is based on the cosine distance using 512-D feature vectors extracted from the last fc layer. The batch size and the weight decay are set to 64 and 5e-4 respectively. Images are resized to 256 × 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>For training from scratch, SGD is used to optimise the network for 350 epochs. The learning rate starts from 0.065 and is decayed by 0.1 at the 150-th, the 225-th, and the 300-th epoch, respectively. Data augmentation includes random flip, random crop and random patch. <ref type="bibr" target="#b7">6</ref> For fine-tuning, we train the network with AMSGrad <ref type="bibr" target="#b103">[102]</ref> and the initial learning rate of 0.0015 for 250 epochs. The learning rate is decayed using the cosine annealing strategy <ref type="bibr" target="#b104">[103]</ref> (without restart). During the first 10 epochs, the ImageNet pre-trained base network is frozen. Only the randomly initialised classifier is open for training <ref type="bibr" target="#b105">[104]</ref>. Data augmentation includes random flip and random erasing <ref type="bibr" target="#b106">[105]</ref>. <ref type="table" target="#tab_4">Table 3</ref>, we have the following observations. (1) OSNet achieves the best overall performance, outperforming most recently published methods by a clear margin. It is evident that the performance on re-ID benchmarks, especially Market1501 and Duke, has been saturated lately. Therefore, the improvements obtained by OSNet are significant. Crucially, the improvements are achieved with much smaller model size-most top-performing re-ID models are based on the ResNet50 backbone, which has more than 23.5 million parameters (except extra customised modules), whereas our OSNet has only 2.2 million parameters. Notably, OSNet is around 6× smaller than the automatically searched model, Auto-ReID <ref type="bibr" target="#b91">[90]</ref>, but obtains better performance on three out of four datasets. These results verify the effectiveness of omni-scale feature learning for re-ID, achieved by an extremely compact network. As OSNet is orthogonal to some methods such as the image generation-based DGNet <ref type="bibr" target="#b102">[101]</ref>, they can be combined to potentially boost the re-ID performance in practice. (2) OS-Net yields strong performance with or without ImageNet pre-training. Among the very few existing lightweight re-ID models that can be trained from scratch (Auto-ReID, HAN and BraidNet in the top group), OSNet exhibits more significant advantages. For instance, in terms of mAP, on Market1501, OSNet beats Auto-ReID, HAN and BraidNet by 6.4%, 5.3% and 11.5%, respectively. Compared with Mo-bileNetV2, which is a general-purpose lightweight CNN, OSNet achieves a large margin consistently at a similar model size. Overall, these results demonstrate the versatility of OSNet: it enables effective feature tuning from generic object categorisation tasks, and offers robustness against model overfitting when trained from scratch on datasets of moderate size. (3) Compared with re-ID models <ref type="bibr" target="#b6">[5]</ref>, <ref type="bibr" target="#b7">[6]</ref>, <ref type="bibr" target="#b30">[29]</ref>, <ref type="bibr" target="#b36">[35]</ref>, <ref type="bibr" target="#b38">[37]</ref>, <ref type="bibr" target="#b47">[46]</ref> also based on multi-scale/multistream architectures, namely Inception or ResNeXt, OSNet 6. RandomPatch works by (1) constructing a patch pool that stores randomly extracted image patches and (2) pasting a random patch selected from the patch pool onto an input image at random position. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Big Re-ID Datasets From</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Small Re-ID Datasets</head><p>Small re-ID datasets are more challenging for deep re-ID models because they have much less training images and classes than the big datasets. <ref type="table" target="#tab_6">Table 4</ref> compares OSNet with six state-of-the-art deep re-ID methods. On VIPeR, we observe that OSNet outperforms all alternatives by a significant margin (more than 11%). GRID is even more challenging than VIPeR because it has only 250 training images of 125 identities. Moreover, it was captured by real (operational) analogue CCTV cameras installed in busy public spaces, presenting more observation noise. OSNet remains the best on GRID, marginally above JLML <ref type="bibr" target="#b41">[40]</ref>, which is the current stateof-the-art. On CUHK01, which has around 1,900 training images, OSNet significantly outperforms Spindle and JLML by 6.7% and 16.8%, respectively. Overall, the performance of OSNet on these small datasets is excellent, indicating its promising advantage in real-world applications without large-scale training data.   datasets like MSMT17 (model 2 vs. 1). This means our design helps maintain the representational power while shrinking the model size by more than <ref type="figure" target="#fig_1">3×. (2)</ref> vs. ResNeXt-like design: OSNet is transformed into a ResNeXt-like architecture by making all streams homogeneous in depth while preserving the unified AG, which refers to model 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We observe that this variant is clearly outperformed by the primary model, which further validates the necessity of the omni-scale design.</p><p>(3) Multi-scale fusion strategy: We change the way how features of different scales are aggregated. The baselines are concatenation (model 4) and addition (model 5). The primary model is clearly better than the two baselines. Nevertheless, models 4 and 5 are still much better than the single-scale architecture (model 9). (4) Unified AG vs. separate AGs: When separate AGs are learned for each feature stream, the model size is increased and the nice property in gradient computation (Eq. (4)) vanishes. Empirically, unifying AG improves the performance (model 1 vs. 6), despite having less parameters. (5) Channel-wise gates vs. stream-wise gates: By turning the channel-wise gates into stream-wise gates (model 7), the performance deteriorates.</p><p>As feature channels represent numerous visual concepts and encapsulate sophisticated correlations <ref type="bibr" target="#b107">[106]</ref>, it is advantageous to use channel-specific gates. (6) Dynamic gates vs. static gates: In model 8, feature streams are fused by static (learned-and-then-fixed) channel-wise gates to mimic the design in <ref type="bibr" target="#b48">[47]</ref>. As a result, the performance drops noticeably compared with that of dynamic gating (model 1). Therefore, adapting the scale fusion for individual input images is essential. (7) Evaluation on stream cardinality: The results are substantially boosted from T = 1 (model 9) to T = 2 (model 10), and gradually progress to T = 4 (model 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Shrinking Hyperparameters</head><p>We can trade-off between model size, computations and performance by adjusting the width multiplier β and the image resolution multiplier γ. <ref type="table" target="#tab_8">Table 6</ref> shows that by keeping one multiplier fixed and shrinking the other, the R1 drops off smoothly. It is worth noting that 92.2% R1 accuracy is obtained by a much shrunken version of OSNet with merely 0.2M parameters and 82.3M mult-adds (β = 0.25). Compared with the results in <ref type="table" target="#tab_4">Table 3</ref>, we can see that the shrunken OSNet is still very competitive against the latest models (most being 100× bigger in size). This indicates that OSNet is a superior fit for efficient deployment in resource-constrained devices such as surveillance cameras with AI processors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualisation of Unified Aggregation Gate</head><p>As the gating vectors produced by the AG inherently encode the way how the omni-scale feature streams are aggregated, we can understand what the AG sub-network has learned by visualising images of similar gating vectors. To this end, we 1) concatenate the gating vectors of four streams in the last bottleneck as the representations of test images, 2) perform k-means clustering, and 3) select top-15 images closest to the cluster centres. <ref type="figure" target="#fig_9">Fig. 6</ref> shows four example clusters where images within the same cluster exhibit similar patterns, i.e. combinations of global-scale and local-scale appearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualisation of Learned Features</head><p>To understand how our designs help OSNet learn discriminative features, we visualise the activations of the last convolutional feature maps to investigate where the network focuses on to extract features. Following <ref type="bibr" target="#b108">[107]</ref>, the activation maps are computed as the sum of absolute-valued feature maps along the channel dimension followed by a spatial 2 normalisation. <ref type="figure" target="#fig_10">Fig. 7</ref>  compares the activation maps of OSNet and the single-scale baseline (model 9 in <ref type="table" target="#tab_5">Table 5</ref>). It is clear that OSNet can capture the local discriminative patterns of Person A (e.g., the clothing logo) which distinguish Person A from Person B. In contrast, the single-scale model over-concentrates on the face region, which is unreliable for re-ID due to low resolution of surveillance images. This qualitative result shows that our multi-scale design and unified aggregation gate enable OSNet to identify subtle differences between visually similar persons-a vital ability for accurate re-ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cross-Domain Person Re-Identification</head><p>In this section, we evaluate the domain-generalisable OSNet with IN, i.e. OSNet-AIN, in the cross-dataset re-ID setting. In particular, we aim to assess the generalisation performance of OSNet-AIN by first training the model on a source dataset and then directly testing its performance on an unseen target dataset without the need for per-domain model adaptation. This differs significantly from the current state-of-the-art unsupervised domain adaptation (UDA) methods <ref type="bibr" target="#b8">[7]</ref>, <ref type="bibr">[10]</ref>, <ref type="bibr" target="#b113">[112]</ref>, <ref type="bibr" target="#b114">[113]</ref>, which require per-domain adaptation on the target domain data (hence more computationally expensive and less scalable).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture Search</head><p>We first discuss the experimental details regarding how OSNet-AIN is searched. For the dataset to perform NAS, we choose MSMT17, which contains the largest camera network (15 cameras) and has diverse image qualities/styles (collected in four days of different weather conditions within a month). Once the network architecture    is found, we directly transfer it to other re-ID datasets without re-searching. The over-parameterised network <ref type="figure" target="#fig_6">(Fig. 5</ref>) is trained from scratch using SGD, batch size of 512, initial learning rate of 0.1 and weight decay of 5e-4 for 120 epochs on 8 Tesla V100 32GB GPUs. The learning rate is annealed down to zero using the cosine annealing trick <ref type="bibr" target="#b104">[103]</ref> without restart. The softmax temperature λ (Eq. (8)) starts from 10 and decreases by 0.5 every 20 epochs (the minimum is fixed to 1. 7 ) Though a larger Monte Carlo sampling number S in Eqs. (12) &amp; (15) is theoretically better for convergence, we empirically found that setting S = 1 worked well and greatly shortened the training time. The objective function is the cross-entropy loss with label smoothing. Random flip and colour jittering are used for data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and Settings</head><p>Following the recent UDA re-ID works <ref type="bibr" target="#b8">[7]</ref>, <ref type="bibr">[9]</ref>, <ref type="bibr">[10]</ref>, <ref type="bibr" target="#b113">[112]</ref>, <ref type="bibr" target="#b114">[113]</ref>, we experiment with Market1501→Duke, Duke→Market1501, MSMT17→Market1501/Duke, and Market1501/Duke→MSMT17. When the source dataset is MSMT17, we use all 126,441 images of 4,101 identities for model training, following <ref type="bibr" target="#b113">[112]</ref>, <ref type="bibr" target="#b114">[113]</ref>. OSNet-AIN is first pre-trained on ImageNet and then fine-tuned on a source dataset for re-ID on a target dataset. The training pipeline and details for this cross-domain setting follow those used in the same-domain setting, except that the maximum epoch is 100 for Market1501/Duke and 50 for MSMT17 and the data augmentation includes random flip and colour jittering. <ref type="table">Table 7</ref> shows that OSNet-AIN significantly improves upon OSNet by 7.7% R1 7. Setting λ &lt; 1 makes the training unstable. and 4.6% mAP on Market1501→Duke and 8.8% R1 and 6.6% mAP on Duke→Market1501. This justifies the effectiveness of IN for cross-domain re-ID. Comparing OSNet with IBN-Net, we observe that our basic omni-scale backbone is already stronger than the IN-equipped ResNet50 model. This suggests that our omni-scale network design is not only effective for learning discriminative re-ID features for source datasets, but also helps the learning of generalisable person features for unseen target datasets. This is because omniscale features capture both global-and local-scale patterns, intuitively a domain-agnostic capability. <ref type="table" target="#tab_9">Table 8</ref> further tests the model performance effect of IN in the same-domain re-ID setting by comparing OSNet-AIN with OSNet. We observe that IN slightly decreases the performance. This is not surprising: during feature learning IN progressively removes dataset-specific features that are detrimental to cross-domain re-ID but potentially beneficial to same-domain recognition. This is thus a price one has to pay to make the model more generalisable to unseen domains. Note that the performance of OSNet-AIN in the same-domain setting is still very competitive when compared with the state-of-the-art alternatives in <ref type="table" target="#tab_4">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Instance Normalisation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Search vs. Engineering</head><p>To justify the contribution of our architecture search algorithm, we hand-engineer an OSNet+IN model by mimicking the design rule in IBN-Net. Specifically, we add IN only to the lowest layers in OSNet (conv1 &amp; conv2 as shown in <ref type="table" target="#tab_2">Table 1</ref>), which we call OSNet-IBN. <ref type="table">Table 7</ref> shows that OSNet-AIN significantly outperforms OSNet-IBN by 4.5% R1 and 2.9% mAP on Market1501→Duke and 3.2% R1 and 3.2% mAP on Duke→Market1501. This strongly demonstrates the superiority of our architecture search algorithm over handcrafted architecture design. Given that the search space for the entire network contains 4 6 = 4096 different configurations, it is much more efficient to learn the network configuration rather than exhaustively trying all possible choices. <ref type="table" target="#tab_10">Table 9</ref> compares OSNet-AIN with current state-of-the-art cross-domain re-ID methods based on UDA, which obtain unfair gains by using the target data. It is clear that OSNet-AIN achieves promising results on the target datasets despite only using source data-it outperforms most UDA methods in terms of R1. When the source dataset is large such as MSMT17, OSNet-AIN substantially improves the cross-domain performance (R1) from 52.4% to 71.1% on Duke and 61.0% to 70.1% on Market1501, which are close to the performance of the latest UDA methods. It is noted that most top-performing UDA methods (ECN, HHL, CamStyle and ATNet in the small source case) rely on image-to-image translation models such as CycleGAN <ref type="bibr" target="#b115">[114]</ref>  to synthesise target-style images and have complex adaptation procedures for each target domain. These thus severely hinder their deployment in real-world applications where out-of-the-box solution is desired. In contrast, OSNet-AIN enables adaptation-free, plug-and-play deployment once trained on a source dataset. Notably, in the more challenging scenario where the source dataset is small but the target dataset is large, i.e. Market1501→MSMT17, as shown in <ref type="table" target="#tab_2">Table 10</ref>, OSNet-AIN can achieve performance on par with ECN-the latter benefits from more unlabelled target data from MSMT17.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparative Results</head><p>To further demonstrate our model's capability, we conduct evaluation on multi-source domain generalisation <ref type="bibr" target="#b116">[115]</ref> where a model is trained using multiple source datasets, such as a combination of Market1501, Duke, and CUHK03, and tested on an unseen target dataset, such as MSMT17. Please see Appendix A and <ref type="table" target="#tab_2">Table 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we presented OSNet, a lightweight CNN architecture that is capable of learning omni-scale feature representations for person re-ID. Compared with existing re-ID CNNs, OSNet has the unique ability to learn multiscale features explicitly inside each building block, where the unified aggregation gate dynamically fuses multi-scale features to produce omni-scale features. To improve crossdomain generalisation, we equipped OSNet with instance normalisation via differentiable architecture search, resulting in a domain-adaptive variant called OSNet-AIN. In the same-domain re-ID setting, the results showed that OSNet achieves state-of-the-art performance while being much smaller than ResNet-based opponents. In the crossdomain re-ID setting, OSNet-AIN exhibited a remarkable generalisation ability on unseen target datasets, beating most recent UDA methods without using per-domain model adaptation on target domain data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A MULTI-SOURCE DOMAIN GENERALISATION</head><p>To further demonstrate the effectiveness of OSNet-AIN, we conduct experiments in the multi-source domain generalisation setting following <ref type="bibr" target="#b116">[115]</ref>, <ref type="bibr" target="#b9">8</ref> i.e. a model is trained using multiple source datasets rather than a single dataset. The aim is to show that integrating instance normalisation into OSNet via architecture search is better than handengineering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>Following <ref type="bibr" target="#b116">[115]</ref>, we use the four largest re-ID datasets, namely Market1501, Duke, CUHK03, 9 and MSMT17. In particular, three datasets are used for training and the remaining one is used for testing. During model training, we only use the training split of the source datasets.</p><p>Training Details All models are trained using the crossentropy loss with label smoothing. We simply build a single large classification layer for classifying all identities combined from different datasets. We keep the training parameters the same as those used in Sec. 4.2. The details of the training parameters can be found in our Github repository. 10</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The results are presented in <ref type="table" target="#tab_2">Table 11</ref>. Comparing OSNet with OSNet-IBN, we observe that using instance normalisation layers improves the plain OSNet's performance, especially on the most challenging test domain (MSMT17). By automating the architecture engineering process for integrating instance normalisation layers, the performance is further improved (OSNet-AIN vs. OSNet-IBN). <ref type="bibr">TABLE 11</ref> Results in the multi-source domain generalisation setting using MSMT17 (MS), Market1501 (M), Duke (D), and CUHK03 (C). <ref type="table" target="#tab_2">MS+D+C→M  MS+M+C→D  MS+D+M→C  D+M+C→MS  mAP  R1  mAP  R1  mAP  R1  mAP  R1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Example images from four person re-ID datasets showing that discriminative and generalisable features are essential for re-ID. Each sub-figure contains, from left to right, a query image, a true match, and a false match (distractor).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>A schematic of OSNet building block. R: Receptive field size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>, 2 .</head><label>2</label><figDesc>https://github.com/KaiyangZhou/deep-person-reid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>(a) Standard and (b) Lite 3×3 convolution. DW: Depth-Wise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>(a) Baseline bottleneck. (b) OSNet bottleneck. AG: Aggregation Gate. The first/last 1×1 layers used to reduce/restore feature dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Our architecture search space consists of four different omniscale residual (OSR) blocks each with a learnable parameter π.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>A classification layer (linear fc + softmax) is mounted on the top of OSNet. The training 5. Throughout this paper, we use the v1 version for MSMT17.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>(</head><label></label><figDesc>b) Male + black jacket + blue jeans. (c) Back bags + yellow T-shirt + black shorts. (d) Green T-shirt.(a) Hoody + back bag.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 .</head><label>6</label><figDesc>Image clusters of similar gating vectors. The visualisation shows that the proposed unified aggregation gate is capable of learning the combination of homogeneous and heterogeneous scales conditioned on the input data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 .</head><label>7</label><figDesc>Visual attention insight. Each triplet contains, from left to right, the original image, activation map of OSNet, and the single-scale baseline. OSNet can detect subtle differences between visually similar persons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and T. Xiang are with the University of Surrey, Guildford, GU2 7XH, UK. E-mail: {k.zhou, y.yang, t.xiang}@surrey.ac.uk</figDesc><table /><note>• A. Cavallaro is with Queen Mary University of London, London, E1 4NS, UK. E-mail: a.cavallaro@qmul.ac.uk</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>(b).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Input</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">1x1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Lite 3x3</cell><cell>Lite 3x3</cell><cell cols="2">Lite 3x3</cell><cell cols="2">Lite 3x3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Lite 3x3</cell><cell cols="2">Lite 3x3</cell><cell cols="2">Lite 3x3</cell></row><row><cell></cell><cell>Input</cell><cell></cell><cell></cell><cell cols="2">Lite 3x3</cell><cell cols="2">Lite 3x3 Lite 3x3</cell></row><row><cell></cell><cell>1x1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Lite 3x3</cell><cell>AG</cell><cell>AG</cell><cell></cell><cell>AG</cell><cell></cell><cell>AG</cell></row><row><cell>Residual</cell><cell>1x1 + ReLU</cell><cell>⇥</cell><cell>⇥</cell><cell>+</cell><cell>⇥</cell><cell>⇥</cell><cell>shared</cell></row><row><cell></cell><cell>Output</cell><cell></cell><cell cols="2">1x1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Residual</cell><cell></cell><cell>+</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ReLU</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Output</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell cols="2">(b)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1</head><label>1</label><figDesc>Omni-scale network architecture for person re-ID. Input image size is 256×128. AIN: Automatic search + Instance Normalisation.</figDesc><table><row><cell>stage</cell><cell>output</cell><cell>OSNet</cell><cell>OSNet-AIN</cell></row><row><cell>conv1</cell><cell>128×64, 64</cell><cell cols="2">7×7 conv, stride 2</cell></row><row><cell>pool</cell><cell>64×32, 64</cell><cell cols="2">3×3 max pool, stride 2</cell></row><row><cell>conv2</cell><cell>64×32, 256 64×32, 256</cell><cell>OS block OS block</cell><cell>OS+IN in block OS+IN in block</cell></row><row><cell>transition</cell><cell>64×32, 256 32×16, 256</cell><cell cols="2">1×1 conv 2×2 average pool, stride 2</cell></row><row><cell>conv3</cell><cell>32×16, 384 32×16, 384</cell><cell>OS block OS block</cell><cell>OS block OS+IN in block</cell></row><row><cell>transition</cell><cell>32×16, 384 16×8, 384</cell><cell cols="2">1×1 conv 2×2 average pool, stride 2</cell></row><row><cell>conv4</cell><cell>16×8, 512 16×8, 512</cell><cell>OS block OS block</cell><cell>OS+IN in block OS block</cell></row><row><cell>conv5</cell><cell>16×8, 512</cell><cell></cell><cell>1×1 conv</cell></row><row><cell>gap</cell><cell>1×1, 512</cell><cell cols="2">global average pool</cell></row><row><cell>fc</cell><cell>1×1, 512</cell><cell></cell><cell>fc</cell></row><row><cell cols="2"># params</cell><cell>2.2M</cell><cell>2.2M</cell></row><row><cell cols="2">Mult-Adds</cell><cell>978.9M</cell><cell>978.9M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc>Statistics of person re-ID datasets.</figDesc><table><row><cell>Dataset</cell><cell># IDs</cell><cell># images</cell><cell># cameras</cell></row><row><cell>Market1501 [81]</cell><cell>1,501</cell><cell>32,668</cell><cell>6</cell></row><row><cell>CUHK03 [3]</cell><cell>1,467</cell><cell>28,192</cell><cell>2</cell></row><row><cell>Duke [82], [83]</cell><cell>1,812</cell><cell>36,411</cell><cell>8</cell></row><row><cell>MSMT17 [84]</cell><cell>4,101</cell><cell>126,411</cell><cell>15</cell></row><row><cell>VIPeR [85]</cell><cell>632</cell><cell>1,264</cell><cell>2</cell></row><row><cell>GRID [86]</cell><cell>251</cell><cell>1,275</cell><cell>6</cell></row><row><cell>CUHK01 [87]</cell><cell>971</cell><cell>3,882</cell><cell>2</cell></row><row><cell cols="4">designed to have a low computational cost by sharing</cell></row><row><cell cols="4">computations with multiple streams. Therefore, its struc-</cell></row><row><cell cols="4">ture, which includes mixed operations of convolution and</cell></row><row><cell cols="4">pooling, was hand-crafted. ResNeXt has multiple equal-</cell></row><row><cell cols="4">scale streams, thus learning features at the same scale. 2)</cell></row><row><cell cols="4">Second, Inception/ResNeXt aggregates features by concate-</cell></row><row><cell cols="4">nation/addition while OSNet uses the unified AG (Eq. (3))</cell></row><row><cell cols="4">to facilitate the learning of heterogeneous-scale features.</cell></row></table><note>Critically, this means that the fusion in OSNet is dynamic and adaptive to each individual input image, which is more effective in dealing with disjoint label space in person re- ID. 3) Third, OSNet uses factorised convolutions and thus the building block and subsequently the whole network is lightweight. Therefore, the OSNet architecture is fundamen- tally different from Inception and ResNeXt in nature and design. While the AG borrows the design from SENet [78], they differ conceptually with separate purposes. SENet aims to re-calibrate feature channels by re-scaling the activation values for a single stream. Whereas, OSNet aims to selec- tively fuse multiple feature streams of different receptive field sizes for learning omni-scale features.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3</head><label>3</label><figDesc>Results on big re-ID datasets. It is noteworthy that OSNet surpasses most published methods by a clear margin on all datasets with only 2.2 million parameters, far less than the best-performing ResNet-based methods. -: not reported. †: reproduced by us.</figDesc><table><row><cell>Method</cell><cell>Venue</cell><cell>Backbone</cell><cell>Params (M)</cell><cell cols="2">Market1501 R1 mAP</cell><cell cols="2">CUHK03 R1 mAP</cell><cell>R1</cell><cell cols="2">Duke mAP</cell><cell cols="2">MSMT17 R1 mAP</cell></row><row><cell></cell><cell></cell><cell cols="2">Trained from scratch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MobileNetV2  † [12]</cell><cell>CVPR'18</cell><cell>MobileNetV2</cell><cell>2.2</cell><cell>87.0</cell><cell>69.5</cell><cell>46.5</cell><cell>46.0</cell><cell cols="2">75.2</cell><cell>55.8</cell><cell>50.9</cell><cell>27.0</cell></row><row><cell>BraidNet [89]</cell><cell>CVPR'18</cell><cell>BraidNet</cell><cell>-</cell><cell>83.7</cell><cell>69.5</cell><cell>-</cell><cell>-</cell><cell cols="2">76.4</cell><cell>59.5</cell><cell>-</cell><cell>-</cell></row><row><cell>HAN [5]</cell><cell>CVPR'18</cell><cell>Inception</cell><cell>4.5</cell><cell>91.2</cell><cell>75.7</cell><cell>41.7</cell><cell>38.6</cell><cell cols="2">80.5</cell><cell>63.8</cell><cell>-</cell><cell>-</cell></row><row><cell>Auto-ReID [90]</cell><cell>ICCV'19</cell><cell>Auto</cell><cell>13.1</cell><cell>90.7</cell><cell>74.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>OSNet (ours)</cell><cell>This work</cell><cell>OSNet</cell><cell>2.2</cell><cell>93.6</cell><cell>81.0</cell><cell>57.1</cell><cell>54.2</cell><cell cols="2">84.7</cell><cell>68.6</cell><cell>71.0</cell><cell>43.3</cell></row><row><cell></cell><cell></cell><cell cols="3">Pre-trained on ImageNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SVDNet [91]</cell><cell>ICCV'17</cell><cell>ResNet</cell><cell>&gt;23.5</cell><cell>82.3</cell><cell>62.1</cell><cell>41.5</cell><cell>37.3</cell><cell cols="2">76.7</cell><cell>56.8</cell><cell>-</cell><cell>-</cell></row><row><cell>PDC [35]</cell><cell>ICCV'17</cell><cell>Inception</cell><cell>&gt;6.8</cell><cell>84.1</cell><cell>63.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>58.0</cell><cell>29.7</cell></row><row><cell>HAP2S [92]</cell><cell>ECCV'18</cell><cell>ResNet</cell><cell>&gt;23.5</cell><cell>84.6</cell><cell>69.4</cell><cell>-</cell><cell>-</cell><cell cols="2">75.9</cell><cell>60.6</cell><cell>-</cell><cell>-</cell></row><row><cell>DPFL [46]</cell><cell>ICCVW'17</cell><cell>Inception</cell><cell>&gt;6.8</cell><cell>88.6</cell><cell>72.6</cell><cell>40.7</cell><cell>37.0</cell><cell cols="2">79.2</cell><cell>60.6</cell><cell>-</cell><cell>-</cell></row><row><cell>DaRe [45]</cell><cell>CVPR'18</cell><cell>DenseNet</cell><cell>&gt;23.5</cell><cell>89.0</cell><cell>76.0</cell><cell>63.3</cell><cell>59.0</cell><cell cols="2">80.2</cell><cell>64.5</cell><cell>-</cell><cell>-</cell></row><row><cell>PNGAN [93]</cell><cell>ECCV'18</cell><cell>ResNet</cell><cell>&gt;23.5</cell><cell>89.4</cell><cell>72.6</cell><cell>-</cell><cell>-</cell><cell cols="2">73.6</cell><cell>53.2</cell><cell>-</cell><cell>-</cell></row><row><cell>GLAD [29]</cell><cell>ACM MM'17</cell><cell>Inception</cell><cell>&gt;6.8</cell><cell>89.9</cell><cell>73.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>61.4</cell><cell>34.0</cell></row><row><cell>KPM [31]</cell><cell>CVPR'18</cell><cell>ResNet</cell><cell>&gt;23.5</cell><cell>90.1</cell><cell>75.3</cell><cell>-</cell><cell>-</cell><cell cols="2">80.3</cell><cell>63.2</cell><cell>-</cell><cell>-</cell></row><row><cell>MLFN [6]</cell><cell>CVPR'18</cell><cell>ResNeXt</cell><cell>32.5</cell><cell>90.0</cell><cell>74.3</cell><cell>52.8</cell><cell>47.8</cell><cell cols="2">81.0</cell><cell>62.8</cell><cell>-</cell><cell>-</cell></row><row><cell>FDGAN [94]</cell><cell>NeurIPS'18</cell><cell>ResNet</cell><cell>&gt;23.5</cell><cell>90.5</cell><cell>77.7</cell><cell>-</cell><cell>-</cell><cell cols="2">80.0</cell><cell>64.5</cell><cell>-</cell><cell>-</cell></row><row><cell>DuATM [24]</cell><cell>CVPR'18</cell><cell>DenseNet</cell><cell>&gt;7.0</cell><cell>91.4</cell><cell>76.6</cell><cell>-</cell><cell>-</cell><cell cols="2">81.8</cell><cell>64.6</cell><cell>-</cell><cell>-</cell></row><row><cell>Bilinear [37]</cell><cell>ECCV'18</cell><cell>Inception</cell><cell>&gt;6.8</cell><cell>91.7</cell><cell>79.6</cell><cell>-</cell><cell>-</cell><cell cols="2">84.4</cell><cell>69.3</cell><cell>-</cell><cell>-</cell></row><row><cell>G2G [95]</cell><cell>CVPR'18</cell><cell>ResNet</cell><cell>&gt;23.5</cell><cell>92.7</cell><cell>82.5</cell><cell>-</cell><cell>-</cell><cell cols="2">80.7</cell><cell>66.4</cell><cell>-</cell><cell>-</cell></row><row><cell>DeepCRF [96]</cell><cell>CVPR'18</cell><cell>ResNet</cell><cell>26.1</cell><cell>93.5</cell><cell>81.6</cell><cell>-</cell><cell>-</cell><cell cols="2">84.9</cell><cell>69.5</cell><cell>-</cell><cell>-</cell></row><row><cell>PCB [23]</cell><cell>ECCV'18</cell><cell>ResNet</cell><cell>27.2</cell><cell>93.8</cell><cell>81.6</cell><cell>63.7</cell><cell>57.5</cell><cell cols="2">83.3</cell><cell>69.2</cell><cell>68.2</cell><cell>40.4</cell></row><row><cell>SGGNN [97]</cell><cell>ECCV'18</cell><cell>ResNet</cell><cell>&gt;23.5</cell><cell>92.3</cell><cell>82.8</cell><cell>-</cell><cell>-</cell><cell cols="2">81.1</cell><cell>68.2</cell><cell>-</cell><cell>-</cell></row><row><cell>Mancs [98]</cell><cell>ECCV'18</cell><cell>ResNet</cell><cell>&gt;25.1</cell><cell>93.1</cell><cell>82.3</cell><cell>65.5</cell><cell>60.5</cell><cell cols="2">84.9</cell><cell>71.8</cell><cell>-</cell><cell>-</cell></row><row><cell>AANet [99]</cell><cell>CVPR'19</cell><cell>ResNet</cell><cell>&gt;23.5</cell><cell>93.9</cell><cell>83.4</cell><cell>-</cell><cell>-</cell><cell cols="2">87.7</cell><cell>74.3</cell><cell>-</cell><cell>-</cell></row><row><cell>CAMA [100]</cell><cell>CVPR'19</cell><cell>ResNet</cell><cell>&gt;23.5</cell><cell>94.7</cell><cell>84.5</cell><cell>66.6</cell><cell>64.2</cell><cell cols="2">85.8</cell><cell>72.9</cell><cell>-</cell><cell>-</cell></row><row><cell>IANet [25]</cell><cell>CVPR'19</cell><cell>ResNet</cell><cell>&gt;23.5</cell><cell>94.4</cell><cell>83.1</cell><cell>-</cell><cell>-</cell><cell cols="2">87.1</cell><cell>73.4</cell><cell>75.5</cell><cell>46.8</cell></row><row><cell>DGNet [101]</cell><cell>CVPR'19</cell><cell>ResNet</cell><cell>&gt;23.5</cell><cell>94.8</cell><cell>86.0</cell><cell>65.6</cell><cell>61.1</cell><cell cols="2">86.6</cell><cell>74.8</cell><cell>77.2</cell><cell>52.3</cell></row><row><cell>Auto-ReID [90]</cell><cell>ICCV'19</cell><cell>Auto</cell><cell>13.1</cell><cell>94.5</cell><cell>85.1</cell><cell>73.3</cell><cell>69.3</cell><cell>-</cell><cell></cell><cell>-</cell><cell>78.2</cell><cell>52.5</cell></row><row><cell>OSNet (ours)</cell><cell>This work</cell><cell>OSNet</cell><cell>2.2</cell><cell>94.8</cell><cell>86.7</cell><cell>72.3</cell><cell>67.8</cell><cell cols="2">88.7</cell><cell>76.6</cell><cell>79.1</cell><cell>55.1</cell></row><row><cell cols="4">is clearly better. As discussed in Sec. 3, this is attributed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">to the unique ability of OSNet to learn heterogeneous-scale</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">features by combining multiple homogeneous-scale features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">with the dynamic and unified AG.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc></figDesc><table /><note>evaluates our architectural design choices for omni-scale feature learning. The primary model is model 1. T denotes the stream cardinality in Eq. (2). The results are summarised as follows. (1) vs. standard convolu- tions: Overall, factorising convolutions does not significantly harm the performance while has a positive effect on large</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4</head><label>4</label><figDesc>Comparison with deep methods on small re-ID datasets at rank-1.</figDesc><table><row><cell>Method</cell><cell cols="4">Backbone VIPeR GRID CUHK01</cell></row><row><cell>MuDeep [47]</cell><cell>Inception</cell><cell>43.0</cell><cell>-</cell><cell>-</cell></row><row><cell>DeepAlign [42]</cell><cell>Inception</cell><cell>48.7</cell><cell>-</cell><cell>-</cell></row><row><cell>JLML [40]</cell><cell>ResNet</cell><cell>50.2</cell><cell>37.5</cell><cell>69.8</cell></row><row><cell>Spindle [34]</cell><cell>Inception</cell><cell>53.8</cell><cell>-</cell><cell>79.9</cell></row><row><cell>GLAD [29]</cell><cell>Inception</cell><cell>54.8</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">HydraPlus-Net [44] Inception</cell><cell>56.6</cell><cell>-</cell><cell>-</cell></row><row><cell>OSNet (ours)</cell><cell>OSNet</cell><cell>68.0</cell><cell>38.2</cell><cell>86.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5</head><label>5</label><figDesc>Ablation study on omni-scale residual learning.</figDesc><table><row><cell>Model</cell><cell>Architecture</cell><cell>CUHK03 MSMT17 R1 mAP R1 mAP</cell></row><row><cell>1</cell><cell cols="2">T = 4 + unified AG (primary model) 57.1 54.2 71.0 43.3</cell></row><row><cell>2</cell><cell>T = 4 w/ full conv + unified AG</cell><cell>59.1 56.2 70.5 43.2</cell></row><row><cell>3</cell><cell>T = 4 (same depth) + unified AG</cell><cell>54.4 52.8 69.1 40.4</cell></row><row><cell>4</cell><cell>T = 4 + concatenation</cell><cell>52.2 50.5 66.4 37.7</cell></row><row><cell>5</cell><cell>T = 4 + addition</cell><cell>52.5 51.1 64.5 36.3</cell></row><row><cell>6</cell><cell>T = 4 + separate AGs</cell><cell>53.6 51.0 68.3 39.4</cell></row><row><cell>7</cell><cell>T = 4 + unified AG (stream-wise)</cell><cell>54.4 51.9 67.9 40.4</cell></row><row><cell>8</cell><cell>T = 4 + learned-and-fixed gates</cell><cell>52.9 50.8 64.5 36.2</cell></row><row><cell>9</cell><cell>T = 1</cell><cell>43.3 42.1 52.9 26.8</cell></row><row><cell cols="2">10 T = 2 + unified AG</cell><cell>52.2 50.0 65.6 37.0</cell></row><row><cell cols="2">11 T = 3 + unified AG</cell><cell>54.0 51.8 67.7 39.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 6</head><label>6</label><figDesc>Results of varying width multiplier β and resolution multiplier γ for OSNet. For input size, γ = 0.75: 192 × 96; γ = 0.5: 128 × 64; γ = 0.25: 64 × 32.</figDesc><table><row><cell>β</cell><cell cols="2"># params</cell><cell>γ</cell><cell>Mult-Adds</cell><cell cols="2">Market1501 R1 mAP</cell></row><row><cell>1.0</cell><cell></cell><cell>2.2M</cell><cell>1.0</cell><cell>978.9M</cell><cell>94.8</cell><cell>86.7</cell></row><row><cell>0.75</cell><cell></cell><cell>1.3M</cell><cell>1.0</cell><cell>571.8M</cell><cell>94.5</cell><cell>84.1</cell></row><row><cell>0.5</cell><cell></cell><cell>0.6M</cell><cell>1.0</cell><cell>272.9M</cell><cell>93.4</cell><cell>82.6</cell></row><row><cell>0.25</cell><cell></cell><cell>0.2M</cell><cell>1.0</cell><cell>82.3M</cell><cell>92.2</cell><cell>77.8</cell></row><row><cell>1.0</cell><cell></cell><cell>2.2M</cell><cell>0.75</cell><cell>550.7M</cell><cell>94.4</cell><cell>83.7</cell></row><row><cell>1.0</cell><cell></cell><cell>2.2M</cell><cell>0.5</cell><cell>244.9M</cell><cell>92.0</cell><cell>80.3</cell></row><row><cell>1.0</cell><cell></cell><cell>2.2M</cell><cell>0.25</cell><cell>61.5M</cell><cell>86.9</cell><cell>67.3</cell></row><row><cell>0.75</cell><cell></cell><cell>1.3M</cell><cell>0.75</cell><cell>321.7M</cell><cell>94.3</cell><cell>82.4</cell></row><row><cell>0.75</cell><cell></cell><cell>1.3M</cell><cell>0.5</cell><cell>143.1M</cell><cell>92.9</cell><cell>79.5</cell></row><row><cell>0.75</cell><cell></cell><cell>1.3M</cell><cell>0.25</cell><cell>35.9M</cell><cell>85.4</cell><cell>65.5</cell></row><row><cell>0.5</cell><cell></cell><cell>0.6M</cell><cell>0.75</cell><cell>153.6M</cell><cell>92.9</cell><cell>80.8</cell></row><row><cell>0.5</cell><cell></cell><cell>0.6M</cell><cell>0.5</cell><cell>68.3M</cell><cell>91.7</cell><cell>78.5</cell></row><row><cell>0.5</cell><cell></cell><cell>0.6M</cell><cell>0.25</cell><cell>17.2M</cell><cell>85.4</cell><cell>66.0</cell></row><row><cell>0.25</cell><cell></cell><cell>0.2M</cell><cell>0.75</cell><cell>46.3M</cell><cell>91.6</cell><cell>76.1</cell></row><row><cell>0.25</cell><cell></cell><cell>0.2M</cell><cell>0.5</cell><cell>20.6M</cell><cell>88.7</cell><cell>71.8</cell></row><row><cell>0.25</cell><cell></cell><cell>0.2M</cell><cell>0.25</cell><cell>5.2M</cell><cell>79.1</cell><cell>56.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE 7</cell><cell></cell></row><row><cell cols="7">Ablation study for instance normalisation and architecture search.</cell></row><row><cell>Method</cell><cell></cell><cell cols="5">Market1501→Duke R1 R5 R10 mAP R1 Duke→Market1501 R5 R10 mAP</cell></row><row><cell cols="7">IBN-Net [19] 43.7 59.1 65.2 24.3 50.7 69.1 76.3 23.5</cell></row><row><cell>OSNet</cell><cell></cell><cell cols="5">44.7 59.6 65.4 25.9 52.2 67.5 74.7 24.0</cell></row><row><cell cols="2">OSNet-IBN</cell><cell cols="5">47.9 62.7 68.2 27.6 57.8 74.0 79.5 27.4</cell></row><row><cell cols="2">OSNet-AIN</cell><cell cols="5">52.4 66.1 71.2 30.5 61.0 77.0 82.5 30.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8</head><label>8</label><figDesc>Performance of OSNet-AIN in the same-domain re-ID setting.</figDesc><table><row><cell>Method</cell><cell cols="2">Market1501 R1 mAP</cell><cell>R1</cell><cell>Duke mAP</cell></row><row><cell>OSNet</cell><cell>94.8</cell><cell>86.7</cell><cell>88.2</cell><cell>76.7</cell></row><row><cell>OSNet-AIN</cell><cell>94.2</cell><cell>84.4</cell><cell>87.9</cell><cell>74.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9</head><label>9</label><figDesc>Comparison with current state-of-the-art unsupervised domain adaptation methods in the cross-domain re-ID setting. OSNet-AIN achieves highly comparable performance despite only using the source training data without per-domain model adaptation. U : Unlabelled. Duke (U ) 46.4 62.3 68.0 26.2 Duke + Market1501 (U ) 57.7 75.8 82.4 26.7 TJ-AIDL [110] CVPR'18 Market1501 + Duke (U ) 44.3 59.6 65.0 23.0 Duke + Market1501 (U ) 58.2 74.8 81.1 26.5 ATNet [10] CVPR'19 Market1501 + Duke (U ) 45.1 59.5 64.2 24.9 Duke + Market1501 (U ) 55.7 73.2 79.4 25.6 TABLE 10 Cross-domain results on the more challenging MSMT17 dataset.</figDesc><table><row><cell>Method</cell><cell>Venue</cell><cell cols="2">Source</cell><cell>R1</cell><cell cols="3">Target: Duke R5 R10 mAP</cell><cell>Source</cell><cell>Target: Market1501 R1 R5 R10 mAP</cell></row><row><cell>MMFA [108]</cell><cell cols="7">BMVC'18 Market1501 + Duke (U ) 45.3 59.8 66.3 24.7</cell><cell>Duke + Market1501 (U )</cell><cell>56.7 75.0 81.8 27.4</cell></row><row><cell cols="8">SPGAN [109] CVPR'18 Market1501 + CamStyle [9] TIP'19 Market1501 + Duke (U ) 48.4 62.5 68.9 25.1</cell><cell>Duke + Market1501 (U )</cell><cell>58.8 78.2 84.3 27.4</cell></row><row><cell>HHL [8]</cell><cell cols="7">ECCV'18 Market1501 + Duke (U ) 46.9 61.0 66.7 27.2</cell><cell>Duke + Market1501 (U )</cell><cell>62.2 78.8 84.0 31.4</cell></row><row><cell>ECN [7]</cell><cell cols="7">CVPR'19 Market1501 + Duke (U ) 63.3 75.8 80.4 40.4</cell><cell>Duke + Market1501 (U )</cell><cell>75.1 87.6 91.6 43.0</cell></row><row><cell>SSG [111]</cell><cell>ICCV'19</cell><cell cols="6">Market1501 + Duke (U ) 73.0 80.6 83.2 53.4</cell><cell>Duke + Market1501 (U )</cell><cell>80.0 90.0 92.4 58.3</cell></row><row><cell cols="2">OSNet-AIN (ours) This work</cell><cell cols="2">Market1501</cell><cell cols="4">52.4 66.1 71.2 30.5</cell><cell>Duke</cell><cell>61.0 77.0 82.5 30.6</cell></row><row><cell>MAR [112]</cell><cell>CVPR'19</cell><cell cols="2">MSMT17+Duke (U )</cell><cell cols="2">67.1 79.8</cell><cell>-</cell><cell>48.0</cell><cell cols="2">MSMT17+Market1501 (U ) 67.7 81.9</cell><cell>-</cell><cell>40.0</cell></row><row><cell>PAUL [113]</cell><cell>CVPR'19</cell><cell cols="2">MSMT17+Duke (U )</cell><cell cols="4">72.0 82.7 86.0 53.2</cell><cell cols="2">MSMT17+Market1501 (U ) 68.5 82.4 87.4 40.1</cell></row><row><cell cols="2">OSNet-AIN (ours) This work</cell><cell cols="2">MSMT17</cell><cell cols="4">71.1 83.3 86.4 52.7</cell><cell>MSMT17</cell><cell>70.1 84.1 88.6 43.3</cell></row><row><cell>Method</cell><cell cols="2">Source</cell><cell cols="3">Target: MSMT17 R1 R5 R10 mAP</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ECN [7]</cell><cell cols="5">Market1501 + MSMT17 (U ) 25.3 36.3 42.1 8.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSG [111]</cell><cell cols="5">Market1501 + MSMT17 (U ) 31.6 -49.6 13.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>OSNet-AIN (ours)</cell><cell cols="2">Market1501</cell><cell cols="3">23.5 34.5 40.2 8.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ECN [7]</cell><cell cols="2">Duke + MSMT17 (U )</cell><cell cols="3">30.2 41.5 46.8 10.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSG [111]</cell><cell cols="2">Duke + MSMT17 (U )</cell><cell cols="3">32.2 -51.2 13.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>OSNet-AIN (ours)</cell><cell cols="2">Duke</cell><cell cols="3">30.3 42.2 47.9 10.2</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. The subtle difference between these two orders is when the channel width is increased: pointwise → depthwise increases the channel width before spatial aggregation.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The 767/700 split is used for both training and testing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Please see im osnet ain x1 0 softmax 256x128 amsgrad cosine</title>
		<imprint/>
	</monogr>
	<note>yaml</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical gaussian descriptor for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-level factorisation net for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generalizing a person retrieval model hetero-and homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Camstyle: A novel data augmentation method for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive transfer network for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Domain generalization: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.02503</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Two at once: Enhancing learning and generalization capacities via ibn-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Batch-instance normalization for adaptively style-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-E</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dual attention matching network for context-aware feature sequence based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Interaction-and-aggregation network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Glad: globallocal-alignment descriptor for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Horizontal pyramid matching for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">End-to-end deep kronecker-product matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mask-guided contrastive attention model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spindle net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attentionaware compositional network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Part-aligned bilinear representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Eliminating background-bias for robust person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Densely semantically aligned person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Person re-identification by deep joint learning of multi-loss classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning deep context-aware features over body and latent parts for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deeply-learned partaligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">The devil is in the middle: Exploiting mid-level representations for cross-domain instance matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08106</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hydraplus-net: Attentive deep features for pedestrian analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Resource aware person reidentification across multiple resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Person re-identification by deep learning multi-scale representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-scale deep learning architectures for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Interleaved group convolutions,&quot; in ICCV</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Condensenet: An efficient densenet using learned group convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Generalizable person re-identification by domain-invariant mapping network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Domain adaptive ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07325</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Episodic training for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep domainadversarial image generation for domain generalisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning to generate novel domains for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Domain generalization with mixstyle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Metareg: Towards domain generalization using meta-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Snas: stochastic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Monte carlo gradient estimation in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.10652</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV-W</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PETS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Multi-camera activity correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Human reidentification with transferred metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Re-ranking person reidentification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Person re-identification with cascaded pairwise convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Auto-reid: Searching for a part-aware convnet for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Svdnet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Hard-aware point-to-set deep metric for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Pose-normalized image generation for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Fdgan: Pose-guided feature distilling gan for robust person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Deep groupshuffling random walk for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Group consistent similarity learning via deep crf for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Person reidentification with deep similarity-guided graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Mancs: A multi-task attentional network with curriculum sampling for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Aanet: Attribute attention network for person re-identifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-P</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Towards rich feature discovery with class activation maps augmentation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">On the convergence of adam and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Deep transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05244</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Net2vec: Quantifying and explaining how concepts are encoded by filters in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Multi-task mid-level feature alignment network for unsupervised cross-dataset person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Imageimage domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Transferable joint attributeidentity deep learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Self-similarity grouping: A simple unsupervised cross domain adaptation approach for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by soft multilabel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Patch-based discriminative feature learning for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Unpaired image-toimage translation using cycle-consistent adversarial networkss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Learning to generalize unseen domains via memorybased multi-source meta-learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
