<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Anatomy-aware 3D Human Pose Estimation with Bone-based Pose Decomposition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhili</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
						</author>
						<title level="a" type="main">Anatomy-aware 3D Human Pose Estimation with Bone-based Pose Decomposition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-3D pose</term>
					<term>Bone</term>
					<term>Length</term>
					<term>Direction</term>
					<term>Long skip connections</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose a new solution to 3D human pose estimation in videos. Instead of directly regressing the 3D joint locations, we draw inspiration from the human skeleton anatomy and decompose the task into bone direction prediction and bone length prediction, from which the 3D joint locations can be completely derived. Our motivation is the fact that the bone lengths of a human skeleton remain consistent across time. This promotes us to develop effective techniques to utilize global information across all the frames in a video for high-accuracy bone length prediction. Moreover, for the bone direction prediction network, we propose a fully-convolutional propagating architecture with long skip connections. Essentially, it predicts the directions of different bones hierarchically without using any time-consuming memory units (e.g. LSTM). A novel joint shift loss is further introduced to bridge the training of the bone length and bone direction prediction networks. Finally, we employ an implicit attention mechanism to feed the 2D keypoint visibility scores into the model as extra guidance, which significantly mitigates the depth ambiguity in many challenging poses. Our full model outperforms the previous best results on Human3.6M and MPI-INF-3DHP datasets, where comprehensive evaluation validates the effectiveness of our model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>3 D human pose estimation in videos has been widely studied in recent years. It has extensive applications in action recognition, sports analysis and human-computer interaction. Current state-of-the-art approaches <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> typically decompose the task into 2D keypoint detection followed by 3D pose estimation. Given an input video, they first detect the 2D keypoints of each frame, and then predict the 3D joint locations of a frame based on the 2D keypoints.</p><p>When estimating the 3D joint locations from 2D keypoints, the challenge is to resolve depth ambiguity, as multiple 3D poses with different joint depths can be projected to the same 2D keypoints. Exploiting temporal information from the video has been demonstrated to be effective for reducing this ambiguity. Typically, to predict the 3D joint locations of a frame in a video, recent approaches <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> utilize temporal networks that additionally feed the adjacent frames' 2D keypoints as input. These approaches consider the adjacent local frames most associated with the current frame, and extract their information as extra guidance. However, such approaches are limited to exploiting information only from the neighboring frames. Given a 1-minute input video with a frame rate of 50, even though we choose the existing temporal network with largest temporal window size (i.e 243 frames) <ref type="bibr" target="#b0">[1]</ref>, it is limited to using a concentrated short segment (about one-twelfth length of the video) to predict a single frame. Such a design can easily make existing temporal networks fail when the current frame and its adjacent input frames correspond to a complex pose, because none of the input frames provide reliable and high-confidence information to the networks.</p><p>Considering this, our first contribution is proposing a novel approach that can effectively capture the knowledge from both local and distant frames to estimate the 3D joint locations of the current frame, by cleverly exploiting the anatomic properties of the human skeleton. We refer to it as anatomy awareness. Specifically, based on the anatomy of the human skeleton, we decompose the task of 3D joint location prediction into two sub-tasks -bone direction prediction and bone length prediction. We demonstrate that the combination of the two new tasks are essentially equivalent to the original task. The motivation is based on the fact that the bone lengths of a person remain consistent in a video over time (This can be verified by 3D human pose datasets such as Human3.6M and MPI-INF-3DHP). Hence, when we predict the bone lengths of a particular frame, we can leverage the frames distributed over the duration of the entire video for more accurate and smooth prediction. Note that although Sun et al. <ref type="bibr" target="#b5">[6]</ref> transform the task into a generic bone-based representation, such a generic representation does not allow them to utilize that critical bone length consistency. In contrast, we decompose the task explicitly into bone direction and bone length prediction. We demonstrate that this explicit design leads to significant advantages over either the generic representation design in <ref type="bibr" target="#b5">[6]</ref> or imposing a bone length consistency loss across frames.</p><p>However, it is nontrivial to implement this explicit design. One problem for training the proposed bone length prediction network is that the training dataset typically contains only a few skeletons. For example, the training set of Human3.6M contains 5 actors corresponding to 5 bone length settings. Directly training the network on the data from the 5 actors leads to serious overfitting. Therefore, we adopt the fullyconnected residual network for bone length prediction and propose two effective mechanisms to prevent overfitting via a network design and data augmentation.</p><p>As for the bone directions, we adopt the temporal convolu-tional network in <ref type="bibr" target="#b0">[1]</ref> to predict the direction of each bone in the 3D space for each frame. Motivated by <ref type="bibr" target="#b4">[5]</ref>, we believe it is beneficial to predict the directions of different bones hierarchically, instead of all at once as in <ref type="bibr" target="#b0">[1]</ref>. Following the human skeleton anatomy, the directions of simple torso bones (e.g. lumbar vertebra) with less motion variation should be predicted first, and then guide the prediction of challenging limb bones (e.g. arms and legs). This strategy is applied straightforwardly by a recurrent neural network (RNN) with different joints predicted step by step in <ref type="bibr" target="#b4">[5]</ref> for a single frame. However, the high computation complexity of RNN precludes the network from holding a large temporal window which has been shown to improve performance. To solve this issue, based on <ref type="bibr" target="#b0">[1]</ref>, we further propose a high-performance fully-convolutional propagating architecture, which contains multiple sub-networks with each predicting the directions of all the bones. The hierarchical prediction is implicitly performed via long skip connections between adjacent subnetworks.</p><p>Additionally, motivated by <ref type="bibr" target="#b5">[6]</ref>, we create an effective joint shift loss for the two sub-tasks (i.e., bone direction prediction and bone length prediction) to learn jointly. The joint shift loss penalizes the relative joint shift between all long-range joint pairs, for example the left hand and right foot. Thus, it provides an extra strong supervision for the two networks to be trained to coordinate with each other and produce robust predictions.</p><p>Last but not least, we propose a simple yet effective approach to further reduce the depth ambiguity. Specifically, we incorporate 2D keypoint visibility scores into the model as a new feature, which indicates the probability of each 2D keypoint being visible in a frame and provides extra knowledge of the depth relation between specific joints. We argue that the scores are useful to those poses with body parts occluded or when the relative depth matters. For example, if a person keeps her/his hands in front of the chest in a frontal view, our model will be confused on whether the hands are in front of the chest (visible) or behind the back (occluded), since the occluded 2D keypoints can still be predicted sometimes. Furthermore, We adopt an implicit attention mechanism to dynamically adjust the importance of the visibility scores for better performance.</p><p>Our contributions are summarized as follows:</p><p>• We explicitly decompose the task of 3D joint estimation into bone direction prediction and bone length prediction. As such, the bone length prediction branch can fully utilize frames across the entire video. • We propose a new fully-convolutional architecture for hierarchical bone direction prediction. • We propose a high-performance bone length prediction network, two mechanisms are created to effectively prevent overfitting. • We feed the visibility scores of 2D keypoint detection into the model to better resolve the depth ambiguity. • Our model is inspired by the human skeleton anatomy and achieves the state-of-the-art performance on Human3.6M and MPI-INF-3DHP datasets.</p><p>II. RELATED WORK 3D human pose estimation has received much attention in recent years. To predict the 3D joint location from 2D image input, previous works of 3D pose estimation typically fall into two categories based on the training pipeline. For the approaches of the first category, they created an end-toend convolutional neural network (CNN) model to directly predict the 3D joint location from the original input images. To establish a strong baseline, Pavlakos et al. <ref type="bibr" target="#b6">[7]</ref> integrated the volumetric representation with a coarse-to-fine supervision scheme to figure out the 3D joint locations by the predicted 3D volumetric heat maps. Based on the ConvNet pose estimator and the volumetric heap map representation proposed by <ref type="bibr" target="#b6">[7]</ref>, recent approaches mainly made progress from two aspects. On the one hand, human-structure constraints such as the human shape constraints <ref type="bibr" target="#b7">[8]</ref>, body articulation constraints <ref type="bibr" target="#b8">[9]</ref> and the joint angle constraints <ref type="bibr" target="#b9">[10]</ref> were employed to prevent invalid pose prediction. On the other hand, effective training approaches were proposed, making the estimation process differentiable <ref type="bibr" target="#b10">[11]</ref> and enabling the model to learn from weakly labeled data <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. To further enable the pose estimator to predict full 3D human mesh instead of the joint locations, Kanazawa et al. <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> proposed end-toend CNN frameworks for reconstructing the full 3D mesh of a human body from an image or a video. These approaches based on image-level input can directly capture rich knowledge contained in images. However, without intermediate feature and supervision, the model's performance will also be affected by the image's background, lighting and person's clothing. More importantly, the large dimension of image-level input disables the 3D model from receiving a large number of images as input, bottlenecking the performance of 3D pose estimation in video.</p><p>For the approaches of the second category, they built a 3D joint estimation model on top of a high-performance 2D keypoint detector. Given an input image, these approaches first utilized the 2D keypoint detector to predict the image' 2D keypoints. The predicted 2D keypoints were then lifted as the 3D joint estimation model's input to predict the final 3D joint locations. As an earlier work, Chen et al. <ref type="bibr" target="#b15">[16]</ref> regarded the 3D pose estimation as a matching problem. They found the best matching 3D pose of the 2D keypoint input from the 3D pose pool by a nearest-neighbor (NN) model. Considering that the ground-truth 3D pose of the input may be non-corresponding to all the 3D poses in the pool, Martinez et al. <ref type="bibr" target="#b16">[17]</ref> proposed an effective fully-connected residual network to regress the 3D joint locations from 2D keypoint input. In addition to utilizing effective human-structure information as the approaches of the first category, based on <ref type="bibr" target="#b16">[17]</ref>, recent approaches of this category further improved the pose estimation performance by hierarchical joint prediction <ref type="bibr" target="#b4">[5]</ref>, 2D keypoint refinement <ref type="bibr" target="#b17">[18]</ref> and view-invariant constraint <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Overall, the approaches in such a "image-2D-3D" pipeline outperform the end-to-end counterparts. One important reason is that the 2D detector can be trained by large-scale indoor/outdoor images. It provides the 3D model a strong intermediate feature to build upon.</p><p>When estimating the 3D poses in a video, recent approaches exploited temporal information into the model to alleviate incoherent predictions. As an earlier work, Mehta et al. <ref type="bibr" target="#b19">[20]</ref> applied simple temporal filtering across 2D and 3D poses from previous frames to predict a temporally consistent 3D pose. As Long Short Term Memory networks (LSTM) were created to adaptively capture information from temporal input by the well-designed input gate, output gate and forget gate, Lin et al. <ref type="bibr" target="#b20">[21]</ref> presented the LSTM-based Recurrent 3D Pose Sequence Machine. It automatically learns the image-dependent structural constraint and sequence-dependent temporal context by a multi-stage sequential refinement. Similar to <ref type="bibr" target="#b20">[21]</ref>, Rayat et al. <ref type="bibr" target="#b3">[4]</ref> predicted temporally consistent 3D poses by learning the temporal context of a sequence using sequence-to-sequence LSTM-based network. Considering the high computational complexity of LSTM, Pavllo et al. <ref type="bibr" target="#b0">[1]</ref> further introduced a temporal fully-convolutional model which enables parallel processing of multiple frames and supports very long 2D keypoint sequence as input. All these approaches essentially leverage the adjacent frames to benefit the current frame's prediction. Compared with them, we are the first to make all the frames in a video contribute to the 3D prediction. Motivated by <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b13">[14]</ref> that created effective sub-tasks for human pose estimation and mesh recovery, we propose a novel solution to decompose the 3D pose estimation task into two bone-based sub-tasks. It should be noticed that Sun et al. <ref type="bibr" target="#b5">[6]</ref> also transformed the 3D joint into a bonebased representation. They trained the model to regress short and long range relative shifts between different joints. We demonstrate that completely decomposing the task into the bone length and bone direction prediction achieves the best performance and makes better use of the relative joint shift supervision.</p><p>III. OUR MODEL In this section, we formally present our 3D pose estimation model. In section III-A, we first describe the overall anatomyaware framework that decomposes the 3D joint location prediction task into bone length and direction prediction. In section III-B, we present the fully-convolutional propagating network for hierarchical bone direction prediction. In Section III-C, the architecture and training details of bone length prediction network are presented. In Section III-D, we describe the framework's overall training strategy. In section III-E, an implicit attention mechanism is introduced to feed the keypoint visibility scores into the model as extra guidance. Our framework's overall architecture is shown as <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Anatomy-aware Framework</head><p>As in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b2">[3]</ref>, given the predicted 2D keypoints of each frame in a video, we aim at predicting the normalized 3D locations of j pre-defined joints for each frame. The 3D location of joint "Pelvis" is commonly defined as the origin of the 3D coordinates. Given a human joint set that contains j joints as in <ref type="figure">Fig. 2</ref>, they correspond to (j − 1) directed bones with each joint being the vertex of at least one bone. This enables us to transform the 3D joint coordinates to the presentation of bone lengths and bone directions.  Formally, to predict the 3D joint locations of a specific (i.e. current) frame, we decompose the task to predict the length and direction of each bone. For the k-th joint, its 3D location − → J k can be derived as:</p><formula xml:id="formula_0">− → J k = b∈B k − → D b · L b<label>(1)</label></formula><p>Here − → D b and L b are the direction and length of bone b, respectively. B k contains all the bones in the path from "Pelvis" to the k-th joint.</p><p>We use two separate sub-networks to predict the bone lengths and directions of the current frame, respectively, as bone length prediction needs global input to ensure consistency across all the frames, whereas bone directions should be estimated within a local temporal window. Meanwhile, to ensure consistency between predicted bone lengths and directions, motivated by <ref type="bibr" target="#b5">[6]</ref>, we add a joint shift loss between the two predictions in addition to their own losses, as shown in <ref type="figure">Fig. 1</ref>. Specifically, the joint shift loss is defined as follows:</p><formula xml:id="formula_1">L JS = k1,k2∈P,k1&lt;k2 X k1,k2 JS − Y k1,k2 JS 1 2 (2) Here Y k1,k2</formula><p>JS is the 3-dimensional ground-truth relative joint shift of the current frame from the k 1 -th joint to the k 2 -th joint, X k1,k2 JS is the corresponding predicted relative joint shift derived from the predicted bone lengths and bone directions of the current frame. P contains all the joint pairs that are not directly connected as a bone. With the joint shift loss, the two sub-networks are connected and enforced to learn from each other jointly. We describe the details of the two sub-networks in the following two sections. </p><formula xml:id="formula_2">(b× , o) d (b× , 2o) d s s (b× , o) d s 2 (b× , 2o) d s 2 (b× , 2o) d s (b× , 2o) d s 2 (b, 3(j-1)) (b, 3(j-1)) (b, 3(j-1)) Fig. 3.</formula><p>The architecture of the bone direction prediction network. Long skip connections are added between adjacent sub-networks. We illustration the dimension of each input/output. b is the batch size. o is the output channel number of fully-connected layer. s is the stride of 1D convolution layer in the network (s = 3). n is the size of the 2D keypoint set. d is the input frame number of the bottom sub-network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Bone Direction Prediction Network</head><p>We adopt the temporal fully-convolutional network proposed by Pavllo et al. <ref type="bibr" target="#b0">[1]</ref> as the backbone architecture of our bone direction prediction network. Specifically, the 2D keypoints of d consecutive frames are concatenated to form the input to the network, with the 2D keypoints of the current frame in the center. In essence, to predict the bone directions of the current frame, the temporal network captures the information of the current frame and the context from its adjacent frames as well. A bone direction loss based on mean squared error is applied to train the network:</p><formula xml:id="formula_3">L D = X D − Y D 2 2 (3)</formula><p>Here X D and Y D represent the predicted and ground-truth 3(j − 1)-dimensional bone direction vector of the current frame, respectively.</p><p>It should be noted that the joint shift loss introduced in Section III-A makes the predicted directions of different bones mutually relevant. For example, if the predicted direction of the left lower arm is inaccurate, the predicted direction of the left upper arm will also be affected, since the model is encouraged to regress a long range shift from left shoulder to left wrist. Intuitively, it would benefit the overall prediction if we could first predict those easy and high-confident cases, and let them guide the subsequent prediction of other joints. As poses may vary significantly, it is difficult to pre-determine the hierarchy of the prediction. Motivated by <ref type="bibr" target="#b4">[5]</ref>, here we propose a fully-convolutional propagating architecture with long skip connections, and let the network itself to learn the prediction hierarchy instead, as in <ref type="figure">Fig. 3</ref>.</p><p>Specifically, the architecture is a stack of several subnetworks, with each sub-network being a temporal fullyconvolutional network with residual blocks proposed by <ref type="bibr" target="#b0">[1]</ref>. The output of each sub-network is the predicted bone directions of the current frame. Except the top sub-network, we temporally duplicate the output of each sub-network d s times as the input to the next sub-network. For each residual block of a specific sub-network, we concatenate its output with the output of the corresponding residual block in the adjacent upper subnetwork on channel level. This forms the long skip connections between adjacent sub-networks. We adopt an independent training strategy for each sub-network, that is, we train each sub-network by the loss of the bone direction prediction network, the back propagation is blocked between different sub-networks. By doing that, the bottom networks would not be affected by the upper ones, and instead would propagate high-confident predictions to guide subsequent predictions. In the process, the model automatically learns the hierarchical order of the prediction. In Section IV, we demonstrate the effectiveness of the proposed architecture.</p><formula xml:id="formula_4">2D Keypoint of l Frames (b×l, 2n) (b×l, o) (b×l, o) (b×l, o) Bone Length Self-attention Module (b×l, 3j) (b×l, j-1) (b, j-1) Bone Lengths 3D</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Bone Length Prediction Network</head><p>As discussed in Section III-A, the prediction of bone lengths requires global inputs from the entire video. However, taking too many frames as the input would make the computation prohibitively expensive. To capture the global context efficiently, we choose to randomly sample l frames across the entire video as the input to the network. The detailed structure of the network is shown as <ref type="figure">Fig. 4</ref>.</p><p>We adopt the fully-connected residual network for bone length prediction. Specifically, it has the same structure and layer number as the bottom sub-network of the bone direction prediction network. However, since the randomly sampled frames do not have temporal connections, we replace each 1D convolution layer by the fully-connected layer in the network. This adapts the network for single-frame input instead of multi-frame consecutive inputs. The fully-connected network predicts the (j − 1) bone lengths of each sampled frame.</p><p>Intuitively, we can average the predicted bone lengths of each sampled frame as the predicted bone lengths of the current frame. In such a way, a similar bone length loss can be applied to train the fully-connected network:</p><formula xml:id="formula_5">L L = X L − Y L 2 2</formula><p>(4) Here X L and Y L are the predicted and ground-truth (j − 1)dimensional bone length vector of the current frame.</p><p>However, since the training datasets usually only contain very limited number of actors, and the bone lengths in the videos performed by the same actor are identical. Such a training loss would lead to severe overfitting. To solve this problem, instead of directly predicting the bone lengths, the fully-connected residual network is modified to predict the 3D joint locations of each sampled frame, supervised by the mean per joint position error (MPJPE) loss as in <ref type="bibr" target="#b0">[1]</ref>:</p><formula xml:id="formula_6">L J = 1 j j k=1 X k J − Y k J 1 2<label>(5)</label></formula><p>Here X k J and Y k J are the predicted and ground-truth 3dimensional 3D joint locations of the k-th joint. Since each frame would predict a set of 3D joint locations, and the number of input frames l is usually large, minimizing the MPJPE for the predictions of all the frames would make the convergence speed of the bone length prediction network much faster than the bone direction prediction network. This decreases the performance of jointly training them by Equation 2. We choose to randomly sample one frame from l input frames and calculate its corresponding MPJPE loss. We find that such a training strategy works quite well as shown in the experiments.</p><p>Since the 3D joint locations would vary in each frame, the overfitting problem would be largely avoided. The bone lengths of each of the l frame can then be derived from the 3D joint locations accordingly.</p><p>When averaging the bone length predictions from the input frames, the prediction accuracy for a specific bone depends on the poses. For example, some of the bones in a certain pose are hardly visible due to occlusion or foreshortening, making the prediction unreliable. Motivated by the self-attention mechanism that is widely used for vision and language tasks <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, we further incorporate a self-attention module at the top of the fully-connected network to predict the bone length vector X L of the current frame:</p><formula xml:id="formula_7">A i = exp(γW X iJ ) l i=1 exp(γW X iJ ) X L = l i=1 A i X iL<label>(6)</label></formula><p>Here X iJ and X iL are the predicted 3j-dimentional 3D joint location vector and the corresponding derived bone length vector of the i-th input frame. W is a learnable matrix of the self-attention module. indicates element-wise multiplication. A i is the (j − 1)-dimensional attention weights that indicate the bone-specific importance of the i-th frame's predicted bone lengths for X L . γ is a hyper-parameter to control the degree of attention. During training, the fullyconnected residual network and the bone length self-attention module are optimized independently by L J (Equation 5) and L L (Equation 4).</p><p>To further solve the overfitting problem of the self-attention module, we augment the training data by generating samples with variant bone lengths. In particular, for each training video, we randomly create a new group of (j − 1) bone lengths. We modify the ground-truth 3D joint locations of each frame to make them accordant with the new bone lengths. Because the camera parameter is available, we can reconstruct the 2D keypoints of each frame from its modified ground-truth 3D joint locations. For each training iteration, we additionally feed a batch of randomly sampled l frames from the augmented videos and use the corresponding 2D keypoints and bone lengths to optimize the self-attention module by L L . As the self-attention module is only used for bone length reweighting, we consider it valid to train this module by a combination of predicted 2D keypoints and reconstructed clean 2D keypoints. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Overall Loss Function</head><p>By combining the losses in each sub-network and the joint shift loss, the overall loss function for our framework is given as:</p><formula xml:id="formula_8">L = λ D L D + λ L L L + λ J L J + λ JS L JS<label>(7)</label></formula><p>Here λ D , λ L , λ J and λ JS are hyper-parameters regulating the importance of each term.</p><p>During training, only the parameters of the bone direction prediction network are updated by the joint shift loss. Essentially, the joint shift loss supervises the model to predict robust bone directions that match with the predicted bone lengths for long range objectives. In Section IV, we prove that the proposed anatomy-aware framework better exerts the joint shift loss's potential than <ref type="bibr" target="#b5">[6]</ref>.</p><p>During inference, to estimate the 3D joint locations of a specific frame, we adopt the same strategy as the training process. We still randomly sample l frames of the video to predict the bone lengths of this frame. We find that taking all the frames of the video as input for bone length prediction does not lead to better performance. In Section IV, we provided more details regarding the frame sampling strategy for bone length prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Incorporating the Visibility Scores</head><p>Our 3D joint prediction network takes the predicted 2D keypoints as the input, which sometimes have ambiguities. For example, when a person has his/her legs crossed in a front view, the corresponding 2D keypoints cannot provide information about which leg is in the front. A more common situation happens when the person put his/her hands in front of the chest or behind the back, the 3D model will be confused by the relative depth between the hands and chest. Even though temporal information is exploited, the above problems still exist.</p><p>We provide a simple yet effective approach to solve the problem without feeding the original images into the model. Specifically, we predict the visibility score of each 2D keypoint in a frame, and incorporate it into the model for 3D joint estimation. The visibility score indicates the confidence of each keypoint being visible in the frame, which can be extracted from most 2D keypoint detectors. Compared with the position, a keypoint's visibility is easy to be predicted, this score is thus reliable.</p><p>We argue that the importance of a specific keypoint's visibility score is related to the corresponding pose. For example, the visibility scores of the hands become useless if the hands are stretched far away from the body. Therefore, we adopt an implicit attention mechanism to adaptively adjust the importance of the visibility scores.</p><p>Given the 2D keypoint sequence of d consecutive frames as the input of the network in Section III-B, we feed the keypoint visibility score sequence of the d frames as in <ref type="figure" target="#fig_3">Fig. 5</ref>. An 1D convolutional block is first applied, which maps the visibility score sequence into a temporal hidden feature that has the same dimension as the hidden feature of the 2D keypoint sequence. After that, we do element-wise multiplication for the two temporal hidden features as the weighted visibility score feature. In the end, we concatenate the weighted visibility score feature and the hidden feature of the 2D keypoint sequence on channel level and feed the concatenated feature to the next 1D convolution layer of the network. The whole process can be expressed as follows:</p><formula xml:id="formula_9">O I = [C 1D (X I ) C 1D (V I ); C 1D (X I )]<label>(8)</label></formula><p>Here X I , V I and O I represent the input 2D keypoint sequence, the input keypoint visibility score sequence and the outputted concatenated features. C 1D represent the operations performed by a combination of 1D convolution layer, batch normalization layer and ReLU unit.[·; ·] indicates concatenation. Similar to <ref type="bibr" target="#b28">[29]</ref>, the hidden feature of the 2D keypoint sequence can be regarded as implicit attention weights to adjust the visibility score importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Datasets and Evaluation</head><p>We evaluate the proposed model on two well established 3D human pose estimation datasets: Human3.6M <ref type="bibr" target="#b29">[30]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b30">[31]</ref>.</p><p>• Human3.6M contains 3.6 million video frames with the corresponding annotated 3D and 2D human joint positions, from 11 actors. Each actor performs 15 different activities captured from 4 camera views. Following previous works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b16">[17]</ref>, the model is trained on five subjects (S1, S5, S6, S7, S8) and evaluated on two subjects (S9 and S11) on a 17-joint skeleton. We follow the standard protocols to evaluate the models on Human3.6M. The first one (i.e. Protocol 1) is the mean per-joint position error (MPJPE) in millimeters that measures the mean Euclidean distance between the predicted and ground-truth joint positions without any transformation. The second one (i.e. Protocol 2) is the normalized variant P-MPJPE after aligning the predicted 3D pose with the ground-truth using a similarity transformation. In addition, to measure the smoothness of predictions over time, which is important for video, we also report the joint velocity errors (MPJVE) created by <ref type="bibr" target="#b0">[1]</ref> corresponding to the MPJPE of the first derivative of the 3D pose sequences. • MPI-INF-3DHP is a recently proposed 3D dataset consisting of both constrained indoor and complex outdoor scenes. It records 8 actors performing 8 activities from 14 camera views. Following <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b30">[31]</ref>, on a 14-joint skeleton, we consider all the 8 actors in the training set and select sequences from 8 camera views in total (5 chest-high cameras, 2 head-high cameras and 1 knee-high camera) for training. Evaluation is performed on the independent MPI-INF-3DHP test set that has different scenes, camera views and relatively different actions from the training set. This design implicitly covers the cross-dataset evaluation. We report the Percentage of Correct Keypoints (PCK) within 150mm range, Area Under Curve (AUC), and MPJPE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>For Human3.6M, we use the predicted 2D keypoints released by <ref type="bibr" target="#b0">[1]</ref> from the Cascaded Pyramid Network (CPN) as the input of our 3D pose model. For MPI-INF-3DHP, the predicted 2D keypoints are acquired from the pretrained AlphaPose model <ref type="bibr" target="#b31">[32]</ref>. In addition to the 2D keypoints, the keypoint visibility scores for both datasets are also extracted from the pretrained AlphaPose model.</p><p>We use the Adam optimizer to train our model in an endto-end manner. For each training iteration, the mini-batch size is set to 1024 for both original samples and augmented samples. We set λ D = 0.02, λ L = 0.05, λ J = 1 and λ JS = 0.1 for the loss terms in Equation 7. For the bone length selfattention module, we set γ = 10 in Equation <ref type="bibr" target="#b7">8</ref>. The sampled frame number of the bone length prediction network l is set to 50 for both the training and inference process. For the proposed architecture in Section III-B, the number of subnetworks is set to 2. As in <ref type="bibr" target="#b0">[1]</ref>, the output channel number of each 1D convolution layer and fully-connected layer is set to 1024. For actual implementation, instead of manually deriving the 3D joint locations and relative joint shifts from the predicted bone lengths and bone directions, we regress the two objectives by feeding the concatenation of the predicted bone length vector and bone direction vector into two fullyconnected layers, respectively. The fully-connected layers are trained together with the whole network. This achieves slightly better performance. <ref type="table" target="#tab_1">Table I</ref> shows the quantitative results of our proposed full model and other baselines on Human3.6M. Following <ref type="bibr" target="#b0">[1]</ref>, we present the performance of our 81-frame and 243-frame models which receive 81 and 243 consecutive frames, respectively, as the input of the bone direction prediction network. We also experiment with a causal version of our model to enable real-time prediction. During the training/inference process, the causal model only receives d consecutive and l randomly sampled frames from the past/current frames for the current frame's estimation. Overall, our model has low average error on both Protocol 1, Protocol 2 and MPJVE. On a great number of actions, we achieve the best performance. Compared with the baseline model <ref type="bibr" target="#b0">[1]</ref> that shares the same 2D keypoint detector, our model achieves more smooth prediction with lower MPJVE and achieves significantly better performance on complex activities such as "Sitting" (-3.4mm in Protocol 1) and "Sitting down" (-5.6mm in Protocol 1). We attribute it to the accurate prediction of the bone lengths for these activities. Even though the person bends his/her body, based on the predicted bone lengths, the joint shift loss can effectively guide the model to predict high-quality bone directions. <ref type="figure" target="#fig_4">Fig. 6</ref>  shows the visualized qualitative results from the baseline and our full model on "Sitting" and "Sitting down" poses. Moreover, our model sharply improves the lower bound of 3D pose estimation when using the ground-truth 2D keypoints as input. For this experiment, data augmentation is not applied as it can be regarded as using extra ground truth 2D keypoints. From <ref type="table" target="#tab_1">Table II</ref>, the gap between our model and the baseline is nearly 5mm on Protocol 1. It indicates that if the performance of the bottom 2D keypoint detector is improved, our model can further boost the improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment results</head><p>Following <ref type="bibr" target="#b0">[1]</ref>, we report the model parameter number and an estimate of the floating-point operations (FLOPs) per frame at inference time to compare different models' computation complexity. The FLOPs are estimated in the same way as <ref type="bibr" target="#b0">[1]</ref>. Besides, to evaluate the models' inference efficiency, we report the inference frame per second (FPS) of different models by letting them estimate the 3D poses of a 10,000-frame test video frame by frame on a single GeForce GTX 2080 Ti GPU. As shown in <ref type="table" target="#tab_1">Table III</ref>, our 9-frame model holds similar computation complexity to the 243-frame baseline model <ref type="bibr" target="#b0">[1]</ref>. It achieves lower MPJPE with sharply fewer input frames, demonstrating the effectiveness of our proposed approach. On the other hand, even though the proposed model' inference speed is about 3.5 times lower than <ref type="bibr" target="#b0">[1]</ref>, they still hold an acceptable FPS, which is significantly higher than common 2D keypoint detection models <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b36">[37]</ref>. As the complete 3D pose estimation process is a combination of 2D keypoint detection and 3D pose estimation, the inference speed of the proposed model will not be the bottleneck.</p><p>We further investigate the sensitivity of different hyperparameters mentioned in Section IV-B. Overall, as shown in <ref type="table" target="#tab_1">Table IV</ref>, the model's performance is insensitive to the setting of different hyper-parameters, indicating its strong robustness. Indeed, keeping increasing the sub-network number of the bone direction prediction network can still reduce the MPJPE. We set the final sub-network number to 2 as a good trade-off between the performance and the computational complexity. <ref type="table">Table V</ref> shows the quantitative results of our full model and other baselines on MPI-INF-3DHP. Overall, MPI-INF-3DHP contains fewer training samples than human3.6M. This leads to better performance for the 81-frame models than the 243frame models. Still, our model outperforms the baselines by a large margin.</p><p>Overall, the comparison with the baseline model <ref type="bibr" target="#b0">[1]</ref> on <ref type="table" target="#tab_1">Table I</ref>, II, V demonstrates that our superior performance is not only from the strong 2D keypoint detector, but also highly related to the proposed high-performance 3D model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>We next perform ablation experiments on Human3.6M under Protocol 1. For all the comparisons, we use the 81-frame models for the baseline <ref type="bibr" target="#b0">[1]</ref> and ours. They receive 81 consecutive frames as the input to predict the 3D joint locations and the bone directions of the current frame, respectively.</p><p>We first show how each proposed module improves the model's performance in <ref type="table" target="#tab_1">Table VI</ref>. For the very naive anatomyaware framework, we adopt Baseline as the bone direction prediction network. We train the bone length prediction network and bone direction prediction network by the bone  length loss and bone direction loss, respectively. We observe a drop of performance from 47.7mm to 48.4mm caused by the overfitting of the bone length prediction network. Once we handle the overfitting by applying the MPJPE loss and doing data augmentation which can be regarded as intrinsic parts of our anatomy-aware framework, MPJPE sharply drops to 46.3mm that is 1.4mm lower than Baseline. This demonstrates the framework's effectiveness. Moreover, we find that the joint shift loss, the fully-convolutional propagating architecture and the attentive feeding of visibility score reduce the error about 0.5mm, 0.7mm and 0.5mm, respectively. In the end, if we apply them to the baseline 81-frame work without the proposed anatomy-aware structure, the MPJPE increases to 46.4mm.</p><p>To further validate the proposed modules, the following models are compared to answer the following questions:</p><p>• Q: Comparison between the proposed anatomy-aware network and the generic bone based representation. M: Baseline + Composition A: Sun et al. <ref type="bibr" target="#b5">[6]</ref> propose compositional pose regression that transforms the task into a generic bone based representation. To demonstrate the effectiveness of the our anatomy-aware framework that decomposes the task explicitly into bone length and bone direction, we apply compositional pose regression on Baseline. Specifically, exactly as <ref type="bibr" target="#b5">[6]</ref>, we modify Baseline to predict the bones and train it by the long range and short range joint shift loss (i.e. compositional loss). The MPJPE is 47.4mm. Compared with our comparable anatomy-aware framework (without feeding LSC and ScoreATT) whose MPJPE is 45.8mm, this suggests that the explicit bone length and bone direction representation is more effective than the generic bone based representation because it can utilize global information across all the frames. It also makes better use of the relative joint shift supervision as our model obtains larger improvement from JSL. • Q: Comparison between our anatomy-aware network and directly imposing a bone length consistency loss. M: Baseline + ConsistencyLoss A: A bone length consistency loss (as opposed to an explicit hard design) can be imposed in a straightforward manner across frames. To evaluate this idea, we further add a training loss term on Baseline to reduce the predicted bone length difference between randomly sampled frame pairs of a same video. The best MPJPE is 47.7mm. This indicates the uselessness of utilizing the bone length consistency by a form of training loss for this supervised learning task, compared with our solution. • Q: Whether the distant frames indeed help the prediction of bone length. M: AF(consecutive) + ML + BoneAtt + AUG + JSL A: To validate this, we investigate the model that receives l consecutive local frames as the input of the bone length prediction network for training/inference, with the current one in the center. As Section IV-B, l is still set to 50 for both the training and inference process. The error increases from 45.8mm to 46.7mm. This demonstrates that randomly sampling frames from the entire video for bone length prediction indeed improves the model's performance, which is consistent with our motivation to decompose the task. • Q: Whether the long skip connections and the independent training strategy indeed help the prediction of bone direction.  M: AF + ML + BoneAtt + AUG + JSL + Baseline-D A: It should be noticed that for the bone direction prediction network, LSC is two times deeper than Baseline. For fair comparison, we further design Baseline-D as the bone direction prediction network. Its structure is same as LSC, but with the long skip connections removed. Also, same as Baseline, the loss is only applied at the top and the backpropagation is not blocked between different sub-networks. The MPJPE is still 45.8mm. This indicates the uselessness of simply increasing the layer/parameter number of the temporal network. • Q: Whether utilizing implicit attention mechanism is an effective way to feed the visibility score feature. M: AF + ML + BoneAtt + AUG + JSL + LSC + Fusion A: We create a model for which we directly concatenate the visibility scores with the input 2D keypoints before feeding them into the network instead of utilizing implicit attention. The MPJPE is 44.8mm. It proves that the implicit attention mechanism provides a more effective way to feed the visibility score feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Real-time 3D Pose Estimation</head><p>As mentioned in Section IV-C, our causal version model support real-time 3D human pose estimation. For the realtime 3D pose estimation, the inference speed is important and highly related to the frame selection strategy of the bone length prediction network. In this section, we compare different frame selection strategies for bone length prediction during the inference process. For all the comparisons, same as Section IV-D, we choose the 81-frame models for the baseline <ref type="bibr" target="#b0">[1]</ref> and ours. It should be noticed that the 3D pose estimation model can only leverage the information of the current and past frames.</p><p>• BS-causal refers to the baseline model with causal convolutions <ref type="bibr" target="#b0">[1]</ref>. To predict the 3D joint locations of the current frame, the input of the baseline model contains the 2D keypoint sequence of 81 consecutive frames with the current frame in the rightmost. The MPJPE is 49.8mm. • Ours-random refers to our real-time model that adopts the same frame selection strategy as our standard model presented in the main paper. For the bone direction prediction network, we input the 2D keypoint sequence of 81 consecutive frames as BS-causal. To predict the bone lengths of the t-th frame, we randomly sample M frames before the (t + 1)-th frame and input their 2D keypoints to the bone length prediction network. If M is larger than t, we input the 2D keypoints of all the frames before the (t + 1)-th frame. • Ours-firstframe refers to our real-time model that doesn't need to iteratively compute the bone lengths. Specifically, during the inference process, to predict the bone lengths of the t-th frame, we always select the 2D keypoints of the first M frames of the video as input. In this situation, the model does not need to recalculate the bone lengths from the (M + 1)-th frame. If M is larger than t, we input the 2D keypoints of all the frames before the (t + 1)-th frame. • Ours-consecutive refers to our real-time model that receives consecutive local frames for bone length prediction. To predict the bone length of the t-th frame, we input the 2D keypoint sequence of M consecutive frames with the t-th frame in the rightmost. Still, if M is larger than t, we input the 2D keypoints of all the frames before the (t + 1)-th frame. During the training process, for all the three models we propose (i.e. Ours-random, Ours-firstframe, Oursconsecutive), we still randomly sample 50 frames and input their 2D keypoints to the bone length prediction network. We don't observe further improvement of the model's performance when inputting the 2D keypoints of the first 50/50 consecutive local frames to train Ours-firstframe/Ours-consecutive as their inference process. <ref type="figure" target="#fig_5">Fig. 7</ref> shows how different settings of the input frame number M for inference influences the models' performance. As we expect, Ours-random achieves best performance when M is small. Moreover, randomly sampling 50 frames is sufficient for Ours-random to reach the best performance. However, it needs to predict the bone lengths frame by frame. On the other hand, Ours-firstframe achieves slightly better performance than Ours-consecutive. It indicates that predicting the bone lengths from the first frames of the video is more accurate than from consecutive local frames. We attribute it to the fact that the person's poses at the beginning of a video are commonly simpler on Human3.6M, this benefits the bone length prediction. To our surprise, even though we just update the bone lengths for the first 50 frames of each video and fix them from the 51-th frame, Ours-firstframes still achieves great performance. The MPJPE is 47.5mm, which is 2.3mm lower than the baseline. More importantly, it is more efficient than Ours-random. Tables VII shows different models' relative average inference speed on Human3.6M test set, M is set to 50 for all the models. The inference speed of Ours-firstframes is about two times lower than BS-causal, because its bone direction prediction network that proposed in Section 3.2 of the main paper is nearly two times deeper than the baseline. However, Ours-firstframes is more efficient than Ours-random without updating the bone lengths frame by frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We present a new solution to estimating the human 3D pose. Instead of directly regressing the 3D joint locations, we transform the task into predicting the bone lengths and directions. For bone length prediction, we make use of the frames across the entire video and propose an effective fullyconnected residual network with a bone length re-weighting mechanism. For bone direction prediction, we add along skip connections into a fully-convolutional architecture for hierarchical prediction. Extensive experiments have demonstrated that the combination of bone length and bone direction is an effective intermediate representation to bridge the 2D keypoints and 3D joint locations.</p><p>In recent years, as 3D human pose estimation has become a significant research topic for researchers to study, multiple directions are demonstrated to be promising for exploration. First, effective data augmentation algorithms <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b37">[38]</ref> are continuously proposed to guide the model to handle occluded or complex pose inputs. Moreover, the creation of generative adversarial network (GAN) <ref type="bibr" target="#b38">[39]</ref> enables a number of approaches <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b39">[40]</ref> to utilize GAN for realistic and reasonable pose prediction, even in a weakly supervised setting. In addition, high-performance temporal models <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref> are created, which support very long 2D keypoint sequence as input and can adaptively capture significant information from keyframes. These directions are regarded as general directions since the proposed temporal models, adversarial training, and data augmentation algorithms can be generally applied to different research tasks other than 3D human pose estimation. In this paper, we focus on a more fundamental aspect of human pose estimation and create an effective learning representation for this task. We believe that exploring the human pose's learning representation is promising as the human body is a special Kinematic Tree-based structure different from other objects. The motion of the human body is drove by joint rotation with fixed bone lengths. We are delighted to see the human pose's learning representation evolved from the joint level to the bone vector level to the bone length/direction level that constantly improves human pose estimation. Based on our work, it may be illuminating for future works to keep exploring the relationship between the tasks of bone length prediction and bone direction prediction. Currently, we adopt two independent networks to predict bone directions and bone lengths. It is valuable to study whether the model can further improve the performance of each task by utilizing the knowledge captured from the other task. Applying the joint shift loss is one useful way. However, we believe that capturing this relationship at the network level as <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref> for visual question answering and image-text matching will make extra improvement for accurate and smooth 3D human pose estimation in video.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>The overview of the proposed anatomy-aware framework. It predicts the bone directions and bone lengths of the current frame using consecutive local frames and randomly sampled frames across the entire video, respectively. The joint and bone representation of a human pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Detailed network structure for feeding the visibility scores. d is the input frame number of the bone direction prediction network, s and o are the stride and output channel number of 1D convolution layer, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Qualitative comparison between the proposed 243-frame model and the baseline 243-frame model [1] on typical poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>The influence of the input frame number M on different real-time models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I QUANTITATIVE</head><label>I</label><figDesc>COMPARISONS BETWEEN THE ESTIMATED POSE AND THE GROUND-TRUTH ON HUMAN3.6M UNDER PROTOCOLS 1,2 AND MPJVE. (*) WE REPORT THE RESULT WITHOUT DATA AUGMENTATION USING VIRTUAL CAMERAS.</figDesc><table><row><cell>Protocol 1</cell><cell cols="2">Dir. Disc.</cell><cell>Eat</cell><cell cols="5">Greet Phone Photo Pose Purch.</cell><cell>Sit</cell><cell cols="7">SitD. Smoke Wait WalkD. Walk WalkT. Avg</cell></row><row><cell>Martinez et al. [17] ICCV'17</cell><cell>51.8</cell><cell>56.2</cell><cell>58.1</cell><cell>59.0</cell><cell>69.5</cell><cell>78.4</cell><cell>55.2</cell><cell>58.1</cell><cell>74.0</cell><cell>94.6</cell><cell>62.3</cell><cell>59.1</cell><cell>65.1</cell><cell>49.5</cell><cell>52.4</cell><cell>62.9</cell></row><row><cell>Sun et al. [6] ICCV'17</cell><cell>52.8</cell><cell>54.8</cell><cell>54.2</cell><cell>54.3</cell><cell>61.8</cell><cell>67.2</cell><cell>53.1</cell><cell>53.6</cell><cell>71.7</cell><cell>86.7</cell><cell>61.5</cell><cell>53.4</cell><cell>61.6</cell><cell>47.1</cell><cell>53.4</cell><cell>59.1</cell></row><row><cell>Pavlakos et al. [12] CVPR'18</cell><cell>48.5</cell><cell>54.4</cell><cell>54.4</cell><cell>52.0</cell><cell>59.4</cell><cell>65.3</cell><cell>49.9</cell><cell>52.9</cell><cell>65.8</cell><cell>71.1</cell><cell>56.6</cell><cell>52.9</cell><cell>60.9</cell><cell>44.7</cell><cell>47.8</cell><cell>56.2</cell></row><row><cell>Yang et al. [9] CVPR'18</cell><cell>51.5</cell><cell>58.9</cell><cell>50.4</cell><cell>57.0</cell><cell>62.1</cell><cell>65.4</cell><cell>49.8</cell><cell>52.7</cell><cell>69.2</cell><cell>85.2</cell><cell>57.4</cell><cell>58.4</cell><cell>43.6</cell><cell>60.1</cell><cell>47.7</cell><cell>58.6</cell></row><row><cell>Luvizon et al. [33] CVPR'18</cell><cell>49.2</cell><cell>51.6</cell><cell>47.6</cell><cell>50.5</cell><cell>51.8</cell><cell>60.3</cell><cell>48.5</cell><cell>51.7</cell><cell>61.5</cell><cell>70.9</cell><cell>53.7</cell><cell>48.9</cell><cell>57.9</cell><cell>44.4</cell><cell>48.9</cell><cell>53.2</cell></row><row><cell>Hossain &amp; Little [4] ECCV'18</cell><cell>48.4</cell><cell>50.7</cell><cell>57.2</cell><cell>55.2</cell><cell>63.1</cell><cell>72.6</cell><cell>53.0</cell><cell>51.7</cell><cell>66.1</cell><cell>80.9</cell><cell>59.0</cell><cell>57.3</cell><cell>62.4</cell><cell>46.6</cell><cell>49.6</cell><cell>58.3</cell></row><row><cell>Lee et al. [5] ECCV'18</cell><cell>40.2</cell><cell>49.2</cell><cell>47.8</cell><cell>52.6</cell><cell>50.1</cell><cell>75.0</cell><cell>50.2</cell><cell>43.0</cell><cell>55.8</cell><cell>73.9</cell><cell>54.1</cell><cell>55.6</cell><cell>58.2</cell><cell>43.3</cell><cell>43.3</cell><cell>52.8</cell></row><row><cell>Chen et al. [3] CVPR'19</cell><cell>41.1</cell><cell>44.2</cell><cell>44.9</cell><cell>45.9</cell><cell>46.5</cell><cell>39.3</cell><cell>41.6</cell><cell>54.8</cell><cell>73.2</cell><cell>46.2</cell><cell>48.7</cell><cell>42.1</cell><cell>35.8</cell><cell>46.6</cell><cell>38.5</cell><cell>46.3</cell></row><row><cell cols="2">Pavllo et al. [1] (243 frames, Causal) CVPR'19 45.9</cell><cell>48.5</cell><cell>44.3</cell><cell>47.8</cell><cell>51.9</cell><cell>57.8</cell><cell>46.2</cell><cell>45.6</cell><cell>59.9</cell><cell>68.5</cell><cell>50.6</cell><cell>46.4</cell><cell>51.0</cell><cell>34.5</cell><cell>35.4</cell><cell>49.0</cell></row><row><cell>Pavllo et al. [1] (243 frames) CVPR'19</cell><cell>45.2</cell><cell>46.7</cell><cell>43.3</cell><cell>45.6</cell><cell>48.1</cell><cell>55.1</cell><cell>44.6</cell><cell>44.3</cell><cell>57.3</cell><cell>65.8</cell><cell>47.1</cell><cell>44.0</cell><cell>49.0</cell><cell>32.8</cell><cell>33.9</cell><cell>46.8</cell></row><row><cell>Lin et al. [2] BMVC'19</cell><cell>42.5</cell><cell>44.8</cell><cell>42.6</cell><cell>44.2</cell><cell>48.5</cell><cell>57.1</cell><cell>42.6</cell><cell>41.4</cell><cell>56.5</cell><cell>64.5</cell><cell>47.4</cell><cell>43.0</cell><cell>48.1</cell><cell>33.0</cell><cell>35.1</cell><cell>46.6</cell></row><row><cell>Cai et al. [34] ICCV'19</cell><cell>44.6</cell><cell>47.4</cell><cell>45.6</cell><cell>48.8</cell><cell>50.8</cell><cell>59.0</cell><cell>47.2</cell><cell>43.9</cell><cell>57.9</cell><cell>61.9</cell><cell>49.7</cell><cell>46.6</cell><cell>51.3</cell><cell>37.1</cell><cell>39.4</cell><cell>48.8</cell></row><row><cell>Cheng et al. [35] ICCV'19 (*)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>44.8</cell></row><row><cell>Yeh et al. [36] NIPS'19</cell><cell>44.8</cell><cell>46.1</cell><cell>43.3</cell><cell>46.4</cell><cell>49.0</cell><cell>55.2</cell><cell>44.6</cell><cell>44.0</cell><cell>58.3</cell><cell>62.7</cell><cell>47.1</cell><cell>43.9</cell><cell>48.6</cell><cell>32.7</cell><cell>33.3</cell><cell>46.7</cell></row><row><cell>Xu et al. [18] CVPR'20</cell><cell>37.4</cell><cell>43.5</cell><cell>42.7</cell><cell>42.7</cell><cell>46.6</cell><cell>59.7</cell><cell>41.3</cell><cell>45.1</cell><cell>52.7</cell><cell>60.2</cell><cell>45.8</cell><cell>43.1</cell><cell>47.7</cell><cell>33.7</cell><cell>37.1</cell><cell>45.6</cell></row><row><cell>Ours (243 frames, Causal)</cell><cell>42.5</cell><cell>45.4</cell><cell>42.3</cell><cell>45.2</cell><cell>49.1</cell><cell>56.1</cell><cell>43.8</cell><cell>44.9</cell><cell>56.3</cell><cell>64.3</cell><cell>47.9</cell><cell>43.6</cell><cell>48.1</cell><cell>34.3</cell><cell>35.2</cell><cell>46.6</cell></row><row><cell>Ours (81 frames)</cell><cell>42.1</cell><cell>43.8</cell><cell>41.0</cell><cell>43.8</cell><cell>46.1</cell><cell>53.5</cell><cell>42.4</cell><cell>43.1</cell><cell>53.9</cell><cell>60.5</cell><cell>45.7</cell><cell>42.1</cell><cell>46.2</cell><cell>32.2</cell><cell>33.8</cell><cell>44.6</cell></row><row><cell>Ours (243 frames)</cell><cell>41.4</cell><cell>43.5</cell><cell>40.1</cell><cell>42.9</cell><cell>46.6</cell><cell>51.9</cell><cell>41.7</cell><cell>42.3</cell><cell>53.9</cell><cell>60.2</cell><cell>45.4</cell><cell>41.7</cell><cell>46.0</cell><cell>31.5</cell><cell>32.7</cell><cell>44.1</cell></row><row><cell>Protocol 2</cell><cell cols="2">Dir. Disc.</cell><cell>Eat</cell><cell cols="5">Greet Phone Photo Pose Purch.</cell><cell>Sit</cell><cell cols="7">SitD. Smoke Wait WalkD. Walk WalkT. Avg</cell></row><row><cell>Martinez et al. [17] ICCV'17</cell><cell>39.5</cell><cell>43.2</cell><cell>46.4</cell><cell>47.0</cell><cell>51.0</cell><cell>56.0</cell><cell>41.4</cell><cell>40.6</cell><cell>56.5</cell><cell>69.4</cell><cell>49.2</cell><cell>45.0</cell><cell>49.5</cell><cell>38.0</cell><cell>43.1</cell><cell>47.7</cell></row><row><cell>Sun et al. [6] ICCV'17</cell><cell>42.1</cell><cell>44.3</cell><cell>45.0</cell><cell>45.4</cell><cell>51.5</cell><cell>53.0</cell><cell>43.2</cell><cell>41.3</cell><cell>59.3</cell><cell>73.3</cell><cell>51.0</cell><cell>44.0</cell><cell>48.0</cell><cell>38.3</cell><cell>44.8</cell><cell>48.3</cell></row><row><cell>Pavlakos et al. [12] CVPR'18</cell><cell>34.7</cell><cell>39.8</cell><cell>41.8</cell><cell>38.6</cell><cell>42.5</cell><cell>47.5</cell><cell>38.0</cell><cell>36.6</cell><cell>50.7</cell><cell>56.8</cell><cell>42.6</cell><cell>39.6</cell><cell>43.9</cell><cell>32.1</cell><cell>36.5</cell><cell>41.8</cell></row><row><cell>Yang et al. [9] CVPR'18</cell><cell>26.9</cell><cell>30.9</cell><cell>36.3</cell><cell>39.9</cell><cell>43.9</cell><cell>47.4</cell><cell>28.8</cell><cell>29.4</cell><cell>36.9</cell><cell>58.4</cell><cell>41.5</cell><cell>30.5</cell><cell>29.5</cell><cell>42.5</cell><cell>32.2</cell><cell>37.7</cell></row><row><cell>Hossain &amp; Little [4] ECCV'18</cell><cell>35.7</cell><cell>39.3</cell><cell>44.6</cell><cell>43.0</cell><cell>47.2</cell><cell>54.0</cell><cell>38.3</cell><cell>37.5</cell><cell>51.6</cell><cell>61.3</cell><cell>46.5</cell><cell>41.4</cell><cell>47.3</cell><cell>34.2</cell><cell>39.4</cell><cell>44.1</cell></row><row><cell>Chen et al. [3] CVPR'19</cell><cell>36.9</cell><cell>39.3</cell><cell>40.5</cell><cell>41.2</cell><cell>42.0</cell><cell>34.9</cell><cell>38.0</cell><cell>51.2</cell><cell>67.5</cell><cell>42.1</cell><cell>42.5</cell><cell>37.5</cell><cell>30.6</cell><cell>40.2</cell><cell>34.2</cell><cell>41.6</cell></row><row><cell cols="2">Pavllo et al. [1] (243 frames, Causal) CVPR'19 35.1</cell><cell>37.7</cell><cell>36.1</cell><cell>38.8</cell><cell>38.5</cell><cell>44.7</cell><cell>35.4</cell><cell>34.7</cell><cell>46.7</cell><cell>53.9</cell><cell>39.6</cell><cell>35.4</cell><cell>39.4</cell><cell>27.3</cell><cell>28.6</cell><cell>38.1</cell></row><row><cell>Pavllo et al. [1] (243 frames) CVPR'19</cell><cell>34.1</cell><cell>36.1</cell><cell>34.4</cell><cell>37.2</cell><cell>36.4</cell><cell>42.2</cell><cell>34.4</cell><cell>33.6</cell><cell>45.0</cell><cell>52.5</cell><cell>37.4</cell><cell>33.8</cell><cell>37.8</cell><cell>25.6</cell><cell>27.3</cell><cell>36.5</cell></row><row><cell>Lin et al. [2] BMVC'19</cell><cell>32.5</cell><cell>35.3</cell><cell>34.3</cell><cell>36.2</cell><cell>37.8</cell><cell>43.0</cell><cell>33.0</cell><cell>32.2</cell><cell>45.7</cell><cell>51.8</cell><cell>38.4</cell><cell>32.8</cell><cell>37.5</cell><cell>25.8</cell><cell>28.9</cell><cell>36.8</cell></row><row><cell>Cai et al. [34] ICCV'19</cell><cell>35.7</cell><cell>37.8</cell><cell>36.9</cell><cell>40.7</cell><cell>39.6</cell><cell>45.2</cell><cell>37.4</cell><cell>34.5</cell><cell>46.9</cell><cell>50.1</cell><cell>40.5</cell><cell>36.1</cell><cell>41.0</cell><cell>29.6</cell><cell>33.2</cell><cell>39.0</cell></row><row><cell>Cheng et al. [35] ICCV'19 (*)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>34.1</cell></row><row><cell>Xu et al. [18] CVPR'20</cell><cell>31.0</cell><cell>34.8</cell><cell>34.7</cell><cell>34.4</cell><cell>36.2</cell><cell>43.9</cell><cell>31.6</cell><cell>33.5</cell><cell>42.3</cell><cell>49.0</cell><cell>37.1</cell><cell>33.0</cell><cell>39.1</cell><cell>26.9</cell><cell>31.9</cell><cell>36.2</cell></row><row><cell>Ours (243 frames, Causal)</cell><cell>33.6</cell><cell>36.0</cell><cell>34.4</cell><cell>36.6</cell><cell>37.5</cell><cell>42.6</cell><cell>33.5</cell><cell>33.8</cell><cell>44.4</cell><cell>51.0</cell><cell>38.3</cell><cell>33.6</cell><cell>37.7</cell><cell>26.7</cell><cell>28.2</cell><cell>36.5</cell></row><row><cell>Ours (81 frames)</cell><cell>33.1</cell><cell>35.3</cell><cell>33.4</cell><cell>35.9</cell><cell>36.1</cell><cell>41.7</cell><cell>32.8</cell><cell>33.3</cell><cell>42.6</cell><cell>49.4</cell><cell>37.0</cell><cell>32.7</cell><cell>36.5</cell><cell>25.5</cell><cell>27.9</cell><cell>35.6</cell></row><row><cell>Ours (243 frames)</cell><cell>32.6</cell><cell>35.1</cell><cell>32.8</cell><cell>35.4</cell><cell>36.3</cell><cell>40.4</cell><cell>32.4</cell><cell>32.3</cell><cell>42.7</cell><cell>49.0</cell><cell>36.8</cell><cell>32.4</cell><cell>36.0</cell><cell>24.9</cell><cell>26.5</cell><cell>35.0</cell></row><row><cell>MPJVE</cell><cell cols="2">Dir. Disc.</cell><cell>Eat</cell><cell cols="5">Greet Phone Photo Pose Purch.</cell><cell>Sit</cell><cell cols="7">SitD. Smoke Wait WalkD. Walk WalkT. Avg</cell></row><row><cell>Pavllo et al. [1] (243 frames) CVPR'19</cell><cell>3.0</cell><cell>3.1</cell><cell>2.2</cell><cell>3.4</cell><cell>2.3</cell><cell>2.7</cell><cell>2.7</cell><cell>3.1</cell><cell>2.1</cell><cell>2.9</cell><cell>2.3</cell><cell>2.4</cell><cell>3.7</cell><cell>3.1</cell><cell>2.8</cell><cell>2.8</cell></row><row><cell>Ours (243 frames)</cell><cell>2.7</cell><cell>2.8</cell><cell>2.0</cell><cell>3.1</cell><cell>2.0</cell><cell>2.4</cell><cell>2.4</cell><cell>2.8</cell><cell>1.8</cell><cell>2.4</cell><cell>2.0</cell><cell>2.1</cell><cell>3.4</cell><cell>2.7</cell><cell>2.4</cell><cell>2.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II QUANTITATIVE</head><label>II</label><figDesc>COMPARISONS OF MODELS TRAINED/EVALUATED ON HUMAN3.6M USING THE GROUND-TRUTH 2D INPUT.</figDesc><table><row><cell></cell><cell cols="2">Protocol 1 Protocol 2</cell></row><row><cell>Martinez et al. [17]</cell><cell>45.5</cell><cell>37.1</cell></row><row><cell>Hossain &amp; Little [4]</cell><cell>41.6</cell><cell>31.7</cell></row><row><cell>Lee et al. [5]</cell><cell>38.4</cell><cell>-</cell></row><row><cell>Pavllo et al. (243 frames) [1]</cell><cell>37.2</cell><cell>27.2</cell></row><row><cell>Ours (243 frames)</cell><cell>32.3</cell><cell>25.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III COMPUTATIONAL</head><label>III</label><figDesc>COMPLEXITY, MPJPE, AND FRAME PER SECOND (FPS) OF DIFFERENT MODELS UNDER PROTOCOL 1 ON HUMAN3.6. THE COMPUTATIONAL COMPLEXITY IS COMPUTED WITHOUT TEST-TIME AUGMENTATION USED BY ALL THE MODELS. THE TWO NUMBERS OF "≈ FLOPS" FOR OUR MODELS REPRESENT THE ESTIMATED FLOATING-POINT OPERATIONS OF THE BONE DIRECTION PREDICTION NETWORK AND THE BONE LENGTH PREDICTION NETWORK, RESPECTIVELY. BASELINE REPRESENTS THE BASELINE 81-FRAME MODEL [1]. OTHER ROWS REPRESENTS THE PROPOSED ANATOMY-AWARE FRAMEWORK THAT DECOMPOSES THE TASKS INTO BONE LENGTH PREDICTION AND BONE DIRECTION PREDICTION. ML REFERS TO THE USE OF MPJPE LOSS TO SOLVE THE OVERFITTING PROBLEM OF THE FULLY-CONNECTED RESIDUAL NETWORK AS SECTION III-C. BONEATT REFERS TO THE FEEDING OF THE BONE LENGTH SELF-ATTENTION MODULE FOR BONE LENGTH RE-WEIGHTING, INSTEAD OF DIRECTLY AVERAGING THE PREDICTED BONE LENGTHS. AUG REFERS TO THE APPLYING OF DATA AUGMENTATION TO SOLVE THE OVERFITTING PROBLEM OF THE SELF-ATTENTION MODULE. JSL, LSC AND SCOREATT REFER TO THE APPLYING OF THE JOINT SHIFT LOSS, THE INCORPORATING OF THE FULLY-CONVOLUTIONAL PROPAGATING ARCHITECTURE AND THE FEEDING OF VISIBILITY SCORES BY AN IMPLICIT ATTENTION MECHANISM, RESPECTIVELY.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Parameters</cell><cell>≈ FLOPs</cell><cell>MPJPE</cell><cell>FPS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Hossain &amp; Little [4] ECCV'18</cell><cell>16.96M</cell><cell>33.88M</cell><cell>58.3</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Pavllo et al. [1] (27 frames) CVPR'19 w/o dilation</cell><cell>29.53M</cell><cell>59.03M</cell><cell>49.3</cell><cell>486</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Pavllo et al. [1] (27 frames) CVPR'19</cell><cell>8.56M</cell><cell>17.09M</cell><cell>48.8</cell><cell>1492</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Pavllo et al. [1] (81 frames) CVPR'19</cell><cell>12.75M</cell><cell>25.48M</cell><cell>47.7</cell><cell>1121</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Pavllo et al. [1] (243 frames) CVPR'19</cell><cell>16.95M</cell><cell>33.87M</cell><cell>46.8</cell><cell>863</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Ours (9 frames)</cell><cell></cell><cell>18.24M</cell><cell>29.97M + 4.58M</cell><cell>46.3</cell><cell>759</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Ours (27 frames)</cell><cell></cell><cell>31.88M</cell><cell>53.03M + 8.67M</cell><cell>45.3</cell><cell>410</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Ours (81 frames)</cell><cell></cell><cell>45.53M</cell><cell>76.1M + 12.76M</cell><cell>44.6</cell><cell>315</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Ours (243 frames)</cell><cell></cell><cell>59.18M</cell><cell>99.17M + 16.95M</cell><cell>44.1</cell><cell>264</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE VI</cell></row><row><cell cols="8">TABLE IV PARAMETER SENSITIVITY TEST FOR OUR 81-FRAME MODEL UNDER PROTOCOL 1 ON HUMAN3.6. λD λL λJS γ l Num. of sub-network MPJPE 0.002 0.05 0.1 10 50 2 44.9 0.02 0.05 0.1 10 50 2 44.6 0.2 0.05 0.1 10 50 2 44.6 0.02 0.005 0.1 10 50 2 45.0 0.02 0.5 0.1 10 50 2 44.6 0.02 0.05 0.01 10 50 2 45.0 0.02 0.05 1 10 50 2 44.9 0.02 0.05 0.1 1 50 2 44.8 0.02 0.05 0.1 10 50 2 44.6 0.02 0.05 0.1 100 50 2 45.3 0.02 0.05 0.1 10 10 2 45.0 0.02 0.05 0.1 10 50 2 44.6 0.02 0.05 0.1 10 100 2 44.6 0.02 0.05 0.1 10 50 1 45.3 0.02 0.05 0.1 10 50 2 44.6</cell><cell cols="2">COMPARISON OF DIFFERENT MODELS UNDER PROTOCOL 1 ON HUMAN3.6M. ML BoneAtt AUG JSL LSC ScoreAtt MPJPE Baseline 47.7</cell></row><row><cell>0.02</cell><cell>0.05</cell><cell>0.1</cell><cell>10</cell><cell>50</cell><cell>3</cell><cell></cell><cell>44.4</cell><cell cols="2">Baseline</cell><cell>46.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>48.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>46.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>46.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AF</cell><cell>46.3</cell></row><row><cell cols="8">TABLE V QUANTITATIVE COMPARISONS OF DIFFERENT MODELS ON MPI-INF-3DHP.</cell><cell></cell><cell>45.8 45.1 44.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">PCK AUC MPJPE</cell><cell></cell></row><row><cell></cell><cell cols="4">Mehta et al. [31] 3DV'17</cell><cell>75.7</cell><cell>39.3</cell><cell>117.6</cell><cell></cell></row><row><cell></cell><cell cols="4">Mehta et al. [20] ACM ToG'17</cell><cell>76.6</cell><cell>40.4</cell><cell>124.7</cell><cell></cell></row><row><cell cols="5">Pavllo et al. [1] (81 frames) CVPR'19</cell><cell>86.0</cell><cell>51.9</cell><cell>84.0</cell><cell></cell></row><row><cell cols="6">Pavllo et al. [1] (243 frames) CVPR'19 85.5</cell><cell>51.5</cell><cell>84.8</cell><cell></cell></row><row><cell></cell><cell cols="4">Lin et al. [2] BMVC'19</cell><cell>82.4</cell><cell>49.6</cell><cell>81.9</cell><cell></cell></row><row><cell></cell><cell cols="3">Ours (81 frames)</cell><cell></cell><cell>87.9</cell><cell>54.0</cell><cell>78.8</cell><cell></cell></row><row><cell></cell><cell cols="3">Ours (243 frames)</cell><cell></cell><cell>87.8</cell><cell>53.8</cell><cell>79.1</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII RELATIVE</head><label>VII</label><figDesc>AVERAGE REAL-TIME INFERENCE SPEED ON ALL THE VIDEOS OF HUMAN3.6M TEST SET.</figDesc><table><row><cell></cell><cell>Relative Inference Speed</cell></row><row><cell>BS-causal</cell><cell>1.0</cell></row><row><cell>Ours-random</cell><cell>0.28</cell></row><row><cell>Ours-firstframes</cell><cell>0.46</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Trajectory space factorization for deep videobased 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08289</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly-supervised discovery of geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="895" to="905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rayat Imtiaz Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Propagating lstm: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2602" to="2611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7025" to="7034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5255" to="5264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-supervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-toend recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5614" to="5623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3d human pose estimation= 2d pose estimation+ matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7035" to="7043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep kinematics analysis for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="899" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">View invariant 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent 3d pose sequence machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="810" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multitask autoencoder model for recovering human poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Electronics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="5060" to="5068" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="186" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multimodal transformer with multiview visual representation for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep multimodal neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12070</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Beyond rnns: Positional self-attention with co-attention for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8658" to="8665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video dialog via multigrained convolutional self-attention context multi-modal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4453" to="4466" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multimodal residual learning for visual qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-O</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="361" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5137" to="5146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Occlusion-aware networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="723" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Chirality nets for human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00029</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Openpose: realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cascaded deep monocular 3d human pose estimation with evolutionary training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6173" to="6183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7782" to="7791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="299" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="201" to="216" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
