<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PolarNet: An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
							<email>yangzhang@knights.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixiang</forename><surname>Zhou</surname></persName>
							<email>zhouzixiang@knights.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>David</surname></persName>
							<email>philip.j.david4.civ@mail.mil</email>
							<affiliation key="aff1">
								<orgName type="department">Computational and Information Sciences Directorate</orgName>
								<orgName type="laboratory">Army Research Laboratory</orgName>
								<orgName type="institution">U.S</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
							<email>xyyue@berkeley.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical Engineering and Computer Sciences</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerong</forename><surname>Xi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
							<email>boqinggo@outlook.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
							<email>foroosh@cs.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PolarNet: An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The need for fine-grained perception in autonomous driving systems has resulted in recently increased research on online semantic segmentation of single-scan LiDAR. Despite the emerging datasets and technological advancements, it remains challenging due to three reasons: (1) the need for near-real-time latency with limited hardware;</p><p>(2) uneven or even long-tailed distribution of LiDAR points across space; and (3) an increasing number of extremely fine-grained semantic classes. In an attempt to jointly tackle all the aforementioned challenges, we propose a new LiDAR-specific, nearest-neighbor-free segmentation algorithm -PolarNet. Instead of using common spherical or bird's-eye-view projection, our polar bird's-eye-view representation balances the points across grid cells in a polar coordinate system, indirectly aligning a segmentation network's attention with the long-tailed distribution of the points along the radial axis. We find that our encoding scheme greatly increases the mIoU in three drastically different segmentation datasets of real urban LiDAR single scans while retaining near real-time throughput.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>There has been a great surge of LiDAR point cloud data over the last decade, especially in the self-driving domain. In order to make use of the LiDAR point clouds in various downstream applications, it is vital to develop automatic analytic methods to make sense of the data. In this paper, we focus on the online fine-grained semantic segmentation of * Contributed equally. <ref type="bibr">â€ </ref>  LiDAR point clouds. Similar to image semantic segmentation, the task is to assign a semantic label to each of the points given an input point cloud.</p><p>While several large-scale LiDAR point clouds datasets are publicly available <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b2">3]</ref>, it is until recent that the semantic segmentation labels, provided by <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref>, are able to match their scales. The lag between the release of massive point clouds and the readiness of semantic segmentation labels indicates the challenge for human raters to provide point-wise labels and the demand for automatic and fast semantic segmentation solutions for LiDAR scans.</p><p>We consider to use end-to-end deep neural networks for the single-scan semantic segmentation of LiDAR point clouds. Before studying the network architecture or advanced training algorithms, however, we first focus on the input to the network. What constitutes a good input representation of one LiDAR point cloud scan? We draw inspirations from several related domains to answer this question.</p><p>In image segmentation, the perception field <ref type="bibr" target="#b38">[39]</ref> is one of the most principled considerations in designing highperforming CNN. It determines how much context a neural network can "perceive" before it classifies a pixel to a semantic class. In general, large perception fields improve performance. Techniques to enlarge the perception fields of convolutional neural networks include dilated convolution <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b4">5]</ref>, feature pyramid <ref type="bibr" target="#b16">[17]</ref>, etc.</p><p>When it comes to the LiDAR point clouds, we conjecture that not only the size but also the shape of the perception field matters. If we view a LiDAR scan from a bird's-eye view, the points are organized in rings of various radii (cf. <ref type="figure" target="#fig_0">Figures 2 and 3)</ref>. As a result, the regular Cartesian coordinate would distribute the points into the grid cells in a nonuniform manner. Cells that are close to the sensor have to condense many points by each cell, blurring out fine-details of the points. In contrast, cells that are far away from the sensor each contain very sparse points, supplying limited cues for the neural network to label the points in such a cell.</p><p>To this end, we propose to let the CNN perception fields track the special ring structure by partitioning a LiDAR scan with a polar grid. This simple change of the representation of the input to a neural network turns out to be very effective, boosting various semantic segmentation networks' performances by significant margins.</p><p>Existing works on the LiDAR scan understanding, however, fail to track the ring structure. Wu et al. <ref type="bibr" target="#b35">[36]</ref> convert the point-cloud segmentation problem to a depth map segmentation problem by spherically projecting points onto an image. Zhang et al. <ref type="bibr" target="#b40">[41]</ref> handcraft a bird's-eye-view (BEV) representation of the point cloud and yet represent it by regular grids. Yang et al. <ref type="bibr" target="#b37">[38]</ref> employ a similar BEV representation for object detection from the LiDAR point clouds.</p><p>On the one hand, the works above show it is promising to employ BEV representations of the LiDAR scans in segmentation and detection. On the other hand, however, we contend they fail to fully take advantage of the structures revealed from BEV. We boost the vanilla BEV representations in two major ways. One is the polar grid to track the ring structures in the LiDAR scans. The other is that we learn, instead of handcrafting, the local features per grid cell.</p><p>While polar coordination is no stranger to pre-DL computer vision <ref type="bibr" target="#b1">[2]</ref>, it is rare in CNN given the images as well as feature matrices are essentially Cartesian. To fully integrate the polar BEV representation with a 2D CNN, we first redesign the BEV quantization scheme. Instead of quantizing points based on their Cartesian coordinates on the XY plane, we now assign points according to their top-down polar coordinates as shown in <ref type="figure" target="#fig_0">Fig. 3</ref>. Mimicking BEV's circular pattern with increasing sparsity, polar BEV significantly balance the points per grid by near one order of magnitude (c.f. <ref type="figure" target="#fig_2">Fig. 4</ref>). Inspired by Lang et al. <ref type="bibr" target="#b15">[16]</ref>, we then learn a simplified PointNet to transform points in each grid into a fix-length representation vector.</p><p>Since we quantize the points in polar coordinate, ideally the feature matrix should be in polar coordinate as well. To ensure the consistency of the perception field in the downstream CNN, we arrange those feature vectors into a polar grid whose leftmost and rightmost column are connected. We also modified the downstream CNN to be capable to convolve continuously on the polar grid. After obtaining the discrete prediction, which is also a polar grid, we map it back to the points in Cartesian space and evaluate the performance. Our pipeline is visualized in <ref type="figure">Fig. 2</ref>.</p><p>We validate our approach on SemanticKITTI <ref type="bibr" target="#b0">[1]</ref>, A2D2 <ref type="bibr" target="#b9">[10]</ref> and Paris-Lille-3D <ref type="bibr" target="#b25">[26]</ref> datasets. Results show that our approach outperforms the state of art method by 2.1%, 4.5% and 3.7%, respectively, on mean intersectionover-union (mIoU) evaluation metric with merely 1/3 of its parameters and MACs.</p><p>The contributions of our work are summarised as follows:</p><p>â€¢ We propose a more suitable LiDAR scan representation which takes the imbalanced spatial distribution of points into consideration.</p><p>â€¢ Our presented PolarNet network, which is trained endto-end using our polar grid data representation, surpasses the state of art method on public benchmarks withlow computational cost as shown in <ref type="figure">Fig. 1</ref>.</p><p>â€¢ We provide thorough analysis on the semantic segmentation performance based on different backbone segmentation networks using a polar grid compared to other representations, such as Cartesian BEV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Point cloud applications and methods</head><p>Most current point cloud applications focus on general point clouds in which points are densely distributed on object surfaces, such as single 3D object shape recognition <ref type="bibr" target="#b33">[34]</ref>, indoor point cloud segmentation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b26">27]</ref>, and reconstruction of outdoor scenes from point clouds <ref type="bibr" target="#b29">[30]</ref>. Despite sharing different tasks, in order to reach their goals, they must address a similar core problem: how to extract contextual information, whether local or global, from points that are irregularly distributed in space. Judging by the approach of aggregating context information, there are mainly two ways this is done: parameterized <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b12">13]</ref> and non-parameterized <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref>. Other works voxelize the points and then apply a 3D volume segmentation /detection algorithm <ref type="bibr" target="#b30">[31]</ref>. The representative work of the latter approach is the famous PointNet <ref type="bibr" target="#b21">[22]</ref> algorithm. PointNet and its successor <ref type="bibr" target="#b22">[23]</ref> individually process each point and then use a set function to aggregate context information among those points. The parameterized ones are more commonly seen in the graph-based approaches <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b14">15]</ref>, where the points are modeled as a graph via KNN and then convoluted based on their graph connectivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">LiDAR applications and methods</head><p>Although LiDAR sensors provide highly accurate distance measurement regardless of lighting conditions, the point clouds generated from LiDAR are more sparse in space, which makes it more challenging to extract information from. Besides, processing resources in systems where LiDAR sensors are typically used, such as in self-driving vehicles, are often restrictive, requiring real-time performance from embedded hardware. To address this issue, researchers have proposed different representations for the 3D data, which can be categorized into front-view and bird'seye-view (BEV). Although different representations of the LiDAR 3D point clouds are used, each quantizes the points into a compressed 2D snapshot of the scene that may be processed by a 2D neural network, thus avoiding expensive graph neural networks or 3D operations.</p><p>Front view representations include depth image-like and spherical projections. Depth map or viewing frustum approaches apply a pinhole camera model to project 3D point clouds onto a 2D image grid. <ref type="bibr" target="#b20">[21]</ref> clustered points according to the frustum, where a 3D deep neural network is used within to identify the object. In spherical projection, points are projected onto a 2D spherical grid for a dense representation. SqueezeSeg <ref type="bibr" target="#b34">[35]</ref> and Squeeze-SegV2 <ref type="bibr" target="#b35">[36]</ref> used spherical projections to represent point clouds for a light 2D semantic segmentation network, which is able to achieve real-time performance. The prediction result is further smoothed through a conditional random field (CRF) model and then re-projected back to a 3D point cloud. RangeNet++ <ref type="bibr" target="#b18">[19]</ref> replaced the backbone network of SqueezeNet and CRF in SqueezeSeg to YOLOv3 <ref type="bibr" target="#b23">[24]</ref> Darknet and a GPU-based K-nearest neighbor search to achieve a better segmentation result. Being an empirically better representation than the depth map, BEV represents point clouds from a top-down perspective without losing any scale and range information and is widely used for LiDAR detection <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b36">37]</ref> and recently also for segmentation <ref type="bibr" target="#b40">[41]</ref>. PIXOR <ref type="bibr" target="#b37">[38]</ref> encoded the feature of each cell after discretizating point clouds into BEV representation as occupancy and normalized reflectance. Next, a neural network with 2D convolutional layers is used for 3D object detection. PointPillars <ref type="bibr" target="#b15">[16]</ref> improved this idea by adding a PointNet model on the BEV representation.</p><p>There are many LiDAR object detection datasets in existence, such as the Waymo Open Dataset <ref type="bibr" target="#b28">[29]</ref> and the KITTI 3D detection dataset <ref type="bibr" target="#b8">[9]</ref>. LiDAR scan semantic segmentation datasets, conversely, are somewhat rare. To our knowledge, there are only three so far: the Audi dataset <ref type="bibr" target="#b9">[10]</ref>, Paris-Lille-3D <ref type="bibr" target="#b25">[26]</ref> and the Semantic KITTI dataset <ref type="bibr" target="#b0">[1]</ref>. Other point cloud segmentation datasets, such as Semantic3D <ref type="bibr" target="#b10">[11]</ref>, are out of the scope of online LiDAR segmentation. Annotating RGB images for semantic segmentation algorithm development is a laborious task; however, the task of annotating LiDAR data for semantic segmentation is even more difficult and less intuitive, which might be the reason for so few LiDAR segmentation datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">2D semantic segmentation</head><p>2D semantic segmentation networks, which evolved from Fully Convolutional Networks (FCN) <ref type="bibr" target="#b17">[18]</ref>, have demonstrated a significant improvement on various benchmarks in recent years. Similar to the success in other computer vision tasks, such as pose estimation and object detection, most efficient semantic segmentation networks <ref type="bibr" target="#b39">[40]</ref> adopt an encoder-decoder structure, where a 2D image feature map is first reduced to extract high level contextual information and then expanded to retrieve spatial information. Among these networks, DeepLab <ref type="bibr" target="#b3">[4]</ref> and Unet <ref type="bibr" target="#b24">[25]</ref> are two well-known successful representatives, both of which are designed to fuse multi-scale contextual information together. DeepLab and its successors <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> took advantage of diluted convolution filters to increase the reception field while Unet added skip connections to directly concatenate different levels of semantic features and is proven to be more efficient in images with irregular and coarse edges, like medical images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Statement</head><p>Given a training dataset of N LiDAR scans {(P i , L i )|i = 1, . . . , N }, P i âˆˆ R niÃ—4 is the ith point set containing n i LiDAR points. Each row of P i consists of four features representing one LiDAR point p, namely (x, y, z, reflection). (x, y, z) is the Cartesian coordinate of the point relative to the scanner. The reflection is the intensity of returning laser beam. L i âˆˆ Z ni contains the object labels for each point p j in P i .</p><p>Our goal is to learn a segmentation model f (Â· ; Î¸) parameterized by Î¸ so that the difference between the prediction f (P i ) and L i is minimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Bird's-eye-view Partitioning</head><p>Although a point cloud scan consists of scattered observations of the surrounding 3D environment, empirically, one may represent it as a top-down snapshot of the scene with minimum information loss. <ref type="bibr" target="#b6">[7]</ref> proposes to input such top-down orthogonal projections directly into a 2D detec-  <ref type="figure">Figure 2</ref>. Overview of our model. For a given LiDAR point cloud, we first quantize the points into grids using their polar BEV coordinates. For each of those grid cells, we use a simplified KNN-free PointNet to transform points in it to a fixed-length representation. The representation is then assigned to its corresponding location in the ring matrix. We input the matrix to the ring CNN, which is composed of ring convolution modules. Finally, the CNN outputs a quantized prediction and we decode it to the point domain. tion network to detect objects in 3D point clouds. And it is later on used in point cloud segmentation <ref type="bibr" target="#b40">[41]</ref>. By taking a 2D top-down image as the input, the network outputs a tensor of the same dimensional shape with each spatial location encoding the class prediction for each voxel along the z-axis of that location. This elegant approach accelerates the segmentation process by taking advantage of years of research in 2D CNNs. It also avoids expensive 3D segmentation and 3D graph operations. The original motivation of the BEV was to represent the scene with a top-down image to speed up the downstream task-specific CNNs. Based on years of experience designing CNN architectures, researchers choose BEV representations to closely resemble the appearance of natural images so as to maximally utilize the downstream CNNs, which happen to be designed for natural images. Hence, initial BEV representations created top-down projections of the point clouds. Recently, variants of the initial BEV attempt to encode each pixel in the BEV with rich different heights <ref type="bibr" target="#b37">[38]</ref>, reflection <ref type="bibr" target="#b27">[28]</ref> and even learned representations <ref type="bibr" target="#b15">[16]</ref>. However, one thing remained unchanged: the BEV methods used a Cartesian grid partition as shown in <ref type="figure" target="#fig_0">Fig. 3(a)</ref>.</p><p>A grid is the fundamental image representation, but it may not be the best representation for BEV. A BEV is a compromise between performance and precision. By observing a BEV image, we immediately notice that points densely concentrated in the middle grid cells and peripheral grid cells stay totally empty. Uneven partitioning does not  only waste computational power, but also limits feature representiveness for the center grid cells. Besides, points with different labels might be assigned to a single cell. The minor points' predictions will be suppressed by the majority in the output since the final prediction is on voxel-level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Polar Bird's-eye-view</head><p>How do we address this imbalance? Based on the ringlike structure presented in the LiDAR scan top-down view, we present our Polar partitioning replacing the Cartesian partitioning in <ref type="figure" target="#fig_0">Fig. 3</ref>.</p><p>Instead of quantizing points in a Cartesian coordinate system, we first calculate each point's azimuth and radius on the XY plane with the sensor's location as the origin. We then assign points to grid cells based on their quantized azimuth and radius.</p><p>We find the benefit of polar BEV to be twofold. First, it more evenly distributes the points. To verify this claim, we computed a statistic on the validation split of the Se-manticKITTI dataset <ref type="bibr" target="#b0">[1]</ref>. As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, the points per polar grid cell is much less than in the Cartesian BEV when the cell is close to the sensor. This indicates that the representation for the densely occupied grid is finer. With the same number of grid cells, the traditional BEV grid cell has on average 0.7 Â± 3.2 points while polar BEV grid cell has on average 0.7 Â± 1.4 points. The difference between the standard deviations indicates that, overall, the points are more evenly distributed across the polar BEV grids.</p><p>The second benefit of the polar BEV is that the more bal-anced point distribution lessens the burden on predictors.</p><p>Since we reshape 2D network output to voxel prediction for point prediction, unavoidably, some points with different groundtruth labels will be assigned to the same voxel. And some of them will be misclassified no matter what. With the Cartesian BEV, on average, 98.75% of points in every grid cell share the same label. And this number jumps to 99.3% in the polar BEV. This indicates that points in the polar BEV are less subjected to misclassification due to the spatial representation. Considering that small objects are more likely to be overwhelmed by majority labels in a voxel, this 0.6% difference might have a more profound impact in the eventual mIoU. To further investigate the mIoU upper bound, we set each point's prediction as the majority label of its assigned voxel. It turns out that the Cartesian BEV's mIoU reaches 97.3% in the sanity check. And the polar BEV reaches 98.5%. The higher upper bound in the polar BEV will likely increase the downstream model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Learning the Polar Grid</head><p>Instead of arbitrarily handcrafting the features for each grid, we capture the distribution of points in each grid with a fixed-length representation. It is produced by a learnable simplified PointNet <ref type="bibr" target="#b21">[22]</ref> h followed by a max-pooling. The network only contains fully-connected layers, batchnormalization and ReLu layers. The feature in the i, j-th grid cell in a scan is:</p><formula xml:id="formula_0">fea i,j = MAX({h(p)|w i &lt; p x &lt; w i+1 , l j &lt; p y &lt; l j+1 })</formula><p>(1) where w and l are the quantization sizes. p x and p y are locations of point p in the map. Note that the locations and quantization sizes could be either polar or Cartesian. We do not quantize the input point cloud along the z-axis. Similar to <ref type="bibr" target="#b15">[16]</ref>, our learned representation represents the entire vertical column of a grid.</p><p>If the representation is learned in the polar coordinate system, the two sides of the feature matrix will be connected along the azimuth-axis in physical space as shown in <ref type="figure">Fig. 2</ref>. We developed a discrete convolution which we refer to as a ring convolution. The ring convolution kernel will convolve the matrix assuming the matrix is connected on both ends of the radius axis. Meanwhile, gradients located in the opposite side can propagate back to the other side through this ring convolution kernel. By replacing the normal convolution with the ring convolution in a 2D network, the network will be able to end-to-end process the polar grid without ignoring its connectivity. This provides models with extended receptive fields. Since it is a 2D neural network, the eventual prediction will also be a polar grid whose feature dimension equals to the multiplication of quantized height channel and number of classes. We can then reshape the prediction to a 4D matrix to derive a voxel-based segmentation loss.</p><p>As readers may notice, most CNNs are technically capable of processing polar grids if convolutions are replaced with ring convolutions. We refer to a network with ringconvolutions that is trained to process polar grids as a ring CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We present our experimental setup, results and ablation study in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We use the SemanticKITTI <ref type="bibr" target="#b0">[1]</ref>, A2D2 <ref type="bibr" target="#b9">[10]</ref> and Paris-Lille-3D <ref type="bibr" target="#b25">[26]</ref> datasets in our experiments.</p><p>SemanticKITTI is a point-level re-annotation of the LiDAR part of the famous KITTI dataset <ref type="bibr" target="#b8">[9]</ref>. It has a total of 43551 scans sampled from 22 sequences collected in different cities in Germany. It has 104452 points per scan on average and each scan is collected by a single Velodyne HDL-64E laser scanner shown in <ref type="figure" target="#fig_3">Fig.5(a)</ref>. There are 19 challenging classes in total. The most frequent class, 'vegetation', has 4.82 Ã— 10 7 times more points than the least frequency class, 'motorcyclist'. Obviously, this is a heavily imbalanced and challenging dataset. We follow Se-manticKITTIs subset split protocol and use ten sequences for training, one for validation and the rest of them for testing. We present several baselines that have been presented with SemanticKITTI. We report the segmentation performance on the SemanticKITTI testing subset by uploading our segmentation prediction to their evaluation server.</p><p>A2D2 dataset is a comprehensive autonomous driving dataset developed by Audi. It includes a 38-class segmentation annotation. Despite that the A2D2 data is presented as 3D points in space, these points distribute differently from the KITTI counterparts. We present an example in <ref type="figure" target="#fig_3">Fig. 5(b)</ref>. First of all, a single sensor creates a panoramic LiDAR scan in the KITTI dataset. Meanwhile, A2D2 uses five asynchronous LiDAR sensors where each sensor covers a potion of the surrounding view. Hence almost all the A2D2 reconstructed LiDAR views do not cover all degrees. Secondly, as shown in <ref type="figure" target="#fig_3">Fig. 5(b)</ref>, A2D2 LiDAR sensors do not necessarily produce horizontal scanlines. Our goal is to simulate a vehicle's immediate perception during operation. We first project all LiDAR points back to the vehicle coordinate system. We then manually create (semi-)panoramic LiDAR compositions from any partial scans asynchronously generated within a time window of 50ms. Since sensors are not available all of the time, some generated scans are left incomplete. This heterogeneous composition poses a great challenge for all segmentation algorithms, including ours. With the aforementioned LiDAR panoramic stitching, we create 22408, 2774 and 13264 training, validation and test scans, respectively.</p><p>In contrast to the other two datasets, Paris-Lille-3D provides 3 aggregated point clouds, which are built from continuous LiDAR scans of streets in Paris and Lille collected with one tilted rear-mounted Velodyne HDL-32E. Each point is annotated with one of nine segmentation classes, its timestamp and its world coordinate. Given scanner trajectory and points' timestamps, we extract individual scans from the registered point clouds. We record one scan every 50ms. Each scan is made of points within +/-100ms, e.g. 5(c). In total, we create 5112, 1205 and 1273 training, validation and test scans, respectively. We upload the testing predictions for Paris-Lille-3D to their evaluation server to obtain the official testing results. Since Paris-Lille-3D accepts composition predictions only, we aggregate multiscan predictions via max-voting.</p><p>Voxelization: After analyzing the spatial distribution of points in the SemanticKITTI, A2D2 and Paris-Lille-3D training split, we respectively fixed the Cartesian BEV grid spaces to be [x : Â±50m, y : Â±50m, z : âˆ’3 âˆ¼ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Baselines and Metric</head><p>SqueezeSeg: As the pioneer work in this field, Wu et al. <ref type="bibr" target="#b34">[35]</ref> converted this problem to a 2D segmentation problem by projecting LiDAR points onto a spherical surface surrounding the sensor. They also added a CRF to further improve the end results by enforcing the neighboring label consistency . Besides the vanilla Squeeze-Seg and SqueezeSeg-v2, Behley et al. <ref type="bibr" target="#b0">[1]</ref> replaced the SqueezeNet backbone with YOLO <ref type="bibr" target="#b23">[24]</ref> Darknet-53. This over-parameterization further improved the results by more than 10% on SemanticKITTI over SqueezeSeg-v2. In addition, RangeNet++ <ref type="bibr" target="#b18">[19]</ref> includes a KNN-based postprocessing method which is used after the CNN segmentation network to reduce the error created by the discretization of spherical intermediate representation.</p><p>PointNet <ref type="bibr" target="#b21">[22]</ref>: PointNet is a simplistic network able to predict point semantic segmentation. It individually processes each point with a fully connected network first. Then it summaries a global representation by max pooling the features of all points. The predictor predicts each points class from the concatenation of that points features and the global representation. PointNet++ <ref type="bibr" target="#b22">[23]</ref> is an empirical improvement obtained by adding hierarchical pooling and context representation to vanilla PointNet. TangentConv </p><formula xml:id="formula_1">IoU c = |P c âˆ© G c | |P c âˆª G c | .<label>(2)</label></formula><p>Given the unique properties of LiDAR applications, we also report models' single scan prediction latency, maximum frames-per-second with largest possible batch size (FPS), average multiply-accumulate operations per scan (MAC), and number of model parameters. We report the average on the entire validation split with the same GPU. We do not down-sample points in points-related models.</p><p>We use official implementations or reported results for our baselines. We implemented our own network in Pytorch <ref type="bibr" target="#b19">[20]</ref>. We use torch Geometric <ref type="bibr" target="#b7">[8]</ref> to parallelize points max pooling in each grid. <ref type="table" target="#tab_3">Table 1</ref> shows the performance comparison between our approaches and multiple baselines. The results demonstrate that our polar bird's-eye-view segmentation network based on Unet outperforms the state of the art method even with a smaller number of parameters and lower latency. As shown in this table, point-based methods like Point-Net and TangentConv are inefficient when used with large LiDAR point clouds and poor in segmentation accuracy. For per class IoU, our BEV approaches achieves improvements in most classes, especially in those classes that are irregular and sparsely distributed in space, which matches with the scale and range preserving properties of the polar BEV. We also notice particularly low performance on "other-ground" and "motorcyclist." Investigation suggests they are visually indistinguishable from other classes. By SemanticKITTI's definition, "other-ground" is essentially sidewalk/terrain like ground but serving other purposes, e.g., traffic islands. As for motorcyclist, it is challenging even for a human to tell a motorcyclist from person or bicyclist because the motorcycle itself is often largely occluded. Besides, motorcyclists are the rarest class in the datasetconstitute 0.004% of the training points and only one instance appears in the official validation sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">SemanticKITTI Segmentation Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">A2D2 Segmentation Experiment</head><p>We present our A2D2 results in <ref type="table">Table.</ref> 2. Our method undoubtedly outperforms other baselines in terms of both mIoU and speed. By observing mIoU, we see A2D2 to be a challenging dataset. Despite being the leading method, our mIoU using only LiDAR data on this dataset is merely 23% while our mIoU on SemanticKITTI is 54%. Our methods also double the IoU in multiple classes such as bicycle, pedestrian, small-vehicle, traffic-light, sidebars, signal corpus. parking area and dash-line. The dataset is indeed challenging since both baselines and our methods achieved near zero IoU in multiple classes as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Paris-Lille-3D Segmentation Experiment</head><p>As indicated by the Paris-Lille-3D segmentation results in <ref type="table">Table 4</ref>, PolarNet outperforms DarkNet53 by 3.7% in mIoU. The segmentation performances are interestingly diverse. PolarNet greatly improved the results in barrier since it is mostly far away from vehicle. However Cartesian Unet has great advantage in the trash can, which has very few samples in both training and validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Impact of Projection Methods</head><p>In <ref type="table" target="#tab_5">Table 3</ref>, we show the results of SemanticKITTI mIoU with different segmentation backbone networks, include SqueezeSeg, Resnet-50-FCN, DRN-DeepLab and Resnet-101-DeepLab, on three different projection methods: spherical projection proposed in SqueezeSeg <ref type="bibr" target="#b34">[35]</ref>, Cartesian BEV and our polar BEV. For spherical projection, we followed the setup of projecting point clouds with zenith angles ranging from âˆ’25 â€¢ to 3 â€¢ into [64, 2048] grids in the projected sphere plane as in <ref type="bibr" target="#b18">[19]</ref>. The results show that no matter what segmentation network is used, BEV always considerably outperforms spherical projection methods. The inferior performance of spherical projection can be explained in two ways. First, since point clouds are directly projected onto 2D sphere coordinate, spherical projection suffers more from the error generated from quantization.</p><p>Second, distance information is lost during projection even when explicitly encoded into features, which enables points distant in space to locate in neighboring 2D grids and easily get misclassified as the same label. Meanwhile, experiments also show that polar BEV achieves a comparable and better performance than Cartesian BEV for each backbone network. Since LiDAR point clouds are sparse in space and discontinuous due to occlusion, quantization creates irregular and inconsistent edges in 2D representations. Such inconsistency allows Unet to stand out from those backbone segmentation networks and achieve the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Augmenting LiDAR Segmentation</head><p>In addition, we analyze the effects of different training settings on the validation mIoU result in <ref type="table">Table 5</ref>. The baseline is our polar BEV Unet network with grid size of <ref type="bibr">[256,</ref><ref type="bibr">256,</ref><ref type="bibr" target="#b31">32]</ref>. "RC" denotes using the ring convolution kernel rather than a normal 2D convolution in the backbone network. "9F" denotes we use 2 Cartesian coordinates, 3 residual distances from the center of the assigned grid and 1 reflection in addition to 3 polar coordinates, totaling 9 features as the input of our CNN network for each point. "FA" denotes we add 25% probability each to randomly flip a point cloud along x, y and x + y axes for data augmentation. "FS" denotes we fix the volume space of BEV based on our statistical analysis mentioned before. "TG" denotes we tuned the grid size to be [480, 360, 32] after trying different grid size configurations to reach the best performance. From <ref type="table">Table 5</ref>, we can see that fixing volume space contributes the most significant improvement of 2.8% increase in mIoU by making scale invariant in each scan. These augmentations are applied to the Cartesian BEV network as well in all other experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">mIoU vs. Distance to Sensor</head><p>Furthermore, we sort the point-wise predictions in validation split w.r.t. the distance from the sensor and analyze the mIoU result at different distances. <ref type="figure">Fig. 6</ref> shows that with the increase of distance, mIoU reduces simultaneously. The reason for this pattern is that distant points are more rare and separated in space, which makes it harder for the segmentation network to extract contextual information from the BEV representation. This observation is the same as in <ref type="bibr" target="#b0">[1]</ref>. However, the most intriguing conclusion we obtain from this figure relates to the different BEV representations:   <ref type="figure">Figure 6</ref>. Points distance to sensor vs. their IoU in different networks and projections. Clearly, closer points benefits the most from polar BEV regardless of backbone networks. polar BEV overall gets higher mIoU in close range than Cartesian BEV due to the more evenly distributed points in this BEV representation, as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. This grants polar BEV superior mIoU on closer points, which are the majority in a scan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a novel data representation for the online, single-scan LiDAR point cloud semantic segmentation problem. Our approach addresses the problem of long-tailed spatial distribution of LiDAR point clouds by quantizing points into polar bird's-eye-view (BEV) grids, where we encode points into fixed size representations through a trainable PointNet. Built upon the polar grid representation, our PolarNet network achieves a significant improvement in mIoU over state-of-the-art methods on the SemanticKITTI, A2D2, and Paris-Lille-3D datasets with fewer parameters, more throughput, and lower inference latency. Moreover, our experiments show universal improvement among different segmentation networks using our polar BEV compared to spherical projection and Cartesian BEV, indicating that our polar grid is a superior yet general LiDAR point cloud data representation for the online semantic segmentation problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Two BEV quantization strategies. Each grid cell on the image denotes one feature in a feature map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Grid cell distance from the sensor vs. logarithmically spaced mean number of points per grid cell. The traditional BEV representation allocates most of its grid cells to the further end with few points in them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>PolarNet outperforms baselines despite different scanline patterns in datasets. Zoom in for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc><ref type="bibr" target="#b29">[30]</ref>:Tatarchenko and Park et al.  propose to use tangent convolutions on surface geometry to predict segmentation classes for 3D point clouds.RandLA[12]: Hu et al. propose to segment large scale point clouds with a local feature aggregation module.We report accuracy, per-class IoU and mIoU. mIoU is the mean over all semantic classes of class intersection over union. A class c's intersection over union, (IoU c ), refers to the intersection of the class prediction and ground truth divided by their union:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Point-level SemanticKITTI<ref type="bibr" target="#b0">[1]</ref> segmentation mIoU vs. multiplyaccumulate operations per scan on the same GPU. Our Unet-based PolarNet not only significantly outperforms Cartesian-BEV Unet, PointNet, SqueezeSeg and SqueezeSeg's overparameterized variants (connected by line), but also retains remarkably low computational cost.</figDesc><table><row><cell></cell><cell>55%</cell><cell></cell><cell>PolarNet (Ours)</cell><cell></cell></row><row><cell>Semantic KITTI test mIoU</cell><cell>35% 40% 45% 50%</cell><cell>Squeezesegv2</cell><cell>Cartesian Unet</cell><cell>DarkNet53</cell></row><row><cell></cell><cell>30%</cell><cell>Squeezeseg</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell cols="3">50 100 150 200 250 300 350 400 MACs (billion)</cell></row><row><cell cols="2">Figure 1.</cell><cell></cell><cell></cell><cell></cell></row></table><note>Now at Google. Code at https://github.com/edwardzhou130/PolarSeg</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>1.5m], [x : Â±50m, y : Â±50m, z : âˆ’3 âˆ¼ 9m] and [x : Â±15m, y : Â±15m, z : âˆ’3 âˆ¼ 12m] and respectively [distance : 3 âˆ¼ 50m, z : âˆ’3 âˆ¼ 1.5m], [distance : 0 âˆ¼ 50m, z : âˆ’3 âˆ¼ 9m] and [distance : 0 âˆ¼ 15m, z : âˆ’3 âˆ¼ 12m] for our polar BEV to include more than 99% of points for each scan on average. Points exceeding this range are assigned to the closest BEV grid cell. In addition, we set the respective grid sizes as [480, 360, 32], [320, 320, 32] and [320, 320, 32].</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Segmentation results on test split of SemanticKITTI. 9% 13.1% 0.9% 85.4% 26.9% 54.3% 4.5% 57.4% 29.0% 60.0% 24.3% 53.7% 17.5% 24.5% 7% 11.6% 10.2% 17.1% 20.2% 0.5% 82.9% 15.2% 61.7% 9.0% 82.8% 44.2% 75.5% 42.5% 55.5% 30.2% 22.2% 7% 81.8% 18.5% 17.9% 13.4% 14.0% 20.1% 25.1% 3.9% 88.6% 45.8% 67.6% 17.7% 73.7% 41.1% 71.8% 35.8% 60.2% 20.2% 36.8% 49.9% 86.4% 24.5% 32.7% 25.5% 22.6% 36.2% 33.6% 4.7% 91.8% 64.8% 74.6% 27.9% 84.1% 55.0% 78.3% 50.1% 64.0% 38.0% 52.2% 91.4% 25.7% 34.4% 25.7% 23.0% 38.3% 38.8% 4.8% 91.8% 65.0% 75.2% 27.8% 87.4% 58.6% 80.5% 55.1% 64.6% 47.9% 94.2% 26.0% 25.8% 40.1% 38.9% 49.2% 48.2% 7.2% 90.7% 60.3% 73.7% 20.4% 86.9% 56.3% 81.4% 66.8% 49.2% 47.7% 38.6% 50.7% 92.7% 26.8% 23.1% 26.7% 24.2% 48.1% 41.0% 4.4% 86.7% 52.3% 67.2% 12.9% 89.5% 57.7% 80.8% 62.5% 62.5% 50.3% 53.0% 54.3% 93.8% 40.3% 30.1% 22.9% 28.5% 43.2% 40.2% 5.6% 90.8% 61.7% 74.4% 21.7% 90.0% 61.3% 84.0% 65.5% 67.8% 51.8% 57.5%</figDesc><table><row><cell>Model</cell><cell cols="4">FPS Latency MACs Params</cell><cell>Acc</cell><cell>mIoU</cell><cell>car</cell><cell>bicycle</cell><cell>motorcycle</cell><cell>truck</cell><cell>other-vehicle</cell><cell>person</cell><cell>bicyclist</cell><cell>motorcyclist</cell><cell>road</cell><cell>Per class IoU parking</cell><cell>sidewalk</cell><cell>other-ground</cell><cell>building</cell><cell>fence</cell><cell>vegetation</cell><cell>trunk</cell><cell>terrain</cell><cell>pole</cell><cell>traffic-sign</cell></row><row><cell>PointNet [22]</cell><cell>11.5</cell><cell>0.087s</cell><cell>141B</cell><cell>3.5M</cell><cell>-</cell><cell cols="2">14.6% 46.3%</cell><cell>1.3%</cell><cell>0.3%</cell><cell>0.1%</cell><cell>0.8%</cell><cell>0.2%</cell><cell>0.2%</cell><cell cols="4">0.0% 61.6% 15.8% 35.7%</cell><cell>1.4%</cell><cell cols="3">41.4% 12.9% 31.0%</cell><cell>4.6%</cell><cell>17.6%</cell><cell>2.4%</cell><cell>3.7%</cell></row><row><cell>PointNet++ [23]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>6M</cell><cell>-</cell><cell cols="2">20.1% 53.7%</cell><cell>1.9%</cell><cell>0.2%</cell><cell>0.9%</cell><cell>0.2%</cell><cell>0.9%</cell><cell>1.0%</cell><cell cols="4">0.0% 72.0% 18.7% 41.8%</cell><cell>5.6%</cell><cell cols="5">62.3% 16.9% 46.5% 13.8% 30.0%</cell><cell>6.0%</cell><cell>8.9%</cell></row><row><cell cols="24">Squeezeseg [35] 12.TangentConv [30] 49.2 0.031s 13B 0.9M -29.5% 68.8% 16.0% 4.1% 3.3% 3.6% ---0.4M -35.9% 86.8% 1.3% 12.Squeezesegv2 [36] 36.7 0.036s 14B 0.9M -39.3%</cell></row><row><cell>DarkNet53 [1]</cell><cell>12.7</cell><cell>0.087s</cell><cell>378B</cell><cell>50M</cell><cell cols="19">87.9% 52.2%</cell></row><row><cell>RangeNet++ [19]</cell><cell>-</cell><cell>-</cell><cell>378B</cell><cell>50M</cell><cell cols="19">89.9% 55.9%</cell></row><row><cell>RandLA [12]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.2M</cell><cell>-</cell><cell cols="18">53.1%</cell></row><row><cell cols="2">Unet w/ Cartesian BEV 19.7</cell><cell>0.051s</cell><cell>134B</cell><cell>14M</cell><cell cols="19">87.5%</cell></row><row><cell>PolarNet</cell><cell>16.2</cell><cell>0.062s</cell><cell>135B</cell><cell>14M</cell><cell>90.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Segmentation results on test split of A2D2. 0% 16.4% 15.4% 0.2% 8.6% 63.8% 0.0% 16.8% 61.7% 0.6% 0.1% 0.0% 14.8% 24.7% 12.7% 33.2% 0.0% 0% 17.2% 15.2% 0.8% 6.1% 68.5% 0.0% 15.5% 63.8% 0.4% 0.3% 0.0% 17.3% 23.8% 13.3% 35.6% 0.0% 5% 20.3% 27.0% 7.3% 20.3% 66.0% 1.9% 25.2% 54.7% 6.5% 12.7% 0.0% 20.3% 26.8% 21.4% 42.5% 0.0% 4% 23.9% 23.8% 10.1% 18.2% 69.7% 9.6% 49.1% 58.5% 0.0% 11.3% 0.0% 28.3% 37.6% 24.8% 42.</figDesc><table><row><cell>Model</cell><cell cols="7">FPS Latency MACs Params</cell><cell cols="2">Acc</cell><cell>mIoU</cell><cell>car</cell><cell></cell><cell>bicycle</cell><cell></cell><cell>pedestrian</cell><cell></cell><cell>truck</cell><cell></cell><cell>small</cell><cell>vehicles</cell><cell>traffic</cell><cell>signal</cell><cell>traffic</cell><cell>sign</cell><cell>utility</cell><cell>vehicle</cell><cell cols="3">Per class IoU sidebars speed</cell><cell>bumper</cell><cell></cell><cell>curbstone</cell><cell></cell><cell>solid line</cell><cell>irrelevant</cell><cell>signs</cell><cell>road</cell><cell>blocks</cell><cell>tractor</cell><cell>non-</cell><cell>drivable</cell><cell>street</cell><cell>zebra</cell><cell>crossing</cell></row><row><cell>Squeezeseg [35]</cell><cell>87.5</cell><cell cols="2">0.009s</cell><cell></cell><cell>15B</cell><cell>0.9M</cell><cell></cell><cell>-</cell><cell></cell><cell>8.9%</cell><cell cols="2">9.7%</cell><cell cols="2">0.0%</cell><cell cols="6">0.0% 15.8% 0.0%</cell><cell cols="6">0.7% 64.4% 0.0%</cell><cell>0.4%</cell><cell cols="3">0.0%</cell><cell cols="6">2.2% 15.6% 0.5% 15.9% 0.0%</cell><cell>0.0%</cell><cell>0.0%</cell></row><row><cell>Squeezesegv2 [36]</cell><cell>67.1</cell><cell cols="2">0.015s</cell><cell></cell><cell>15B</cell><cell>0.9M</cell><cell></cell><cell cols="29">81.5.8%</cell><cell>0.0%</cell></row><row><cell>DarkNet53 [1]</cell><cell>16.1</cell><cell cols="2">0.063s</cell><cell></cell><cell>378B</cell><cell>50M</cell><cell></cell><cell cols="29">82.6.3%</cell><cell>0.0%</cell></row><row><cell cols="2">Unet w/ Cartesian BEV 49.5</cell><cell cols="2">0.028s</cell><cell></cell><cell>60B</cell><cell>14M</cell><cell></cell><cell cols="29">83.9.5%</cell><cell>0.0%</cell></row><row><cell>PolarNet</cell><cell>38.4</cell><cell cols="2">0.031s</cell><cell></cell><cell>60B</cell><cell>14M</cell><cell></cell><cell cols="29">85.8% 0.0%</cell><cell>14.8%</cell><cell>0.0%</cell></row><row><cell>Model</cell><cell cols="2">mIoU</cell><cell>obstacles /</cell><cell>trash</cell><cell>poles</cell><cell>RD</cell><cell>restricted</cell><cell>area</cell><cell>animals</cell><cell>grid</cell><cell>structure</cell><cell>signal</cell><cell>corpus</cell><cell>drivable</cell><cell>cobblestone</cell><cell>electronic</cell><cell>traffic</cell><cell>slow drive</cell><cell>area</cell><cell>nature</cell><cell cols="4">Per class IoU object parking area</cell><cell>sidewalk</cell><cell></cell><cell>ego car</cell><cell>painted</cell><cell cols="2">driv. instr.</cell><cell>traffic</cell><cell>guide obj.</cell><cell>dashed</cell><cell>line</cell><cell>RD normal</cell><cell>street</cell><cell>sky</cell><cell>buildings</cell><cell>blurred</cell><cell>area</cell><cell>rain dirt</cell></row><row><cell>Squeezeseg [35]</cell><cell>8.9%</cell><cell></cell><cell cols="2">0.0%</cell><cell>0.3%</cell><cell cols="3">0.0%</cell><cell cols="3">0.0% 0.0%</cell><cell cols="2">0.0%</cell><cell cols="2">0.0%</cell><cell cols="2">0.0%</cell><cell cols="13">0.0% 64.5% 0.0% 13.7% 0.0% 0.0%</cell><cell cols="2">0.1%</cell><cell cols="4">0.2% 77.7% 10.4% 27.7% 0.0% 0.0%</cell></row><row><cell>Squeezesegv2 [36]</cell><cell cols="4">16.4% 0.2%</cell><cell>5.2%</cell><cell cols="3">29.5%</cell><cell cols="5">0.0% 10.3% 5.5%</cell><cell cols="2">2.7%</cell><cell cols="2">0.0%</cell><cell cols="19">1.9% 76.4% 3.8% 29.2% 0.0% 6.4% 12.4% 17.1% 85.8% 12.1% 50.9% 0.0% 0.0%</cell></row><row><cell>DarkNet53 [1]</cell><cell cols="4">17.2% 3.9%</cell><cell>7.6%</cell><cell cols="3">38.7%</cell><cell cols="5">0.0% 10.8% 4.4%</cell><cell cols="2">3.3%</cell><cell cols="2">0.0%</cell><cell cols="13">0.0% 77.9% 3.1% 31.5% 0.0% 9.4%</cell><cell cols="6">7.3% 15.7% 86.4% 12.9% 55.2% 0.0% 0.0%</cell></row><row><cell cols="6">Unet w/ Cartesian BEV 20.3% 4.3% 11.0%</cell><cell cols="3">44.7%</cell><cell cols="7">0.0% 11.8% 11.9% 6.4%</cell><cell cols="2">0.0%</cell><cell cols="19">0.0% 81.6% 11.9% 35.1% 0.0% 6.9% 13.7% 20.2% 89.2% 5.8% 56.1% 0.0% 0.0%</cell></row><row><cell>PolarNet</cell><cell cols="5">23.9% 8.0% 11.0%</cell><cell cols="3">55.6%</cell><cell cols="7">0.0% 14.8% 11.9% 7.0%</cell><cell cols="2">0.0%</cell><cell cols="19">4.4% 81.6% 12.8% 42.5% 0.0% 12.7% 11.5% 31.8% 90.3% 9.2% 57.0% 0.0% 0.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>How projection methods impact models' segmentation performance on val split of SemanticKITTI. 6% 82.3% 1.5% 13.7% 65.8% 15.5% 20.3% 31.2% 0.0% 92.1% 32.4% 75.6.2% 0.1% 77.3% 31.6% 78.1% 43.9% 66.8% 36.6% 25.2% 5% 92.1% 22.8% 36.2% 57.5% 24.6% 42.5% 63.9% 0.0% 92.1% 43.6% 77.5% 1.7% 90.0% 46.9% 84.4% 56.0% 73.1% 53.3% 40.2% 4% 82.6% 3.1% 24.5% 51.1% 18.3% 27.3% 23.9% 0.0% 93.0% 37.2% 77.4% 0.2% 76.8% 42.1% 79.7% 46.2% 68.7% 39.2% 32.9% 7% 90.4% 14.1% 20.3% 51.4% 37.3% 39.3% 42.3% 0.0% 87.6% 30.6% 68.0% 1.5% 86.5% 33.0% 83.2% 49.2% 69.8% 44.3% 39.0% 2% 91.6% 19.4% 35.0% 34.6% 20.8% 50.8% 55.1% 0.0% 92.5% 38.6% 77.5% 1.1% 88.5% 44.4% 84.8% 59.7% 70.6% 56.7% 40.2% 6% 81.0% 0.6% 17.1% 58.9% 12.1% 21.3% 24.7% 0.0% 92.5% 33.5% 76.4% 0.0% 76.0% 40.4% 78.6% 45.7% 68.3% 35.1% 28.6% Cartesian BEV 11.8 0.090s 107B 60M 50.4% 92.6% 17.8% 41.9% 62.0% 24.2% 42.0% 66.3% 0.0% 87.1% 27.2% 69.6% 0.4% 87.4% 41.5% 84.7% 54.8% 71.0% 48.7% 39.1% 6% 91.5% 30.7% 38.8% 46.4% 24.0% 54.1% 62.2% 0.0% 92.4% 47.1% 78.0% 1.8% 89.1% 45.5% 85.4% 59.6% 72.3% 58.1% 42.2% Table 4. Segmentation results on test split of Paris-Lille-3D. Cartesian BEV 80.9% 40.3% 96.0% 44.0% 38.4% 42.8% 12.7% 12.4% 12.1% 70.4% 33.60% PolarNet 87.5% 43.7% 96.8% 69.1% 32.2% 27.6% 2.4% 27.5% 12.1% 74.0% 51.60%Table 5. Improvement break down. RC denotes ring convolution. 9F denotes using 9 features to describe each point. FA denotes flip augmentation. FS denotes fixed volume space. TG denotes tuned grid size.</figDesc><table><row><cell>Model</cell><cell cols="2">Projection</cell><cell cols="6">FPS Latency MACs Params mIoU</cell><cell>car</cell><cell>bicycle</cell><cell></cell><cell>motorcycle</cell><cell>truck</cell><cell>other-vehicle</cell><cell>person</cell><cell>bicyclist</cell><cell>motorcyclist</cell><cell>road</cell><cell cols="2">Per class IoU parking</cell><cell>sidewalk</cell><cell>other-ground</cell><cell>building</cell><cell>fence</cell><cell>vegetation</cell><cell>trunk</cell><cell>terrain</cell><cell>pole</cell><cell>traffic-sign</cell></row><row><cell></cell><cell cols="2">Spherical</cell><cell>83.6</cell><cell>0.012s</cell><cell>14B</cell><cell>0.9M</cell><cell cols="5">31.8% 79.4% 0.0%</cell><cell>0.0%</cell><cell>3.2%</cell><cell>1.3%</cell><cell>0.0%</cell><cell cols="4">0.0% 0.0% 90.9% 19.8%</cell><cell cols="2">74.7%</cell><cell>0.0% 75.3% 31.6% 80.6% 37.3% 71.1% 13.2% 26.3%</cell></row><row><cell>Squeezeseg</cell><cell cols="3">Cartesian BEV 19.5</cell><cell>0.051s</cell><cell>101B</cell><cell>1.5M</cell><cell cols="13">42.6% 90.4% 15.2% 16.6% 13.5% 16.8% 39.0% 45.8% 0.0% 85.7% 25.3%</cell><cell cols="2">65.2%</cell><cell>0.0% 86.1% 32.1% 79.7% 54.4% 60.1% 50.9% 33.2%</cell></row><row><cell></cell><cell cols="2">Polar BEV</cell><cell>17.8</cell><cell>0.056s</cell><cell>105B</cell><cell>1.5M</cell><cell cols="13">42.2% 89.8% 22.1% 19.8% 14.2% 9.2% 37.0% 14.3% 0.4% 83.7% 15.8%</cell><cell cols="2">65.6%</cell><cell>0.0% 85.9% 40.2% 85.6% 54.2% 72.1% 54.9% 36.7%</cell></row><row><cell>Resnet-FCN</cell><cell cols="19">Spherical 41.Cartesian BEV 11.7 38.6 0.048s 92B 117M 0.088s 197B 117M 49.2% 89.9% 28.2% 15.6% 56.5% 30.5% 41.0% 66.1% 0.0% 88.6% 38.3%</cell><cell cols="2">71.5%</cell><cell>6.1% 86.5% 30.4% 81.5% 52.2% 65.7% 46.7% 39.3%</cell></row><row><cell cols="10">Polar BEV 52.DRN-DL 11.5 0.091s 200B 117M Spherical 39.1 0.038s 94B 41M 43.Cartesian BEV 10.0 0.100s 171B 41M 46.Polar BEV 9.9 0.101s 173B 41M 51.Resnet-DL Spherical 89.5 0.031s 45B 59M 41.Polar BEV 11.7 0.094s 109B 60M 53.Model Acc Per class IoU mIoU ground building pole bollard trash can</cell><cell>barrier</cell><cell>pedestrian</cell><cell>car</cell><cell>vegetation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Squeezesegv2 [36]</cell><cell cols="5">87.3% 36.9% 95.9% 82.7% 18.7%</cell><cell>9.9%</cell><cell>3.8%</cell><cell cols="2">15.2%</cell><cell>3.4%</cell><cell>49.9%</cell><cell>52.8%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DarkNet53 [1]</cell><cell></cell><cell cols="6">88.9% 40.0% 96.7% 84.9% 19.5% 16.7%</cell><cell>4.8%</cell><cell cols="2">17.6%</cell><cell>3.4%</cell><cell cols="2">58.2% 57.9%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="12">Unet w/ RC 9F FA FS TG mIoU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">46.9%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Ã—</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">47.4%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Ã—</cell><cell>Ã—</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">48.5%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Ã—</cell><cell>Ã—</cell><cell>Ã—</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">50.6%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Ã—</cell><cell>Ã—</cell><cell>Ã—</cell><cell></cell><cell>Ã—</cell><cell></cell><cell></cell><cell cols="3">53.4%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Ã—</cell><cell>Ã—</cell><cell>Ã—</cell><cell></cell><cell>Ã—</cell><cell>Ã—</cell><cell></cell><cell cols="3">54.9%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>50 55 60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Squeezeseg DRN Deeplab Resnet Deeplab Resnet FCN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>mIoU</cell><cell>40 45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>25 30</cell><cell cols="2">Polar BEV Cartesian BEV</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell></cell><cell>10</cell><cell cols="2">20 Distance (m)</cell><cell>30</cell><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Se-manticKITTI: A dataset for semantic scene understanding of lidar sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Milioto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9297" to="9307" />
		</imprint>
	</monogr>
	<note>Sven Behnke, Cyrill Stachniss, and Jurgen Gall</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shape context: A new descriptor for shape matching and object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="831" to="837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11027</idno>
		<title level="m">nuScenes: A multimodal dataset for autonomous driving</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop on representation learning on graphs and manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Oleksandr Vorobiov, and Peter Schuberth. A2D2: AEV autonomous driving dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohannes</forename><surname>Kassahun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mentar</forename><surname>Mahmudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Ricou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><surname>Durgesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenz</forename><surname>Hauswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Viet Hoang Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Mhlegg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiffany</forename><surname>Dorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudesh</forename><surname>Jnicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiragkumar</forename><surname>Mirashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Savani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sturm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">NET: a new large-scale point cloud classification benchmark. ISPRS Annals of Photogrammetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Semantic3d</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing and Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="91" to="98" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">RandLA-Net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent slice networks for 3d segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2626" to="2635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">PointPillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="12697" to="12705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">RangeNet++: Fast and accurate LiDAR semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS autodiff workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgbd data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Paris-Lille-3D: A large and high-quality groundtruth urban point cloud dataset for automatic segmentation and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Roynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franois</forename><surname>Goulette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Complex-yolo: An euler-region-proposal for real-time 3D object detection on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Milz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Amende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst-Michael</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="197" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xerxes</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijaysai</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04838</idno>
		<title level="m">perception for autonomous driving: Waymo open dataset</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyne</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on 3D Vision</title>
		<meeting>the International Conference on 3D Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="537" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velikovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph attention networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3D object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8445" to="8453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Robotics and Automation</title>
		<meeting>the International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1887" to="1893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Squeezesegv2: Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Robotics and Automation</title>
		<meeting>the International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4376" to="4382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">PIXOR: Realtime 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="472" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient convolutions for real-time semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on 3D Vision</title>
		<meeting>the International Conference on 3D Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sensor fusion for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Candra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avideh</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zakhor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Robotics and Automation</title>
		<meeting>the International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1850" to="1857" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
