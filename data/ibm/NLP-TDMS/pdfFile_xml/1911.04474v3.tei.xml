<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TENER: Adapting Transformer Encoder for Named Entity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bocao</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaonan</forename><surname>Li</surname></persName>
							<email>lixiaonan1208@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<email>xpqiu@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TENER: Adapting Transformer Encoder for Named Entity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bidirectional long short-term memory networks (BiLSTMs) have been widely used as an encoder for named entity recognition (NER) task. Recently, the fully-connected self-attention architecture (aka Transformer) is broadly adopted in various natural language processing (NLP) tasks owing to its parallelism and advantage in modeling the longrange context. Nevertheless, the performance of the vanilla Transformer in NER is not as good as it is in other NLP tasks. In this paper, we propose TENER, a NER architecture adopting adapted Transformer Encoder to model the character-level features and wordlevel features. By incorporating the directionaware, distance-aware and un-scaled attention, we prove the Transformer-like encoder is just as effective for NER as other NLP tasks. Experiments on six NER datasets show that TENER achieves superior performance than the prevailing BiLSTM-based models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The named entity recognition (NER) is the task of finding the start and end of an entity in a sentence and assigning a class for this entity. NER has been widely studied in the field of natural language processing (NLP) because of its potential assistance in question generation <ref type="bibr" target="#b42">(Zhou et al., 2017)</ref>, relation extraction <ref type="bibr" target="#b24">(Miwa and Bansal, 2016)</ref>, and coreference resolution <ref type="bibr" target="#b9">(Fragkou, 2017)</ref>. Since <ref type="bibr" target="#b5">(Collobert et al., 2011)</ref>, various neural models have been introduced to avoid hand-crafted features <ref type="bibr" target="#b16">(Huang et al., 2015;</ref><ref type="bibr" target="#b22">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b18">Lample et al., 2016)</ref>.</p><p>NER is usually viewed as a sequence labeling task, the neural models usually contain three components: word embedding layer, context encoder layer, and decoder layer <ref type="bibr" target="#b16">(Huang et al., 2015;</ref><ref type="bibr">Ma * Corresponding author. and Hovy, 2016;</ref><ref type="bibr" target="#b18">Lample et al., 2016;</ref><ref type="bibr" target="#b4">Chiu and Nichols, 2016;</ref><ref type="bibr" target="#b3">Chen et al., 2019;</ref><ref type="bibr" target="#b12">Gui et al., 2019b)</ref>. The difference between various NER models mainly lies in the variance in these components.</p><p>Recurrent Neural Networks (RNNs) are widely employed in NLP tasks due to its sequential characteristic, which is aligned well with language. Specifically, bidirectional long short-term memory networks (BiLSTM) <ref type="bibr" target="#b14">(Hochreiter and Schmidhuber, 1997</ref>) is one of the most widely used RNN structures. <ref type="bibr" target="#b16">(Huang et al., 2015)</ref> was the first one to apply the BiLSTM and Conditional Random Fields (CRF) <ref type="bibr" target="#b17">(Lafferty et al., 2001)</ref> to sequence labeling tasks. Owing to BiLSTM's high power to learn the contextual representation of words, it has been adopted by the majority of NER models as the encoder <ref type="bibr" target="#b22">(Ma and Hovy, 2016;</ref><ref type="bibr" target="#b18">Lample et al., 2016;</ref><ref type="bibr" target="#b12">Gui et al., 2019b)</ref>.</p><p>Recently, Transformer <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> began to prevail in various NLP tasks, like machine translation <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref>, language modeling <ref type="bibr" target="#b29">(Radford et al., 2018)</ref>, and pretraining models <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref>. The Transformer encoder adopts a fully-connected self-attention structure to model the long-range context, which is the weakness of RNNs. Moreover, Transformer has better parallelism ability than RNNs. However, in the NER task, Transformer encoder has been reported to perform poorly <ref type="bibr" target="#b13">(Guo et al., 2019)</ref>, our experiments also confirm this result. Therefore, it is intriguing to explore the reason why Transformer does not work well in NER task.</p><p>In this paper, we analyze the properties of Transformer and propose two specific improvements for NER.</p><p>The first is that the sinusoidal position embedding used in the vanilla Transformer is aware of distance but unaware of the directionality. In addition, this property will lose when used in the Louis Vuitton founded Louis Vuitton Inc. in 1854 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PER PER</head><p>ORG ORG ORG TIME <ref type="figure">Figure 1</ref>: An example for NER. The relative direction is important in the NER task, because words before "Inc." are mostly to be an organization, words after "in" are more likely to be time or location. Besides, the distance between words is also important, since only continuous words can form an entity, the former "Louis Vuitton" can not form an entity with the "Inc.". vanilla Transformer. However, both the direction and distance information are important in the NER task. For example in <ref type="figure">Fig 1,</ref> words after "in" are more likely to be a location or time than words before it, and words before "Inc." are mostly likely to be of the entity type "ORG". Besides, an entity is a continuous span of words. Therefore, the awareness of distance might help the word better recognizes its neighbor. To endow the Transformer with the ability of direction-and distanceawareness, we adopt the relative positional encoding <ref type="bibr" target="#b31">(Shaw et al., 2018;</ref>. instead of the absolute position encoding. We propose a revised relative positional encoding that uses fewer parameters and performs better.</p><p>The second is an empirical finding. The attention distribution of the vanilla Transformer is scaled and smooth. But for NER, a sparse attention is suitable since not all words are necessary to be attended. Given a current word, a few contextual words are enough to judge its label. The smooth attention could include some noisy information. Therefore, we abandon the scale factor of dot-production attention and use an un-scaled and sharp attention.</p><p>With the above improvements, we can greatly boost the performance of Transformer encoder for NER.</p><p>Other than only using Transformer to model the word-level context, we also tried to apply it as a character encoder to model word representation with character-level information. The previous work has proved that character encoder is necessary to capture the character-level features and alleviate the out-of-vocabulary (OOV) problem <ref type="bibr" target="#b18">(Lample et al., 2016;</ref><ref type="bibr" target="#b22">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b4">Chiu and Nichols, 2016;</ref><ref type="bibr" target="#b36">Xin et al., 2018)</ref>. In NER, CNN is commonly used as the character encoder. However, we argue that CNN is also not perfect for representing character-level information, be-cause the receptive field of CNN is limited, and the kernel size of the CNN character encoder is usually 3, which means it cannot correctly recognize 2-gram or 4-gram patterns. Although we can deliberately design different kernels, CNN still cannot solve patterns with discontinuous characters, such as "un..ily" in "unhappily" and "unnecessarily". Instead, the Transformer-based character encoder shall not only fully make use of the concurrence power of GPUs, but also have the potentiality to recognize different n-grams and even discontinuous patterns. Therefore, in this paper, we also try to use Transformer as the character encoder, and we compare four kinds of character encoders.</p><p>In summary, to improve the performance of the Transformer-based model in the NER task, we explicitly utilize the directional relative positional encoding, reduce the number of parameters and sharp the attention distribution. After the adaptation, the performance raises a lot, making our model even performs better than BiLSTM based models. Furthermore, in the six NER datasets, we achieve state-of-the-art performance among models without considering the pre-trained language models or designed features.</p><p>2 Related Work 2.1 Neural Architecture for NER <ref type="bibr" target="#b5">Collobert et al. (2011)</ref> utilized the Multi-Layer Perceptron (MLP) and CNN to avoid using taskspecific features to tackle different sequence labeling tasks, such as Chunking, Part-of-Speech (POS) and NER. In <ref type="bibr" target="#b16">(Huang et al., 2015)</ref>, BiLSTM-CRF was introduced to solve sequence labeling questions. Since then, the BiLSTM has been extensively used in the field of NER <ref type="bibr" target="#b4">(Chiu and Nichols, 2016;</ref><ref type="bibr" target="#b8">Dong et al., 2016;</ref><ref type="bibr" target="#b22">Ma and Hovy, 2016)</ref>.</p><p>Despite BiLSTM's great success in the NER task, it has to compute token representations one by one, which massively hinders full exploitation of GPU's parallelism. Therefore, CNN has been proposed by <ref type="bibr" target="#b33">(Strubell et al., 2017;</ref><ref type="bibr" target="#b11">Gui et al., 2019a)</ref> to encode words concurrently. In order to enlarge the receptive field of CNNs, <ref type="bibr" target="#b33">(Strubell et al., 2017)</ref> used iterative dilated CNNs (ID-CNN).</p><p>Since the word shape information, such as the capitalization and n-gram, is important in recognizing named entities, CNN and BiLSTM have been used to extract character-level informa-tion <ref type="bibr" target="#b4">(Chiu and Nichols, 2016;</ref><ref type="bibr" target="#b18">Lample et al., 2016;</ref><ref type="bibr" target="#b22">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b33">Strubell et al., 2017;</ref><ref type="bibr" target="#b3">Chen et al., 2019)</ref>.</p><p>Almost all neural-based NER models used pretrained word embeddings, like Word2vec and Glove <ref type="bibr" target="#b26">(Pennington et al., 2014;</ref><ref type="bibr" target="#b23">Mikolov et al., 2013)</ref>. And when contextual word embeddings are combined, the performance of NER models will boost a lot <ref type="bibr" target="#b28">(Peters et al., 2017</ref><ref type="bibr" target="#b27">(Peters et al., , 2018</ref><ref type="bibr" target="#b0">Akbik et al., 2018)</ref>. ELMo introduced by <ref type="bibr" target="#b27">(Peters et al., 2018)</ref> used the CNN character encoder and BiLSTM language models to get contextualized word representations. Except for the BiLSTM based pre-trained models, BERT was based on Transformer <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transformer</head><p>Transformer was introduced by <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref>, which was mainly based on self-attention. It achieved great success in various NLP tasks. Since the self-attention mechanism used in the Transformer is unaware of positions, to avoid this shortage, position embeddings were used <ref type="bibr" target="#b34">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b7">Devlin et al., 2018)</ref>. Instead of using the sinusoidal position embedding <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> and learned absolute position embedding, <ref type="bibr" target="#b31">Shaw et al. (2018)</ref> argued that the distance between two tokens should be considered when calculating their attention score.  reduced the computation complexity of relative positional encoding from O(l 2 d) to O(ld), where l is the length of sequences and d is the hidden size.  derived a new form of relative positional encodings, so that the relative relation could be better considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Transformer Encoder Architecture</head><p>We first introduce the Transformer encoder proposed in <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref>. The Transformer encoder takes in an matrix H ∈ R l×d , where l is the sequence length, d is the input dimension. Then three learnable matrix W q , W k , W v are used to project H into different spaces. Usually, the matrix size of the three matrix are all R d×d k , where d k is a hyper-parameter. After that, the scaled dotproduct attention can be calculated by the following equations,</p><formula xml:id="formula_0">Q, K, V = HWq, HW k , HWv,<label>(1)</label></formula><formula xml:id="formula_1">At,j = QtK T j ,<label>(2)</label></formula><formula xml:id="formula_2">Attn(K, Q, V ) = softmax( A √ d k )V,<label>(3)</label></formula><p>where Q t is the query vector of the tth token, j is the token the tth token attends. K j is the key vector representation of the jth token. The softmax is along the last dimension. Instead of using one group of W q , W k , W v , using several groups will enhance the ability of self-attention. When several groups are used, it is called multi-head selfattention, the calculation can be formulated as follows,</p><formula xml:id="formula_3">Q (h) , K (h) , V (h) = HW (h) q , HW (h) k , HW (h) v , (4) head (h) = Attn(Q (h) , K (h) , V (h) ),<label>(5)</label></formula><formula xml:id="formula_4">MultiHead(H) = [head (1) ; ...; head (n) ]WO,<label>(6)</label></formula><p>where n is the number of heads, the superscript h represents the head index.</p><p>[head (1) ; ...; head (n) ] means concatenation in the last dimension. Usu-</p><formula xml:id="formula_5">ally d k × n = d, which means the output of [head (1) ; ...; head (n) ] will be of size R l×d . W o is a learnable parameter, which is of size R d×d .</formula><p>The output of the multi-head attention will be further processed by the position-wise feedforward networks, which can be represented as follows,</p><formula xml:id="formula_6">FFN(x) = max(0, xW1 + b1)W2 + b2,<label>(7)</label></formula><p>where</p><formula xml:id="formula_7">W 1 , W 2 , b 1 , b 2 are learnable parameters, and W 1 ∈ R d×d f f , W 2 ∈ R d f f ×d , b 1 ∈ R d f f , b 2 ∈ R d . d f f is a hyper-parameter.</formula><p>Other components of the Transformer encoder includes layer normalization and Residual connection, we use them the same as <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Position Embedding</head><p>The self-attention is not aware of the positions of different tokens, making it unable to capture the sequential characteristic of languages. In order to solve this problem, <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> suggested to use position embeddings generated by sinusoids of varying frequency. The tth token's position embedding can be represented by the following equations</p><formula xml:id="formula_8">P Et,2i = sin(t/10000 2i/d ),<label>(8)</label></formula><formula xml:id="formula_9">P Et,2i+1 = cos(t/10000 2i/d ),<label>(9)</label></formula><p>where i is in the range of [0, d 2 ], d is the input dimension. This sinusoid based position embedding makes Transformer have an ability to model the position of a token and the distance of each two tokens. For any fixed offset k, P E t+k can be represented by a linear transformation of P E t <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref>.  <ref type="figure">Figure 2</ref>: Model structure of TENER for English NER tasks. In TENER, Transformer encoder is used not only to extract the word-level contextual information, but also to encode character-level information in a word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Model</head><p>In this paper, we utilize the Transformer encoder to model the long-range and complicated interactions of sentence for NER. The structure of proposed model is shown in <ref type="figure">Fig 2.</ref> We detail each parts in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Embedding Layer</head><p>To alleviate the problems of data sparsity and outof-vocabulary (OOV), most NER models adopted the CNN character encoder <ref type="bibr" target="#b22">(Ma and Hovy, 2016;</ref><ref type="bibr" target="#b39">Ye and Ling, 2018;</ref><ref type="bibr" target="#b3">Chen et al., 2019)</ref> to represent words. Compared to BiLSTM based character encoder <ref type="bibr" target="#b18">(Lample et al., 2016;</ref><ref type="bibr" target="#b10">Ghaddar and Langlais, 2018)</ref>, CNN is more efficient. Since Transformer can also fully exploit the GPU's parallelism, it is interesting to use Transformer as the character encoder. A potential benefit of Transformer-based character encoder is to extract different n-grams and even uncontinuous character patterns, like "un..ily" in "unhappily" and "uneasily". For the model's uniformity, we use the "adapted Transformer" to represent the Transformer introduced in next subsection.</p><p>The final word embedding is the concatenation of the character features extracted by the character encoder and the pre-trained word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoding Layer with Adapted Transformer</head><p>Although Transformer encoder has potential advantage in modeling long-range context, it is not working well for NER task. In this paper, we propose an adapted Transformer for NER task with two improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Direction-and Distance-Aware Attention</head><p>Inspired by the success of BiLSTM in NER tasks, we consider what properties the Transformer lacks compared to BiLSTM-based models. One observation is that BiLSTM can discriminatively collect the context information of a token from its left and right sides. But it is not easy for the Transformer to distinguish which side the context information comes from. Although the dot product between two sinusoidal position embeddings is able to reflect their distance, it lacks directionality and this property will be broken by the vanilla Transformer attention. To illustrate this, we first prove two properties of the sinusoidal position embeddings. Property 1. For an offset k and a position t, P E T t+k P E t only depends on k, which means the dot product of two sinusoidal position embeddings can reflect the distance between two tokens.</p><p>Proof. Based on the definitions of Eq.(8) and Eq.(9), the position embedding of t-th token is</p><formula xml:id="formula_10">P Et =        sin(c0t) cos(c0t)</formula><p>. . .</p><formula xml:id="formula_11">sin(c d 2 −1 t) cos(c d 2 −1 t)        ,<label>(10)</label></formula><p>where d is the dimension of the position embedding, c i is a constant decided by i, and its value is 1/10000 2i/d . Therefore,</p><formula xml:id="formula_12">P E T t P E t+k = d 2 −1 j=0</formula><p>[sin(cjt) sin(cj(t + k)) + cos(cjt) cos(cj(t + k))]</p><formula xml:id="formula_13">= d 2 −1 j=0 cos(cj(t − (t + k))) (12) = d 2 −1 j=0 cos(cjk),<label>(11)</label></formula><p>where Eq.(11) to Eq. <ref type="formula" target="#formula_0">(12)</ref> is based on the equation cos(x − y) = sin(x) sin(y) + cos(x) cos(y). <ref type="figure">Figure 3</ref>: Dot product between two sinusoidal position embeddings whose distance is k. It is clear that the product is symmetrical, and with the increment of |k|, it has a trend to decrease, but this decrease is not monotonous. <ref type="figure">Figure 4</ref>: The upper line is the product between P E T t P E t+k . The lower two lines are the products of P E T t W P E t+k with two random W s. Although P E T t P E t+k can reflect the distance, the P E T t W P E t+k has no clear pattern.</p><p>Property 2. For an offset k and a position t, P E T t P E t−k = P E T t P E t+k , which means the sinusoidal position embeddings is unware of directionality.</p><p>Proof. Let j = t − k, according to property 1, we have</p><formula xml:id="formula_15">P E T t P E t+k = P E T j P E j+k<label>(14)</label></formula><p>= P E T t−k P Et.</p><p>The relation between d, k and P E T t P E t+k is displayed in <ref type="figure">Fig 3.</ref> The sinusoidal position embeddings are distance-aware but lacks directionality.</p><p>However, the property of distance-awareness also disappears when P E t is projected into the query and key space of self-attention. Since in vanilla Transformer the calculation between P E t and P E t+k is actually P E T t W T q W k P E t+k , where W q , W k are parameters in Eq.(1). Mathematically, it can be viewed as P E T t W P E t+k with only one parameter W . The relation between P E T t P E t+k and P E T t W P E t+k is depicted in <ref type="figure">Fig 4</ref>. Therefore, to improve the Transformer with direction-and distance-aware characteristic, we calculate the attention scores using the equations below:</p><formula xml:id="formula_17">Q, K, V = HWq, H d k , HWv,<label>(16)</label></formula><formula xml:id="formula_18">Rt−j = [. . . sin( t − j 10000 2i/d k ) cos( t − j 10000 2i/d k ) . . .] T ,<label>(17)</label></formula><formula xml:id="formula_19">A rel t,j = QtK T j + QtR T t−j + uK T j + vR T t−j ,<label>(18)</label></formula><formula xml:id="formula_20">Attn(Q, K, V ) = softmax(A rel )V,<label>(19)</label></formula><p>where t is index of the target token, j is the index of the context token, Q t , K j is the query vector and key vector of token t, j respectively, <ref type="formula" target="#formula_0">(18)</ref> is the attention score between two tokens; Q T t R t−j is the tth token's bias on certain relative distance; u T K j is the bias on the jth token; v T R t−j is the bias term for certain distance and direction.</p><formula xml:id="formula_21">W q , W v ∈ R d×d k . To get H d k ∈ R l×d k , we first split H into d/d k partitions in the second dimension, then for each head we use one parti- tion. u ∈ R d k , v ∈ R d k are learnable parame- ters, R t−j is the relative positional encoding, and R t−j ∈ R d k , i in Eq.(17) is in the range [0, d k 2 ]. Q T t K j in Eq.</formula><p>Based on Eq.(17), we have</p><formula xml:id="formula_22">Rt, R−t =        sin(c0t) cos(c0t) . . . sin(c d 2 −1 t) cos(c d 2 −1 t)        ,        − sin(c0t) cos(c0t) . . . − sin(c d 2 −1 t) cos(c d 2 −1 t)        ,<label>(20)</label></formula><p>because sin(−x) = − sin(x), cos(x) = cos(−x). This means for an offset t, the forward and backward relative positional encoding are the same with respect to the cos(c i t) terms, but is the opposite with respect to the sin(c i t) terms. Therefore, by using R t−j , the attention score can distinguish different directions and distances. The above improvement is based on the work <ref type="bibr" target="#b31">(Shaw et al., 2018;</ref>. Since the size of NER datasets is usually small, we avoid direct multiplication of two learnable parameters, because they can be represented by one learnable parameter. Therefore we do not use W k in Eq.(16). The multi-head version is the same as Eq. <ref type="formula" target="#formula_4">(6)</ref>, but we discard W o since it is directly multiplied by W 1 in Eq.(7). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Un-scaled Dot-Product Attention</head><p>The vanilla Transformer use the scaled dotproduct attention to smooth the output of softmax function. In Eq. <ref type="formula" target="#formula_2">(3)</ref>, the dot product of key and value matrices is divided by the scaling factor √ d k .</p><p>We empirically found that models perform better without the scaling factor √ d k . We presume this is because without the scaling factor the attention will be sharper. And the sharper attention might be beneficial in the NER task since only few words in the sentence are named entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CRF Layer</head><p>In order to take advantage of dependency between different tags, the Conditional Random Field (CRF) was used in all of our models. Given a sequence s = [s 1 , s 2 , ..., s T ], the corresponding golden label sequence is y = [y 1 , y 2 , ..., y T ], and Y(s) represents all valid label sequences. The probability of y is calculated by the following equation</p><formula xml:id="formula_23">P (y|s) = T t=1 e f (y t−1 ,y t ,s) Y(s) y T t=1 e f (y t−1 ,y t ,s) ,<label>(21)</label></formula><p>where f (y t−1 , y t , s) computes the transition score from y t−1 to y t and the score for y t . The optimization target is to maximize P (y|s). When decoding, the Viterbi Algorithm is used to find the path achieves the maximum probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>We evaluate our model in two English NER datasets and four Chinese NER datasets.</p><p>(1) CoNLL2003 is one of the most evaluated English NER datasets, which contains four differ-ent named entities: <ref type="bibr">PERSON, LOCATION, OR-GANIZATION, and MISC (Sang and Meulder, 2003)</ref>.</p><p>(2) OntoNotes 5.0 is an English NER dataset whose corpus comes from different domains, such as telephone conversation, newswire. We exclude the New Testaments portion since there is no named entity in it <ref type="bibr" target="#b3">(Chen et al., 2019;</ref><ref type="bibr" target="#b4">Chiu and Nichols, 2016)</ref>. This dataset has eleven entity names and seven value types, like CARDINAL, MONEY, LOC.</p><p>(3) Weischedel <ref type="formula" target="#formula_0">(2011)</ref> released OntoNotes 4.0. In this paper, we use the Chinese part. We adopted the same pre-process as <ref type="bibr" target="#b2">(Che et al., 2013)</ref>.</p><p>(4) The corpus of the Chinese NER dataset MSRA came from news domain <ref type="bibr" target="#b19">(Levow, 2006)</ref>.</p><p>(5) Weibo NER was built based on text in Chinese social media Sina Weibo <ref type="bibr" target="#b25">(Peng and Dredze, 2015)</ref>, and it contained 4 kinds of entities.</p><p>(6) Resume NER was annotated by .</p><p>Their statistics are listed in <ref type="table" target="#tab_0">Table 1</ref>. For all datasets, we replace all digits with "0", and use the BIOES tag schema. For English, we use the Glove 100d pre-trained embedding <ref type="bibr" target="#b26">(Pennington et al., 2014)</ref>. For the character encoder, we use 30d randomly initialized character embeddings. More details on models' hyper-parameters can be found in the supplementary material. For Chinese, we used the character embedding and bigram embedding released by . All pretrained embeddings are finetuned during training. In order to reduce the impact of randomness, we ran all of our experiments at least three times, and its average F1 score and standard deviation are reported.</p><p>We used random-search to find the optimal hyper-parameters, hyper-parameters and their ranges are displayed in the supplemental material. We use SGD and 0.9 momentum to optimize the model. We run 100 epochs and each batch has 16 samples. During the optimization, we use the triangle learning rate <ref type="bibr" target="#b32">(Smith, 2017)</ref> where the learning rate rises to the pre-set learning rate at the first 1% steps and decreases to 0 in the left 99% steps. The model achieves the highest development performance was used to evaluate the test set. The hyper-parameter search range and other settings can be found in the supplementary material. Codes are available at https://github. com/fastnlp/TENER.    and <ref type="bibr" target="#b11">(Gui et al., 2019a)</ref>, respectively. "w/ scale" means TENER using the scaled attention in <ref type="figure">Eq.(19)</ref>. * their results are not directly comparable with ours, since they used 100d pre-trained character and bigram embeddings. Other models use the same embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on Chinese NER Datasets</head><p>We first present our results in the four Chinese NER datasets. Since Chinese NER is directly based on the characters, it is more straightforward to show the abilities of different models without considering the influence of word representation. As shown in <ref type="table" target="#tab_2">Table 2</ref>, the vanilla Transformer does not perform well and is worse than the BiL-STM and CNN based models. However, when relative positional encoding combined, the performance was enhanced greatly, resulting in better results than the BiLSTM and CNN in all datasets. The number of training examples of the Weibo dataset is tiny, therefore the performance of the Transformer is abysmal, which is as expected since the Transformer is data-hungry. Nevertheless, when enhanced with the relative positional encoding and unscaled attention, it can achieve even better performance than the BiLSTM-based model. The superior performance of the adapted Transformer in four datasets ranging from small datasets to big datasets depicts that the adapted Transformer is more robust to the number of training examples than the vanilla Transformer. As the last line of Table 2 depicts, the scaled attention will deteriorate the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on English NER datasets</head><p>The comparison between different NER models on English NER datasets is shown in <ref type="table" target="#tab_4">Table 3</ref>. The poor performance of the Transformer in the NER datasets was also reported by <ref type="bibr" target="#b13">(Guo et al., 2019)</ref>. Although performance of the Transformer is higher than <ref type="bibr" target="#b13">(Guo et al., 2019)</ref>, it still lags behind the BiLSTM-based models <ref type="bibr" target="#b22">(Ma and Hovy, 2016)</ref>. Nonetheless, the performance is massively enhanced by incorporating the relative positional encoding and unscaled attention into the Transformer. The adaptation not only makes the Transformer achieve superior performance than BiL-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>CoNLL2003 OntoNotes 5.0</p><p>BiLSTM-CRF <ref type="bibr" target="#b16">(Huang et al., 2015)</ref> 88.83 <ref type="bibr">-BiLSTM-CRF (Chiu and Nichols, 2016)</ref> 90.91 ± 0.20 86.12 ± 0.22</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN</head><p>BiLSTM-BiLSTM-CRF <ref type="bibr" target="#b18">(Lample et al., 2016)</ref> 90.94 CNN-BiLSTM-CRF <ref type="bibr" target="#b22">(Ma and Hovy, 2016)</ref> 91.21 ID-CNN <ref type="bibr" target="#b33">(Strubell et al., 2017)</ref> 90.54 ± 0.18 86.84 ± 0.19 LM-LSTM-CRF  91.24 ± 0.12 <ref type="bibr">CRF+HSCRF (Ye and Ling, 2018)</ref> 91.26 ± 0.1</p><p>BiLSTM-BiLSTM-CRF <ref type="bibr" target="#b1">(Akhundov et al., 2018)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>91.11</head><p>LS+BiLSTM-CRF <ref type="bibr" target="#b10">(Ghaddar and Langlais, 2018)</ref> 90.52 ± 0.20 86.57 ± 0.1 CN 3  91.1 GRN <ref type="bibr" target="#b3">(Chen et al., 2019)</ref> 91    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis of Different Character Encoders</head><p>The character-level encoder has been widely used in the English NER task to alleviate the data sparsity and OOV problem in word representation. In this section, we cross different character-level encoders (BiLSTM, CNN, Transformer encoder and our adapted Transformer encoder (AdaTrans for short) ) and different word-level encoders (BiL-STM, ID-CNN and AdaTrans) to implement the NER task. Results on CoNLL2003 and OntoNotes 5.0 are presented in <ref type="table" target="#tab_6">Table 5a</ref> and <ref type="table" target="#tab_6">Table 5b</ref>, respectively.</p><p>The ID-CNN encoder is from <ref type="bibr" target="#b33">(Strubell et al., 2017)</ref>, and we re-implement their model in Py-Torch. For different combinations, we use random search to find its best hyper-parameters. Hyperparameters for character encoders were fixed. The details can be found in the supplementary material.</p><p>For the results on CoNLL2003 dataset which is depicted in <ref type="table" target="#tab_6">Table 5a</ref>, the AdaTrans performs as good as the BiLSTM in different character en-coder scenario averagely. In addition, from Table 5b, we can find the pattern that the AdaTrans character encoder outpaces the BiLSTM and CNN character encoders when different word-level encoders being used. Moreover, no matter what character encoder being used or none being used, the AdaTrans word-level encoder gets the best performance. This implies that when the number of training examples increases, the AdaTrans character-level and word-level encoder can better realize their ability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Convergent Speed Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose TENER, a model adopting Transformer Encoder with specific customizations for the NER task. Transformer Encoder has a powerful ability to capture the long-range context. In order to make the Transformer more suitable to the NER task, we introduce the direction-aware, distance-aware and un-scaled attention. Experiments in two English NER tasks and four Chinese NER tasks show that the performance can be massively increased. Under the same pretrained embeddings and external knowledge, our proposed modification outperforms previous models in the six datasets. Meanwhile, we also found the adapted Transformer is suitable for being used as the English character encoder, because it has the potentiality to extract intricate patterns from char-acters. Experiments in two English NER datasets show that the adapted Transformer character encoder performs better than BiLSTM and CNN character encoders.</p><p>6 Supplemental Material</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Character Encoder</head><p>We exploit four kinds of character encoders. For all character encoders, the randomly initialized character embeddings are 30d. The hidden size of BiLSTM used in the character encoder is 50d in each direction. The kernel size of CNN used in the character encoder is 3, and we used 30 kernels with stride 1. For Transformer and adapted Transformer, the number of heads is 3, and every head is 10d, the dropout rate is 0.15, the feedforward dimension is 60. The Transformer used the sinusoid position embedding. The number of parameters for the character encoder (excluding character embedding) when using BiLSTM, CNN, Transformer and adapted Transformer are 35830, 3660, 8460 and 6600 respectively. For all experiments, the hyper-parameters of character encoders stay unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Hyper-parameters</head><p>The hyper-parameters and search ranges for different encoders are presented in <ref type="table">Table 6</ref>, <ref type="table" target="#tab_8">Table 7</ref> and <ref type="table" target="#tab_9">Table 8</ref>.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Convergent speed in the development dataset of OntoNotes 5.0 for four kinds of models. We compare the convergent speed of BiLSTM, ID-CNN, Transformer, and TENER in the development set of the OntoNotes 5.0. The curves are shown in Fig 5. TENER converges as fast as the BiLSTM model and outperforms the vanilla Transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Details of Datasets.</figDesc><table><row><cell></cell><cell>Dataset</cell><cell>Type</cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>English</cell><cell>CoNLL2003 OntoNotes 5.0</cell><cell cols="4">Sentence 14.0k 3.2k 3.5k Token 203.6k 51.4k 46.4k Sentence 59.9k 8.5k 8.3k Token 1088.5k 147.7k 152.7k</cell></row><row><cell></cell><cell>OntoNotes 4.0</cell><cell cols="4">Sentence 15.7k 4.3k 4.3k Token 491.9k 200.5k 208.1k</cell></row><row><cell>Chinese</cell><cell>MSRA Weibo</cell><cell cols="4">Sentence 46.4k 4.4k 4.4k Token 2169.9k 172.6k 172.6k Sentence 1.4k 0.3k 0.3k Token 73.5k 14.4k 14.8k</cell></row><row><cell></cell><cell>Resume</cell><cell>Sentence Token</cell><cell cols="3">3.8k 0.5k 0.5k 124.1k 13.9k 15.1k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>± 0.78 93.43 ± 0.26 66.49 ± 0.30 88.35 ± 0.60 TENER(Ours) 58.17 ± 0.22 95.00 ± 0.25 72.43 ± 0.39 92.74 ± 0.27 w/ scale 57.40 ± 0.3 94.00 ± 0.51 71.72 ± 0.08 91.67 ± 0.23</figDesc><table><row><cell>Models</cell><cell>Weibo</cell><cell>Resume</cell><cell cols="2">OntoNotes4.0 MSRA</cell></row><row><cell>BiLSTM ♣</cell><cell>56.75</cell><cell>94.41</cell><cell>71.81</cell><cell>91.87</cell></row><row><cell>ID-CNN ♠</cell><cell>-</cell><cell>93.75</cell><cell>62.25</cell><cell>-</cell></row><row><cell cols="2">CAN-NER  *  (Zhu and Wang, 2019) 59.31</cell><cell>94.94</cell><cell>73.64</cell><cell>92.97</cell></row><row><cell>Transformer</cell><cell>46.38</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The F1 scores on Chinese NER datasets. ♣ , ♠ are results reported in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: The F1 scores on English NER datasets. We</cell></row><row><cell>only list results based on non-contextualized embed-</cell></row><row><cell>dings, and methods utilized pre-trained language mod-</cell></row><row><cell>els, pre-trained features, or higher dimension word vec-</cell></row><row><cell>tors are excluded. TENER (Ours) uses the Trans-</cell></row><row><cell>former encoder both in the character-level and word-</cell></row><row><cell>level. "w/ scale" means TENER using the scaled atten-</cell></row><row><cell>tion in Eq.(19). "w/ CNN-char" means TENER using</cell></row><row><cell>CNN as character encoder instead of AdaTrans.</cell></row><row><cell>STM based models, but also unveil the new state-</cell></row><row><cell>of-the-art performance in two NER datasets when</cell></row><row><cell>only the Glove 100d embedding and CNN char-</cell></row><row><cell>acter embedding are used. The same deteriora-</cell></row><row><cell>tion of performance was observed when using the</cell></row><row><cell>scaled attention. Besides, if ELMo was used (Pe-</cell></row><row><cell>ters et al., 2018), the performance of TENER can</cell></row><row><cell>be further boosted as depicted in Table 4.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance of models with ELMo as their embeddings in English NER datasets. "BiLSTM" is our run. In the larger OntoNotes5.0, TENER achieves much better F1 score.</figDesc><table><row><cell>Char</cell><cell cols="2">Word BiLSTM</cell><cell>ID-CNN</cell><cell>AdaTrans</cell></row><row><cell cols="2">No Char</cell><cell cols="3">88.34 ± 0.32 87.30 ± 0.15 88.37 ± 0.27</cell></row><row><cell cols="2">BiLSTM</cell><cell cols="3">91.32 ± 0.13 89.99 ± 0.14 91.29 ± 0.12</cell></row><row><cell>CNN</cell><cell></cell><cell cols="3">91.22 ± 0.10 90.17 ± 0.02 91.45 ± 0.07</cell></row><row><cell cols="5">Transformer 91.12 ± 0.10 90.05 ± 0.13 91.23 ± 0.06</cell></row><row><cell cols="2">AdaTrans</cell><cell cols="3">91.38 ± 0.15 89.99 ± 0.05 91.33 ± 0.05</cell></row><row><cell></cell><cell></cell><cell cols="2">(a) CoNLL2003</cell></row><row><cell>Char</cell><cell cols="2">Word BiLSTM</cell><cell>ID-CNN</cell><cell>AdaTrans</cell></row><row><cell cols="2">No Char</cell><cell cols="3">85.20 ± 0.23 84.26 ± 0.07 85.80 ± 0.10</cell></row><row><cell cols="2">BiLSTM</cell><cell cols="3">87.85 ± 0.09 87.38 ± 0.17 88.12 ± 0.16</cell></row><row><cell>CNN</cell><cell></cell><cell cols="3">87.79 ± 0.14 87.10 ± 0.06 88.25 ± 0.11</cell></row><row><cell cols="5">Transformer 88.01 ± 0.06 87.31 ± 0.10 88.20 ± 0.07</cell></row><row><cell cols="2">AdaTrans</cell><cell cols="3">88.12 ± 0.17 87.51 ± 0.11 88.43 ± 0.12</cell></row><row><cell></cell><cell></cell><cell cols="2">(b) OntoNotes 5.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell>: F1 scores in the CoNLL2003 and OntoNotes</cell></row><row><cell>5.0.</cell><cell>"Char" means character-level encoder, and</cell></row><row><cell cols="2">"Word" means word-level encoder. "AdaTrans" means</cell></row><row><cell cols="2">our adapted Transformer encoder.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>The hyper-parameters and hyper-parameter search ranges for ID-CNN.</figDesc><table><row><cell></cell><cell>Chinese</cell><cell>English</cell></row><row><cell>number of layers</cell><cell>[1, 2]</cell><cell>[1, 2]</cell></row><row><cell>number of head</cell><cell>[4, 6, 8, 10]</cell><cell>[8, 10, 12, 14]</cell></row><row><cell>head dimension</cell><cell cols="2">[32, 48, 64, 80, 96] [64, 80, 96, 112, 128]</cell></row><row><cell>learning rate</cell><cell>[1e-3, 5e-4, 7e-4]</cell><cell>[9e-4, 7e-4, 5e-4]</cell></row><row><cell>transformer dropout</cell><cell>0.15</cell><cell>0.15</cell></row><row><cell>fc dropout</cell><cell>0.4</cell><cell>0.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>The hyper-parameters and hyper-parameter search ranges for Transformer and adapted Transformer in Chinese and English NER datasets.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Sequence labeling: A practical approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adnan</forename><surname>Akhundov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Trautmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Groh</surname></persName>
		</author>
		<idno>abs/1808.03926</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Named entity recognition with bilingual constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="52" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">GRN: Gated relation network to enhance convolutional neural network for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijia</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borje</forename><surname>Karlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Named entity recognition with bidirectional lstm-cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Viet Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Characterbased LSTM-CRF with radical-level features for chinese named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanhai</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Hattori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Di</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NLPCC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Applying named entity recognition and co-reference resolution for segmenting english texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlina</forename><surname>Fragkou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="325" to="346" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Robust lexical features for improved neural network named-entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Ghaddar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Langlais</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03489</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cnn-based chinese NER with lexicon rethinking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruotian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lujun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4982" to="4988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A lexicon-based graph neural network for chinese ner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicheng</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Startransformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1315" to="1325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Music transformer: Generating music with long-term structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Zhi Anna</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<meeting>the Eighteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The third international chinese language processing bakeoff: Word segmentation and named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gina-Anne</forename><surname>Levow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Workshop on Chinese Language Processing</title>
		<meeting>the Fifth Workshop on Chinese Language Processing<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07-22" />
			<biblScope unit="page" from="108" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Empower sequence labeling with task-aware neural language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">Fangzheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5253" to="5260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Contextualized non-local neural networks for sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaichen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie Chi Kit</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6762" to="6769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.00770</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Named entity recognition for chinese social media with jointly trained embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="548" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NACCL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1756" to="1765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cyclical learning rates for training neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting><address><addrLine>Santa Rosa, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-03-24" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Fast and accurate entity recognition with iterated dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02098</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Ontonotes release 4.0</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning better internal structure of words for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhuti</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-David</forename><surname>Ruvini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2584" to="2593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Five-stroke based cnn-birnn-crf network for chinese named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongshen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanrong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NLPCC</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="184" to="195" />
		</imprint>
	</monogr>
	<note>English number of layers [1, 2] hidden size [200, 400, 600, 800, 1200] learning rate [0.01, 0.007, 0.005</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Table 6: The hyper-parameters and hyper-parameter search ranges for BiLSTM</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Hybrid semimarkov CRF for neural sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Zhi-Xiu Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03838</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sentencestate LSTM for text representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="317" to="327" />
		</imprint>
	</monogr>
	<note>ACL (1)</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Chinese NER using lattice LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1554" to="1564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural question generation from text: A preliminary study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NLPCC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="662" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">CAN-NER: convolutional attention network for chinese named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuying</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3384" to="3393" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
