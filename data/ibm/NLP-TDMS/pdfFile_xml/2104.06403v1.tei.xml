<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lite-HRNet: A Lightweight High-Resolution Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Image Processing and Intelligent Control School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Image Processing and Intelligent Control School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
							<email>nsang@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Image Processing and Intelligent Control School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Lite-HRNet: A Lightweight High-Resolution Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an efficient high-resolution network, Lite-HRNet, for human pose estimation. We start by simply applying the efficient shuffle block in ShuffleNet to HRNet (high-resolution network), yielding stronger performance over popular lightweight networks, such as MobileNet, ShuffleNet, and Small HRNet.</p><p>We find that the heavily-used pointwise (1 × 1) convolutions in shuffle blocks become the computational bottleneck. We introduce a lightweight unit, conditional channel weighting, to replace costly pointwise (1 × 1) convolutions in shuffle blocks. The complexity of channel weighting is linear w.r.t the number of channels and lower than the quadratic time complexity for pointwise convolutions. Our solution learns the weights from all the channels and over multiple resolutions that are readily available in the parallel branches in HRNet. It uses the weights as the bridge to exchange information across channels and resolutions, compensating the role played by the pointwise (1 × 1) convolution. Lite-HRNet demonstrates superior results on human pose estimation over popular lightweight networks. Moreover, Lite-HRNet can be easily applied to semantic segmentation task in the same lightweight manner. The code and models have been publicly available at https://github.com/HRNet/Lite-HRNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation requires high-resolution representation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45]</ref> to achieve high performance. Motivated by the increasing demand for model efficiency, this paper studies the problem of developing efficient highresolution models under computation-limited resources.</p><p>Existing efficient networks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b52">53]</ref> are mainly designed from two perspectives. One is to borrow the design from classification networks, such as MobileNet <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16]</ref> and ShuffleNet <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b56">57]</ref>, to reduce the redundancy in matrixvector multiplication, where convolution operations dominate the cost. The other is to mediate the spatial information loss with various tricks, such as encoder-decoder architectures <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26]</ref>, and multi-branch architectures <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b58">59]</ref>.</p><p>We first study a naive lightweight network by simply combining the shuffle block in ShuffleNet and the highresolution design pattern in HRNet <ref type="bibr" target="#b40">[41]</ref>. HRNet has shown a stronger capability among large models in positionsensitive problems, e.g., semantic segmentation, human pose estimation, and object detection. It remains unclear whether high resolution helps for small models. We empirically show that the direct combination outperforms Shuf-fleNet, MobileNet, and Small HRNet 1 .</p><p>To further achieve higher efficiency, we introduce an efficient unit, named conditional channel weighting, performing information exchange across channels, to replace the costly pointwise (1 × 1) convolution in a shuffle block. The channel weighting scheme is very efficient: the complexity is linear w.r.t the number of channels and lower than the quadratic time complexity for the pointwise convolution. For example, with the multi-resolution features of 64 × 64 × 40 and 32 × 32 × 80, the conditional channel weighting unit can reduce the shuffle block's whole computation complexity by 80%.</p><p>Unlike the regular convolutional kernel weights learned as model parameters, the proposed scheme weights are conditioned on the input maps and computed across channels through a lightweight unit. Thus, they contain the information in all the channel maps and serve as a bridge to exchange information through channel weighting. Furthermore, we compute the weights from parallel multiresolution channel maps that are readily available HRNet so that the weights contain richer information and are strengthened. We call the resulting network, Lite-HRNet.</p><p>The experimental results show that Lite-HRNet outperforms the simple combination of shuffle blocks and HRNet (which we call naive Lite-HRNet). We believe that the superiority is because the computational complexity reduction is more significant than the loss of information exchange in the proposed conditional channel weighting scheme.</p><p>Our main contributions include:</p><p>• We simply apply the shuffle blocks to HRNet, leading a lightweight network naive Lite-HRNet. We empirically show superior performance over MobileNet, ShuffleNet, and Small HRNet. • We present an improved efficient network, Lite-HRNet. The key point is that we introduce an efficient conditional channel weighting unit to replace the costly 1 × 1 convolution in shuffle blocks, and the weights are computed across channels and resolutions. • Lite-HRNet is the state-of-the-art in terms of complexity and accuracy trade-off on COCO and MPII human pose estimation and easily generalized to semantic segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Efficient blocks for classification. Separable convolutions and group convolutions have been increasingly popular in lightweight networks, such as MobileNet <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b15">16]</ref>, IGCV3 <ref type="bibr" target="#b36">[37]</ref>, and ShuffleNet <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b27">28]</ref>. Xception <ref type="bibr" target="#b8">[9]</ref> and MobileNetV1 <ref type="bibr" target="#b16">[17]</ref> disentangle one normal convolution into depthwise convolution and pointwise convolution. Mo-bileNetV2 and IGCV3 <ref type="bibr" target="#b36">[37]</ref> further combine linear bottlenecks that are about low-rank kernels. MixNet <ref type="bibr" target="#b38">[39]</ref> applies mixed kernels on the depthwise convolutions. Effi-cientHRNet <ref type="bibr" target="#b29">[30]</ref> introduces the mobile convolutions into HigherHRNet <ref type="bibr" target="#b7">[8]</ref>.</p><p>The information across channels are blocked in group convolutions and depthwise convolutions. The pointwise convolutions are heavily used to address it but are very costly in lightweight network design. To reduce the complexity, grouping 1 × 1 convolutions with channel shuffling <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b27">28]</ref> or interleaving <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b36">37</ref>] are used to keep information exchange across channels. Our proposed solution is a lightweight manner performing information exchange across channels to replace costly 1×1 convolutions. Mediating spatial information loss. The computation complexity is positively related to spatial resolution. Reducing the spatial resolution with mediating spatial information loss is another way to improve efficiency. Encoderdecoder architecture is used to recover the spatial resolution, such as ENet <ref type="bibr" target="#b33">[34]</ref> and SegNet <ref type="bibr" target="#b1">[2]</ref>. ICNet <ref type="bibr" target="#b59">[60]</ref> applies different computations to different resolution inputs to reduce the whole complexity. BiSeNet <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b49">50]</ref> decouples the detail information and context information with different lightweight sub-networks. Our solution follows the high- resolution pattern in HRNet to maintain the high-resolution representation through the whole process.</p><p>Convolutional weight generation and mixing. Dynamic filter networks <ref type="bibr" target="#b20">[21]</ref> dynamically generates the convolution filters conditioned on the input. Meta-Network <ref type="bibr" target="#b28">[29]</ref> adopts a meta-learner to generate weights to learn crosstask knowledge. CondINS <ref type="bibr" target="#b39">[40]</ref> and SOLOV2 <ref type="bibr" target="#b42">[43]</ref> apply this design to the instance segmentation task, generating the parameters of the mask sub-network for each instance. CondConv <ref type="bibr" target="#b47">[48]</ref> and Dynamic Convolution <ref type="bibr" target="#b4">[5]</ref> learn a series of weights to mix the corresponding convolution kernels for each sample, increasing the model capacity.</p><p>Attention mechanism <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b53">54]</ref> can be regarded as a kind of conditional weight generation. SENet <ref type="bibr" target="#b18">[19]</ref> uses global information to learn the weights to excite or suppress the channel maps. GENet <ref type="bibr" target="#b17">[18]</ref> expands on this by gathering local information to exploit the contextual dependencies. CBAM <ref type="bibr" target="#b43">[44]</ref> exploits the channel and spatial attention to refine the features.</p><p>The proposed conditional channel weighting scheme can be, in some sense, regarded as a conditional channel-wise 1 × 1 convolution. Besides its cheap computation, we exploit an extra effect and use the conditional weights as the bridge to exchange information across channels.</p><p>Conditional architecture. Different from normal networks, conditional architecture can achieve dynamic width, depth, or kernels. SkipNet <ref type="bibr" target="#b41">[42]</ref> uses a gated network to skip some convolutional blocks to reduce complexity selectively. Spatial Transform Networks <ref type="bibr" target="#b19">[20]</ref> learn to warp the <ref type="figure">Figure 2</ref>. Illustration of the Small HRNet architecture. It consists of a high-resolution stem as the first stage, gradually adding high-to-low resolution streams as the main body. The main body has a sequence of stages, each containing parallel multi-resolution streams and repeated multi-resolution fusions. The details are given in <ref type="bibr">Section 3.</ref> feature map conditioned on the input. Deformable Convolution <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b60">61]</ref> learns the offsets for the convolution kernels conditioned on each spatial location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Naive Lite-HRNet</head><p>Shuffle blocks. The shuffle block in ShuffleNet V2 <ref type="bibr" target="#b27">[28]</ref> first splits the channels into two partitions. One partition passes through a sequence of 1×1 convolution, 3×3 depthwise convolution, and 1 × 1 convolution, and the output is concatenated with the other partition. Finally, the concatenated channels are shuffled, as illustrated in <ref type="figure" target="#fig_0">Figure 1 (a)</ref>. HRNet. The HRNet <ref type="bibr" target="#b40">[41]</ref> starts from a high-resolution convolution stem as the first stage, gradually adding high-tolow resolution streams one by one as new stages. The multiresolution streams are connected in parallel. The main body consists of a sequence of stages. In each stage, the information across resolutions is exchanged repeatedly. We follow the Small HRNet design 2 and use fewer layers and smaller width to form our network. The stem of Small HRNet consists of two 3 × 3 convolutions with stride 2. Each stage in the main body contains a sequence of residual blocks and one multi-resolution fusion. <ref type="figure">Figure 2</ref> illustrates the structure of Small HRNet. Simple combination. We adopt the shuffle block to replace the second 3 × 3 convolution in the stem of Small HR-Net, and replace all the normal residual blocks (formed with two 3 × 3 convolutions). The normal convolutions in the multi-resolution fusion are replaced by the separable convolutions <ref type="bibr" target="#b8">[9]</ref>, resulting in a naive Lite-HRNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Lite-HRNet</head><p>1 × 1 convolution is costly. The 1 × 1 convolution performs a matrix-vector multiplication at each position:</p><formula xml:id="formula_0">Y = W ⊗ X,<label>(1)</label></formula><p>where X and Y are input and output maps, and W is the 1 × 1 convolutional kernel. It serves a critical role of exchanging information across channels as the shuffle operation and the depthwise convolution have no effect on information exchange across channels. The 1 × 1 convolution is of quadratic time complexity (Θ(C 2 )) with respect to the number (C) of channels. The 3 × 3 depthwise convolution is of linear time complexity (Θ(9C) <ref type="bibr" target="#b2">3</ref> ). In the shuffle block, the complexity of two 1 × 1 convolutions is much higher than that of the depthwise convolution: Θ(2C 2 ) &gt; Θ(9C), for the usual case C &gt; 5. <ref type="table" target="#tab_1">Table 2</ref> shows an example of the complexity comparison between 1 × 1 convolutions and depthwise convolutions. Conditional channel weighting. We propose to use the element-wise weighting operation to replace the 1 × 1 convolution in naive Lite-HRNet, which has s branches in the sth stage. The element-wise weighting operation for the sth resolution branch is written as,</p><formula xml:id="formula_1">Y s = W s X s ,<label>(2)</label></formula><p>where W s is a weight map, a 3-d tensor of size W s × H s × C s , and is the element-wise multiplication operator.</p><p>The complexity is linear with respect to the channel number Θ(C), and much lower than 1 × 1 convolution in the shuffle block.</p><p>We compute the weights by using the channels for a single resolution and the channels across all the resolutions, as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (b), and show that the weights play a role of exchanging information across channels and resolutions. Cross-resolution weight computation. Considering the s-th stage, there are s parallel resolutions, and s weight maps W 1 , W 2 , . . . , W s , each for the corresponding resolution. We compute the s weight maps from all the channels across resolutions using a lightweight function H s (·),</p><formula xml:id="formula_2">(W 1 , W 2 , . . . , W s ) = H s (X 1 , X 2 , . . . , X s ),<label>(3)</label></formula><p>where {X 1 , . . . , X s } are the input maps for the s resolutions. X 1 corresponds to the highest resolution, and X s corresponds to the s-th highest resolution. We implement the lightweight function H s (·) as following. We perform adaptive average pooling (AAP) on</p><formula xml:id="formula_3">{X 1 , X 2 , . . . , X s−1 }: X 1 = AAP(X 1 ), X 2 = AAP(X 2 ), . . . , X s−1 = AAP(X s−1 )</formula><p>, in which the AAP pools any input size to a given output size W s × H s . Then we concatenate {X 1 , X 2 , . . . , X s−1 } and X s together, followed by a sequence of 1 × 1 convolution, ReLU, 1 × 1 convolution, and sigmoid, generating weight maps consisting of s partitions, W 1 , W 2 , . . . , W s (each for one resolution):  </p><formula xml:id="formula_4">(X 1 , X 2 , . . . , X s ) → Conv. → ReLU → Conv. → sigmoid → (W 1 , W 2 , . . . , W s ).<label>(4)</label></formula><formula xml:id="formula_5">(2C 2 s + NsCs) 0.25M CCW w/ multi-resolution weights 2( s 1 Cs) 2 + s 1 NsCs 0.26M CCW 2( s 1 Cs) 2 + 2 s 1 (C 2 s + NsCs) 0.51M</formula><p>Here, the weights at each position for each resolution depend on the channel feature at the same position from the average-pooled multi-resolution channel maps. This is why we call the scheme as cross-resolution weight computation. The s − 1 weight maps, W 1 , W 2 , . . . , W s−1 , are upsampled to the corresponding resolutions, outputting W 1 , W 2 , . . . , W s−1 , for the subsequent element-wise channel weighting. We show that the weight maps serves as a bridge for information exchange across channels and resolutions. Each element of the weight vector w si at the position i (from the weight map W s ) receives the information from all the input channels of all the s resolutions at the same pooling region, which is easily verified from the operations in <ref type="bibr">Equation 4</ref>. Through such a weight vector, each of the output channels at this position,</p><formula xml:id="formula_6">y si = w si x si ,<label>(5)</label></formula><p>receives the information from all the input channels at the same position across all the resolutions. In other words, the channel weighting scheme plays the role as well as the 1×1 convolution in terms of exchanging information.</p><p>On the other hand, the function H s (·) is applied on the small resolution, and thus the computation complexity is very light. <ref type="table" target="#tab_1">Table 2</ref> illustrates that the whole unit has much lower complexity than 1 × 1 convolution. Spatial weight computation. For each resolution, we also compute the spatial weights which are homogeneous to spatial positions: the weight vector w si at all positions are the same. The weights depend on all the pixels of the input channels in a single resolution:</p><formula xml:id="formula_7">w s = F s (X s ).<label>(6)</label></formula><p>Here, the function F s (·) is implemented as:</p><formula xml:id="formula_8">X s → GAP → FC → ReLU → FC → sigmoid → w s .</formula><p>The global average pooling (GAP) operator serves as a role of gathering the spatial information from all the positions. By weighting the channels with the spatial weights, y si = w s x si , each element in the output channels receives the contribution from all the positions of all the input channels. We compare the complexity between 1 × 1 convolutions and conditional channel weighting unit in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instantiation.</head><p>The Lite-HRNet consists of a highresolution stem and the main body to maintain the highresolution representation. The stem has one 3 × 3 convolution with stride 2 and a shuffle block, as the first stage. The main body has a sequence of modularized modules.  Each module consists of two conditional channel weighting blocks and one multi-resolution fusion. Each resolution branch's channel dimensions are C, 2C, 4C, and 8C, respectively. <ref type="table" target="#tab_0">Table 1</ref> describes the detailed structures.</p><p>Connection. The conditional channel weighting scheme shares the same philosophy to the conditional convolutions <ref type="bibr" target="#b47">[48]</ref>, dynamic filters <ref type="bibr" target="#b20">[21]</ref>, and squeeze-excitenetwork <ref type="bibr" target="#b18">[19]</ref>. Those works learn the convolution kernels or the mixture weights by sub-network conditioned on the input features for increasing the model capacity. Our method instead exploits an extra effect and uses the weights learned from all the channels as a bridge to exchange information across channels and resolutions. It can replace costly 1 × 1 convolutions in lightweight networks. Besides, we introduce multi-resolution information to boost weight learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our approach on two human pose estimation datasets, COCO <ref type="bibr" target="#b26">[27]</ref> and MPII <ref type="bibr" target="#b0">[1]</ref>. Following the state-of-the-art top-down framework, our approach estimates K heatmaps to indicate the keypoint location confidence. We perform a comprehensive ablation on COCO and report the comparisons with other methods on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setting</head><p>Datasets. COCO <ref type="bibr" target="#b26">[27]</ref> has over 200K images and 250K person instances with 17 keypoints. Our models are trained on train2017 dataset (includes 57K images and 150K person instances) and validated on val2017 (includes 5K images) and test-dev2017 (includes 20K images).</p><p>The MPII Human Pose dataset <ref type="bibr" target="#b0">[1]</ref> contains around 25K images with full-body pose annotations taken from realworld activities. There are over 40K person instances, split 12K instances for testing, and others for training.</p><p>Training. The network is trained on 8 NVIDIA V100 GPUs with mini-batch size 32 per GPU. We adopt Adam optimizer with an initial learning rate of 2e −3 .</p><p>The human detection boxes are expanded to have a fixed  Testing. For COCO, following <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b32">33]</ref>, we adopt the two-stage top-down paradigm (detect the person instance via a person detector and predict keypoints) with the person detectors provided by SimpleBaseline <ref type="bibr" target="#b45">[46]</ref>. For MPII, we adopt the standard testing strategy to use the provided person boxes. We estimate the heat maps via a post-gaussian filter and average the original and flipped images' predicted heat maps. A quarter offset in the direction from the highest response to the second-highest response is applied to obtain each keypoint location. Evaluation. We adopt the OKS-based mAP metric on COCO, where OKS (Object Keypoint Similarity) defines the similarity between different human poses. We report standard average precision and recall scores: AP (the mean of AP scores at 10 positions, OKS = 0.50, 0.55, . . . , 0.90, 0.95), AP 50 (AP at OKS = 0.50), AP 75 , AR and AR 50 . For MPII, we use the standard metric PCKH @0.5 (head-normalized probability of correct keypoint) to evaluate the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>COCO val. The results of our method and other state-ofthe-art methods are reported in  <ref type="figure">Figure 4</ref> (a). <ref type="figure" target="#fig_1">Figure 3</ref> shows the visual results on COCO from Lite-HRNet-30. COCO test-dev. <ref type="table" target="#tab_3">Table 4</ref> reports the comparison results of our networks and other state-of-the-art methods. Our Lite-HRNet-30 achieves 69.7 AP score. It is significantly better than the small networks, and is more efficient in terms of GFLOPs and parameters. Compared to the large networks, our Lite-HRNet-30 outperforms Mask-RCNN <ref type="bibr" target="#b13">[14]</ref>, G-RMI <ref type="bibr" target="#b32">[33]</ref>, and Integral Pose Regression <ref type="bibr" target="#b37">[38]</ref>. Although there is a performance gap with some large networks, our networks have far lower GFLOPs and parameters. MPII val. <ref type="table" target="#tab_4">Table 5</ref> reports the results of our net-works and other lightweight networks. Our Lite-HRNet-18 achieves better accuracy with much lower GFLOPs than MobileNetV2, MobileNetV3, ShuffleNetV2, Small HRNet-W16. With increasing the model size, as Lite-HRNet-30, the improvement gap becomes larger. Our Lite-HRNet-30 achieves 87.0 PCKh @0.5, improving MobileNetV2, Mo-bileNetV3, ShuffleNetV2 and Small HRNet-W16 by 1.6, 2.7, 4.2, and 6.8 points, respectively. <ref type="figure">Figure 4 (b)</ref> shows the comparison of accuracy and complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We perform ablations on two datasets: COCO and MPII, and report the results on the validation sets. The input size is 256 × 192 for COCO, and 256 × 256 for MPII. Naive Lite-HRNet vs. Small HRNet. We empirically study that the shuffle blocks combined into HRNet improve performance. <ref type="figure">Figure 4</ref> shows the comparison to Small HRNet-W16 <ref type="bibr" target="#b3">4</ref> . We can see that naive Lite-HRNet achieves higher AP scores with lower computation complexity. On COCO val, naive Lite-HRNet improves AP over the Small HRNet-W16 by 7.3 points, and the GFLOPs and parameters are less than half. When increasing to similar parameters as wider naive Lite-HRNet, the improvement is enlarged to 10.5 points, as shown in <ref type="figure">Figure 4</ref> (a). On MPII val, naive Lite-HRNet outperforms the Small HRNet-W16 by 5.1 points, while the wider network outperforms 6.6 points, as illustrated in <ref type="figure">Figure 4 (b)</ref>. Conditional channel weighting vs. 1 × 1 convolution. We compare the performance between 1 × 1 convolution (wider naive Lite-HRNet) and conditional channel weighting (Lite-HRNet). We simply remove one or two 1 × 1 convolutions in the shuffle blocks in wider naive Lite-HRNet. <ref type="table" target="#tab_5">Table 6</ref> shows the studies on the COCO val and MPII val sets. 1 × 1 convolutions can exchange the information across channels, important to representation learning. On COCO val, dropping two 1 × 1 convolutions leads to 4.4 AP points decrease for wider naive Lite-HRNet, and also reduces almost 40% FLOPs. Our conditional channel weighting improves by 3.5 AP points over dropping two 1 × 1 convolutions with only increasing 16M FLOPs. The AP score is comparable with the wider naive Lite-HRNet by using only 65% FLOPs. Increasing the depth of Lite-HRNet leads to 1.5 AP improvements with similar FLOPs as wider naive Lite-HRNet and slightly larger #parameters than wider naive Lite-HRNet. The observations on MPII val are consistent (see <ref type="table" target="#tab_5">Table 6</ref>). The AP improvement is because that our lightweight weighting operations can make the network capacity improved, by exploring the multi-resolution information using cross-resolution channel weighting and deepening the network, if taking similar FLOPs with naive version. Spatial and multi-resolution weights. We empirically study how spatial weights and multi-resolution weights influence the performance, as shown in <ref type="table" target="#tab_7">Table 7</ref>.</p><p>On COCO val, the spatial weights achieve 1.3 AP increase, and the multi-resolution weights obtain 1.7 point gain. The FLOPs of both operations are cheap. With both spatial and cross-resolution weights, our network improves by 3.5 points. <ref type="table" target="#tab_7">Table 7</ref> reports the consistent improvements on MPII val. These studies validate the efficiency and effectiveness of the spatial and cross-resolution weights.</p><p>We conduct the experiments by changing the arrangement order of the spatial weighting and cross-resolution weighting, which achieves similar performance. The experiments with only two spatial weights or two cross-resolution weights, lead to an almost 0.3 drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Application to Semantic Segmentation</head><p>Dataset. Cityscapes <ref type="bibr" target="#b9">[10]</ref> includes 30 classes and 19 of them are used for semantic segmentation task. The dataset contains 2,975, 500, and 1,525 finely-annotated images for training, validation, and test sets, respectively. In our experiments, we only use the fine annotated images. Training. Our models are trained from scratch with the SGD algorithm <ref type="bibr" target="#b21">[22]</ref>. The initial rate is set to 1e −2 with a "poly" learning rate strategy <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b50">51]</ref> with a multiplier of (1 − iter max iters ) 0.9 each iteration. The total iterations are 160K with 16 batch size, and the weight decay is 5e −4 . We randomly horizontally flip, scale ([0.5, 2]), and crop the input images to a fixed size (512 × 1024) for training. Results. We do not adopt testing tricks, e.g., slidingwindow and multi-scale evaluation, beneficial to performance improvement but time-consuming. <ref type="table" target="#tab_8">Table 8</ref> shows that Lite-HRNet-18 achieves 72.8% mIoU with only 1.95 GFLOPs and Lite-HRNet-30 achieves 75.3% mIoU with 3.02 GFLOPs, outperforming the hand-crafted methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b11">12]</ref> and NAS-based methods <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24]</ref> , and comparable with SwiftNetRN-18 <ref type="bibr" target="#b31">[32]</ref> that is far computationally intensive (104 GFLOPs).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Building block. (a) The shuffle block. (b) Our conditional channel weighting block. The dotted line indicates the representation from other resolutions and the weights distributed to other resolutions. H= cross-resolution weighting function. F=spatial weighting function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Example qualitative results on COCO pose estimation: containing viewpoint change, occlusion, and multiple persons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Structure of Lite-HRNet. The stem contains one stride 2 3 × 3 convolution and one shuffle block. The main body has three stages, each of which has a sequence of modules. Each module consists of two conditional channel weight blocks and one fusion block. N in Lite-HRNet-N indicates the number of layers. resolution branch indicates this stage contains the feature stream of the corresponding resolution. ccw = conditional channel weight.</figDesc><table><row><cell>layer</cell><cell>output size</cell><cell>operator</cell><cell>resolution branch</cell><cell>#output channels</cell><cell>repeat</cell><cell cols="2">#modules Lite-HRNet-18 Lite-HRNet-30</cell></row><row><cell>image</cell><cell>256 × 256</cell><cell></cell><cell>1×</cell><cell>3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>stem</cell><cell>64 × 64</cell><cell>conv2d shuffle block</cell><cell>2× 4×</cell><cell>32 32</cell><cell>1 1</cell><cell>1</cell><cell>1</cell></row><row><cell>stage 2</cell><cell>64 × 64</cell><cell>ccw block fusion block</cell><cell>4× 8× 4× 8×</cell><cell>40, 80 40, 80</cell><cell>2 1</cell><cell>2</cell><cell>3</cell></row><row><cell>stage 3</cell><cell>64 × 64</cell><cell>ccw block fusion block</cell><cell>4× 8× 16× 4× 8× 16×</cell><cell>40, 80, 160 40, 80, 160</cell><cell>2 1</cell><cell>4</cell><cell>8</cell></row><row><cell>stage 4</cell><cell>64 × 64</cell><cell>ccw block fusion block</cell><cell>4× 8× 16× 32× 4× 8× 16× 32×</cell><cell>40, 80, 160, 320 40, 80, 160, 320</cell><cell>2 1</cell><cell>2</cell><cell>3</cell></row><row><cell>FLOPs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>273.4M</cell><cell>425.3M</cell></row><row><cell>#Params</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.1M</cell><cell>1.8M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Computational complexity comparison: 1 × 1 convolution vs. conditional channel weight. Xs ∈ R Hs×Ws×Cs are the input channel maps for the s resolution, X1 corresponds to the highest resolution. Ns = HsWs. For example, the shape of X1 and X2 are 64 × 64 × 40 and 32 × 32 × 80, respectively. single/cross-resolution=single/cross resolution information exchange.</figDesc><table><row><cell>model</cell><cell>single-resolution</cell><cell>cross-resolution</cell><cell>Theory Complexity</cell><cell>Example FLOPs</cell></row><row><cell>1 × 1 convolution</cell><cell></cell><cell></cell><cell>s 1 NsC 2 s</cell><cell>12.5M</cell></row><row><cell>3 × 3 depthwise convolution</cell><cell></cell><cell></cell><cell>s 1 9NsCs</cell><cell>2.1M</cell></row><row><cell>CCW w/ spatial weights</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>s 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparisons on the COCO val set. pretrain = pretrain the backbone on ImageNet. #Params and FLOPs are calculated for the pose estimation network, and those for human detection and keypoint grouping are not included. model backbone pretrain input size #Params GFLOPs AP AP 50 AP 75 AP M AP L AR</figDesc><table><row><cell>Large networks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8-stage Hourglass [31]</cell><cell>8-stage Hourglass</cell><cell>N</cell><cell>256 × 192</cell><cell>25.1M</cell><cell>14.3</cell><cell>66.9</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell></row><row><cell>CPN [7]</cell><cell>ResNet-50 [15]</cell><cell>Y</cell><cell>256 × 192</cell><cell>27.0M</cell><cell>6.20</cell><cell>68.6</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell></row><row><cell>SimpleBaseline [46]</cell><cell>ResNet-50</cell><cell>Y</cell><cell>256 × 192</cell><cell>34.0M</cell><cell>8.90</cell><cell>70.4</cell><cell>88.6</cell><cell>78.3</cell><cell>67.1</cell><cell cols="2">77.2 76.3</cell></row><row><cell>HRNetV1 [41]</cell><cell>HRNetV1-W32</cell><cell>N</cell><cell>256 × 192</cell><cell>28.5M</cell><cell>7.10</cell><cell>73.4</cell><cell>89.5</cell><cell>80.7</cell><cell>70.2</cell><cell cols="2">80.1 78.9</cell></row><row><cell>DARK [55]</cell><cell>HRNetV1-W48</cell><cell>Y</cell><cell>128 × 96</cell><cell>63.6M</cell><cell>3.6</cell><cell>71.9</cell><cell>89.1</cell><cell>79.6</cell><cell>69.2</cell><cell>78.0</cell><cell>77.9</cell></row><row><cell>Small networks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MobileNetV2 1× [36]</cell><cell>MobileNetV2</cell><cell>Y</cell><cell>256 × 192</cell><cell>9.6M</cell><cell>1.48</cell><cell>64.6</cell><cell>87.4</cell><cell>72.3</cell><cell>61.1</cell><cell>71.2</cell><cell>70.7</cell></row><row><cell>MobileNetV2 1×</cell><cell>MobileNetV2</cell><cell>Y</cell><cell>384 × 288</cell><cell>9.6M</cell><cell>3.33</cell><cell>67.3</cell><cell>87.9</cell><cell>74.3</cell><cell>62.8</cell><cell>74.7</cell><cell>72.9</cell></row><row><cell>ShuffleNetV2 1× [28]</cell><cell>ShuffleNetV2</cell><cell>Y</cell><cell>256 × 192</cell><cell>7.6M</cell><cell>1.28</cell><cell>59.9</cell><cell>85.4</cell><cell>66.3</cell><cell>56.6</cell><cell>66.2</cell><cell>66.4</cell></row><row><cell>ShuffleNetV2 1×</cell><cell>ShuffleNetV2</cell><cell>Y</cell><cell>384 × 288</cell><cell>7.6M</cell><cell>2.87</cell><cell>63.6</cell><cell>86.5</cell><cell>70.5</cell><cell>59.5</cell><cell>70.7</cell><cell>69.7</cell></row><row><cell>Small HRNet</cell><cell>HRNet-W16</cell><cell>N</cell><cell>256 × 192</cell><cell>1.3M</cell><cell>0.54</cell><cell>55.2</cell><cell>83.7</cell><cell>62.4</cell><cell>52.3</cell><cell>61.0</cell><cell>62.1</cell></row><row><cell>Small HRNet</cell><cell>HRNet-W16</cell><cell>N</cell><cell>384 × 288</cell><cell>1.3M</cell><cell>1.21</cell><cell>56.0</cell><cell>83.8</cell><cell>63.0</cell><cell>52.4</cell><cell>62.6</cell><cell>62.6</cell></row><row><cell cols="2">DY-MobileNetV2 1× [5] DY-MobileNetV2</cell><cell>Y</cell><cell>256 × 192</cell><cell>16.1M</cell><cell>1.01</cell><cell>68.2</cell><cell>88.4</cell><cell>76.0</cell><cell>65.0</cell><cell>74.7</cell><cell>74.2</cell></row><row><cell>DY-ReLU 1× [6]</cell><cell>MobileNetV2</cell><cell>Y</cell><cell>256 × 192</cell><cell>9.0M</cell><cell>1.03</cell><cell>68.1</cell><cell>88.5</cell><cell>76.2</cell><cell>64.8</cell><cell>74.3</cell><cell>−</cell></row><row><cell>Lite-HRNet</cell><cell>Lite-HRNet-18</cell><cell>N</cell><cell>256 × 192</cell><cell>1.1M</cell><cell>0.20</cell><cell>64.8</cell><cell>86.7</cell><cell>73.0</cell><cell>62.1</cell><cell>70.5</cell><cell>71.2</cell></row><row><cell>Lite-HRNet</cell><cell>Lite-HRNet-18</cell><cell>N</cell><cell>384 × 288</cell><cell>1.1M</cell><cell>0.45</cell><cell>67.6</cell><cell>87.8</cell><cell>75.0</cell><cell>64.5</cell><cell>73.7</cell><cell>73.7</cell></row><row><cell>Lite-HRNet</cell><cell>Lite-HRNet-30</cell><cell>N</cell><cell>256 × 192</cell><cell>1.8M</cell><cell>0.31</cell><cell>67.2</cell><cell>88.0</cell><cell>75.0</cell><cell>64.3</cell><cell>73.1</cell><cell>73.3</cell></row><row><cell>Lite-HRNet</cell><cell>Lite-HRNet-30</cell><cell>N</cell><cell>384 × 288</cell><cell>1.8M</cell><cell>0.70</cell><cell>70.4</cell><cell>88.7</cell><cell>77.7</cell><cell>67.5</cell><cell>76.3</cell><cell>76.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparisons on the COCO test-dev set. #Params and FLOPs are calculated for the pose estimation network, and those for human detection and keypoint grouping are not included.</figDesc><table><row><cell></cell><cell cols="2">model</cell><cell></cell><cell></cell><cell></cell><cell>backbone</cell><cell></cell><cell cols="2">input size</cell><cell cols="2">#Params</cell><cell cols="2">GFLOPs</cell><cell>AP</cell><cell>AP 50</cell><cell></cell><cell>AP 75</cell><cell>AP M</cell><cell>AP L</cell><cell>AR</cell></row><row><cell cols="3">Large networks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Mask-RCNN [14]</cell><cell></cell><cell cols="4">ResNet-50-FPN</cell><cell></cell><cell>−</cell><cell>−</cell><cell></cell><cell>−</cell><cell></cell><cell cols="2">63.1 87.3</cell><cell></cell><cell>68.7</cell><cell>57.8</cell><cell>71.4</cell><cell>−</cell></row><row><cell cols="2">G-RMI [33]</cell><cell></cell><cell></cell><cell cols="3">ResNet-101</cell><cell></cell><cell cols="2">353 × 257</cell><cell cols="2">42.6M</cell><cell>57.0</cell><cell></cell><cell cols="2">64.9 85.5</cell><cell></cell><cell>71.3</cell><cell>62.3</cell><cell>70.0</cell><cell>69.7</cell></row><row><cell cols="4">Integral Pose Regression [38]</cell><cell cols="3">ResNet-101</cell><cell></cell><cell cols="2">256 × 256</cell><cell cols="2">45.0M</cell><cell>11.0</cell><cell></cell><cell cols="2">67.8 88.2</cell><cell></cell><cell>74.8</cell><cell>63.9</cell><cell>74.0</cell><cell>−</cell></row><row><cell cols="2">CPN [7]</cell><cell></cell><cell></cell><cell cols="4">ResNet-Inception</cell><cell cols="2">384 × 288</cell><cell>−</cell><cell></cell><cell>−</cell><cell></cell><cell cols="2">72.1 91.4</cell><cell></cell><cell>80.0</cell><cell>68.7</cell><cell>77.2</cell><cell>78.5</cell></row><row><cell cols="2">RMPE [13]</cell><cell></cell><cell></cell><cell cols="3">PyraNet [49]</cell><cell></cell><cell cols="2">320 × 256</cell><cell cols="2">28.1M</cell><cell>26.7</cell><cell></cell><cell cols="2">72.3 89.2</cell><cell></cell><cell>79.1</cell><cell>68.0</cell><cell>78.6</cell><cell>−</cell></row><row><cell cols="4">SimpleBaseline [46]</cell><cell cols="3">ResNet-152</cell><cell></cell><cell cols="2">384 × 288</cell><cell cols="2">68.6M</cell><cell>35.6</cell><cell></cell><cell cols="2">73.7 91.9</cell><cell></cell><cell>81.1</cell><cell>70.3</cell><cell>80.0</cell><cell>79.0</cell></row><row><cell cols="3">HRNetV1 [41]</cell><cell></cell><cell cols="4">HRNetV1-W32</cell><cell cols="2">384 × 288</cell><cell cols="2">28.5M</cell><cell>16.0</cell><cell></cell><cell cols="2">74.9 92.5</cell><cell></cell><cell>82.8</cell><cell>71.3</cell><cell>80.9</cell><cell>80.1</cell></row><row><cell cols="3">HRNetV1 [41]</cell><cell></cell><cell cols="4">HRNetV1-W48</cell><cell cols="2">384 × 288</cell><cell cols="2">63.6M</cell><cell>32.9</cell><cell></cell><cell cols="2">75.5 92.5</cell><cell></cell><cell>83.3</cell><cell>71.9</cell><cell>81.5</cell><cell>80.5</cell></row><row><cell cols="2">DARK [55]</cell><cell></cell><cell></cell><cell cols="4">HRNetV1-W48</cell><cell cols="2">384 × 288</cell><cell cols="2">63.6M</cell><cell>32.9</cell><cell></cell><cell>76.2</cell><cell>92.5</cell><cell></cell><cell>83.6</cell><cell>72.5</cell><cell>82.4</cell><cell>81.1</cell></row><row><cell cols="3">Small networks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">MobileNetV2 1×</cell><cell></cell><cell cols="3">MobileNetV2</cell><cell></cell><cell cols="2">384 × 288</cell><cell cols="2">9.8M</cell><cell>3.33</cell><cell></cell><cell>66.8</cell><cell>90.0</cell><cell></cell><cell>74.0</cell><cell>62.6</cell><cell>73.3</cell><cell>72.3</cell></row><row><cell cols="3">ShuffleNetV2 1×</cell><cell></cell><cell cols="3">ShuffleNetV2</cell><cell></cell><cell cols="2">384 × 288</cell><cell cols="2">7.6M</cell><cell>2.87</cell><cell></cell><cell>62.9</cell><cell>88.5</cell><cell></cell><cell>69.4</cell><cell>58.9</cell><cell>69.3</cell><cell>68.9</cell></row><row><cell cols="3">Small HRNet</cell><cell></cell><cell cols="3">HRNet-W16</cell><cell></cell><cell cols="2">384 × 288</cell><cell cols="2">1.3M</cell><cell>1.21</cell><cell></cell><cell>55.2</cell><cell>85.8</cell><cell></cell><cell>61.4</cell><cell>51.7</cell><cell>61.2</cell><cell>61.5</cell></row><row><cell cols="2">Lite-HRNet</cell><cell></cell><cell></cell><cell cols="4">Lite-HRNet-18</cell><cell cols="2">384 × 288</cell><cell cols="2">1.1M</cell><cell>0.45</cell><cell></cell><cell>66.9</cell><cell>89.4</cell><cell></cell><cell>74.4</cell><cell>64.0</cell><cell>72.2</cell><cell>72.6</cell></row><row><cell cols="2">Lite-HRNet</cell><cell></cell><cell></cell><cell cols="4">Lite-HRNet-30</cell><cell cols="2">384 × 288</cell><cell cols="2">1.8M</cell><cell>0.70</cell><cell></cell><cell>69.7</cell><cell>90.7</cell><cell></cell><cell>77.5</cell><cell>66.9</cell><cell>75.0</cell><cell>75.4</cell></row><row><cell></cell><cell>GFLOPs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mAP</cell><cell></cell><cell>GFLOPs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(a)</cell><cell>0.2 0.4 0.6 0.8 1 1.2 1.6 1.4</cell><cell>1.48 64.6</cell><cell>59.9 1.28</cell><cell cols="2">0.54 55.2</cell><cell>0.3 65.7</cell><cell cols="2">0.2 64.8</cell><cell>0.31 67.2</cell><cell>70 72 54 56 58 60 62 64 66 68</cell><cell>(b)</cell><cell>2 2.2 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8</cell><cell>1.97 85.4</cell><cell>82.8 1.7</cell><cell cols="2">80.2 0.72</cell><cell>0.41 86.8</cell><cell>0.27 86.1</cell><cell>0.42 87</cell><cell>90 78 80 82 84 86 88</cell></row><row><cell></cell><cell cols="2">M B V 2</cell><cell>S F V 2</cell><cell>S H R</cell><cell cols="2">W L H -1 8</cell><cell>L H -1 8</cell><cell></cell><cell>L H -3 0</cell><cell></cell><cell></cell><cell cols="2">M B V 2</cell><cell>S F V 2</cell><cell>S H R</cell><cell cols="2">W L H -1 8</cell><cell>L H -1 8</cell><cell>L H -3 0</cell></row></table><note>PCKh Figure 4. Illustration of the complexity and accuracy comparison on the COCO val and MPII val sets. (a) Comparison on COCO val with 256 × 192 input size. (b) Comparison on MPII val with 256 × 256 input size. MBV2= MobileNet V2. SFV2= ShuffleNet V2. SHR= Small HRNet-W16. (W)LH= (Wider Naive) Lite-HRNet.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparisons on the MPII val set. The FLOPs is computed with the input size 256 × 256.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>a series of data augmentation operations, containing random</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>rotation ([−30°, 30°]), random scale ([0.75, 1.25]), and ran-</cell></row><row><cell>model</cell><cell>#Params</cell><cell>GFLOPs</cell><cell>PCKh</cell><cell>dom flipping for both datasets and additional half body data</cell></row><row><cell>MobileNetV2 1×</cell><cell>9.6M</cell><cell>1.97</cell><cell>85.4</cell><cell>augmentation for COCO.</cell></row><row><cell>MobileNetV3 1×</cell><cell>8.7M</cell><cell>1.82</cell><cell>84.3</cell><cell></cell></row><row><cell>ShuffleNetV2 1×</cell><cell>7.6M</cell><cell>1.70</cell><cell>82.8</cell><cell></cell></row><row><cell>Small HRNet-W16</cell><cell>1.3M</cell><cell>0.72</cell><cell>80.2</cell><cell></cell></row><row><cell>Lite-HRNet-18</cell><cell>1.1M</cell><cell>0.27</cell><cell>86.1</cell><cell></cell></row><row><cell>Lite-HRNet-30</cell><cell>1.8M</cell><cell>0.42</cell><cell>87.0</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>go through</cell><cell></cell></row></table><note>aspect ratio of 4: 3, and then crop the box from the images. The image size is resized to 256 × 192 or 384 × 288 for COCO, and 256×256 for MPII. Each image will</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Ablation about conditional channel weight vs. 1 × 1 convolutions on the COCO val and MPII val sets. The input size of COCO is 256 × 192, while 256 × 256 for MPII. Wider NLite-NRNet = wider naive Lite-HRNet.</figDesc><table><row><cell>model</cell><cell>#Params</cell><cell>MFLOPs</cell><cell>AP</cell><cell>COCO AP 50</cell><cell>AP 75</cell><cell>AR</cell><cell>MPII MFLOPs</cell><cell>PCKh</cell></row><row><cell>Small HRNet-W16</cell><cell>1.3M</cell><cell>551.7</cell><cell>55.2</cell><cell>83.7</cell><cell>62.4</cell><cell>62.1</cell><cell>735.5</cell><cell>80.2</cell></row><row><cell>Naive Lite-HRNet-18</cell><cell>0.7M</cell><cell>194.8</cell><cell>62.5</cell><cell>85.4</cell><cell>69.6</cell><cell>68.8</cell><cell>259.6</cell><cell>85.3</cell></row><row><cell>Wider Naive Lite-HRNet-18</cell><cell>1.3M</cell><cell>311.1</cell><cell>65.7</cell><cell>87</cell><cell>73.3</cell><cell>71.8</cell><cell>418.7</cell><cell>86.8</cell></row><row><cell>Wider NLite-HRNet-18 (one 1 × 1 conv. dropped)</cell><cell>1.1M</cell><cell>248.4</cell><cell>63.6</cell><cell>86.1</cell><cell>70.7</cell><cell>69.8</cell><cell>331.0</cell><cell>86.0</cell></row><row><cell>Wider NLite-HRNet-18 (two 1 × 1 conv. dropped)</cell><cell>0.9M</cell><cell>188.9</cell><cell>61.3</cell><cell>85.3</cell><cell>68.7</cell><cell>67.7</cell><cell>251.7</cell><cell>85.3</cell></row><row><cell>Lite-HRNet-18</cell><cell>1.1M</cell><cell>205.2</cell><cell>64.8</cell><cell>86.7</cell><cell>73.0</cell><cell>71.2</cell><cell>273.4</cell><cell>86.1</cell></row><row><cell>Lite-HRNet-30</cell><cell>1.8M</cell><cell>319.2</cell><cell>67.2</cell><cell>88.0</cell><cell>75.0</cell><cell>73.3</cell><cell>425.3</cell><cell>87.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>. Our Lite-HRNet-</cell></row><row><cell>30, trained from scratch with the 256 × 192 input size,</cell></row><row><cell>achieves 67.2 AP score, outperforming other light-weight</cell></row><row><cell>methods. Compared to MobileNetV2, Lite-HRNet im-</cell></row><row><cell>proves AP by 2.6 points with only 20% GFLOPs and pa-</cell></row><row><cell>rameters. Compared to ShuffleNetV2, our Lite-HRNet-18</cell></row><row><cell>and Lite-HRNet-30 achieve 4.9 and 7.3 points gain, re-</cell></row><row><cell>spectively. The complexity of our network is much lower</cell></row><row><cell>than ShuffleNetV2. Compared to Small HRNet-W16, Lite-</cell></row><row><cell>HRNet improves over 10 AP points. Compared to large</cell></row><row><cell>networks, e.g., Hourglass and CPN, our networks achieve</cell></row><row><cell>comparable AP score with far lower complexity.</cell></row><row><cell>With the input size 384 × 288, our Lite-HRNet-18 and</cell></row><row><cell>Lite-HRNet-30 achieve 67.6 and 70.4 AP, respectively. Due</cell></row><row><cell>to the efficient conditional channel weighting, Lite-HRNet</cell></row><row><cell>achieves a better balance between accuracy and computa-</cell></row><row><cell>tional complexity, as shown in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Ablation about spatial and multi-resolution weights. on the COCO val and MPII val sets. The input size of COCO is 256 × 192, while 256 × 256 for MPII. CCW=conditional channel weight computation, Wider NLite-NRNet = wider naive Lite-HRNet.</figDesc><table><row><cell>model</cell><cell>#Params</cell><cell>MFLOPs</cell><cell>AP</cell><cell>COCO AP 50</cell><cell>AP 75</cell><cell>AR</cell><cell>MPII MFLOPs</cell><cell>PCKh</cell></row><row><cell>Wider NLite-HRNet-18 (two 1 × 1 conv. dropped)</cell><cell>0.9M</cell><cell>188.9</cell><cell>61.3</cell><cell>85.3</cell><cell>68.7</cell><cell>67.7</cell><cell>251.7</cell><cell>85.3</cell></row><row><cell>Lite-HRNet-18 (CCW only w/ spatial weights)</cell><cell>0.9M</cell><cell>190.6</cell><cell>62.6</cell><cell>85.8</cell><cell>69.8</cell><cell>69.1</cell><cell>254.0</cell><cell>85.4</cell></row><row><cell>Lite-HRNet-18 (CCW only w/ multi-resolution weights)</cell><cell>0.9M</cell><cell>203.5</cell><cell>63.0</cell><cell>85.7</cell><cell>70.5</cell><cell>69.4</cell><cell>271.1</cell><cell>85.8</cell></row><row><cell>Lite-HRNet-18</cell><cell>1.1M</cell><cell>205.2</cell><cell>64.8</cell><cell>86.7</cell><cell>73.0</cell><cell>71.2</cell><cell>273.4</cell><cell>86.1</cell></row><row><cell>Lite-HRNet-30</cell><cell>1.8M</cell><cell>319.2</cell><cell>67.2</cell><cell>88.0</cell><cell>75.0</cell><cell>73.3</cell><cell>425.3</cell><cell>87.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Segmentation results on Cityscapes. P = pretrain the backbone on ImageNet. * indicates the complexity is estimated from the original paper.</figDesc><table><row><cell>model</cell><cell cols="4">P #Params GFLOPs resolution</cell><cell>val test</cell></row><row><cell>Hand-crafted networks</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ICNet [59]</cell><cell>Y</cell><cell>−</cell><cell>28.3</cell><cell cols="2">1024 × 2048 67.7 69.5</cell></row><row><cell>BiSeNetV1 A [53]</cell><cell cols="2">Y 5.8M</cell><cell>14.8</cell><cell cols="2">768 × 1536 69.0 68.4</cell></row><row><cell>BiSeNetV1 B [53]</cell><cell cols="2">Y 49.0M</cell><cell>55.3</cell><cell cols="2">768 × 1536 74.8 74.7</cell></row><row><cell>DFANet A' [23]</cell><cell cols="2">Y 7.8M</cell><cell>1.7</cell><cell cols="2">512 × 1024 − 70.3</cell></row><row><cell>SwiftNet [32]</cell><cell cols="2">Y 11.8M</cell><cell>26.0</cell><cell cols="2">512 × 1024 70.2 −</cell></row><row><cell>SwiftNet [32]</cell><cell cols="2">Y 11.8M</cell><cell>104</cell><cell cols="2">1024 × 2048 75.4 75.5</cell></row><row><cell>Fast-SCNN [35]</cell><cell>N</cell><cell>−</cell><cell>−</cell><cell cols="2">1024 × 2048 68.6 68.0</cell></row><row><cell>ShelfNet [62]</cell><cell>Y</cell><cell>−</cell><cell>36.9</cell><cell cols="2">1024 × 2048 − 74.8</cell></row><row><cell cols="2">BiSeNetV2 Small [50] N</cell><cell>−</cell><cell>21.15</cell><cell cols="2">512 × 1024 73.4 72.6</cell></row><row><cell>MoibleNeXt [12]</cell><cell cols="2">Y 4.5M</cell><cell cols="3">10.1  *  1024 × 2048 75.5 −</cell></row><row><cell cols="3">MobileNet V2 0.5 [36] Y 0.3M</cell><cell>3.73</cell><cell cols="2">512 × 1024 68.6 −</cell></row><row><cell>HRNet-W16 [41]</cell><cell cols="2">Y 2.0M</cell><cell>7.8</cell><cell cols="2">512 × 1024 68.6 −</cell></row><row><cell>NAS-based networks</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CAS [58]</cell><cell>Y</cell><cell>−</cell><cell>−</cell><cell cols="2">768 × 1536 71.6 70.5</cell></row><row><cell>DF1-Seg-d8 [24]</cell><cell>Y</cell><cell>−</cell><cell>−</cell><cell cols="2">1024 × 2048 72.4 71.4</cell></row><row><cell>FasterSeg [4]</cell><cell cols="2">Y 4.4M</cell><cell>28.2</cell><cell cols="2">1024 × 2048 73.1 71.5</cell></row><row><cell>GAS [25]</cell><cell>Y</cell><cell>−</cell><cell>−</cell><cell cols="2">769 × 1537 − 71.8</cell></row><row><cell>MobileNetV3 [16]</cell><cell cols="2">Y 1.5M</cell><cell>9.1</cell><cell cols="2">1024 × 2048 72.4 72.6</cell></row><row><cell cols="3">MobileNet V3-Small Y 0.5M</cell><cell>2.7</cell><cell cols="2">512 × 1024 68.4 69.4</cell></row><row><cell>Lite-HRNet-18</cell><cell cols="2">N 1.1M</cell><cell>1.95</cell><cell cols="2">512×1024 73.8 72.8</cell></row><row><cell>Lite-HRNet-30</cell><cell cols="2">N 1.8M</cell><cell>3.02</cell><cell cols="2">512×1024 76.0 75.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Small HRNet is available at https://github.com/HRNet/ HRNet-Semantic-Segmentation. It simply reduces the depth and the width of the original HRNet.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https : / / github . com / HRNet / HRNet -Semantic -Segmentation</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In terms of time complexity, the constant 9 should be ignored. We keep it for analysis convenience.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Available from https : / / github . com / HRNet / HRNet -Semantic-Segmentation)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by the National Natural Science Foundation of China (No. 61433007 and 61876210).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SegNet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fasterseg: Searching for faster real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno>2020. 8</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR</title>
		<meeting>International Conference on Learning Representations (ICLR</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic convolution: Attention over convolution kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="11030" to="11039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic relu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5386" to="5395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rethinking bottleneck structure for efficient mobile network design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Zhou Daquan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2334" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gather-excite: Exploiting feature context in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9401" to="9411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dfanet: Deep feature aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Partial order pruning: for best speed/accuracy trade-off in neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9145" to="9153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph-guided architecture search for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiwen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4203" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Meta networks. Proceedings of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">2554</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Efficienthrnet: Efficient scaling for lightweight high-resolution multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Neff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aneri</forename><surname>Sheth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Furgurson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Tabkhi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">In defense of pre-trained imagenet architectures for real-time semantic segmentation of road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12607" to="12616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Chris Bregler, and Kevin Murphy. Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nori</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangpil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenio</forename><surname>Culurciello</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Fast-scnn: fast semantic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Inverted residuals and linear bottlenecks: Mobile networks for classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1801</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">IGCV3: interleaved low-rank group convolutions for efficient deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixconv</surname></persName>
		</author>
		<title level="m">Mixed depthwise convolutional kernels. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Skipnet: Learning dynamic routing in convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="409" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Solov2: Dynamic and fast instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3d human pose estimation via explicit compositional depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12378" to="12385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Interleaved structured sparse convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guotian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Condconv: Conditionally parameterized convolutions for efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ngiam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1281" to="1290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Representative graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="379" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Context prior for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12416" to="12425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Distribution-aware coordinate representation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Interleaved group convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Customizable architecture search for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="405" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Shelfnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juntang</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
