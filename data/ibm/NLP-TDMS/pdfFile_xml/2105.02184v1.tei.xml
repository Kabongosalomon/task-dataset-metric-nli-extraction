<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 PolarMask++: Enhanced Polar Representation for Single-Shot Instance Segmentation and Beyond</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 PolarMask++: Enhanced Polar Representation for Single-Shot Instance Segmentation and Beyond</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Instance Segmentation</term>
					<term>Object Detection</term>
					<term>Polar Representation</term>
					<term>Fully Convolutional Network !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reducing the complexity of the pipeline of instance segmentation is crucial for real-world applications. This work addresses this issue by introducing an anchor-box free and single-shot instance segmentation framework, termed PolarMask, which reformulates the instance segmentation problem as predicting the contours of objects in the polar coordinate, with several appealing benefits. (1) The polar representation unifies instance segmentation (masks) and object detection (bounding boxes) into a single framework, reducing the design and computational complexity. (2) Two modules are carefully designed (i.e. soft polar centerness and polar IoU loss) to sample high-quality center examples and optimize polar contour regression, making the performance of PolarMask does not depend on the bounding box prediction results and thus becomes more efficient in training. (3) PolarMask is fully convolutional and can be easily embedded into most off-the-shelf detection methods. To further improve the accuracy of the framework, a Refined Feature Pyramid is introduced to further improve the feature representation at different scales, termed PolarMask++. Extensive experiments demonstrate the effectiveness of both PolarMask and PolarMask++, which achieve competitive results on instance segmentation in the challenging COCO dataset with single-model and single-scale training and testing, as well as new state-of-the-art results on rotate text detection and cell segmentation. We hope the proposed polar representation can provide a new perspective for designing algorithms to solve single-shot instance segmentation. The codes and models are available at: github.com/xieenze/PolarMask.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>I NSTANCE segmentation is one of the most fundamental tasks in computer vision. As the object mask of an object instance provides more accurate boundary information than its bounding box does, instance segmentation improves the performances of numerous downstream vision applications, such as text detection and recognition in visual navigation, cell segmentation in biotechnology, defect localization in manufacturing, so on and so forth.</p><p>However, instance segmentation is challenging because it requires predicting both the location and the semantic mask of each object instance in an image. Therefore, instance segmentation has been typically solved by firstly detecting bounding boxes and secondly performing semantic segmentation within each detected bounding box. This is a two-stage pipeline adopted by twostage instance segmentation methods, such as Mask R-CNN <ref type="bibr" target="#b19">[20]</ref>, PANet <ref type="bibr" target="#b38">[39]</ref> and Mask Scoring R-CNN <ref type="bibr" target="#b27">[28]</ref>. The above pipeline is straightforward and has achieved good performance, but may suffer from heavy computational overhead, that its their ability in real-time applications in limited.</p><p>To address the above issue, recent trend has spent more effort in designing simpler single-stage pipeline for bounding box detection <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b72">[73]</ref> and instance segmentation <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b74">[75]</ref>. This is also the main focus of this work, which designs a conceptually simple and unified mask  where PolarMask is a bounding box-free method because polar coordinate is a unified and more flexible representation than bounding box (e.g. a bounding box is a polar mask with only four rays). Polar representation has more advantages than bounding boxes. For example, when the object instances are rotated and crowded, the original bounding boxes are not tied and the adjacent boxes are easily suppressed by non-maximum suppression (NMS) as shown by the boxes in red dots, leading to mis-detected instances.</p><p>Our PolarMask system directly predicts center point and ray lengths of the object in the polar coordinate without relying on bounding box, thus being suitable in challenging situations such as heavy rotations and crowded objects.</p><p>prediction module that can be easily plugged into many off-theshelf detectors, enabling instance segmentation and rotated object detection.</p><p>arXiv:2105.02184v1 [cs.CV] 5 May 2021</p><p>Intuitively, instance segmentation is usually solved by binary classification in a spatial layout surrounded by bounding boxes, as shown in <ref type="figure" target="#fig_0">Figure 1</ref> <ref type="bibr">(b)</ref>. Such pixel-to-pixel correspondence prediction is luxurious, especially in the single-shot methods. Instead, we show that masks can be recovered successfully and effectively if the contour is obtained. A representative approach to predict contours is shown in <ref type="figure" target="#fig_0">Figure 1(c)</ref>, which localizes the Cartesian coordinates of the points composing the contour. We term it "cartesian representation". An alternative is "polar representation", which applies the angle and the distance as the coordinate to localize points, as shown in <ref type="figure" target="#fig_0">Figure 1(d)</ref>. The polar representation with several advantages, is suitable for instance segmentation. Firstly, the origin point of the polar coordinate can be regarded as the center of an object. Secondly, starting from the origin, a point on the contour can be simply determined by its distance and angle with respect to the origin. Thirdly, the angle is naturally directional and thus renders it convenient to connect multiple points into a contour.</p><p>The cartesian representation may exhibit the above first two properties, but it lacks the advantage of the third one. For example, polar representation can naturally obtain the order of a sequence of points for an object according to a set of different angles. For each point, we only need to regress one parameter (the length of the ray, that is the distance between the center point and the contour). However, for cartesian representation, we cannot naturally give a clear order of the points in the cartesian coordinate without angles. Moreover, for each point in cartesian representation, we need to regress two parameters, x and y coordinates. As a result, cartesian representation is only suitable to represent the bounding box, while polar representation is a unified representation and can represent both bounding box and contour of object because the bounding box is just the simplest contour with 4 angles and rays. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>'s column 2, polar representation can enable box-free instance segmentation, which has large advantages over box-based instance segmentation methods, e.g. Mask R-CNN, on crowd scene. For example, polar representation has shown great advantages over cartesian representation on remote sensing Object detection, scene text detection and cell instance segmentation <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b71">[72]</ref>.</p><p>The polar representation enables us to reformulate instance segmentation as instance center classification and dense coordinate regression in the polar coordinate, whereby we propose Polar-Mask, an anchor-box-free and single-shot instance segmentation method. Specifically, PolarMask takes an image as input and predicts the distance from a sampled positive location (i.e. a candidate object's center) with respect to the object's contour from each angle, and then assembles the predicted points to produce the final mask. To leverage the benefits of the polar representation, we introduce two novel modules, i.e., the soft polar centerness and the polar IoU loss, aiming sample high-quality center examples and ease optimization of the dense coordinate regression problem.</p><p>In summary, the proposed PolarMask has several appealing benefits compared to prior arts. (1) The polar representation unifies instance segmentation and detection into a single framework, integrating the design of the above two technical routes been unified while reducing computational complexity. (2) The performance of PolarMask does not depend on the results of bounding box predictions. By removing the bounding-box branches (which is indispensable for Mask R-CNN), PolarMask has a lower computational cost, being more efficient in training. (3) It is fully convolutional and can be easily embedded into most off-the-shelf detection systems. For example, we instantiate the proposed method by embedding it into the recent object detector FCOS <ref type="bibr" target="#b55">[56]</ref>, which is a simple and low-cost pipeline. It is also worthy noting that PolarMask can be also used with other detectors such as RetinaNet <ref type="bibr" target="#b36">[37]</ref> and YOLO <ref type="bibr" target="#b48">[49]</ref>.</p><p>As shown in <ref type="figure">Figure 2</ref>(f), our resulting pipeline is as simple and clean as FCOS compared to other existing works in (a-e). PolarMask introduces negligible computational overhead, with both simplicity and efficiency, which are the two key factors for single-shot instance segmentation. In this example, PolarMask actually generalizes FCOS in polar representation by optimizing polar centerness and polar IoU. To further improve its accuracy, a Refined Feature Pyramid module is proposed to further improve the feature fusion ability at different scales. As a result, our framework takes advantage of the polar representation, which is much simpler and has fewer modules and efficient processes than the ones based on bounding box prediction as shown in <ref type="figure">Figure  2</ref>. Without bells and whistles, PolarMask relatively improves the mask accuracy by about 25%, showing considerable gains under strict localization metrics. For instance, with only single-scale testing, it achieves competitive or state-of-the-art performances on multiple tasks including instance segmentation, rotated text detection and cell segmentation, such as 38.7% mask mAP on COCO <ref type="bibr" target="#b37">[38]</ref>, 85.4% F-measure on ICDAR2015 <ref type="bibr" target="#b28">[29]</ref>, and 74.2% mAP on DSB2018 <ref type="bibr" target="#b18">[19]</ref>.</p><p>The main contributions of this work are three-fold. (1) We introduce a new perspective to design a single-shot instance segmentation framework, PolarMask, which predicts instance masks and rotated objects in the polar coordinate in an effective and efficient manner. (2) With the polar representation, we propose the polar IoU loss and the soft polar centerness for instance center classification and dense coordinate regression. We show that the proposed IoU loss in polar space can largely ease the optimization and improve accuracy, compared with the standard loss such as the smooth-ł1 loss. In parallel, soft polar centerness improves the previous centreness loss in FCOS <ref type="bibr" target="#b55">[56]</ref> and PolarMask <ref type="bibr" target="#b60">[61]</ref>, leading to further boost in performance. (3) Rich experiments show that state-of-the-art performances of object instance segmentation and rotated object detection can be achieved with low computational overhead in multiple challenging benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">General Object Detection</head><p>We also review algorithms for general object detection, which could be categorized into two-stage and one-stage methods.</p><p>Two-stage detectors mostly follow the R-CNN <ref type="bibr" target="#b16">[17]</ref> pipeline, which firstly generates a set of object proposals and then refines the proposals by a subnetwork in each region. For example, SPP-Net <ref type="bibr" target="#b20">[21]</ref> and Fast R-CNN <ref type="bibr" target="#b15">[16]</ref> have similar region-wise feature extractors, while Faster R-CNN <ref type="bibr" target="#b50">[51]</ref> proposes a Region Proposal Network (RPN) to generate proposals. R-FCN <ref type="bibr" target="#b7">[8]</ref> introduces a position-sensitive RoI Pooling technique to reduce computations of the region-wise subnetwork. These methods improve R-CNN's performance. Based on them, HyperNet <ref type="bibr" target="#b30">[31]</ref> and FPN <ref type="bibr" target="#b35">[36]</ref> involve multi-layer features, accounting for objects in various scales.</p><p>One-stage object detection is another popular research topic. It aims to design a simple pipeline to reduce the cost of the twostage methods, leading to efficient computation. These approaches often drop the step of proposal generation by directly predicting the final outputs following the merit of the classic sliding window  <ref type="figure">Fig. 2</ref> -The overall pipeline of PolarMask compared to the previous representative methods. "Gen" means generation. "Det" represents detection and "Conv" is convolution operation. We can see all the two-stage methods (e.g. MNC(a), FCIS(b) and Mask R-CNN(c)) and one stage method Yolact(d) rely on box detection results, following the paradigm of "detect then segment". Although TensorMask(e) does not need box predictions, its architecture is also complex. It models instances as 4D tensor by using "Structured Tensors Generation" and uses "Tensor Align" to align features, leading to slow inference speed. The proposed PolarMask(f) is much simpler than other methods in pipeline design and does not need box predictions. strategy. For example, YOLO <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> is a representative singlestage object detector. By using a sparse feature map for object detection, it achieved nearly real-time speed but scarifies accuracy. To improve the accuracy and tackle objects with multiple scales, SSD <ref type="bibr" target="#b39">[40]</ref> employs the feature maps that are produced by multiple layers and detects the object with different sizes in different layers. DSSD <ref type="bibr" target="#b14">[15]</ref> extended SSD by utilizing a deconvolution module to fuse the low-level feature and the high-level feature, involving more contextual information for the low-level detector. A similar strategy is also proposed in FPN and RetinaNet <ref type="bibr" target="#b36">[37]</ref>. Meanwhile, RetinaNet further proposes focal loss to address the problem of foreground-background class imbalance.</p><p>Recent trends focus on anchor free one-stage detectors <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b72">[73]</ref>. For example, Cornetnet <ref type="bibr" target="#b31">[32]</ref> directly predicts the object heatmap of top-left and bottom-right and then uses embedding vectors to group them. FCOS <ref type="bibr" target="#b55">[56]</ref> abandons the anchor design by directly regressing four distances between the center and the bounding box. Reppoints <ref type="bibr" target="#b65">[66]</ref> uses deformable convolution to extract features around the boxes. However, none of the above works explored objects in polar space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Rotated Object Detection</head><p>Rotated object detection is also related to this work. It is a challenging task beyond traditional object detection with rich applications such as scene text detection in the real world. Recent advantages in scene text detection are based on deep learning. For example, TextBoxes <ref type="bibr" target="#b33">[34]</ref> modifies anchors and kernels of SSD <ref type="bibr" target="#b39">[40]</ref> to detect large-aspect-radio scene texts. EAST <ref type="bibr" target="#b73">[74]</ref> adopts FCN <ref type="bibr" target="#b41">[42]</ref> to predict a text score map and a final bounding box in the text region. RRD <ref type="bibr" target="#b34">[35]</ref> extracts two types of features for classification and regression respectively for long text line detection. Based on Faster R-CNN, RRPN <ref type="bibr" target="#b45">[46]</ref> adds rotation to both anchors and RoIPooling to detect multi-oriented text regions. SPCNet <ref type="bibr" target="#b61">[62]</ref> uses Mask R-CNN to detect arbitrary-shape text and add text context module and re-score module to further suppress false positives. PSENet <ref type="bibr" target="#b57">[58]</ref> segments the text kernels map and the entire text regions map, and then uses progressively scale expansion to reconstruct the whole text instances. PAN <ref type="bibr" target="#b58">[59]</ref> is based on PSENet by using a light-weight backbone network and FPN. It also learns an embedding vector to distinguish which pixels belong to the corresponding text instance.</p><p>Instead of specially designed modules for rotated text detection, we present a unified polar representation to handle not only instance segmentation but also rotated object detection, by treating scene text as a special case of the mask. In this way, text detection can be easily integrated into PolarMask, and benefit from the advantages of polar representation, resulting in state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Polar Representation in Vision Applications</head><p>Before the deep learning era, polar representation has been used in contour extraction and tracking. For example, Denzler et al <ref type="bibr" target="#b11">[12]</ref> introduce active rays to describe the contour extraction as an energy minimization problem. In practice, the authors use a reference point within the object's contour and shoot rays in different directions from the reference point. They also introduce an energy term to describe the internal elasticity of the rays. Their method can be applied to tracking a pedestrian approaching the camera. In the deep learning era, StarDist <ref type="bibr" target="#b52">[53]</ref> firstly use polar representation to detect cells in microscopic images. They predict a star-convex polygon for every pixel in the feature map. The first branch of StarDist predicts object probabilities on a binary mask to classify two categories (i.e. 'cell' and 'background'). The second branch predicts star-convex polygon distance, to calculate the Euclidean distance of pixel belonging to an object and the pixel on the object boundary by simply following each radial direction k. DARNet <ref type="bibr" target="#b5">[6]</ref> use deep active rays for automatic building segmentation. They use polar coordinates to parameterize a polygon-based contour. This contour then evolves to minimize its energy via gradient descent. The whole training process of DARNet is end-to-end. More related work is ESE-Seg <ref type="bibr" target="#b62">[63]</ref>, which is based on Yolo v3. The key idea of ESE-Seg is to calculate the inner center of object and sample contour points according to the angles at a fixed interval around the inner-center point, where the distance from center to the contour is named shape vector. They also shorten shape vectors and resist noise through Chebyshev polynomial fitting. Finally, they add a regression branch on Yolo to train the network. Following PolarMask, some recent articles <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b71">[72]</ref> also adopt polar representation in their research area, such as scene text detection and rotate object detection in remote sensing, which shows strong potential of polar representation for instance-level detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>Firstly, we reformulate instance mask segmentation in the polar coordinate space in section 3.1. Secondly, we introduce the polar centerness and polar IoU loss functions to optimize the polar coordinate regression problem in section 3.2. Thirdly, section 3.3 provides details of the architecture of PolarMask++, where a Refined Feature Pyramid module is proposed to improve the capacity to detect small objects. The procedures of label generation and model optimization are presented at the end of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of Mask Segmentation in Polar Coordinate</head><p>Polar Representation. As shown in <ref type="figure" target="#fig_1">Fig.3</ref>, given a mask of an object instance, we sample a center point of the object, denoted as (x c , y c ), and a set of points located on the contour of the object, denoted as</p><formula xml:id="formula_0">{(x i , y i )} N i=1</formula><p>. And then starting from the center, n rays are emitted uniformly with the same angle interval ∆θ. For example, n = 36 and ∆θ = 10 • represent 36 rays with 10 • between two adjacent rays. The length of each ray is calculated as the distance between the center and the point on the contour. In this case, we could model a mask in the polar coordinate using one center and n rays. Since the angle interval is a constant, only the lengths of the rays need to be learned. Therefore, we could reformulate instance segmentation as instance center classification and regression of rays' lengths in the polar coordinate.</p><p>Mass Center. To represent an object's center, we consider both the bounding box's center and the mass center and evaluate the upper bound of the mask segmentation performance of them (details in <ref type="figure" target="#fig_0">Figure 10</ref>). We find that the mass center is more advantageous than the box center because the mass center has a larger probability of falling inside an instance compared to its box center. Although for some extreme cases such as a "donut", neither the mass center nor the box center lies inside the instance, the mass center is applicable to most of the cases better than the box center.</p><p>Center Samples. A location (x, y) in an image is considered as a center (positive) sample if it falls into a certain range around the mass center of an object. Otherwise, it is treated as a negative sample. We define the range for sampling positive pixels to be 1.5 times the strides <ref type="bibr" target="#b55">[56]</ref> of the feature map from the mass center to the left, top, right, and bottom of the bounding box. As a result, 9∼16 pixels around the mass center of each instance would be treated as positive examples, leading to two advantages. (1) This would increase the number of positive samples to avoid imbalance between the positive and negative samples in training. (2) More candidate points would represent the mass center of an instance more accurately.</p><p>Distance Regression of Ray in Training. Given a center point of a positive sample (x c , y c ) and a set of points on its contour, the length of n rays are denoted by {d 1 , d 2 , . . . , d n } as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. In this case, instance segmentation is treated as length regression of rays. The detailed computations are presented in section 3.3.2. Here we introduce two special cases. (a) If a ray intersects with the contour in more than one point (i.e. a "concave" boundary), the point with the maximum length with respect to the center is chosen to represent the ray. (b) If a ray is connected to </p><formula xml:id="formula_1">!"# !$% (a) (b) (c) !"# !$% !"# / !$% → 0 !"# / !$% → 1 Fig. 4 -Polar Centerness. (a) is the original image. (b)</formula><p>is the rays regression with a inappropriate center. (c) is the rays regression with an appropriate center. d min and dmax are the minimum and maximum distance of rays. Polar Centerness is used to down-weight such regression tasks as the high diversity of rays' lengths as shown in red lines in the middle plot. These examples are hard to optimize and produce low-quality masks. In (b), the d min /dmax → 0 and in (c), the d min /dmax → 1 , which means the positive sample in (c) is better than (b) and has higher loss weight during training.</p><p>During inference, the polar centerness predicted by the network is multiplied to the classification score, thus can down-weight the low-quality masks.</p><p>a center that lies outside the mask and it does not intersect with the contour given the angle ∆θ, we define its length as a small constant , for example, = 10 −6 .</p><p>The training stage of length regression is non-trivial because of two reasons. Firstly, length regression is a dense distance regression task since every training sample has n rays such as n = 36. It may cause an imbalance between the regression loss and the center classification loss. Secondly, n rays of each instance are correlated. They should be jointly trained, rather than being treated as a set of independent tasks. The above difficulties will be solved by the polar IoU loss function as discussed in Section 3.2.2.</p><p>Mask Assembling in Testing. After training, the network produces the confidence scores of the center and the rays' lengths. We assemble masks from at most 1,000 top-scoring predictions per feature level in a Feature Pyramid Network (FPN). The top predictions from all levels are merged and non-maximum suppression (NMS) is applied to yield the mask. Here we introduce the mask assembling and the NMS procedures.</p><p>Given a center location (x c , y c ) and the lengths of n rays {d 1 , d 2 , . . . , d n }, we can calculate the position of each corre-sponding contour point (x i , y i ),</p><formula xml:id="formula_2">x i = cos θ i × d i + x c<label>(1)</label></formula><formula xml:id="formula_3">y i = sin θ i × d i + y c .<label>(2)</label></formula><p>As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, starting from 0 • , the contour points are connected one by one and finally assembles the entire contour as well as the mask.</p><p>We apply NMS to remove redundant masks. To simplify the process, we calculate the smallest bounding boxes of masks and then apply NMS based on the IoU of the generated bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Polar Representation Optimization</head><p>This section describes how to optimize the prediction of centerness and the regression of rays.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Polar Centerness Prediction</head><p>Centerness <ref type="bibr" target="#b55">[56]</ref> is introduced to suppress low-quality bounding boxes in object detection. However, simply applying centerness in the polar space is sub-optimal because it is designed for regular bounding boxes but not masks.</p><p>Here we define polar centerness. Let {d 1 , d 2 , . . . , d n } be a set of lengths of n rays of an instance, the polar centerness could be represented by,</p><formula xml:id="formula_4">Polar Centerness = min({d 1 , d 2 , . . . , d n }) max({d 1 , d 2 , . . . , d n }) ,<label>(3)</label></formula><p>which assigns higher centerness to a location if the min({d 1 , d 2 , . . . , d n }) value and the max({d 1 , d 2 , . . . , d n }) value are close. The above equation could localize the object center efficiently, as shown in <ref type="figure">Figure 4</ref>. However, we find that sometimes the min({d 1 , d 2 , . . . , d n }) value of the best positive sample is not close to the max({d 1 , d 2 , . . . , d n }) value, especially for complex shapes, making the value of polar centerness small and resulting in two drawbacks. (1) The weight of positive samples becomes small and this is not optimal for solving ray regression. <ref type="bibr" target="#b1">(2)</ref> In test, the final scores of objects would be also small because they rely on the centerness prediction.</p><p>We improve it by introducing a soft mechanism, termed as soft polar centerness, to bridge the gap between min({d 1 , d 2 , . . . , d n }) and max({d 1 , d 2 , . . . , d n }). In details, we divide d i according to its angle into four subsets,</p><formula xml:id="formula_5">D 1 = {d 1 , . . . , d n 4 } ∈ [0 • , 90 • ), D 2 = {d n 4 +1 , . . . , d n 2 } ∈ [90 • , 180 • ), D 3 = {d n 2 +1 , . . . , d 3n 4 } ∈ [180 • , 270 • ), D 4 = {d 3n 4 +1 , . . . , d n } ∈ [270 • , 360 • ).<label>(4)</label></formula><p>Then the soft polar centerness is defined as,</p><formula xml:id="formula_6">Soft Polar Centerness = F (D 1 ) F (D 3 ) × F (D 2 ) F (D 4 ) ,<label>(5)</label></formula><p>where F is a function to calculate a value given a set D i . We investigate three different functions, including (1) the mean of D i , (2) the maximum value of D i , and (3) the first value of D i . Through extensive experiments, we found that the results are comparable and all of them improve the performance of the original polar centerness.</p><p>In the implementation, as shown in <ref type="figure" target="#fig_2">Figure 5</ref>, we add a branch with a single layer to predict the soft polar centerness, which is in parallel with the classification branch. The polar centerness predicted by the network is multiplied by the classification score, thus reducing the weight of low-quality masks. Experiments show that soft polar centerness improves accuracy especially under strict localization metrics such as AP 75 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Polar Ray Regression</head><p>As discussed above, the method of polar segmentation converts the task of instance segmentation into a set of ray regression problems. In object detection and segmentation, the smooth-ł1 loss <ref type="bibr" target="#b16">[17]</ref> and the IoU loss <ref type="bibr" target="#b69">[70]</ref> are the two widely-used loss functions to solve the regression problems. However, both of these functions have certain drawbacks. First, the smooth-ł1 loss does not capture correlations between samples of the same objects, thus resulting in less accurate localization. Second, although the IoU loss directly optimizes the metric of interest (IoU), computing the IoU of the predicted mask and its ground-truth is difficult to implement in parallel.</p><p>This work derives an easy and effective method to compute the mask IoU in the polar space and defines the Polar IoU loss function to optimize the model and achieves competitive performance. Let</p><formula xml:id="formula_7">d min i = min(d i , d * i ) and d max i = max(d i , d * i ), where d i and d *</formula><p>i indicate the target and the predicted length of the i-th ray respectively. The Polar IoU is computed as</p><formula xml:id="formula_8">Polar IoU = n i=1 d min i n i=1 d max i ,<label>(6)</label></formula><p>where we draw the connection between the ray regression and the polar IoU of the predicted mask. Then we can define the Polar IoU loss function as the binary cross entropy (BCE) loss of the Polar IoU to optimize the length of each ray. Since the optimal IoU is always 1, the polar IoU loss function can be represented by the negative logarithm of the Polar IoU,</p><formula xml:id="formula_9">Polar IoU Loss = log n i=1 d max i n i=1 d min i .<label>(7)</label></formula><p>The above polar IoU loss function exhibits two advantageous properties. (1) It is differentiable by using back-propagation and it is easy to implement parallel computations, facilitating a fast training process. (2) It improves the overall performance by a large margin compared with the smooth-ł1 loss function by predicting all the regression rays as a whole, rather than treating them independently. (3) Moreover, Polar IoU loss is able to automatically balance between the classification loss and the regression loss of dense distance prediction. We will discuss this in detail in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Discussions of the Effectiveness of Polar IoU</head><p>Intuitively, optimization of the polar IoU loss in Eqn.7 encourages lengths of the predicted rays to be the same as the target rays. It is derived from the polar mask IoU introduced below. Here we connect the polar IoU to the polar mask IoU that has a continuous formation, showing that Eqn.7 is actually maximizing the mask IoU in the polar space. The polar mask IoU is the ratio between the predicted mask and the ground-truth mask represented by the polar coordinates. As shown in <ref type="figure">Figure 6</ref>, mask IoU can be calculated by using polar integration where d and d * are the target and the predicted lengths of the rays respectively and θ is the angle between rays. We can transform the above continuous form into a discrete form,</p><formula xml:id="formula_10">IoU = 2π 0 1 2 min(d, d * ) 2 dθ 2π 0 1 2 max(d, d * ) 2 dθ ,<label>(8)</label></formula><formula xml:id="formula_11">IoU = lim N →∞ N i=1 1 2 d 2 min ∆θ i N i=1 1 2 d 2 max ∆θ i .<label>(9)</label></formula><p>In fact, when N approaches infinity, this discrete form equals the continuous form. We assume that the rays are uniformly emitted so that we have ∆θ = 2π N , which can further simplify the expression. In practice, we empirically observe that the power of two has a negligible impact on the performance (±0.1 mAP on COCO). Thus, we ignore the power of two and apply the definition in Eqn. 6 to calculate the Polar IoU to approximate the mask IoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network Architecture and Model Training</head><p>The proposed system PolarMask++ is an effective and unified framework, which consists of a backbone network, a modified feature pyramid network, and the task-specific heads. To enable fair comparisons, the setup of the backbone follows FCOS <ref type="bibr" target="#b55">[56]</ref>, which is a representative method for one-stage object detection. Although there are many candidates for the backbone networks, we align the setting with FCOS to show the simplicity and effectiveness of our instance modeling method. As shown in the middle of <ref type="figure" target="#fig_2">Figure 5</ref>, the heads in PolarMask++ contain three branches, including a classification branch, a polar centerness branch, and a mask regression branch, which predict the class label, the polar centerness score and the length of each polar ray of each pixel respectively, where k and n indicate the number of categories and the number of rays.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Refined Feature Pyramid</head><p>Feature Pyramid Network (FPN) <ref type="bibr" target="#b35">[36]</ref> is commonly used in most object detection and instance segmentation methods such as Reti-naNet and Mask R-CNN. FPN has achieved great success because high-level features in backbones have more semantic meanings while shallow low-level features have more content information. In the literature, the Balanced Feature Pyramid (BFP) is proposed <ref type="figure">Fig. 6</ref> -Mask IoU in Polar Representation. Mask IoU (interaction area over union area) in the polar coordinate can be calculated by integrating the differential IoU area in terms of differential angles. Polar IoU loss optimizes the mask regression as a whole, instead of optimizing each ray separately like L1 loss, leading to better performance.</p><formula xml:id="formula_12">Ground Truth Prediction D = { 1 , 2 , … } ෩ = { ෪ 1 , ෪ 2 , … ෪ } ෩ = ‫‬ 0 2 1 2 min( , ෪ ) 2 ‫‬ 0 2 1 2 m ( , ෪ ) 2</formula><p>in <ref type="bibr" target="#b46">[47]</ref> balance the feature in each resolution in object detection.</p><p>In this paper, we propose a Refined Feature Pyramid (RFP) that strengthens the feature pyramid representation in instance segmentation. As shown in <ref type="figure">Figure 7</ref>, given the feature maps from P 3, P 4, P 5, P 6, P 7 with resolution range from 1/8 to 1/128, we first re-scale the feature maps in different levels to the resolution of 1/8, and then fuse them by adding all these feature maps. Next, we use non-local <ref type="bibr" target="#b59">[60]</ref> operation on the fused feature maps to calculate the relationship between long-range and short-range pixels and "refine" the representation of each pixel by using its contextual information. After we get the refined feature maps, we re-scale these feature maps using a similar but reverse procedure. Finally, to generate the final feature representation, the origin feature maps from multiple scales are added with the refined feature maps by using a shortcut connection.</p><p>The Refined Feature Pyramid not only makes high-resolution features and low-resolution features fused more effectively but also builds the relation of pixels on the feature map. In experiments, we find that it benefits object segmentation, especially for  <ref type="figure">Fig. 7</ref> -Refined Feature Pyramid. The feature maps from different stages are integrated into the same size and added, then a non-local block is used for context modeling. At last, the refined feature is integrated to original sizes and a shortcut operate is adopted to get the final feature representation in multiple scales. small instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Label Generation</head><p>Here we explain the procedure of label generation for ray regression in Eqn. 7, as shown in Algorithm 1. Firstly, we obtain the contours of one instance by applying methods such as cv2.findContours in OpenCV. Secondly, we traverse every point on the contour to calculate the distance and the angle from this point to the center of object instance. Finally, we could achieve the distance given the corresponding angle (e.g. when number of rays is 36, we obtain ray length of every ∆θ = 10 • ). When the target angle is missing, we adopt the nearest angle as the supervision. For instance, if the ray of 10 • is missing but that corresponding to 9 • exists, we can use 9 • as the regression target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Model Training</head><p>The proposed model optimizes multiple tasks jointly including label prediction, polar centerness prediction and polar ray regression. We therefor define a multi-task loss function,</p><formula xml:id="formula_13">L = L cls + α 1 L reg + α 2 L ct ,<label>(10)</label></formula><p>where L cls is the classification loss that is formulated as the focal loss function. L reg is the mask regression loss, which is defined in section 3.2.2. And L ct is the loss for soft polar centerness, which is formulated as a binary cross entropy loss. We set the trade-off parameters α 1 and α 2 to '1' in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>To validate the effectiveness of the proposed approaches, we conduct extensive experiments and compare with the recent stateof-the-art methods on three challenging public benchmarks, including a general instance segmentation dataset MSCOCO <ref type="bibr" target="#b37">[38]</ref>, a rotated object detection dataset for text detection ICDAR2015 <ref type="bibr" target="#b28">[29]</ref> and a cell instance segmentation dataset DSB2018 <ref type="bibr" target="#b18">[19]</ref>. All the experiments are implemented in mmDetection <ref type="bibr" target="#b3">[4]</ref> using PyTorch. All networks are trained with 8 NVIDIA Tesla V100 GPUs with 32GB memories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">General Instance Segmentation on COCO</head><p>Experiment Settings. We first examine the performance of the proposed PolarMask++ on the COCO benchmark <ref type="bibr" target="#b37">[38]</ref>, which is a widely used dataset in general object detection and instance segmentation. In COCO, Average Precision (AP) is used to measure the performance. By following common setups <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b19">[20]</ref>, we train the models by using the union of 80K training images and a 35K subset of validation images (trainval35k), and report results on the remaining 5K validation images (minival). We also compare for each point ∈ Contour do 4:</p><p>Calculate distance and angle from point to center 5:</p><p>Append distance to D, angle to A 6:</p><p>Get distance set D, angle set A 7: 8:</p><p>Initialize distance label L D 9:</p><p>for angle θ ∈ [0,10,20,. . . ,360] do 10:</p><p>if Find angle θ in A then 11:</p><p>if angle has multiple distances d then 12:</p><p>Find the maximum d 13: else 14:</p><p>Find corresponding distance d 15:</p><p>else θ not in A 16:</p><p>if Find angle θnear nearby θ in A then 17:</p><p>Find corresponding d // Nearest Interpolation. 18: else 19: d = 10 −6 // Target a minimum number as label. 20:</p><p>Append d to L D 21:</p><p>return L D our results on test-dev with the recent state-of-the-art methods, including both the one-stage and two-stage models. Similar to <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b19">[20]</ref>, we employ ResNet101 and ResNeXt101 as the backbone networks of PolarMask++. In the training phase, we adopt 2× training schedule (i.e. 24 epochs). All the models are trained with 4 samples per GPU and are optimized by using stochastic gradient decent (SGD) with an initial learning rate of 0.02. All the input images are resized to 768×1280 to input the network. For data augmentation, we randomly scale the shorter side of images in the range from 640 to 768 during the training. During inference, we keep the input size of 768×1280 for single scale testing unless otherwise stated. Result Comparisons. <ref type="table">Table 1</ref> reports the performance of Polar-Mask++ against its counterparts including the recent one-stage and two-stage models. Without bells and whistles, PolarMask++ is able to achieve competitive performance with more complex one-stage methods. For example, using a simpler pipeline and half training epochs, PolarMask++ outperforms YOLACT by 2.6 mAP. Moreover, PolarMask++ with deformable convolutional layers <ref type="bibr" target="#b8">[9]</ref> can achieve 37.2 mAP, which is comparable with stateof-the-art methods. We further enlarge the input image from 768 × 1280 to 1280 × 1920 and the performance consistently improved, especially the AP for small objects. This is because there are many small objects (41% objects) in COCO, so a highresolution input image is helpful for small objects. In this case, the best PolarMask++ can achieve 38.7 mAP, outperforming the prior state-of-the-art two-stage method Mask R-CNN by 1.6%. We compare the runtime (i.e. frame per second, FPS) between TensorMask and PolarMask++ with the same image size (short length 800) and device (one V100 GPU). PolarMask++ runs at 14 FPS with the ResNet-101 backbone, which is more than 4 times faster than TensorMask (3 FPS).</p><p>The outputs of PolarMask++ are visualized in <ref type="figure" target="#fig_0">Figure 11</ref>, whereby we have two observations. (1) For objects with regular shapes such as bus and apple, PolarMask++ predicts accurate contours than other methods. (2) For objects with complex and nonregular shapes such as person, PolarMask++ predicts relatively rough contours, while the performance is not satisfactory enough. These objects make PolarMask++ have relatively low performance under high-IoU restriction. For instance, in <ref type="table">Table 1</ref>  <ref type="bibr" target="#b74">75</ref> , the improvement over Mask R-CNN is just 0.6%. We would like to point out that it is the main challenge of polar representation on instance segmentation, and we will put more effort to solve it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Rotated Text Detection</head><p>Experimental Settings. We also evaluate the performance of Po-larMask++ on the rotated detection task of ICDAR2015 <ref type="bibr" target="#b28">[29]</ref>. This dataset deals with scene text detection, a typical rotated detection task. It contains 1000 training samples and 500 test images. All training images are annotated with word-level quadrangles as well as corresponding transcriptions. In ICDAR2015, F-measure, which is the Harmonic mean of precision and recall, is used as the metric.</p><p>Following the common setting <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b64">[65]</ref>, we use ResNet-50 as the backbone network for fair comparisons. Similar to prior arts, the COCO pre-trained model is used to initialize the parameters of the network. We report two kinds of results, including (1) do not use external text detection dataset to pre-train the network; (2) follow <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b61">[62]</ref> to use external text dataset ICDAR2017 to pre-train the network.</p><p>In training, the batch size is 4 images per GPU and a multiscale strategy is adopted to keep the short edge from 768 to 1024. All models are trained by using SGD optimizer for 48 epochs with the initial learning rate as 0.02. The learning rate is divided by 10 at the 32-th and the 44-th epoch. In the test, the short-side of the image is 1024 and we perform the single-scale testing process. We do a grid search then set the NMS threshold 0.3 and the score 0.25 to balance the Precision and Recall.</p><p>Result Comparisons. We compare our method with the recent state-of-the-art methods as shown in <ref type="table">Table 2</ref>. With only singlescale testing, our method achieves 83.4% F-measure without external text dataset for pre-training, outperforming prior bestperforming method by 1%. When an external dataset is used to pre-train the backbone, PolarMask++ achieves 85.4%, establishing a new state-of-the-art performance. With the only postprocess NMS, PolarMask++ can run at 10 FPS, leading to large advantages in speed compared to most of the methods such as PSENet and TextSnake, which typically need more complex pipelines and post-processing steps. The above competitive results on ICDAR2015 verify that PolarMask++ can not only perform instance segmentation but also transfer to rotated object detection, which demonstrates the advantages of polar representation.  In addition, we demonstrate some detection examples in appendix, showing that PolarMask++ can accurately detect arbitrarily oriented text instances, which proves that the polar representation can handle not only instance segmentation but also rotated object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Cell Segmentation</head><p>Experimental Settings. To further evaluate the robustness of PolarMask++, we conduct experiments on the DSB2018 dataset, which is a cell dataset manually annotated real microscopy images of cell nuclei from the 2018 Data Science Bowl. It contains 671 images. The cells in this dataset have different types, magnification, and imaging modalities because they were captured under different conditions. This dataset does not provide training and testing partitions. By following the prior work <ref type="bibr" target="#b52">[53]</ref>, we randomly use 90% of the images for training and 10% for testing. In DSB2018, the evaluation metric is the same as COCO, we use AP to evaluate the performance.</p><p>We use ResNet-50 as the backbone network. The COCO pretrained model is used to initialize the parameters of the network. In training, the batch size is 4 images per GPU and a multi-scale strategy is adopted to keep the short edge of the image from 768 to 1024. All models are trained for 12 epochs by using SGD optimizer with an initial learning rate of 0.02. The learning rate is divided by 10 at the 8-th and 11-th epoch. In the test, the shortside of the image is 1024 and single-scale testing is performed. In DSB2018, as the number of cells may be large, the max objects per image are 400 during inference. By following the previous work, we report AP from AP 50 to AP 90 and the mAP as the metric. Result Comparisons. The results on DSB2018 are shown in <ref type="table" target="#tab_5">Table 3</ref>. Without bells and whistles, PolarMask++ clearly outperforms its counterparts with large margins. For example, Polar-Mask++ surpasses Nuclei R-CNN, which is the recent state-ofthe-art method, by 4.6% mAP. It also has large advantages over Mask R-CNN, which is a state-of-the-art instance segmentation method. We see that the polar representation is able to perform accurately cell segmentation.</p><p>Some results are visualized in appendix. We have two observations. (1) Polar representation has natural advantages to represent cells because cell tends to have regular shapes. (2) PolarMask++ is robust to detect cells with different backgrounds and scales. We believe PolarMask++ can be immediately applied in the industry with a simple scenario such as cell segmentation and corn detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Summarizing the Experimental Results</head><p>The above experimental analyses of general instance segmentation rotated object detection and cell instance segmentation in various benchmarks can be concluded as follows. (1) Polar representation is a flexible and general representation that could model instance, rotated object as well as cell. It has great advantages compared to its counterparts. <ref type="bibr" target="#b1">(2)</ref> The proposed method has a huge superiority on rotated object detection and cell instance segmentation, and the runtime speed is fast. (3) Compared to the prior PolarMask <ref type="bibr" target="#b60">[61]</ref>, the proposed method achieves much better performance, which strongly proves the effectiveness of soft polar centerness and refined feature pyramid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Advantages of PolarMask++ vs. Mask R-CNN</head><p>The PolarMask++ has obviois advantages compared with Mask R-CNN in a crowded scene. Mask R-CNN is a classic bounding-boxdetection based method. As shown in <ref type="figure" target="#fig_5">Figure 8</ref>, when detecting dense objects, Mask R-CNN needs to detect object's bounding box first. However, the bounding boxes of crowd objects are highly overlapped, which will be suppressed by non-maximum suppression (NMS). Moreover, although Mask R-CNN successfully detects the bounding box, there tend to be more than one object in one bounding box, which is also challenging for mask segmentation. However, the advantage of PolarMask++ is that it directly regress the boundary of object from the object center, and does not rely on box detection. In this way, as long as the centers of dense objects do not overlap, PolarMask++ is able to detect these objects and not miss one object easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation Study</head><p>We perform ablation studies on the COCO dataset. It is widely used in general object detection and instance segmentation. In the ablation studies, ResNet-50 <ref type="bibr" target="#b21">[22]</ref> is used as the backbone network, and the same hyper-parameters as above are used. Specifically, our network is trained with stochastic gradient descent (SGD) for 1× training schedule (i.e. 12 epochs) with the initial learning rate being 0.01 and a mini-batch of 4 images per GPU. The learning rate is reduced by a factor of 10 at iteration 8 and 11 epochs, respectively. Weight decay and momentum are set as 0.0001 and 0.9, respectively. We initialize our backbone networks with the weights pre-trained on ImageNet <ref type="bibr" target="#b10">[11]</ref>. The input images are resized to 768×1280.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Verification of Performance Upper Bound</head><p>The concern of polar representation is that it might not model the mask precisely. In this section, we show that this concern may not be necessary. Here we verify the upper bound of PolarMask as the IoU of the predicted mask and the ground-truth mask. The verification results on different numbers of rays are shown in <ref type="figure" target="#fig_0">Figure 10</ref>.    We have the following observations: (1) It can be seen that IoU is almost perfect (i.e. above 90%) when the number of rays increases, showing that polar representation is able to model the mask very well. (2) However, when the rays are more than 72, the performance of polar representation encounters a bottleneck. For instance, 90 rays improve 0.4% compare to 72 rays, and with 120 rays, the upper bound is saturated. (3) It is more reasonable to use mass-center than bounding box-center as the center of an instance because the bounding box center is more likely to fall out of the instance. From these observations, we can easily conclude that the concern about the upper bound of PolarMask++ is not necessary, and using a mass-center is better than a box-center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">Number of Rays</head><p>The number of rays plays an important role in the entire system of PolarMask++. <ref type="table" target="#tab_7">From Table 4a</ref> and <ref type="figure" target="#fig_0">Figure 10</ref>, more rays achieve a higher upper bound and better AP. First, 24 rays improve 1.1% AP compared to 18 rays, while 36 rays further improve 0.3% AP compared to 24 rays. Second, too many rays (e.g. 72 rays) would saturate the performance. The AP of 72 rays is 27.6%, which is 0.1% lower than 36 rays. We have two intuitive explanations. (1) From the upper bound <ref type="figure" target="#fig_0">Figure 10</ref>, when rays increasing from 36 to 72, although the upper bound keeps improving, the improvement is less than increasing the number of rays from 18 to 36. In theory, it shows that there is not much room for improvement when the number of rays is 72. (2) From the perspective of CNN, more rays indicate the network needs to learn more information, impeding network training. According to the above discussions, we use 36 rays for PolarMask++, since 36 rays already depict the mask contours very well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.3">Polar IoU Loss vs. Smooth-Ł1 Loss</head><p>We examine both Polar IoU Loss and Smooth-Ł1 Loss in our architecture. We note that the regression loss of Smooth-Ł1 Loss is significantly larger than the classification loss since our architecture is a task of dense distance prediction. To cope with the imbalance, we select different factor α for the regression of Smooth-Ł1 Loss. Experimental results are shown in  that Polar IoU Loss is more effective than Smooth-Ł1 loss for training the regression task of distances between mass-center and contours. The gap may come from two aspects. First, the Smooth-Ł1 Loss may need more hyper-parameter search to achieve better performance, which can be time-consuming compared to the Polar IoU Loss. Second, Polar IoU Loss predicts all rays of one instance as a whole, which is superior to Smooth-Ł1 Loss.</p><p>In <ref type="figure" target="#fig_6">Figure 9</ref>, we also compare the results using the Smooth-Ł1 Loss and Polar IoU Loss respectively. Smooth-Ł1 Loss exhibits systematic artifacts, suggesting that it lacks supervision of the entire object. Polar IoU Loss shows more smooth and precise contours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.4">Centerness Strategy</head><p>In PolarMask <ref type="bibr" target="#b60">[61]</ref>, the Polar CenterNess is proposed to re-weight positive samples. The comparisons are shown in <ref type="table" target="#tab_7">Table 4c</ref>. Polar Centerness improves by 1.4% AP overall. Particularly, AP <ref type="bibr" target="#b74">75</ref> and AP L considerably increase by 2.3% and 2.6% respectively. The reasons can be summarized as follows: (1) Our Polar CenterNess suppresses the scores of low-quality masks, and thus improve high-IoU metric (i.e. AP75); (2) In the original centerness, larger instances often have larger differences between maximum and minimum lengths of rays, which is exactly the problem that polar centerness solves.</p><p>In PolarMask++, we further improve polar centerness <ref type="bibr" target="#b60">[61]</ref> by proposing its soft version. From <ref type="table" target="#tab_7">Table 4e</ref>, we can find that on the one hand, soft polar centerness can improve more than 0.6% AP compared to the original polar centerness. On the other hand, these three functions lead to nearly the same performance, which indicates that dividing rays of four subsets are essential to improve performance.</p><p>We explain the importance of dividing rays of four subsets as follows. According to our observations, as the complex shapes of objects in COCO dataset, the value of original Polar Centerness tend to be low, because the min({d 1 , d 2 , . . . , d n } are extremely imbalance with respect to the max({d 1 , d 2 , . . . , d n } when object shapes are complicated. As a result, the final classification scores tend to be low, which is harmful to the performance. We hypothesize that the original polar centerness is too "aggressive", so that we propose a soft mechanism. The rays are divided into 4 subsets and we calculate centerness according to the value of these 4 subsets, which will ease the imbalance length problem of the original polar centerness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.5">Strategy of Feature Pyramid</head><p>The feature pyramid network (FPN) is another key component in PolarMask <ref type="bibr" target="#b60">[61]</ref>. As show in  <ref type="figure" target="#fig_0">Fig. 10</ref> -Upper Bound Analysis. Larger number of rays would model instance masks with higher IoU. And mass-center is more effective to represent an instance than the box-center. For example, 90 rays improve 0.4% compared to 72 rays, and the result is saturated when the number of rays approaches 120. <ref type="bibr" target="#b28">29</ref>.8% AP, while using PANet <ref type="bibr" target="#b38">[39]</ref> can only improve 0.1% AP. In contrast, our Refined Feature Pyramid (RFP) can boost up the mAP by 0.4%, especially the performance of small and medium objects. It is reasonable since small objects are usually captured in the shallow layers of FPN, while large objects are usually targeted in the deeper layers. However, shallow layers have high resolution, lacking rich semantic information and relations between pixels. We use RFP to ease these two problems. First, the high-level and low-level features are aggregated and distributed again to help the information flow from deep layers to shallow layers. Second, the non-local module would help build the relationship of pixels on the feature maps. As a result, the performance of small objects is significantly improved. Flops and params of FPN, PAN and RFP. To verify the efficiency of RFP compared with FPN, we calculate the flops and parameters of PolarMask++ with different necks. We set the input size to 1200 × 800. As shown in <ref type="table" target="#tab_7">Table 4f</ref>, the flops of FPN/RFP/PAN is 252.5G/253.6G/258.5G, our RFP only adds 1.1G compared with FPN. However, the PAN's Flops is 258.5G, adding 3.0G Flops than FPN. The parameters of FPN/RFP/PAN is 34.4/36.8/34.7M, our RFP only adds 0.3M parameters than FPN, the ratio of additional parameters is less than 1%. However, PAN adds 2.4M parameters, adding 7% parameters. But the PAN only improves 0.1 AP, while our RFP improves 0.4 AP. In summary, the proposed RFP introduces only one parameterized operator, a non-local module, so the total flops and parameters improve minor. Moreover, the performance of RFP is also better than PAN, which typically needs more parameters and flops. It indicates the design of RFP is superior to PAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.6">Box Branch</head><p>Most of the previous methods of instance segmentation require localizing the bounding box of the object area and then segment the pixels inside the bounding box. For Instance, both Mask R-CNN and Yolact rely on box detection. In contrast, PolarMask++ is capable to directly produce the mask without bounding box detection.</p><p>In this section, we examine whether the additional bounding box detection branch would improve the mask AP. From Table 4d, we see that the bounding box branch contributes little to the performance of mask prediction. As mentioned before, the bounding box can be viewed as the simplest version of the mask with 4 rays only in polar representation. Therefore, the regression of bounding boxes and masks are essentially similar tasks in PolarMask++, so that the improvement brought by multitask training is limited. Moreover, unlike methods based on "detect then segment" paradigm, our framework does not need to detect the bounding box. We could use the predicted boxes to perform NMS or use the predicted masks to generate the minimum boxes to perform NMS. In summary, box detection is not necessary for PolarMask++. Thus, we do not have the bounding box prediction head in PolarMask++ for simplicity and faster speed. <ref type="table" target="#tab_7">Table 4g</ref> shows the results of PolarMask++ when using different backbones. It can be seen that better features extracted by deeper and advanced networks improve the performance. For example, PolarMask++ with ResNet-101 as backbone improves 1.4% mAP compared to ResNet-50, while ResNeXt further improves 2.3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.7">Backbone Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.8">Speed vs. Accuracy</head><p>Larger image sizes yield higher accuracy, but slower inference speeds. <ref type="table" target="#tab_7">Table 4h</ref> shows the speed and accuracy trade-off for different input image scales, which is represented by the shorter image side. The FPS is evaluated on one V100 GPU. Note that here we report the entire inference time, including all components. It shows that PolarMask++ has a strong potential as a real-time instance segmentation system with little modification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Mask Refinement with Post-processing FCN</head><p>PolarMask++ performs well on scene text detection and cell instance segmentation, since the text and cell usually have regular shapes. However, it is challenging for PolarMask++ to get a very fine segmentation mask for an object with an irregular shape, e.g. person and chair. To solve this problem, we straightforwardly add a post-processing FCN network to refine the predicted mask from PolarMask++. Implementation details and qualitative results are shown in appendix.</p><p>Quantitative results. The results are shown in <ref type="table" target="#tab_11">Table 5</ref>. The performance is largely improved from 30.2 AP to 34.1 AP, showing that the refinement FCN is useful.</p><p>On the one hand, we find that for some categories, e.g. person, bicycle and animals, the original mask AP of PolarMask++ is relatively low, because they tend to have irregular shapes. However, when adding an additional refinement network, the mask AP largely improved. This result indicates that pixel-wise segmentation has advantages on objects with irregular shapes.</p><p>On the other hand, for the categories with regular shapes, e.g. backpack, ball, cup, apple and clock, adding mask refinement network can not improve the mask AP, even a little drop. This result shows that polar representation has more advantages to segment objects with regular shapes.</p><p>The time consumption of refinement FCN is minor, only 1.5ms per object. So the mask refinement post-procedure is acceptable in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose PolarMask++, which is a single shot anchor-box free method that unifies instance segmentation and rotated object detection. Different from previous works that typically solve mask prediction as binary classification in a spatial layout, PolarMask++ steps forward to represent a mask by its contour and model the contour by one center and rays emitted from the center to the contour in the polar coordinate space. PolarMask++ is designed almost as simple and clean as single-shot object detectors, introducing negligible computing overhead. We hope that the proposed PolarMask++ framework could provide a new perspective for single-shot instance segmentation and rotated object detection. In the future, we would like to improve the capability of polar representation on objects with more complex shapes, which is the main factor to boost up performance on the COCO dataset. <ref type="figure" target="#fig_0">Fig. 11</ref> -Example results of PolarMask++ on COCO test-dev using ResNeXt-101-DCN as backbone, which achieves 37.2% mask AP (see <ref type="table">Table 1</ref>). Zoom in for best view.  We show mAP and AP of different categories. In the last column, we report the time consumption for each object. We find that mask refinement largely improves the overall mAP from 30.2 to 34.1, improving 3.9 mAP. For specifical categories, on the one hand, some categories with complex and irregular shapes, e.g. person and animals, their mAP is largely boosted up by mask refinement. On the other hand, other categories which has simple and regular shapes, e.g. ball and clock, their mAP is hardly affected by mask refinement. The time consumption of mask refinement is minor, only 1.5ms per object.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 -</head><label>1</label><figDesc>First Row: Comparisons between Cartesian representation and polar representation. "Rep." indicates representation. (a) shows an original image. (b) is its corresponding pixel-wise mask. (c) and (d) represent the mask by using contours in the Cartesian coordinates and Polar coordinates (a center and ray lengths at twelve angles), respectively. Second Row: Comparisons between our PolarMask and the popular Mask R-CNN,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 -</head><label>3</label><figDesc>Mask Assembling. Polar Representation provides a directional angle. The contour points are connected one by one start from 0 • (bold line) and assemble the whole contour. The mask is naturally obtained as the pixels inside the contour are the mask result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 -</head><label>5</label><figDesc>The overall pipeline of PolarMask system. The left part contains the backbone and feature pyramid to extract features of different levels. The middle part is the two heads for classification and polar mask regression. H, W, C are the height, width, channels of feature maps, respectively, and k is the number of categories (e.g., k = 80 on the COCO dataset), n is the number of rays (e.g., n = 36).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(g) Backbone Architecture: All models are based on FPN. Better backbones bring expected gains: deeper networks do better, and ResNeXt improves on ResNet. scale AP AP50 AP75 APS APM APL FPS 400 24.0 40.8 24.4 5.7 25.2 43.0 48.8 600 28.5 48.5 29.2 10.7 31.0 44.2 33.2 800 30.2 52.6 30.8 14.4 32.5 43.1 20.5 (h) Accuracy/speed trade-off on ResNet-50: PolarMask performance with different image scales. The FPS is reported on one V100 GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 -</head><label>8</label><figDesc>The advantage of PolarMask++ on heavy crowd and rotate scene compared with Mask R-CNN. (a) shows a representative original image with large rotations from a text detection dataset MSRA-TD500 [67]. (b) Detection result of PolarMask++. (c) Detection result of Mask R-CNN. The blue solid box and red dot boxes in (c) are the remained and suppressed results by NMS.It shows that polar representation has more advantages than bounding boxes in such situation, because bounding boxes are not tied and the highly-overlapped boxes are easily suppressed by NMS, leading to mis-detected instance. However, our PolarMask system directly predicts center point and ray lengths of object in the polar coordinate without relying on bounding box, thus being suitable in challenging situations such as heavy rotation and crowded objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 -</head><label>9</label><figDesc>Comparisons of visualization results of PolarMask++ when using Smooth-Ł1 loss and Polar IoU loss. Polar IoU Loss achieves more accurate contour of instance, while Smooth-Ł1 Loss exhibits artifacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Algorithm 1 Distance Label Generation(36 rays)    </figDesc><table><row><cell cols="2">Require: Contour: Contour, Center Sample: center,</cell></row><row><cell cols="2">1: function DISTANCE CALCULATE(Contour, center)</cell></row><row><cell>2:</cell><cell>Initialize distance set D, angle set A</cell></row><row><cell>3:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1 -TABLE 2 -</head><label>12</label><figDesc>Instance segmentation mask AP on the COCO test-dev. The standard training strategy<ref type="bibr" target="#b17">[18]</ref> is training by 12 epochs; and 'aug' means data augmentation, including multi-scale and random crop. is training with 'aug', • is without 'aug'. 'PolarMask++(600)' means we scale the short side of test image to 600 for faster speed. Our PolarMask++(600) achieves better speed and performance than real-time instance segmentation method YOLACT. * indicates we enlarge the input input image to 1920 × 1280. It can further boost up the performance, especially helpful for small object segmentation. And the final best performance is higher than Mask R-CNN, showing our method is competitive on general instance segmentation task. The single-scale results on rotate object detection dataset ICDAR2015. Following the COCO setup and without using any tricks, PolarMask++ achieves highest F-measure with or without external data, showing polar representation is suitable for scene text detection and rotate object detection.</figDesc><table><row><cell>, PolarMask++</cell></row></table><note>achieves 64.1% in AP 50 , improving 4.1% compared to Mask R- CNN. However, in AP</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3 -</head><label>3</label><figDesc>The single-scale results on cell segmentation dataset DSB2018. Following the COCO setup and without using any tricks, PolarMask++ achieves highest mAP, making 4.6 mAP higher than previous best method. It shows polar representation has large advantages on cell instance segmentation task.</figDesc><table><row><cell>Note that the result of U-Net and Mask R-CNN are Obtained from StarDist [53].</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>rays AP AP50 AP75 APS APM APL18 26.2 48.7 25.4 11.8 28.2 38.0 24 27.3 49.5 26.9 12.4 29.5 40.1 36 27.7 49.6 27.4 12.6 30.2 39.7 72 27.6 49.7 27.2 12.9 30.0 39.7 (a) Number of Rays: More rays bring a large gain, while too many rays saturate since it already depicts the mask ground-truth well. 20.2 37.9 19.6 8.6 20.6 31.1 Polar IoU 1.00 27.7 49.6 27.4 12.6 30.2 39.7 (b) Polar IoU Loss vs. Smooth-L1 Loss: Polar IoU Loss outperforms Smooth-Ł1 loss, even the best variants of balancing regression loss and classification loss.</figDesc><table><row><cell></cell><cell></cell><cell>loss</cell><cell>α AP AP50 AP75 APS APM APL</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.05 24.7 47.1 23.7 11.3 26.7 36.8</cell></row><row><cell></cell><cell></cell><cell>Smooth-Ł1</cell><cell>0.30 25.1 46.4 24.5 10.6 27.3 37.3</cell></row><row><cell cols="4">1.00 centerness AP AP50 AP75 APS APM APL box branch AP AP50 AP75 APS APM APL</cell></row><row><cell cols="2">Original 27.7 49.6 27.4 12.6 30.2 39.7</cell><cell>w</cell><cell>27.7 49.6 27.4 12.6 30.2 39.7</cell></row><row><cell>Polar</cell><cell>29.1 49.5 29.7 12.6 31.8 42.3</cell><cell>w/o</cell><cell>27.5 49.8 27.0 13.0 30.0 40.0</cell></row><row><cell cols="2">(c) Polar Centerness vs. Centerness: Polar Centerness bring a large gain,</cell><cell cols="2">(d) Box Branch: Box branch makes no difference to performance of mask</cell></row><row><cell cols="2">especially high IoU AP75 and large instance AP L .</cell><cell>prediction.</cell><cell></cell></row><row><cell cols="2">centerness AP AP50 AP75 APS APM APL Polar 29.1 49.5 29.7 12.6 31.8 42.3 Soft (F1) 29.7 51.6 30.4 13.3 32.0 42.7 Soft (F2) 29.7 51.6 30.0 13.9 31.7 42.9 Soft (F 3) 29.8 52.0 30.2 13.9 31.9 43.0 (e) Soft Mechanism: Compare with conference version [61], soft polar centerness can improve the performance without computation overhead.</cell><cell cols="2">neck AP AP50 AP75 APS APM APL Flops Params FPN 29.8 52.0 30.2 13.9 31.9 43.0 252.5 34.4 PAN 29.9 51.8 30.6 13.5 32.1 43.0 258.5 36.8 RFP 30.2 52.6 30.8 14.4 32.5 43.1 253.6 34.7 (f) Strategy of Feature Pyramid: Compare with conference version [61] and PAN [39], our proposed RPF can consistently boost up the performance, especially for small objects.</cell></row><row><cell>backbone</cell><cell>AP AP50 AP75 APS APM APL</cell><cell></cell><cell></cell></row><row><cell>ResNet-50</cell><cell>30.2 52.6 30.8 14.4 32.5 43.1</cell><cell></cell><cell></cell></row><row><cell cols="2">ResNet-101 31.6 54.5 32.2 15.7 34.2 44.8</cell><cell></cell><cell></cell></row><row><cell cols="2">ResNeXt-101 33.9 57.7 34.9 17.3 37.0 47.7</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4 -</head><label>4</label><figDesc>Ablation experiments for the proposed PolarMask++ on MSCOCO dataset. All models are trained on trainval35k and tested on minival, using ResNet50-FPN backbone with 1× training schedule unless otherwise noted.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4b .</head><label>4b</label><figDesc>Our Polar IoU Loss achieves 27.7% AP without balancing regression loss and classification loss. In contrast, the best setting for Smooth-Ł1 Loss achieves 25.1% AP (i.e. 2.6% worse than ours), showing</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4f</head><label>4f</label><figDesc>, the original FPN can achieve</figDesc><table><row><cell>18 81 83</cell><cell>24 85 87</cell><cell>36 87 89</cell><cell>72 88.5 91</cell><cell>90 88.8 91.4</cell><cell>120 88.8 91.4</cell></row><row><cell>%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 5 -</head><label>5</label><figDesc>Effectiveness of Mask Refinement.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scale-aware polar representation for arbitrarily-shaped text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Yolact: Real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dcan: deep contour-aware networks for accurate gland segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2487" to="2496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Mmdetection: Open mmlab detection toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tensormask: A foundation for dense object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Darnet: Deep active ray network for building segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7431" to="7439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3150" to="3158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pixellink: Detecting scene text via instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Active rays: Polar-transformed active contours for real-time contour tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Niemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Real-Time Imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="213" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Textdragon: An end-to-end framework for arbitrary shaped text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Dssd: Deconvolutional single shot detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">data science bowl: Find the nuclei in divergent images to advance medical discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaggle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Single shot text detector with regional attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep direct regression for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kainmueller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07626</idno>
		<title level="m">Patchperpix for instance segmentation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Wordsup: Exploiting word annotations for character based text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4940" to="4949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mask scoring r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6409" to="6418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
		<title level="m">ICDAR 2015 competition on robust reading. In Proc. ICDAR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Foveabox: Beyond anchorbased object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03797</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hypernet: Towards accurate region proposal generation and joint object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="845" to="853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2359" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rotation-sensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5909" to="5918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2016-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning markov clustering networks for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Textsnake: A flexible representation for detecting text of arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Nuclei r-cnn: Improve mask r-cnn for nuclei segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 2nd International Conference on Information Communication and Signal Processing (ICI-CSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="357" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Multi-oriented scene text detection via corner localization and region segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08948</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3111" to="3122" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Libra r-cnn: Towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="821" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep snake for realtime instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8533" to="8542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Yolo9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Cell detection with star-convex polygons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weigert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Broaddus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>Int. Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="265" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">FCOS: Fully convolutional onestage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Textray: Contour-based geometric modeling for arbitrary-shaped scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Shape robust text detection with progressive scale expansion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9336" to="9345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Efficient and accurate arbitrary-shaped text detection with pixel aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8440" to="8449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polarmask</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13226</idno>
		<title level="m">Single shot instance segmentation with polar representation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Scene text detection with supervised pyramid context network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9038" to="9045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Explicit shape encoding for realtime instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5168" to="5177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Textfield: learning a deep direction field for irregular scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5566" to="5579" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Msr: multi-scale shape regression for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02596</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<title level="m">Reppoints: Point set representation for object detection. arXiv: Comp. Res. Repository</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1083" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Hoeppner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09199</idno>
		<title level="m">Object-guided instance segmentation for biological images</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Multi-scale cell instance segmentation with keypoint graph based bounding boxes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Hoeppner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="369" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Unitbox: An advanced object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="516" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Polardet: A fast, more precise detector for rotated target in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08720</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Objects detection for remote sensing images based on polar coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.02988</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<title level="m">Objects as points. arXiv: Comp. Res. Repository</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">East: an efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5551" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="850" to="859" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
