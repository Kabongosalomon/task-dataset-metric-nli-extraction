<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Open-World Text-Guided Face Image Generation and Manipulation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Xia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Hao</forename><surname>Xue</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
						</author>
						<title level="a" type="main">Towards Open-World Text-Guided Face Image Generation and Manipulation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The existing text-guided image synthesis methods can only produce limited quality results with at most 256 2 resolution and the textual instructions are constrained in a small Corpus. In this work, we propose a unified framework for both face image generation and manipulation that produces diverse and high-quality images with an unprecedented resolution at 1024 2 from multimodal inputs. More importantly, our method supports open-world scenarios, including both image and text, without any re-training, fine-tuning, or post-processing. To be specific, we propose a brand new paradigm of text-guided image generation and manipulation based on the superior characteristics of a pretrained GAN model. Our proposed paradigm includes two novel strategies. The first strategy is to train a text encoder to obtain latent codes that align with the hierarchically semantic of the aforementioned pretrained GAN model. The second strategy is to directly optimize the latent codes in the latent space of the pretrained GAN model with guidance from a pretrained language model. The latent codes can be randomly sampled from a prior distribution or inverted from a given image, which provides inherent supports for both image generation and manipulation from multi-modal inputs, such as sketches or semantic labels, with textual guidance. To facilitate text-guided multi-modal synthesis, we propose the MULTI-MODAL CELEBA-HQ, a large-scale dataset consisting of real face images and corresponding semantic segmentation map, sketch, and textual descriptions. Extensive experiments on the introduced dataset demonstrate the superior performance of our proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>C REATING the desired visual contents without tedious manual operations is a difficult but meaningful task. To make the process more readily and user-friendly, recent studies have been leveraging a variety of intermediary modalities as conditional guidance, e.g., sketch <ref type="bibr" target="#b0">[1]</ref>, semantic label <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, or textual description <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Compared with the success of their label and sketch counterparts, most state-of-the-art methods on text-guided image creation are only able to produce relatively low-quality images <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Those aiming at creating high-quality images with textual guidance typically design a multi-stage architecture and train their models in a progressive manner. An initial image with rough shape and color would be refined to a highresolution one. To be more specific, there are usually three stages in the main module, and each stage contains a generator and a discriminator. Three stages are trained at the same time, and progressively generate images of three different scales, i.e., 64 2 → 128 2 → 256 2 . However, when it comes to higher resolution, the multi-stage training process is timeconsuming and cumbersome, making the aforementioned methods unfeasible for high-quality images with 1024 2 resolution. More importantly, high-resolution images contain lots of facial details, such as stubble, freckles, or skin pores, and cannot be obtained by simply upsampling from the <ref type="figure">Fig. 1</ref>. Our TediGAN unifies text-guided image generation and manipulation into one framework, leading to continuous operations from generation to manipulation (a), inherent support of image synthesis from multi-modal inputs (b), and high-resolution synthesis (c). lower-resolutions or such a progressive training procedure. Another serious problem with the previous methods is the poor generalization. Methods trained on small datasets typically fail to handle out-of-distribution data, let alone open-world images and texts with all sorts of complexities.</p><p>Due to the latest developments in generative adversarial networks (GANs), an entirely different image generation paradigm that achieves phenomenal quality, fidelity, and realism has been established. StyleGAN <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, one of the most notable GAN frameworks, introduces a novel stylebased generator architecture and produces images with unmatched resolution and photorealism. Some recent work <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref> has demonstrated that the intermediate latent space W of StyleGAN, inducted from a learned piece-wise continuous mapping, yields less entangled representations and offers more feasible manipulation. The superior characteristics of W space and its variants intrigue numerous researchers to develop advanced GAN inversion techniques <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> to find the inverted codes for real images in the StyleGAN's latent space and perform meaningful manipulation afterward. Overview of our proposed method. We propose two strategies to use a pretrained GAN model. (a) demonstrates the first strategy. The key idea is to project multi-modal embedding into the common W space of StyleGAN. Taking visual and linguistic embedding for example, with the learned the inversion module, we can then learn the visual-linguistic similarity, where the visual embedding w v and linguistic embedding w l are expected to be close enough. The instance-level optimization if for identity preservation. The edited image can be generated from the StyleGAN generator. (b) illustrates the inference of text-guided image manipulation using the text encoder. Given a source image and a text guidance, we first get their embedding w v and w l in W space through corresponding encoders. We then perform style mixing for target layers and get the target latent code w t . The final w t * is obtained through instance-level optimization. For image generation, we can directly obtain the results by feeding the latent codes from the text encoder into the generator. (c) is the illustration of text-guided image manipulation using a pretrained language model. In (d), we show that such optimization can be easily extended to support region-of-interest manipulation.</p><p>The most popular way <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref> is to train an additional encoder to map real images into the W space, which leads to not only faithful reconstruction but also semantically meaningful editing. Furthermore, it is easy to introduce the hierarchically semantic property of the W space to any GAN model by simply learning an extra mapping network. Such properties inspire us to design cross-modal methods for visual content creation using these fixed, pretrained StyleGAN generators.</p><p>In this paper, we propose a very simple yet effective method for Text-guided diverse image generation and manipulation via GAN (abbreviated TediGAN). Our proposed TediGAN, for the first time, unifies the two different tasks, text-guided image generation and manipulation, into one single framework, leading to continuous operations from generation to manipulation and inherent supports for image synthesis from multi-modal inputs, such as sketches or semantic labels with textual guidance, as demonstrated in <ref type="figure">Figure 1</ref>. To be specific, we introduces two strategies in this work. The first strategy, as shown in <ref type="figure">Figure 2</ref> (a) and (b), is to map multi-modal information, including texts, sketches, and labels, into a common latent space of a pretrained StyleGAN. Speaking more concretely, we train a encoder for a specific modality that can obtain latent codes that align with the hierarchically semantic of a pretrained GAN model. It is achieved through three modules. The first inversion module is to train an image encoder where the inverted code of a given image can be found in the W space. The second visual-linguistic similarity module learns linguistic representations that align with the visual representations by projecting both image and text into a common W space. The third instance-level optimization module is for identity preservation during editing. It aims to precisely manipulate the desired attributes consistent with the texts while faithfully reconstructing the unconcerned ones. The second strategy, as shown in <ref type="figure">Figure 2</ref> (c) and (d), optimizes the latent codes directly in the latent space of the aforementioned pre-trained GAN model with the guidance from a pretrained language model. Like StyleGAN, such off-the-shelf language models <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> have helped some cross-modal methods improve performance but are lessaddressed (if not never) in text-guided visual content creation. Both proposed strategies generate diverse and highquality results with a resolution up to 1024 2 and support image creation from multi-modal inputs, such as sketches or semantic labels with textual guidance, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. Our method inherits the diversity and generalizability of the pretrained StyleGAN model and thus provides a reliable guarantee of plausible results no matter how uncommon the given text or image is. Compared with the first strategy, the second one can create images from open-world images or texts and manipulate region-of-interest regions for a given image. Furthermore, to fill the gaps in the text-to-image synthesis dataset for faces, we introduce the MULTI-MODAL CELEBA-HQ dataset to facilitate the research community. Following the format of the two popular text-to-image a smiling young woman with short blonde hair she is young and wears earrings a young woman with long black hair he has high cheekbones and double chin. He is chubby he is young and wears beard he has a five o'clock shadow synthesis datasets, i.e., CUB <ref type="bibr" target="#b15">[16]</ref> for birds and COCO <ref type="bibr" target="#b16">[17]</ref> for natural scenes, we create ten unique descriptions for each image in the CelebA-HQ <ref type="bibr" target="#b17">[18]</ref>. Besides real faces and textual descriptions, the introduced dataset also contains label maps and sketches for the text-guided generation with multi-modal inputs. In sum, this paper makes the following contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We propose a unified framework that can generate diverse images given the same input text, or manipulate the given image with a text, allowing the user to edit the appearance of different attributes interactively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We propose two strategies that uses the superior characteristics of a pretrained GAN model for textguided image generation and manipulation. Both proposed strategies can create high-quality results, for the first time, with a resolution up to 1024 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We introduce the Multi-Modal CelebA-HQ dataset, which consists of multi-modal face images and corresponding textual descriptions, to facilitate the research community.</p><p>A preliminary version of this work has been published as a conference paper <ref type="bibr" target="#b18">[19]</ref>. The journal extension improves over the conference paper mainly in two significant ways:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We propose a new strategy that uses powerful offthe-shelf language models. Different from and complementary to the initial strategy, it supports image creation from open-world images or texts and regionof-interest manipulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We investigate in much more detail than the conference version add considerable discusses, such as open-world texts and images. Such extension allow us to further investigate the current limitations and future directions of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Controllable Image Generation and Manipulation. For interactive and controllable image creation, some methods focus on image synthesis conditioned on a variety of userdefined guidance, e.g., label <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, sketch <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, text <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, or control vector like gaze <ref type="bibr" target="#b25">[26]</ref> or light direction <ref type="bibr" target="#b26">[27]</ref>. For example, Zhu et al. <ref type="bibr" target="#b19">[20]</ref> propose a semantically multi-modal image synthesis method that is able to generate diverse images of different attributes from segmentation masks. Zhu et al. <ref type="bibr" target="#b20">[21]</ref> introduce semantic region-adaptive normalization to control the style of each semantic region individually. Zou et al. <ref type="bibr" target="#b21">[22]</ref> propose a network that colourizes sketches following the instructions provided by the input text specifications. Jo et al. <ref type="bibr" target="#b22">[23]</ref> propose a face editing method where users can edit face images using sketch and color. Both sketch-based and label-based categories put forward high requirements for the user's drawing <ref type="bibr" target="#b27">[28]</ref>. It is challenging to synthesize natural and realistic images from poorly-drawn sketches or labels. Those conditioned on control vectors are typically limited to a small range of applications such as gaze redirection <ref type="bibr" target="#b25">[26]</ref> or relighting <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>. Although some methods, including both sketch-based <ref type="bibr" target="#b22">[23]</ref> and label-based <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b20">[21]</ref> ones, provide an interactive software for users to edit images, some important attributes such as color and texture can only get random results. Compared to providing a color or texture gallery, a more feasible way is to integrate descriptive texts. In general, compared with labels, sketches, and vectors, texts have a much lower cost of learning for users to edit real images. Based on the pros and cons of these user-defined conditions, sketch or label is integrated in our application to define the basic features of a human face like head pose or face shape, etc, while text precisely defines color, texture, and other desired attributes. Text-to-Image Generation. There are primarily two categories of GAN-based text-to-image generation methods.   The first category produces images from texts directly by one generator and one discriminator. For example, Reed et al. <ref type="bibr" target="#b5">[6]</ref> propose to use conditional GANs to generate plausible images from given text descriptions. Tao et al. <ref type="bibr" target="#b29">[30]</ref> propose a simplified backbone that generates high-resolution images directly by Wasserstein distance and fuses the text information into visual feature maps to improve the image quality and text-image consistency. Despite the plainness and conciseness, the one-stage models produce dissatisfied results in terms of both photo-realism and text-relevance in some cases. Thus, another thread of research focuses on multi-stage processing. Zhang et al. <ref type="bibr" target="#b30">[31]</ref> stack two GANs to generate high-resolution images from text descriptions through a sketch-refinement process. They further propose a three-stage architecture <ref type="bibr" target="#b31">[32]</ref> that stacks multiple generators and discriminators, where multi-scale images are generated progressively in a course-to-fine manner. Xu et al. <ref type="bibr" target="#b4">[5]</ref> improve the work of <ref type="bibr" target="#b31">[32]</ref> from two aspects. First, they introduce attention mechanisms to explore fine-grained text and image representations. Second, they propose a Deep Attentional Multimodal Similarity Model (DAMSM) to compute the similarity between the generated image and the sentence. The subsequent studies basically follow the framework of <ref type="bibr" target="#b4">[5]</ref> and have proposed several variants by introducing different mechanisms like attention <ref type="bibr" target="#b32">[33]</ref> or memory writing gate <ref type="bibr" target="#b33">[34]</ref>. However, the multi-stage frameworks produce results that look like a simple combination of visual attributes from different image scales. A concurrent zeroshot text-to-image system called DALL-E <ref type="bibr" target="#b34">[35]</ref> is trained on large-scale unconstrained image-text pairs and can generate arbitrary classes of images from open-world descriptions.</p><p>Despite the ability to generate arbitrary classes of images from open-world descriptions, the quality and resolution of its results are often limited compared with the state-of-theart text-to-image methods that focus on certain classes <ref type="bibr" target="#b35">[36]</ref>.</p><p>Text-Guided Image Manipulation. Text-guided image manipulation is similar to text-to-image generation in terms of producing results that contain desired visual attributes.</p><p>Differently, the modified results should only change certain parts and preserve text-irrelevant contents of the original images. For example, Dong et al. <ref type="bibr" target="#b6">[7]</ref> propose an encoderdecoder architecture to modify an image according to a given text. Nam et al. <ref type="bibr" target="#b24">[25]</ref> disentangle different visual attributes by introducing a text-adaptive discriminator, which can provide finer training feedback to the generator. Li et al. <ref type="bibr" target="#b36">[37]</ref> introduce a multi-stage network with a novel textimage combination module to produce high-quality results. Liu et al. <ref type="bibr" target="#b37">[38]</ref> propose a multi-domain and multi-modal method to explicitly model the visual attributes of an image and learn how to translate them through automatically generated commands. Similar to text-to-image generation, the text-based image manipulation methods with the best performance are basically based on the multi-stage framework. Different from all existing methods, we propose a novel framework that unifies text-guided image generation and manipulation methods and can generate highresolution and diverse images directly without multistage processing.</p><p>Vision-language Representation Learning. One key of textguided image synthesis is to match visual attributes with corresponding words. To do this, current methods usually provide explicit word-level training feedback from the elaborately-designed discriminator <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b38">[39]</ref>. There is also a rich line of work proposed to address a related direction named image-text matching, or visual-semantic alignment, aiming at exploiting the matching relationships and making the corresponding alignments between text and image. Most of them can be categorized into two-branch deep architecture according to the granularity of representations for both modalities, i.e., global <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> or local <ref type="bibr" target="#b41">[42]</ref> representations. The first category employs deep neural networks to extract the global features of both modalities, based on which their similarities are measured <ref type="bibr" target="#b39">[40]</ref>. Another thread of work performs instance-level image-text matching <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, learning the correspondences between words and image regions. Pretraining has also been an increasingly significant approach He has brown hair. He has no beard.</p><p>This woman wears earrings. She has oval face and high bones. She is smiling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>She wears eyeglasses.</head><p>She has oval face and long black hair. She is wearing earrings.</p><p>This old woman has big lips, pale skin, and gray hair. in vision-language modeling, especially those on millions or billions of text-image pairs collected from a variety of publicly available sources on the Internet <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. For example, Radford et al. <ref type="bibr" target="#b13">[14]</ref> propose an efficient method of learning from natural language supervision, called Contrastive Language-Image Pre-training (CLIP), which is trained on 400 million text-image pairs. By contrast, the dataset in <ref type="bibr" target="#b14">[15]</ref> is much larger (1.8B image-text pairs) and noisier. They propose a simple dual-encoder architecture using a contrastive loss that avoids heavy labor on data curation and requires only minimal frequency-based cleaning. Both methods have shown strong potential on the semantic similarity estimation between given texts and images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE TEDIGAN FRAMEWORK</head><p>In this section, we introduce the two proposed strategies for text-guided diverse face image generation and manipulation. In Section 3.1, we introduce the inversion module used in both strategies, i.e., training an image encoder to map the real images to the latent space such that all codes produced by the encoder can be recovered at both the pixellevel and the semantic-level. In Section 3.2, we demonstrate how to use the hierarchical characteristic of W space to train a text encoder. We propose the visual-linguistic similarity learning (Section 3.2.1) that maps the image and text into the same joint embedding space. To preserve identity in manipulation, we propose an instance-level optimization (Section 3.2.2), involving the trained encoder as a regularization to better reconstruct the pixel values without affecting the semantic property of the inverted code. In Section 3.3, we demonstrate how to achieve the same goal through a pretrained powerful language model and provide some necessary discusses and innovative extensions. We extend our method to enable users to generate or manipulate via arbitrary descriptions of facial attributes and concentrate on manipulation in the region of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">StyleGAN Inversion Module</head><p>The inversion module aims at training an image encoder that can map a real face image to the latent space of a fixed StyleGAN model pretrained on the FFHQ dataset <ref type="bibr" target="#b7">[8]</ref>. The reason we invert a trained GAN model instead of training one from scratch is that, in this way, we can go beyond the limitations of a paired text-image dataset. The StyleGAN is trained in an unsupervised setting and covers much higher quality and wider diversity, leading to a latent space that almost covers the full space of facial attributes, which makes our method able to produce satisfactory edited results with images in the wild. In order to facilitate subsequent alignment with text attributes, our goal for inversion is not only to reconstruct the input image by pixel values but also to acquire the inverted code that is semantically meaningful and interpretable <ref type="bibr" target="#b44">[45]</ref>.</p><p>Before introducing our method, we first briefly establish problem settings and notations. A GAN model typically consists of a generator G(·) : Z → X to synthesize fake images and a discriminator D(·) to distinguish real data from the synthesized. In contrast, GAN inversion studies the reverse mapping, which is to find the best latent code z * by inverting a given image x to the latent space of a welltrained GAN. A popular solution is to train an additional encoder E v (·) : X → Z (subscript v means visual). To be specific, a collection of latent codes z s are first randomly sampled from a prior distribution, e.g., normal distribution, and fed into G(·) to get the synthesis x s as the training pairs. The introduced encoder E v (·) takes x s and z s as inputs and supervisions respectively and is trained with</p><formula xml:id="formula_0">min Θ Ev L Ev = ||z s − E v (G(z s ))|| 2 2 ,<label>(1)</label></formula><p>where || · || 2 denotes the l 2 distance and Θ Ev represents the parameters of the encoder E v (·).</p><p>Despite of its fast inference, the aforementioned procedure simply learns a deterministic model with no regard to whether the codes produced by the encoder align with the semantic knowledge learned by G(·). The supervision by only reconstructing z s is not powerful enough to train E v (·), and G(·) is actually not fully used to guide the training of E v (·), leading to the incapability of inverting real images. To solve these problems, we use a totally different strategy to train an encoder for GAN inversion as in <ref type="bibr" target="#b9">[10]</ref>. There are two main differences compared with the conventional framework: (a) the encoder is trained with real images rather than with synthesized images, making it more applicable to real applications; (b) the reconstruction is at the image space instead of latent space, which provides semantic knowledge and accurate supervision and allows integration of powerful image generation losses such as perceptual loss <ref type="bibr" target="#b45">[46]</ref> and LPIPS <ref type="bibr" target="#b46">[47]</ref>. Hence, the training process can be formulated as</p><formula xml:id="formula_1">min Θ Ev LE v = ||x − G(Ev(x))|| 2 2 +λ1||F (x)−F (G(Ev(x)))|| 2 2 −λ2E[Dv(G(Ev(x)))],<label>(2)</label></formula><formula xml:id="formula_2">min Θ Dv LD v = E[Dv(G(Ev(x)))]−E[Dv(x)]+ λ3 2 E[||∇xDv(x)|| 2 2 ],<label>(3)</label></formula><p>where Θ Ev and Θ Dv are learnable parameters, λ 1 and λ 2 are the perceptual and discriminator loss weights, λ 3 is the hyper-parameter for the gradient regularization, and F (·) denotes the VGG feature extraction model. Through the learned image encoder, we can map a real image into the W space and obtain a latent code. The obtained code is guaranteed to align with the semantic domain of the StyleGAN generator and can be further utilized to mine cross-modal similarity between the imagetext instance pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training an Text Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Visual-Linguistic Similarity Learning</head><p>Once the inversion module is trained, given a real image, we can map it into the W space of StyleGAN. The next problem is how to train a text encoder that learns the associations and alignments between image and text. The previous methods usually use a submodule named Deep Attentional Multimodal Similarity Model (DAMSM) <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b47">[48]</ref> as the text-image matching loss, which learns two neural networks that map subregions of the image and words of the sentence to a common semantic space, thus measures the image-text similarity at the word level to compute a finegrained loss for image generation. However, this module is cumbersome and fails to explore the relationships between face attributes and sentence fragments, leading to entangled attributes and limited matching accuracy.</p><p>Instead of training a text encoder in the same way as the image encoder or the aforementioned DAMSM, we Since the texts and images are mapped into the common latent space, we can synthesize images with certain attributes by selecting attribute-specific layers. The control mechanism mixes layers of the style code w s by partially replacing corresponding layers of the content w c . When w s is a randomly sampled latent code, it is the text-to-image generation and when w s is the image embedding, it performs textguided image manipulation.</p><p>propose a visual-linguistic similarity module to project the image and text into a common embedding space, i.e., the W space, as shown in <ref type="figure">Figure 2</ref> (a). Given a real image and its descriptions, we encode them into the W space by using the previously trained image encoder and a text encoder. The obtained latent code is the concatenation of L different C-dimensional w vectors, one for each input layer of StyleGAN. The multi-modal alignment can be trained with</p><formula xml:id="formula_3">min Θ E l L E l = || L i=1 p i (w v i − w l i )|| 2 2 ,<label>(4)</label></formula><p>where Θ E l represents the parameters of the text encoder E l (·) and subscript l means linguistic; w v , w l ∈ W L×C are the obtained image embedding and text embedding;</p><formula xml:id="formula_4">w v = f (E v (x))</formula><p>is the projected code of the image embedding z in the input latent space Z using a non-linear mapping network f : Z → W; w l shares a similar definition; w v and w l are with the same shape L × C, meaning to have L layers and each with a C-dimensional latent code; and p i is the weight of i-th layer in the latent code. Compared with DAMSM, our proposed module is lightweight and easy to train. More importantly, this module achieves instance-level alignment, i.e., learning correspondences between visual and linguistic attributes, by leveraging the disentanglability of StyleGAN. The text encoder is trained with the proposed visual-linguistic similarity loss together with the pairwise ranking loss <ref type="bibr" target="#b6">[7]</ref>, which is omitted from Equation 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Instance-Level Optimization</head><p>One of the main challenges of face manipulation is the identity preservation. Due to the limited representation capability, learning a perfect reverse mapping with an encoder alone is not easy. To preserve identity, some recent methods <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b48">[49]</ref> incorporate a dedicated face recognition loss <ref type="bibr" target="#b49">[50]</ref> to measure the cosine similarity between the The woman is wearing lipstick. She has long straight hair. She is young and attractive. output image and its source. Different from their methods, for text-guided image manipulation, we implement an instance-level optimization module to precisely manipulate the desired attributes consistent with the descriptions while faithfully reconstructing the unconcerned ones. We use the inverted latent code z as the initialization, and the image encoder is included as a regularization to preserve the latent code within the semantic domain of the generator. To summarize, the objective function for optimization is</p><formula xml:id="formula_5">z * = arg min z ||x − G(z)|| 2 2 + λ 1 ||F (x) − F (G(z))|| 2 2 + λ 2 ||z − E v (G(z))|| 2 2 ,<label>(5)</label></formula><p>where x is the original image to manipulate. λ 1 and λ 2 are hyperparameters corresponding to the perceptual loss and the encoder regularization term, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Control Mechanism</head><p>In this section, we demonstrate how the two different tasks (text-to-image generation and text-guide image manipulation) and different modalities (image, text, sketch, and label) can be unified into one framework by a simple control mechanism. Attribute-Specific Selection. Our control mechanism is based on the style mixing of StyleGAN. The layer-wise representation of StyleGAN learns disentanglement of semantic fragments (attributes or objects). In general, different layer w i represents different attributes and is fed into the i-th layer of the generator. Changing the value of a certain layer would change the corresponding attributes of the image. As shown in <ref type="figure" target="#fig_5">Figure 6</ref>, given two codes with the same size w c , w s ∈ W L×C denoting content code and style code, this control mechanism selects attribute-specific layers and mixes those layers of w s by partially replacing corresponding layers of w c . For text-to-image generation, the produced images should be consistent with the textual description, thus w c should be the linguistic code, and randomly sampled latent code with the same size acts as w s to provide diversity. For text-guided image manipulation, w c is the visual embedding while w s is the linguistic embedding; the layers for mixing should be relevant to the text, for the purpose of modifying the relevant attributes only and keeping the unrelated ones unchanged. Supported Modality. The style code w s and content code w c could be sketch, label, image, and noise, as shown in <ref type="figure" target="#fig_5">Figure 6</ref>, which makes our TediGAN feasible for multimodal image synthesis. The control mechanism provides high accessibility, diversity, controllability, and accurateness for image generation and manipulation. Due to the control mechanism, as shown in <ref type="figure">Figure 1</ref>, our method inherently supports continuous operations and multi-modal synthesis for sketches and semantic labels with descriptions. To produce the diverse results, all we need to do is to keep the layers related to the text unchanged and replace the others with the randomly sampled latent code.  <ref type="bibr" target="#b15">(16,</ref><ref type="bibr">512)</ref> and <ref type="bibr" target="#b17">(18,</ref><ref type="bibr">512)</ref>. In general, layers in the generator at lower resolutions control high-level styles such as eyeglasses and head pose, layers in the middle control hairstyle and facial expression, She is wearing lipstick and has brown hair.</p><p>This woman has brown hair and wears lipstick. She is young and attractive.</p><p>He has big lips, pointy nose and straight hair.</p><p>He is attractive and young.  while the final layers control color schemes and fine-grained details. In order to find the correspondence between layers and attributes, we conduct a layerwise analysis by selecting some images with different attributes and performing layerwise style mixing. The results are shown in <ref type="figure" target="#fig_7">Figure 7</ref>. The top figure shows the results of replacing the top-n layers while the bottom demonstrates the results of replacing one specific layer. The real images in the green rectangle are illustrated for comparison with the ones after style mixing. Four sets of images were generated from their respective modality (source content and source style); the rest images were generated by copying a specified subset of styles from style images and taking the rest from the content ones. It is obvious that each layer in StyleGAN roughly controls certain attributes. For example, as illustrated, replacing the first layer of the inverted latent code of Emma Watson with the counterpart of Yann LeCun adds a pair of eyeglasses for her. The layers from 11-14 represent micro features or fine structures, such as stubble, freckles, or skin pores, which can be regarded as the stochastic variation. High-resolution images contain lots of facial details and cannot be obtained by simply upsampling from the lower-resolutions, making the stochastic variations especially important as they improve the visual perception without affecting the main structures and attributes of the synthesized image. Based on empirical observations, we list some attributes represented by different possible layers of a 14-layer StyleGAN in <ref type="table" target="#tab_2">Table I</ref>. When performing style mixing for target attributes, one can refer to <ref type="table" target="#tab_2">Table I</ref> or other layerwise analysis <ref type="bibr" target="#b44">[45]</ref> as a start.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Using A Pretrained Text Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">An optimization problem</head><p>In Section 3.2.1, we introduce a strategy that trains a text encoder to produce text codes sharing the same structure of image codes produced by an image encoder. The improvement of GAN generator goes hand in hand with progress of language models. Since we use a pre-trained generator, what if using a pre-trained language model instead of training a text encoder from scratch? In this section, we demonstrate a simple strategy that feasibly introduces some powerful pretrained language models, e.g., CLIP <ref type="bibr" target="#b13">[14]</ref> or ALIGN <ref type="bibr" target="#b14">[15]</ref>, to replace the visual-linguistic similarity learning module. We take CLIP <ref type="bibr" target="#b13">[14]</ref> as an example. The CLIP model jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of image and text training samples. At test time, given a image and a text, the CLIP encoder computes the cosine similarity S(x, t) between encoded features of given image x and text t.</p><p>In this case, we have the StyleGAN model G pretrained on FFHQ and the pretrained text encoder CLIP ViT/32B text-image semantic similarity network C. The goal is twofold: we want the generated images or manipulated attributes of given images to be firstly visually satisfactory and secondly semantically consistent with the given texts. For image synthesis using a pretrained generator and a pretrained text encoder for the two-fold goal, the most intuitive way is to solve the following optimization problem:</p><formula xml:id="formula_6">z * = arg min z ||z − E v (x)|| 2 2 + λL CLIP (x, t),<label>(6)</label></formula><p>where x = G(z) and L CLIP (x, t) = 1 − S(x, t). For manipulation, given an image x, we obtain its inverted latent code z as the initialization for the optimization, using the inversion module in Section 3.1 or other available inversion methods <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b48">[49]</ref>. For image generation, we use the randomly sampled or mean codes as the initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Parameter Sensitivity</head><p>Generally, the last term in Equation <ref type="formula" target="#formula_6">(6)</ref> is for the semantic consistency between the generated image and the given text, while the other forces the unrelated attributes or regions unchanged and produces results with a plausible appearance. However, Equation <ref type="formula" target="#formula_6">(6)</ref> is very sensitive to the parameter. Different edits require different parameters of the semantic consistent term and it is hard to decide what λ to use before we start. One possible reason for this phenomenon, which we call parameter sensitivity, is that both pretrained models G and C are trained for different tasks and it is hard to find the exact optimum for both models when optimizing a single instance. One solution is to add additional image reconstruction term and perceptual term to help stabilize the optimization process:</p><formula xml:id="formula_7">z * = arg min z ||x − G(z)|| 2 2 + λ 1 ||F (x) − F (G(z))|| 2 2 + λ 2 ||z − E v (G(z))|| 2 2 + λ 3 L CLIP (x, t).<label>(7)</label></formula><p>This optimization can be seen a revised version of instancelevel optimization introduced in Section 3.2.2 by adding a text-image semantic consistent term. The original latent term and the two additional reconstruction and perceptual terms in Equation <ref type="formula" target="#formula_7">(7)</ref> make the image reconstruction higher priority than the goal of making textually consistent changes. We found in experiments that the aforementioned improvements decrease the parameter sensitivity and most cases can be well-handled using one fixed parameter setting. Another solution lies in the choices of optimizers. We use He has bread and black hair.</p><p>This woman has black long hair and wears earrings. She is smiling. <ref type="figure">Fig. 9</ref>. Diverse text-to-image generation.</p><p>the gradient-based algorithm Adam <ref type="bibr" target="#b50">[51]</ref>. Besides gradient descent, some other optimization strategies like Covariance Matrix Adaptation (CMA) <ref type="bibr" target="#b51">[52]</ref> can be used to find optimal results for both models G and C. Some concurrent GAN inversion methods <ref type="bibr" target="#b52">[53]</ref> demonstrate that gradient-free optimizers find better solutions than gradient-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Region-of-interest Manipulation</head><p>Such optimization can be further extended to the regionof-interest manipulation, which focuses the optimization on the selected region <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b53">[54]</ref>. Similar to the inversion module introduced in Section 3.1, we train another image encoder, which takes a masked image x ⊗ m and a mask m, instead of taking an image x as input:</p><formula xml:id="formula_8">min Θ Em L Em = ||x − x r || 2 2 + λ 1 ||F (x) − F (x r )|| + λ 2 ||z − E m (x m )||,<label>(8)</label></formula><p>where x = G(z), x r = G(E m (x m )), x m = (x ⊗ m, m), m is the mask, and (·, ·) operator means channel concatenation. The main difference between Equation <ref type="formula" target="#formula_1">(2)</ref> and Equation (8) is obvious: instead of taking only the masked image as input, E m takes both the masked image x ⊗ m and the mask m. The reason is that, with this mask, we can tell the encoder explicitly which regions are "unknown" and unsupervisedly learn an inherent image representation that allows completion from only partial observations. Given only partial images as input, the encoder is encouraged to simply fit the values of these unknown pixels from the known pixels and produce unpleasing results. The encoder E m provides additional flexibility, allowing the generator to create user-specified visual contents in the region of interest while leaving the unrelated parts of the image unchanged. Once the image encoder is trained, given a source image x, a mask m and a description t, we can manipulate the masked region with given text using </p><formula xml:id="formula_9">z * = arg min z ||x⊗m−x r ⊗m)|| 2 2 +λ 1 ||F (x)−F (x r )|| + λ 2 ||z − E m (x m )|| + λ 3 L CLIP (x r , t).<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments Setup</head><p>Multi-Modal CelebA-HQ Dataset. To facilitate text-guided multi-modal synthesis, we propose the MULTI-MODAL-CELEBA-HQ, a large-scale face image dataset that has 30,000 high-resolution face images by following CelebA-HQ <ref type="bibr" target="#b17">[18]</ref>. Each image has a high-quality semantic segmentation map, a sketch, a descriptive text, and an image with transparent background, as shown in <ref type="figure" target="#fig_8">Figure 8</ref>. For text descriptions, following the format of the popular CUB <ref type="bibr" target="#b15">[16]</ref> and COCO <ref type="bibr" target="#b16">[17]</ref> datasets, we create ten unique single sentence descriptions for each image in CelebA-HQ <ref type="bibr" target="#b17">[18]</ref>. The CelebA-HQ dataset consists of facial images and their attributes, where the attribute labels are used to generate natural he has no bread a smiling young women she is young and attractive she is old a young man this woman has brown hair she is smiling her hair is black <ref type="figure">Fig. 10</ref>. Image synthesis from sketches of different styles. The sketches in the first roware real human-drawn sketches. The second are degradations of one sample randomly chosen from our sketch data, from top to bottom adding noisy scribbles, mask, colorful text, and irregular shape.</p><p>sounding textual descriptions. The total dataset consists of 30000 images, which we divide into 80% training and 20% test samples. A similar dataset <ref type="bibr" target="#b54">[55]</ref> is also proposed but it is not publicly available. For labels, we use the labels from CelebAMask-HQ <ref type="bibr" target="#b55">[56]</ref>, which contains manually-annotated semantic mask of facial attributes corresponding to CelebA-HQ. To produce a sketch for each image, we first apply Photocopy filter in Photoshop to extract edges, which preserves facial details and introduces excessive noise. We then apply the sketch-simplification <ref type="bibr" target="#b56">[57]</ref> to get edge maps resembling hand-drawn sketches. The data generation pipeline is the same as in <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b57">[58]</ref>. We also provided each image with transparent background. For background removing, we use an open-source tool Rembg <ref type="bibr" target="#b58">[59]</ref> and a commercial software remove.bg <ref type="bibr">[60]</ref>. Different backgrounds can be further added using image composition or harmonization methods like DoveNet <ref type="bibr" target="#b59">[61]</ref>. The introduced MULTI-MODAL-CELEBA-HQ DATASET can be used to train and evaluate algorithms of text-to-image-generation, text-guided image manipulation, sketch-to-image generation, and other various GANs for face generation and editing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Models and Evaluation Metric.</head><p>We evaluate our proposed method on text and image partitions, comparing with state-of-the-art approaches AttnGAN <ref type="bibr" target="#b4">[5]</ref>, Con-trolGAN <ref type="bibr" target="#b32">[33]</ref>, DM-GAN <ref type="bibr" target="#b33">[34]</ref>, and DFGAN <ref type="bibr" target="#b29">[30]</ref> for textto-image generation, and comparing with ManiGAN <ref type="bibr" target="#b36">[37]</ref> for text-guided image manipulation. All methods are retrained with the default settings on the proposed Multi-Modal CelebA-HQ dataset. We did not compare with some concurrent methods <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b60">[62]</ref> since either they have compared with our method <ref type="bibr" target="#b60">[62]</ref> or their implementation is not publicly available <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. For evaluation, there are four equally important aspects: quality, diversity, accuracy, and realism. Following the previous methods <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b38">[39]</ref>, we evaluate the quality of generated or manipulated images using Fréchet Inception Distance (FID) <ref type="bibr" target="#b61">[63]</ref>. The image diversity is measured by the Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b46">[47]</ref>. The accuracy and realism are evaluated through a user study. For realism, users are asked to judge which one is more photo-realistic among the given results of the aforementioned methods. The accuracy of generation is evaluated by the similarity between the given description and the corresponding generated image, where users are asked to judge which image is more coherent with the given text. To evaluate the accuracy of manipulation, besides the modified visual attributes of the synthetic image are aligned with the text, users are also asked to judge whether the text-irrelevant contents are preserved. We test accuracy and realism by randomly sampling 50 images with the same conditions and collect more than 20 surveys from different people with various backgrounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Network Architecture</head><p>For experiment in Section 3.2, we use the a 14-layer Style-GAN generator as the pretrained GAN model for inversion. The image encoders are based on <ref type="bibr" target="#b9">[10]</ref>. The text encoder is based on <ref type="bibr" target="#b4">[5]</ref> with an additional non-linear mapping component the same as in the image encoders. We use the recently proposed BERT model <ref type="bibr" target="#b62">[64]</ref> instead of the original LSTM <ref type="bibr" target="#b63">[65]</ref> in <ref type="bibr" target="#b4">[5]</ref> to improve text embedding. For experiments in Section 3.3, we use the CLIP as the pretrained text encoder. For high-resolution results in both parts, we use pSp <ref type="bibr" target="#b12">[13]</ref> and StyleGAN2 <ref type="bibr" target="#b8">[9]</ref> as encoder and generator, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Details</head><p>We first train the StyleGAN inversion module. Once the image encoder learns how to invert a real image into the StyleGAN latent space <ref type="bibr" target="#b7">[8]</ref>, we then train the visual-linguistic similarity module to make the text and image latent code match. For StyleGAN inversion training, We basically follow the framework of Zhu et al. <ref type="bibr" target="#b9">[10]</ref>. The StyleGAN model we used for inversion is trained on FFHQ dataset <ref type="bibr" target="#b7">[8]</ref>, which contains 70,000 high-quality face images, and the latent codes for different layers are different. When training the image encoder E v , the generator is fixed and we only update the encoder and discriminator according to Equation <ref type="formula" target="#formula_1">(2)</ref> and Equation <ref type="formula" target="#formula_2">(3)</ref>. As for the perceptual loss in Equation <ref type="formula" target="#formula_1">(2)</ref>, we take conv4 3 as the VGG output. The hyper-parameters λ 1 in Equation <ref type="formula" target="#formula_1">(2)</ref> and λ 1 in Equation <ref type="formula" target="#formula_5">(5)</ref> and Equation <ref type="formula" target="#formula_6">(6)</ref> are set as 5e −5 , λ 2 = 0.1, λ 2 = 2 and λ 3 in Equation <ref type="formula" target="#formula_2">(3)</ref> is 10. λ 3 in Equation <ref type="formula" target="#formula_7">(7)</ref> is set as 200 in most cases. For visuallinguistic similarity module, we fix the image encoder and original inversion a woman without makeup a young man she's Asian she has grey hair she is tanned original inversion he has no bread a clean-shaven man a smooth-faced man he is beardless he has no facial hair <ref type="figure">Fig. 11</ref>. Image manipulation results of using a pretrained text encoder CLIP <ref type="bibr" target="#b13">[14]</ref>. The first row of images shows manipulated results using texts in (makeup, young, and grey hair ) and out (asia and tanned) of the proposed dataset. The second row of images are manipulated with similar descriptions of beardless.</p><p>train the text encoder using the images and texts in the introduced dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with State-of-the-Art Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Text-to-Image Generation</head><p>Quantitative Comparison. In our experiments, we evaluate the FID and LPIPS on a large number of samples generated from randomly selected text descriptions. To evaluate accuracy and realism, we generate images from 50 randomly sampled texts using different methods. In a user study for accuracy, users are asked to judge which one is the most coherent with the given texts while in another for realism to choose the most photo-realistic image. We use TediGAN-A and TediGAN-B to refer to the two strategies proposed in Section 3.2 and Section 3.3. The results are demonstrated in <ref type="table" target="#tab_2">Table II</ref>. Compared with the state-of-thearts, our method achieves better FID, LPIPS, accuracy, and realism values, which proves that our methods can generate images with the highest quality, diversity, photorealism, and text-relevance. Qualitative Comparison. <ref type="figure" target="#fig_3">Figure 4</ref> demonstrates the visual comparisons of different text-to-image generation methods and our two proposed strategies. As shown, most methods can generate photo-realistic and text-relevant results. When comparing comprehensively, however, the gap of performance is obvious. For example, some of the attributes contained in the text sometimes do not appear in the generated image, e.g., lipstick for AttnGAN, gender for DF-GAN, and hairstyle for ControlGAN. The generated images look like featureless paints and lack essential details that make them indistinguishable from the real images. This "featureless painterly" appearance <ref type="bibr" target="#b7">[8]</ref> would be significantly obvious and irredeemable when generating higher resolution images using the multi-stage training methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> due to their incompetent to generate facial details like stubble, freckles, or skin pores. Furthermore, most existing solutions have limited diversity of the outputs, even if the provided conditions contain different meanings. For example, "has a beard" might mean a goatee, short or long beard, and could have different colors. Our method can not only generate results with diversity but also realise the expectation to change where you want by using the control mechanism.</p><p>To produce diverse results, with the layers related to the text unchanged, the other layers could be replaced by any values sampled from the prior distribution. For example, as shown in the first row of <ref type="figure">Figure 9</ref>, the key visual attributes (women, black long hair, earrings, and smiling) are preserved, while the other attributes, like haircuts, makeups, face shapes, and head poses, show a great degree of diversity. The images in the second row illustrate more precise control ability. We keep the layers representing face shape and head pose the same and change the others. Our method is also able to generate high-quality and diverse results with resolution at 1024 × 1024 from multimodal inputs, e.g., free-drawn sketches or semantic labels with textual descriptions. The results are shown in <ref type="figure" target="#fig_1">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Text-Guided Image Manipulation</head><p>Quantitative Comparison. In our experiments, we evaluate the FID score and conduct a user study on accuracy and realism by selecting images randomly from both CelebA and Non-CelebA datasets with randomly chosen descriptions from our proposed dataset and some open-world descriptions (marked as Open-Text). The quantitative results are shown in <ref type="table" target="#tab_2">Table III</ref>. Compared with ManiGAN <ref type="bibr" target="#b36">[37]</ref>, both of our proposed strategies achieve better FID, accuracy, and realism. This indicates that our method can produce highquality synthetic images, and the modifications are highly aligned with the given descriptions, while preserving other text-irrelevant contents. Qualitative Comparison. <ref type="figure" target="#fig_4">Figure 5</ref> shows the visual comparisons between a recent method ManiGAN <ref type="bibr" target="#b36">[37]</ref> and our two proposed strategies, TediGAN-A and TediGAN-B. ManiGAN produces less satisfactory modified results: the text-relevant regions are not modified and the text-irrelevant ones are changed. In some cases, especially for the difficult attributes and out-of-distribution images, the obtained results are basically the replications of the original images with worse quality. For example, the second row is the results of adding earrings and changing the face shape and hair style of the pictured woman. Our method completes this difficult case while ManiGAN fails to produce required attributes. The images in last two columns are results of out-of-distribution (Non-CelebA), i.e., images from other face dataset such as <ref type="bibr" target="#b64">[66]</ref>, <ref type="bibr" target="#b65">[67]</ref>, <ref type="bibr" target="#b66">[68]</ref>, which illustrate that our method is prepared to produce pleasing results for the in-the-wild images. Compared with TediGAN-A, which is the initial strategy proposed in the conference version <ref type="bibr" target="#b18">[19]</ref>, the newly proposed TediGAN-B can further handle image creation from openworld images or texts and region-of-interest manipulation, which will be discussed in Section 4.5 and Section 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Diving Deep into Open-World Images and Texts</head><p>In this section, we illustrate the results of images and texts with real-world variances. The StyleGAN we used is pretrained on a very large face dataset <ref type="bibr" target="#b7">[8]</ref>. The robust encoders support inverting open-world images, labels, and sketches into the latent space of StyleGAN and enables us to synthesis images from different kinds of images or sketches. The results from texts and sketches of different styles are shown in <ref type="figure">Figure 10</ref>. The sketches in the first row are real human-drawn sketches. The second are different degradations of one sample randomly chosen from our sketch data, from top to bottom adding noisy scribbles, mask, colorful text, and irregular shape. Our method still produces plausible results despite that the model has not seen these difficult variants during training.</p><p>In <ref type="figure">Figure 11</ref>, we show some manipulated results by our TediGAN-B. The first row of images shows manipulated results using texts in (makeup, young, and grey hair) and out of (asia and tanned) the proposed dataset while the second are manipulated with similar descriptions of beardless. The results of image generation with open-world texts can be found in <ref type="figure">Figure 12</ref>. The images in each column are obtained from the same randomly-sampled latent code according to different texts. Each description has the form of "he/she is/has/looks like". We omit these words in the figure and leave only the keywords. We illustrate certain effects of our method in different columns. The first two columns are about changing facial expression. Such experiments are very common in the GAN inversion methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b44">[45]</ref>. Our method directly change the facial expressions without explicitly training a boundaries for attributes in advance like in <ref type="bibr" target="#b44">[45]</ref>. The third column shows precise controllability of the age. In addition to young and old, more detailed instructions are also supported. The images in the next four columns show results of different hairstyles, face shapes, makeups, and small changes around the eye area. These results prove that our method can change the main structure of the face and precisely synthesize the tiny but nonnegligible attributes. The visual attributes can be controlled with explicit descriptions, e.g., beard or blonde, as shown in the first three columns in <ref type="figure">Figure 12</ref>. In the last column, we show some results implicitly controlled by indicating a celebrity, i.e., Joe Biden, Donald Trump, and Barack Obama. 'He looks like somebody' may refer to the identity or some important characteristics of these celebrities like hairstyle or skin color. Obviously, our method regards these characteristics as more important than the identity. <ref type="figure" target="#fig_1">Figure 13</ref> demonstrates that our method can edit painting, caricature, and black and white photo images guided by textual descriptions without any re-training, fine-tuning, or post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Region-of-interest Manipulation</head><p>This section illustrates some results of the region-of-interest editing introduced in Section 4.6. We mask certain regions in the images and inpaint them with text guidance. The results are shown in <ref type="figure" target="#fig_3">Figure 14</ref>. The first two rows are editing results of masked regions. The third row is for comparison. For the single-word descriptions blue and red, the results in the second row are primarily restricted to the designated regions. For the third row, since the the editing areas are not designated, the editing maybe occurs in any possible regions, e.g., blue for eyes and red for clothes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ABLATION STUDY AND DISCUSSION</head><p>At present, we follow the principle of minimization. Each module is the most simplified without any elaboratelydesigned architectures or tricks. Removing any module, the output result cannot be obtained. For each module, we also use the most simple solutions, e.g., the L2-norm in visuallinguistic similarity learning in Section 3.2.1 or the gradientbased algorithm Adam <ref type="bibr" target="#b50">[51]</ref> for optimization in Section 3.3. Adding well-designed modules can undoubtedly improve performance, but this goes against our original intention. Thus, we organize the ablation study by illustrating the effectiveness of each module, discussing the weakness, and analyzing possible reasons. Instance-Level Optimization. The comparison of with or without instance-level optimization is shown in <ref type="figure" target="#fig_4">Figure 15</ref>. As shown in <ref type="figure" target="#fig_4">Figure 15</ref> (c), the inversion results of the image encoder preserve all attributes of the original images, which is sufficient for text-to-image generation since there is no identity to preserve. Manipulating a given image according to a text, however, should not change the unrelated attributes especially one's identity, which is preserved after the instance-level optimization <ref type="figure" target="#fig_4">(Figure 15 (d)</ref>). We also compare with another strategy to preserve identity proposed by a recent inversion-based image synthesis method pSp <ref type="bibr" target="#b12">[13]</ref> that incorporates a dedicated recognition loss <ref type="bibr" target="#b49">[50]</ref> during training. Both methods can find a reasonable latent code for real face images. Despite both achieving the goal of identity preservation, the optional instance-level optimization makes our framework concise and easy to train, and provides a non-deterministic way to refine the final results accordingly. To be specific, pSp produces one deterministic result while ours provides a non-deterministic way that can refine the final results accordingly. For example, one can choose to directly output the results of the encoder, or optimize for steps. Through our empirical observation, 20 iterations are good enough to preserve identity. Visual-Linguistic Similarity. As AttnGAN shown, the global sentence vectors are used to "sketch the primitive shape and colors of objects". This inspires us that the global information of facial images can be seen as main structures such as face shape, which is consistent with the hierarchically semantic property of Style-based generators. The text encoder is trained using our visual-linguistic similarity and a very simple pairwise ranking loss <ref type="bibr" target="#b6">[7]</ref> to align text and image embedding, as described in Section 3.2.1. This pairwise ranking loss maximizes the cosine similarity of mismatching pairs, and minimizes the similarity of matched pairs. Although the learned text embedding can handle near-miss  cases, as shown in <ref type="figure" target="#fig_5">Figure 16</ref>, we found this plain strategy sometimes may lead to insufficient disentanglement of attributes and mismatching of image-text alignment, leaving some room for improvement. Potential Issue with StyleGAN. In our experiments, we found that some text-unrelated attributes are unwantedly changed when manipulating a given image according to a text description. In the first place, we thought this may be caused by the proposed visual language similarity learning module. However, when performing layer-wise stylemixing on the inverted codes of two real images, it turns out that such interference still occurs. This means some important facial attributes remain entangled in the W space. Under ideal circumstances, different facial attributes should be orthogonal (meaning without affecting other attributes). Another inherent defect of StyleGAN is that some attributes, such as hats, necklaces, and earrings, are not well represented in its latent space. This sometimes makes our method perform less satisfactorily when adding or removing jewelry or accessories through natural language descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">LIMITATION AND OUTLOOK</head><p>Expensive Image-specific Optimization. Due the lowdimensionality of the latent code, the directly inverted results often do not resemble the given images. Typically, a slight difference between the identities of the manipulated result and the given image is observed. To preserve identities and unrelated regions, we use iterative optimization during inference in both strategies of our method. As such preservation usually require expensive image-specific optimization at runtime, the whole process is often timeconsuming. Some recent methods <ref type="bibr" target="#b67">[69]</ref>, <ref type="bibr" target="#b68">[70]</ref>, <ref type="bibr" target="#b69">[71]</ref> of image restoration found that inversion-based methods often generate images with low-fidelity. Unlike most GAN inversion methods, which also use pre-trained GANs, they employ the pre-trained GANs as prior, e.g., designing a latent bank in a succinct encoder-bank-decoder architecture. To get rid of restrictions, instead of directly using the unmodified latent space of the style-based generators, we can use these pre-trained GANs to provide rich and diverse priors. The textual guidance can be integrated in a similar way as in the previous text-to-image generation methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b32">[33]</ref>. One Certain Class Limitation. It is a natural idea to adopt our method in dozens of classes where the StyleGAN generates high-resolution images, like cars, birds or any categories in the lsun dataset <ref type="bibr" target="#b70">[72]</ref>. Due to the limitation of StyleGAN itself, however, the state-of-the-art performance of our method is within a narrow image domain. Stylebased generators, not limited to StyleGAN, are suitable for images with certain classes and structures like faces <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b18">[19]</ref>, birds <ref type="bibr" target="#b15">[16]</ref>, or bedrooms <ref type="bibr" target="#b70">[72]</ref>. It may perform lesssatisfactory on datasets that contain various objects or scenes like COCO <ref type="bibr" target="#b16">[17]</ref> or ImageNet <ref type="bibr" target="#b71">[73]</ref>. Some recent generation or restoration methods using pretrained GANs as prior are also limited to certain classes <ref type="bibr" target="#b67">[69]</ref>, <ref type="bibr" target="#b68">[70]</ref>. Training a GAN to support the synthesis of images from many classes is still a challenging problem, especially when the fine-grained representations with multiple semantic levels are required. We notice that a concurrent zero-shot textto-image system called DALL-E <ref type="bibr" target="#b34">[35]</ref> can generate arbitrary classes of images from open-world descriptions. It is trained on a far broader variety of unconstrained images and texts. Roughly speaking, DALL-E is a combination of three separately-trained parts: image encoder and decoder dVAE, text-encoder Transformer, and text-image matching CLIP. Yet notwithstanding the breakthrough of the limitations of one certain class, the quality and resolution of its results are very limited. This limitation of DALL-E is primarily inherited from the dVAE. To break current limitations of our method on open-class and DALL-E on high-resolution, a novel generator that can generate images of arbitrary scenes or structures with plausible quality and resolution is indispensable. Supported Modalities. In addition to text, label, and sketch that supported in our method, other modalities, like 3D shapes or sounds, can also be used as intermediaries in visual content creation. Is it possible for our proposed method to support 3D shapes or sounds? Or in other words, He has / has no beard. She has brown / black hair. what if we want to generate and manipulate 2D images or 3D shapes through oral instructions directly without "lifting a finger"? The simplest way to support verbal orders, i.e., the modality of sound, although not elegant enough, is to turn the sound into the text input we need through a speech recognition system. To generate 3D shapes directly from text or sound, we can easily integrate our method with a concurrent shape reconstruction method <ref type="bibr" target="#b72">[74]</ref> for the reason that we share the same latent space of a pretrained GAN model. In the future, we will exploit elegant solutions for efficient language-driven (including speech and word) generation and manipulation of 2D images and 3D shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this work, we propose a novel method for image synthesis using textual descriptions, where two strategies are developed to use the strong generative powers of Style-GAN. Both strategies can unify the two different tasks (textguided image generation and manipulation) into the same framework and achieves high accessibility, diversity, controllability, and accurateness for facial image generation and manipulation. Through the two proposed strategies using a pretrained StyleGAN as prior and a large-scale multi-modal face dataset, our method can effectively synthesize images from multi-modal inputs with unprecedented quality. Extensive experimental results demonstrate the superiority of our method, in terms of the effectiveness of image synthesis, the capability of generating high-quality results, the extendability for multi-modal inputs, and the controllability of region-of-interest editing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 2. Overview of our proposed method. We propose two strategies to use a pretrained GAN model. (a) demonstrates the first strategy. The key idea is to project multi-modal embedding into the common W space of StyleGAN. Taking visual and linguistic embedding for example, with the learned the inversion module, we can then learn the visual-linguistic similarity, where the visual embedding w v and linguistic embedding w l are expected to be close enough. The instance-level optimization if for identity preservation. The edited image can be generated from the StyleGAN generator. (b) illustrates the inference of text-guided image manipulation using the text encoder. Given a source image and a text guidance, we first get their embedding w v and w l in W space through corresponding encoders. We then perform style mixing for target layers and get the target latent code w t . The final w t * is obtained through instance-level optimization. For image generation, we can directly obtain the results by feeding the latent codes from the text encoder into the generator. (c) is the illustration of text-guided image manipulation using a pretrained language model. In (d), we show that such optimization can be easily extended to support region-of-interest manipulation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Diverse high-resolution results from multimodal inputs with textual guidance. Our method achieves text-guided diverse image generation and manipulation up to an unprecedented resolution at 1024 × 1024.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Comparison of text-to-image generation on our Multi-modal CelebA-HQ dataset. TediGAN-A and -B represents two strategies proposed in Section 3.2 and Section 3.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative comparison of image manipulation using natural language descriptions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Control mechanism of our TediGAN framework. Different layer in the StyleGAN generator represents different attributes. Changing the value of a certain layer would change the corresponding attributes of the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Layerwise analysis of control mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Data sample of the Multi-Modal-CelebA-HQ dataset. From left to right are textual descriptions, real images, semantic labels, sketches, and images with transparent background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 14 .</head><label>14</label><figDesc>Results of region-of-interest editing. The first two rows are editing results of masked regions. The third row is for comparison. For the single-word descriptions blue and red, the results in the second row are primarily restricted to the designated regions. For the third row, since the the editing areas are not designated, the editing maybe occurs in any possible regions, e.g., blue for eyes and red for clothes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 15 .</head><label>15</label><figDesc>Inversion results. (a) original image; (b) inversion result of pSp [13]; (c) inversion result of our image encoder (Section 3.1); (d) inversion results after our optimization (Section 3.2.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 16 .</head><label>16</label><figDesc>Illustration of near-miss cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>If we want to generate images from other modality with text guidance, take the sketch as an example, we can train an additional sketch image encoder E vs in the same way as training the real image encoder and leave the other parts unchanged.</figDesc><table /><note>Layerwise Analysis. For Style-based generators, the image size R is determined by its number of layers L: L = 2 log 2 R− 2. For example, the pre-trained StyleGAN we used in most experiments is to generate images of size 256 × 256, which has 14 layers of the intermediate vector. For a synthesis network trained to generate images of 512 and 1024, the intermediate vector would be of shape</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I</head><label>I</label><figDesc>The empirical layerwise analysis of a 14-layer StyleGAN generator. The 13-th and 14-th layers are omitted since there is basically no visible difference.</figDesc><table><row><cell>n-th</cell><cell>attribute</cell><cell>n-th</cell><cell>attribute</cell></row><row><cell>1</cell><cell>eye glasses</cell><cell>7</cell><cell>hair color</cell></row><row><cell>2</cell><cell>head pose</cell><cell>8</cell><cell>face color</cell></row><row><cell>3</cell><cell>face shape</cell><cell>9</cell><cell>age</cell></row><row><cell>4</cell><cell>hair length, nose, lip</cell><cell>10</cell><cell>gender</cell></row><row><cell>5</cell><cell>cheekbones</cell><cell>11</cell><cell>micro features</cell></row><row><cell>6</cell><cell>chin</cell><cell>12</cell><cell>micro features</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II</head><label>II</label><figDesc>Quantitative comparison of text-to-image generation. We compare the state-of-the-arts and our method in terms of FID, LPIPS, accuracy (Acc.), and realism (Real.). ↓ means the lower the better while ↑ means the opposite.</figDesc><table><row><cell>Method</cell><cell>FID ↓</cell><cell cols="3">LPIPS ↓ Acc. (%) ↑ Real. (%) ↑</cell></row><row><cell>AttnGAN [5]</cell><cell>125.98</cell><cell>0.512</cell><cell>13.0</cell><cell>11.9</cell></row><row><cell cols="2">ControlGAN [33] 116.32</cell><cell>0.522</cell><cell>14.6</cell><cell>13.1</cell></row><row><cell>DFGAN [30]</cell><cell>137.60</cell><cell>0.581</cell><cell>17.3</cell><cell>14.5</cell></row><row><cell>DM-GAN [34]</cell><cell>131.05</cell><cell>0.544</cell><cell>16.4</cell><cell>16.9</cell></row><row><cell>TediGAN-A</cell><cell>106.37</cell><cell>0.456</cell><cell>18.4</cell><cell>22.6</cell></row><row><cell>TediGAN-B</cell><cell>101.42</cell><cell>0.461</cell><cell>20.4</cell><cell>21.0</cell></row><row><cell></cell><cell cols="2">TABLE III</cell><cell></cell><cell></cell></row><row><cell cols="5">Quantitative comparison of text-guided image manipulation. We</cell></row><row><cell cols="5">compare our method with the state-of-the-art ManiGAN [37] in terms of</cell></row><row><cell cols="4">FID, accuracy (Acc.), and realism (Real.).</cell><cell></cell></row><row><cell></cell><cell>Method</cell><cell>FID</cell><cell cols="2">Acc. (%) Real. (%)</cell></row><row><cell></cell><cell cols="2">ManiGAN [37] 117.89</cell><cell>27.4</cell><cell>10.9</cell></row><row><cell>CelebA</cell><cell>TediGAN-A</cell><cell>107.25</cell><cell>34.3</cell><cell>42.6</cell></row><row><cell></cell><cell>TediGAN-B</cell><cell>101.27</cell><cell>38.3</cell><cell>46.5</cell></row><row><cell></cell><cell cols="2">ManiGAN [37] 143.39</cell><cell>16.9</cell><cell>7.4</cell></row><row><cell>Non-CelebA</cell><cell>TediGAN-A</cell><cell>135.47</cell><cell>40.9</cell><cell>45.2</cell></row><row><cell></cell><cell>TediGAN-B</cell><cell>129.20</cell><cell>42.2</cell><cell>47.4</cell></row><row><cell></cell><cell cols="2">ManiGAN [37] 141.51</cell><cell>9.1</cell><cell>10.9</cell></row><row><cell>Open-Text</cell><cell>TediGAN-A</cell><cell>113.57</cell><cell>22.2</cell><cell>43.0</cell></row><row><cell></cell><cell>TediGAN-B</cell><cell>107.32</cell><cell>68.7</cell><cell>44.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interactive sketch &amp; fill: Multiclass sketch-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Text-adaptive generative adversarial networks: Manipulating images with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">AttnGAN: Fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semantic image synthesis via adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5706" to="5714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">In-domain GAN inversion for real image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Gan inversion: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05278</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inverting layers of a large generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Encoding in style: a Style-GAN encoder for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Alaluf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nitzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00951</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Scaling up visual and visionlanguage representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05918</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<title level="m">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tedigan: Text-guided diverse face image generation and manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantically multi-modal image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5467" to="5476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sean: Image synthesis with semantic region-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Language-based colorization of scene sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SC-FEGAN: Face editing generative adversarial network with user&apos;s sketch and color</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to super-resolve blurry face and text images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Text-adaptive generative adversarial networks: manipulating images with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="42" to="51" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Controllable continuous gaze redirection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep singleimage portrait relighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7194" to="7202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cali-sketch: Stroke calibration and completion for high-quality face image generation from poorlydrawn sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Single image portrait relighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Busch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="79" to="80" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">DF-GAN: Deep fusion generative adversarial networks for text-toimage synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Jing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05865</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">StackGAN++: Realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1947" to="1962" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Controllable textto-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">DM-GAN: Dynamic memory generative adversarial networks for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5802" to="5810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12092</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Paint by word</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jahanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10951</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">ManiGAN: Textguided image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Describe what to change: A text-guided unsupervised image-to-image translation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Nadai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lepri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Lightweight generative adversarial networks for text-guided image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Deep captioning with multimodal recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multimodal convolutional neural networks for matching image and sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="201" to="216" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Polysemous visual-semantic embedding for cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1979" to="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Interpreting the latent space of GANs for semantic face editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="586" to="595" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">CPGAN: Full-spectrum contentparsing generative adversarial networks for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Using latent space regression to analyze and leverage compositionality in gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">ArcFace: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Completely derandomized selfadaptation in evolution strategies. evolutionary computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ostermeier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Evoluaionary Computation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Transforming and projecting images into class-conditional generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Image2StyleGAN++: How to edit the embedded images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Conditional image generation and manipulation for user-specified content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bleeker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ibrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ter Hoeve</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Maskgan: Towards diverse and interactive facial image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Learning to simplify: Fully convolutional networks for rough sketch cleanup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">DeepFaceDrawing: Deep generation of face images from sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">TOG</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gatis</surname></persName>
		</author>
		<ptr target="https://github.com/danielgatis/rembg" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Dovenet: Deep image harmonization via domain verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17249</idno>
		<title level="m">Styleclip: Text-driven manipulation of stylegan imagery</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Bert: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Rewards of beauty: the opioid system mediates social motivation in humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chelnokova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Laeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eikemo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Riegels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Løseth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maurud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Willoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leknes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular psychiatry</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="746" to="747" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The Caucasian and North African French Faces (CaNAFF): A face database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Courset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rougier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Palluel-Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Jonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chauvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Review of Social Psychology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">ApdrawingGAN: Generating artistic portrait drawings from face photos with hierarchical gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Towards real-world blind face restoration with generative facial prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Glean: Generative latent bank for large-factor image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Exploiting deep generative prior for versatile image restoration and manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Do 2d gans know 3d shape? unsupervised 3d shape reconstruction from 2d image gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
