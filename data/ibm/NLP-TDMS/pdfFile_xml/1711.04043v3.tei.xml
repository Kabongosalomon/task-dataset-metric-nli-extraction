<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FEW-SHOT LEARNING WITH GRAPH NEURAL NET- WORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Garcia</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
							<email>bruna@cims.nyu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Amsterdam Machine Learning</orgName>
								<orgName type="institution">Lab University of Amsterdam Amsterdam</orgName>
								<address>
									<postCode>1098 XH</postCode>
									<region>NL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Courant Institute of Mathematical Sciences</orgName>
								<orgName type="institution">New York University New York City</orgName>
								<address>
									<postCode>10010</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FEW-SHOT LEARNING WITH GRAPH NEURAL NET- WORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose to study the problem of few-shot learning with the prism of inference on a partially observed graphical model, constructed from a collection of input images whose label can be either observed or not. By assimilating generic message-passing inference algorithms with their neural-network counterparts, we define a graph neural network architecture that generalizes several of the recently proposed few-shot learning models. Besides providing improved numerical performance, our framework is easily extended to variants of few-shot learning, such as semi-supervised or active learning, demonstrating the ability of graph-based models to operate well on 'relational' tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Supervised end-to-end learning has been extremely successful in computer vision, speech, or machine translation tasks, thanks to improvements in optimization technology, larger datasets and streamlined designs of deep convolutional or recurrent architectures. Despite these successes, this learning setup does not cover many aspects where learning is nonetheless possible and desirable.</p><p>One such instance is the ability to learn from few examples, in the so-called few-shot learning tasks. Rather than relying on regularization to compensate for the lack of data, researchers have explored ways to leverage a distribution of similar tasks, inspired by human learning <ref type="bibr" target="#b18">Lake et al. (2015)</ref>. This defines a new supervised learning setup (also called 'meta-learning') in which the input-output pairs are no longer given by iid samples of images and their associated labels, but by iid samples of collections of images and their associated label similarity.</p><p>A recent and highly-successful research program has exploited this meta-learning paradigm on the few-shot image classification task <ref type="bibr" target="#b18">Lake et al. (2015)</ref>; <ref type="bibr" target="#b16">Koch et al. (2015)</ref>; <ref type="bibr" target="#b31">Vinyals et al. (2016)</ref>; <ref type="bibr" target="#b21">Mishra et al. (2017)</ref>; <ref type="bibr" target="#b27">Snell et al. (2017)</ref>. In essence, these works learn a contextual, task-specific similarity measure, that first embeds input images using a CNN, and then learns how to combine the embedded images in the collection to propagate the label information towards the target image.</p><p>In particular, <ref type="bibr" target="#b31">Vinyals et al. (2016)</ref> cast the few-shot learning problem as a supervised classification task mapping a support set of images into the desired label, and developed an end-to-end architecture accepting those support sets as input via attention mechanisms. In this work, we build upon this line of work, and argue that this task is naturally expressed as a supervised interpolation problem on a graph, where nodes are associated with the images in the collection, and edges are given by a trainable similarity kernels. Leveraging recent progress on representation learning for graphstructured data <ref type="bibr" target="#b1">Bronstein et al. (2017)</ref>; <ref type="bibr" target="#b10">Gilmer et al. (2017)</ref>, we thus propose a simple graph-based few-shot learning model that implements a task-driven message passing algorithm. The resulting architecture is trained end-to-end, captures the invariances of the task, such as permutations within the input collections, and offers a good tradeoff between simplicity, generality, performance and sample complexity.</p><p>Besides few-shot learning, a related task is the ability to learn from a mixture of labeled and unlabeled examples -semi-supervised learning, as well as active learning, in which the learner has the Published as a conference paper at ICLR 2018 option to request those missing labels that will be most helpful for the prediction task. Our graphbased architecture is naturally extended to these setups with minimal changes in the training design. We validate experimentally the model on few-shot image classification, matching state-of-the-art performance with considerably fewer parameters, and demonstrate applications to semi-supervised and active learning setups.</p><p>Our contributions are summarized as follows:</p><p>• We cast few-shot learning as a supervised message passing task which is trained end-to-end using graph neural networks. • We match state-of-the-art performance on Omniglot and Mini-Imagenet tasks with fewer parameters.</p><p>• We extend the model in the semi-supervised and active learning regimes.</p><p>The rest of the paper is structured as follows. Section 2 describes related work, Sections 3, 4 and 5 present the problem setup, our graph neural network model and the training, and Section 6 reports numerical experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>One-shot learning was first introduced by <ref type="bibr" target="#b8">Fei-Fei et al. (2006)</ref>, they assumed that currently learned classes can help to make predictions on new ones when just one or few labels are available. More recently, <ref type="bibr" target="#b18">Lake et al. (2015)</ref> presented a Hierarchical Bayesian model that reached human level error on few-shot learning alphabet recongition tasks.</p><p>Since then, great progress has been done in one-shot learning. <ref type="bibr" target="#b16">Koch et al. (2015)</ref> presented a deeplearning model based on computing the pair-wise distance between samples using Siamese Networks, then, this learned distance can be used to solve one-shot problems by k-nearest neighbors classification. <ref type="bibr" target="#b31">Vinyals et al. (2016)</ref> Presented an end-to-end trainable k-nearest neighbors using the cosine distance, they also introduced a contextual mechanism using an attention LSTM model that takes into account all the samples of the subset T when computing the pair-wise distance between samples. <ref type="bibr" target="#b27">Snell et al. (2017)</ref> extended the work from <ref type="bibr" target="#b31">Vinyals et al. (2016)</ref>, by using euclidean distance instead of cosine which provided significant improvements, they also build a prototype representation of each class for the few-shot learning scenario. <ref type="bibr" target="#b20">Mehrotra &amp; Dukkipati (2017)</ref> trained a deep residual network together with a generative model to approximate the pair-wise distance between samples.</p><p>A new line of meta-learners for one-shot learning is rising lately: <ref type="bibr" target="#b23">Ravi &amp; Larochelle (2016)</ref> introduced a meta-learning method where an LSTM updates the weights of a classifier for a given episode. <ref type="bibr" target="#b22">Munkhdalai &amp; Yu (2017)</ref> also presented a meta-learning architecture that learns meta-level knowledge across tasks, and it changes its inductive bias via fast parametrization. <ref type="bibr" target="#b9">Finn et al. (2017)</ref> is using a model agnostic meta-learner based on gradient descent, the goal is to train a classification model such that given a new task, a small amount of gradient steps with few data will be enough to generalize. Lately, <ref type="bibr" target="#b21">Mishra et al. (2017)</ref> used Temporal Convolutions which are deep recurrent networks based on dilated convolutions, this method also exploits contextual information from the subset T providing very good results.</p><p>Another related area of research concerns deep learning architectures on graph-structured data. The GNN was first proposed in <ref type="bibr" target="#b11">Gori et al. (2005)</ref>; <ref type="bibr" target="#b26">Scarselli et al. (2009)</ref>, as a trainable recurrent messagepassing whose fixed points could be adjusted discriminatively. Subsequent works ; <ref type="bibr" target="#b28">Sukhbaatar et al. (2016)</ref> have relaxed the model by untying the recurrent layer weights and proposed several nonlinear updates through gating mechanisms. Graph neural networks are in fact natural generalizations of convolutional networks to non-Euclidean graphs. <ref type="bibr" target="#b3">Bruna et al. (2013)</ref>; <ref type="bibr" target="#b12">Henaff et al. (2015)</ref> proposed to learn smooth spectral multipliers of the graph Laplacian, albeit with high computational cost, and <ref type="bibr" target="#b5">Defferrard et al. (2016)</ref>; <ref type="bibr" target="#b15">Kipf &amp; Welling (2016)</ref> resolved the computational bottleneck by learning polynomials of the graph Laplacian, thus avoiding the computation of eigenvectors and completing the connection with GNNs. In particular, <ref type="bibr" target="#b15">Kipf &amp; Welling (2016)</ref> was the first to propose the use of GNNs on semi-supervised classification problems. We refer the reader to <ref type="bibr" target="#b1">Bronstein et al. (2017)</ref> for an exhaustive literature review on the topic. GNNs and the analogous Neural Message Passing Models are finding application in many different domains. <ref type="bibr" target="#b0">Battaglia et al. (2016)</ref>; <ref type="bibr" target="#b4">Chang et al. (2016)</ref> develop graph interaction networks that learn pairwise particle interactions and apply them to discrete particle physical dynamics. <ref type="bibr" target="#b6">Duvenaud et al. (2015)</ref>; <ref type="bibr" target="#b14">Kearnes et al. (2016)</ref> study molecular fingerprints using variants of the GNN architecture, and <ref type="bibr" target="#b10">Gilmer et al. (2017)</ref> further develop the model by combining it with set representations <ref type="bibr" target="#b30">Vinyals et al. (2015)</ref>, showing state-of-the-art results on molecular prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM SET-UP</head><p>We describe first the general setup and notations, and then particularize it to the case of few-shot learning, semi-supervised learning and active learning.</p><p>We consider input-output pairs (T i , Y i ) i drawn iid from a distribution P of partially-labeled image collections</p><formula xml:id="formula_0">T = {(x 1 , l 1 ), . . . (x s , l s )}, {x 1 , . . . ,x r }, {x 1 , . . . ,x t } ; l i ∈ {1, K}, x i ,x j ,x j ∼ P l (R N ) , and Y = (y 1 , . . . , y t ) ∈ {1, K} t ,<label>(1)</label></formula><p>for arbitrary values of s, r, t and K. Where s is the number of labeled samples, r is the number of unlabeled samples (r &gt; 0 for the semi-supervised and active learning scenarios) and t is the number of samples to classify. K is the number of classes. We will focus in the case t = 1 where we just classify one sample per task T . P l (R N ) denotes a class-specific image distribution over R N . In our context, the targets Y i are associated with image categories of designated imagesx 1 , . . . ,x t ∈ T i with no observed label. Given a training set</p><formula xml:id="formula_1">{(T i , Y i ) i } i≤L , we consider the standard supervised learning objective min Θ 1 L i≤L (Φ(T i ; Θ), Y i ) + R(Θ) ,</formula><p>using the model Φ(T ; Θ) = p(Y | T ) specified in Section 4 and R is a standard regularization objective.</p><p>Few-Shot Learning When r = 0, t = 1 and s = qK, there is a single image in the collection with unknown label. If moreover each label appears exactly q times, this setting is referred as the q-shot, K-way learning.</p><p>Semi-Supervised Learning When r &gt; 0 and t = 1, the input collection contains auxiliary images x 1 , . . . ,x r that the model can use to improve the prediction accuracy, by leveraging the fact that these samples are drawn from common distributions as those determining the output.</p><p>Active Learning In the active learning setting, the learner has the ability to request labels from the sub-collection {x 1 , . . . ,x r }. We are interested in studying to what extent this active learning can improve the performance with respect to the previous semi-supervised setup, and match the performance of the one-shot learning setting with s 0 known labels when s + r = s 0 , s s 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MODEL</head><p>This section presents our approach, based on a simple end-to-end graph neural network architecture. We first explain how the input context is mapped into a graphical representation, then detail the architecture, and next show how this model generalizes a number of previously published few-shot learning architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SET AND GRAPH INPUT REPRESENTATIONS</head><p>The input T contains a collection of images, both labeled and unlabeled. The goal of few-shot learning is to propagate label information from labeled samples towards the unlabeled query image. This propagation of information can be formalized as a posterior inference over a graphical model determined by the input images and labels.</p><p>Following several recent works that cast posterior inference using message passing with neural networks defined over graphs <ref type="bibr" target="#b26">Scarselli et al. (2009)</ref> associate T with a fully-connected graph G T = (V, E) where nodes v a ∈ V correspond to the images present in T (both labeled and unlabeled). In this context, the setup does not specify a fixed similarity e a,a between images x a and x a , suggesting an approach where this similarity measure is learnt in a discriminative fashion with a parametric model similarly as in <ref type="bibr" target="#b10">Gilmer et al. (2017)</ref>, such as a siamese neural architecture. This framework is closely related to the set representation from <ref type="bibr" target="#b31">Vinyals et al. (2016)</ref>, but extends the inference mechanism using the graph neural network formalism that we detail next. In its simplest incarnation, given an input signal F ∈ R V ×d on the vertices of a weighted graph G, we consider a family A of graph intrinsic linear operators that act locally on this signal. The simplest is the adjacency operator A :</p><formula xml:id="formula_2">F → A(F ) where (AF ) i := j∼i w i,j F j , with i ∼ j iff (i, j) ∈ E and w i,j its associated weight. A GNN layer Gc(·) receives as input a signal x (k) ∈ R V ×d k and produces x (k+1) ∈ R V ×d k+1 as x (k+1) l = Gc(x (k) ) = ρ B∈A Bx (k) θ (k) B,l , l = d1 . . . d k+1 ,<label>(2)</label></formula><p>where Θ = {θ</p><formula xml:id="formula_3">(k) 1 , . . . , θ (k) |A| } k , θ (k) B ∈ R d k ×d k+1</formula><p>, are trainable parameters and ρ(·) is a point-wise non-linearity, chosen in this work to be a 'leaky' ReLU <ref type="bibr" target="#b32">Xu et al. (2015)</ref>.</p><p>Authors have explored several modeling variants from this basic formulation, by replacing the pointwise nonlinearity with gating operations <ref type="bibr" target="#b6">Duvenaud et al. (2015)</ref>, or by generalizing the generator family to Laplacian polynomials <ref type="bibr" target="#b5">Defferrard et al. (2016)</ref>; Kipf &amp; Welling (2016); <ref type="bibr" target="#b3">Bruna et al. (2013)</ref>, or including 2 J -th powers of A to A, A J = min(1, A 2 J ) to encode 2 J -hop neighborhoods of each node <ref type="bibr" target="#b2">Bruna &amp; Li (2017)</ref>. Cascaded operations in the form (2) are able to approximate a wide range of graph inference tasks. In particular, inspired by message-passing algorithms, <ref type="bibr" target="#b14">Kearnes et al. (2016)</ref>; <ref type="bibr" target="#b10">Gilmer et al. (2017)</ref> generalized the GNN to also learn edge featuresÃ (k) from the current node hidden representation:Ã where ϕ is a symmetric function parametrized with e.g. a neural network. In this work, we consider a Multilayer Perceptron stacked after the absolute difference between two vector nodes. See eq. 4:</p><formula xml:id="formula_4">(k) i,j = ϕθ(x (k) i , x (k) j ) ,<label>(3)</label></formula><formula xml:id="formula_5">ϕθ(x (k) i , x (k) j ) = MLPθ(abs(x (k) i − x (k) j ))<label>(4)</label></formula><p>Then ϕ is a metric, which is learned by doing a non-linear combination of the absolute difference between the individual features of two nodes. Using this architecture the distance property Symmetry ϕθ(a, b) = ϕθ(b, a) is fulfilled by construction and the distance property Identity ϕθ(a, a) = 0 is easily learned.</p><p>The trainable adjacency is then normalized to a stochastic kernel by using a softmax along each row. The resulting update rules for node features are obtained by adding the edge feature kernelÃ (k) into the generator family A = {Ã (k) , 1} and applying (2). Adjacency learning is particularly important in applications where the input set is believed to have some geometric structure, but the metric is not known a priori, such as is our case.</p><p>In general graphs, the network depth is chosen to be of the order of the graph diameter, so that all nodes obtain information from the entire graph. In our context, however, since the graph is densely connected, the depth is interpreted simply as giving the model more expressive power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Construction of Initial Node Features</head><p>The input collection T is mapped into node features as follows. For images x i ∈ T with known label l i , the one-hot encoding of the label is concatenated with the embedding features of the image at the input of the GNN.</p><formula xml:id="formula_6">x (0) i = (φ(x i ), h(l i )) ,<label>(5)</label></formula><p>where φ is a Convolutional neural network and h(l) ∈ R K + is a one-hot encoding of the label. Architectural details for φ are detailed in Section 6.1.1 and 6.1.2. For imagesx j ,x j with unknown label l i , we modify the previous construction to account for full uncertainty about the label variable by replacing h(l) with the uniform distribution over the K-simplex: V j = (φ(x j ), K −1 1 K ), and analogously forx.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">RELATIONSHIP WITH EXISTING MODELS</head><p>The graph neural network formulation of few-shot learning generalizes a number of recent models proposed in the literature. <ref type="bibr" target="#b16">Koch et al. (2015)</ref> can be interpreted as a single layer message-passing iteration of our model, and using the same initial node embedding (5) x with u selecting the label field from x. In this model, the learning is reduced to learning image embeddings φ(x i ) whose euclidean metric is consistent with the label similarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Siamese Networks Siamese Networks</head><formula xml:id="formula_7">(0) i = (φ(x i ), h i ) , using a non-trainable edge feature ϕ(x i , x j ) = φ(x i ) − φ(x j ) ,Ã (0) = softmax(−ϕ) ,</formula><p>Prototypical Networks Prototypical networks <ref type="bibr" target="#b27">Snell et al. (2017)</ref> evolve Siamese networks by aggregating information within each cluster determined by nodes with the same label. This operation can also be accomplished with a gnn as follows. we consider</p><formula xml:id="formula_8">A (0) i,j = q −1 if l i = l j 0 otherwise.</formula><p>where q is the number of examples per class, and</p><formula xml:id="formula_9">x (1) i = jÃ (0) i,j x (0) j ,</formula><p>where x (0) is defined as in the Siamese Networks. We finally apply the previous kernelÃ (1) = softmax(ϕ) applied to x (1) to yield class prototypes:</p><formula xml:id="formula_10">Y * = jÃ (1) * ,j x (1) j , u .</formula><p>Matching Networks Matching networks <ref type="bibr" target="#b31">Vinyals et al. (2016)</ref> use a set representation for the ensemble of images in T , similarly as our proposed graph neural network model, but with two important differences. First, the attention mechanism considered in this set representation is akin to the edge feature learning, with the difference that the mechanism attends always to the same node embeddings, as opposed to our stacked adjacency learning, which is closer to <ref type="bibr" target="#b29">Vaswani et al. (2017)</ref>. In other words, instead of the attention kernel in <ref type="formula" target="#formula_4">(3)</ref>, matching networks consider attention mechanisms of the formÃ</p><formula xml:id="formula_11">(k) * ,j = ϕ(x (k) * , x (T ) j ), where x (T ) j</formula><p>is the encoding function for the elements of the support set, obtained with bidirectional LSTMs. In that case, the support set encoding is thus computed independently of the target image. Second, the label and image fields are treated separately throughout the model, with a final step that aggregates linearly the labels using a trained kernel. This may prevent the model to leverage complex dependencies between labels and images at intermediate stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">TRAINING</head><p>We describe next how to train the parameters of the GNN in the different setups we consider: fewshot learning, semi-supervised learning and active learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">FEW-SHOT AND SEMI-SUPERVISED LEARNING</head><p>In this setup, the model is asked only to predict the label Y corresponding to the image to classifȳ x ∈ T , associated with node * in the graph. The final layer of the GNN is thus a softmax mapping the node features to the K-simplex. We then consider the Cross-entropy loss evaluated at node * :</p><formula xml:id="formula_12">(Φ(T ; Θ), Y ) = − k y k log P (Y * = y k | T ) .</formula><p>The semi-supervised setting is trained identically -the only difference is that the initial label fields of the node will be filled with the uniform distribution on nodes corresponding tox j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ACTIVE LEARNING</head><p>In the Active Learning setup, the model has the intrinsic ability to query for one of the labels from {x 1 , . . . ,x r }. The network will learn to ask for the most informative label in order to classify the samplex ∈ T . The querying is done after the first layer of the GNN by using a Softmax attention over the unlabeled nodes of the graph. For this we apply a function g(x (1) i ) ∈ R 1 that maps each unlabeled vector node to a scalar value. Function g is parametrized by a two layers neural network. A Softmax is applied over the {1, . . . , r} scalar values obtained after applying g:</p><formula xml:id="formula_13">Attention = Softmax(g(x (1) {1,...,r} ))</formula><p>In order to query only one sample, we set all elements from the Attention ∈ R r vector to 0 except for one. At test time we keep the maximum value, at train time we randomly sample one value based on its multinomial probability. Then we multiply this sampled attention by the label vectors:</p><formula xml:id="formula_14">w · h(l i * ) = Attention , h(l {1,...,r} )</formula><p>The label of the queried vector h(l i * ) is obtained, scaled by the weight <ref type="figure">w ∈ (0, 1)</ref>. This value is then summed to the current representation x (1) i * , since we are using dense connections in our GNN model we can sum this w · h(l i * ) value directly to where the uniform label distribution was concatenated</p><formula xml:id="formula_15">x (1) i * = [Gc(x (0) i * ), x (0) i * ] = [Gc(x (0) i * ), (φ(x i * ), h(l i * ))]</formula><p>After the label has been summed to the current node, the information is forward propagated. This attention part is trained end-to-end with the rest of the network by backpropagating the loss from the output of the GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>For the few-shot, semi-supervised and active learning experiments we used the Omniglot dataset presented by <ref type="bibr" target="#b18">Lake et al. (2015)</ref> and Mini-Imagenet dataset introduced by <ref type="bibr" target="#b31">Vinyals et al. (2016)</ref> which is a small version of ILSVRC-12 <ref type="bibr" target="#b17">Krizhevsky et al. (2012)</ref>. All experiments are based on the q-shot, K-way setting. For all experiments we used the same values q-shot and K-way for both training and testing.</p><p>Code available at: https://github.com/vgsatorras/few-shot-gnn 6.1 DATASETS AND IMPLEMENTATION 6.1.1 OMNIGLOT Dataset: Omniglot is a dataset of 1623 characters from 50 different alphabets, each character/class has been drawn by 20 different people. Following <ref type="bibr" target="#b31">Vinyals et al. (2016)</ref> implementation we split the dataset into 1200 classes for training and the remaining 423 for testing. We augmented the dataset by multiples of 90 degrees as proposed by <ref type="bibr" target="#b25">Santoro et al. (2016)</ref>.</p><p>Architectures: Inspired by the embedding architecture from <ref type="bibr" target="#b31">Vinyals et al. (2016)</ref>, following <ref type="bibr" target="#b21">Mishra et al. (2017)</ref>, a CNN was used as an embedding φ function consisting of four stacked blocks of {3×3-convolutional layer with 64 filters, batch-normalization, 2×2 max-pooling, leaky-relu} the output is passed through a fully connected layer resulting in a 64-dimensional embedding. For the GNN we used 3 blocks each of them composed by 1) a module that computes the adjacency matrix and 2) a graph convolutional layer. A more detailed description of each block can be found at Figure 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">MINI-IMAGENET</head><p>Dataset: Mini-Imagenet is a more challenging dataset for one-shot learning proposed by <ref type="bibr" target="#b31">Vinyals et al. (2016)</ref> derived from the original ILSVRC-12 dataset <ref type="bibr" target="#b17">Krizhevsky et al. (2012)</ref>. It consists of 84×84 RGB images from 100 different classes with 600 samples per class. It was created with the purpose of increasing the complexity for one-shot tasks while keeping the simplicity of a light size dataset, that makes it suitable for fast prototyping. We used the splits proposed by <ref type="bibr" target="#b23">Ravi &amp; Larochelle (2016)</ref> of 64 classes for training, 16 for validation and 20 for testing. Using 64 classes for training, and the 16 validation classes only for early stopping and parameter tuning.</p><p>Architecture: The embedding architecture used for Mini-Imagenet is formed by 4 convolutional layers followed by a fully-connected layer resulting in a 128 dimensional embedding. This light architecture is useful for fast prototyping: 1×{3×3-conv. layer (64 filters), batch normalization, max pool(2, 2), leaky relu}, 1×{3×3-conv. layer (96 filters), batch normalization, max pool(2, 2), leaky relu}, 1×{3×3-conv. layer (128 filters), batch normalization, max pool(2, 2), leaky relu, dropout(0.5)}, 1×{3×3-conv. layer (256 filters), batch normalization, max pool(2, 2), leaky relu, dropout(0.5)}, 1×{ fc-layer (128 filters), batch normalization}. The two dropout layers are useful to avoid overfitting the GNN in Mini-Imagenet dataset. The GNN architecture is similar than for Omniglot, it is formed by 3 blocks, each block is described at <ref type="figure" target="#fig_4">Figure  3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">FEW-SHOT</head><p>Few-shot learning experiments for Omniglot and Mini-Imagenet are presented at <ref type="table" target="#tab_1">Table 1 and Table  2</ref> respectively. We evaluate our model by performing different q-shot, K-way experiments on both datasets. For every few-shot task T , we sample K random classes from the dataset, and from each class we sample q random samples. An extra sample to classify is chosen from one of that K classes.</p><p>Omniglot: The GNN method is providing competitive results while still remaining simpler than other methods. State of the art results are reached in the 5-Way and 20-way 1-shot experiments. In the 20-Way 1-shot setting the GNN is providing slightly better results than <ref type="bibr" target="#b22">Munkhdalai &amp; Yu (2017)</ref> while still being a more simple approach. The TCML approach from <ref type="bibr" target="#b21">Mishra et al. (2017)</ref> is in the same confidence interval for 3 out of 4 experiments, but it is slightly better for the 20-Way 5-shot, although the number of parameters is reduced from ∼5M (TCML) to ∼300K (3 layers GNN).</p><p>At Mini-Imagenet table we are also presenting a baseline "Our metric learning + KNN" where no information has been aggregated among nodes, it is a K-nearest neighbors applied on top of the pair-wise learnable metric ϕ θ (x (0)</p><p>i , x (0) j ) and trained end-to-end, this learnable metric is competitive by itself compared to other state of the art methods. Even so, a significant improvement (from 64.02% to 66.41%) can be seen for the 5-shot 5-Way Mini-Imagenet setting when aggregating information among nodes by using the full GNN architecture. A variety of embedding functions φ are used among the different papers for Mini-Imagenet experiments, in our case we are using a simple network of 4 conv. layers followed by a fully connected layer (Section 6.1.2) which served us to compare between Our GNN and Our metric learning + KNN and it is useful for fast prototyping. More complex embeddings have proven to produce better results, at <ref type="bibr" target="#b21">Mishra et al. (2017)</ref> a deep residual network is used as embedding network φ increasing the accuracy considerably. Regarding the TCML architecture in Mini-Imagenet, the number of parameters is reduced from ∼11M (TCML) to ∼400K (3 layers GNN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5-Way</head><p>20-Way Model 1-shot 5-shot 1-shot 5-shot Pixels <ref type="bibr" target="#b31">Vinyals et al. (2016)</ref> 41.7% 63.2% 26.7% 42.6% Siamese Net <ref type="bibr" target="#b16">Koch et al. (2015)</ref> 97.3% 98.4% 88.2% 97.0% Matching Networks <ref type="bibr" target="#b31">Vinyals et al. (2016)</ref> 98.1% 98.9% 93.8% 98.5% N. Statistician <ref type="bibr" target="#b7">Edwards &amp; Storkey (2016)</ref> 98.1% 99.5% 93.2% 98.1% Res. Pair-Wise <ref type="bibr" target="#b20">Mehrotra &amp; Dukkipati (2017)</ref> --94.8% -Prototypical Networks <ref type="bibr" target="#b27">Snell et al. (2017)</ref> 97.4% 99.3% 95.4% 98.8% ConvNet with Memory  98.4% 99.6% 95.0% 98.6% Agnostic Meta-learner <ref type="bibr" target="#b9">Finn et al. (2017)</ref> 98.7 ±0.4% 99.9 ±0.3% 95.8 ±0.3% 98.9 ±0.2% Meta Networks <ref type="bibr" target="#b22">Munkhdalai &amp; Yu (2017)</ref> 98.9% -97.0% -TCML <ref type="bibr" target="#b21">Mishra et al. (2017)</ref> 98 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5-Way Model</head><p>1-shot 5-shot Matching Networks <ref type="bibr" target="#b31">Vinyals et al. (2016)</ref> 43.6% 55.3% Prototypical Networks <ref type="bibr" target="#b27">Snell et al. (2017)</ref> 46.61% ±0.78% 65.77% ±0.70% Model Agnostic Meta-learner <ref type="bibr" target="#b9">Finn et al. (2017)</ref> 48.70% ±1.84% 63.1% ±0.92% Meta Networks <ref type="bibr" target="#b22">Munkhdalai &amp; Yu (2017)</ref> 49.21% ±0.96 -Ravi &amp; Larochelle <ref type="bibr" target="#b23">Ravi &amp; Larochelle (2016)</ref> 43.4% ±0.77% 60.2% ±0.71% TCML <ref type="bibr" target="#b21">Mishra et al. (2017)</ref> 55.71% ±0.99% 68.88% ±0.92% Our metric learning + KNN 49.44% ±0.28% 64.02% ±0.51% Our GNN 50.33% ±0.36% 66.41% ±0.63% Two strategies can be seen at <ref type="table" target="#tab_3">Tables 3 and 4</ref>. "GNN -Trained only with labeled" is equivalent to the supervised few-shot setting, for example, in the 5-Way 5-shot 20%-labeled setting, this method is equivalent to the 5-way 1-shot learning setting since it is ignoring the unlabeled samples. "GNN -Semi supervised" is the actual semi-supervised method, for example, in the 5-Way 5-shot 20%labeled setting, the GNN receives as input 1 labeled sample per class and 4 unlabeled samples per class.</p><p>Omniglot results are presented at <ref type="table" target="#tab_3">Table 3</ref>, for this scenario we observe that the accuracy improvement is similar when adding images than when adding labels. The GNN is able to extract information from the input distribution of unlabeled samples such that only using 20% of the labels in a 5-shot semi-supervised environment we get same results as in the 40% supervised setting.</p><p>In Mini-Imagenet experiments, <ref type="table" target="#tab_4">Table 4</ref>, we also notice an improvement when using semi-supervised data although it is not as significant as in Omniglot. The distribution of Mini-Imagenet images is more complex than for Omniglot. In spite of it, the GNN manages to improve by ∼2% in the 20% and 40% settings.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">ACTIVE LEARNING</head><p>We performed Active Learning experiments on the 5-Way 5-shot set-up when 20% of the samples are labeled. In this scenario our network will query for the label of one sample from the unlabeled ones. The results are compared with the Random baseline where the network chooses a random sample to be labeled instead of one that maximally reduces the loss of the classification task T .</p><p>Results are shown at <ref type="table">Table 5</ref>. The results of the GNN-Random criterion are close to the Semisupervised results for 20%-labeled samples from <ref type="table" target="#tab_3">Tables 3 and 4</ref>. It means that selecting one random label practically does not improve the accuracy at all. When using the GNN-AL learned criterion, we notice an improvement of ∼ 3.4% for Mini-Imagenet, it means that the GNN manages to correctly choose a more informative sample than a random one. In Omniglot the improvement is smaller since the accuracy is almost saturated and the improving margin is less.</p><p>Method 5-Way 5-shot 20%-labeled GNN -AL 99.62% GNN -Random 99.59%</p><p>Method 5-Way 5-shot 20%-labeled GNN -AL 55.99% ±1.35% GNN -Random 52.56% ±1.18% <ref type="table">Table 5</ref>: Omniglot (left) and Mini-Imagneet (right), average accuracies are shown at both tables, the GNN-AL is the learned criterion that performs Active Learning by selecting the sample that will maximally reduce the loss of the current classification. The GNN -Random is also selecting one sample, but in this case a random one. Mini-Imagenet results are presented with 95% confidence intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>This paper explored graph neural representations for few-shot, semi-supervised and active learning. From the meta-learning perspective, these tasks become supervised learning problems where the input is given by a collection or set of elements, whose relational structure can be leveraged with neural message passing models. In particular, stacked node and edge features generalize the contextual similarity learning underpinning previous few-shot learning models.</p><p>The graph formulation is helpful to unify several training setups (few-shot, active, semi-supervised) under the same framework, a necessary step towards the goal of having a single learner which is able to operate simultaneously in different regimes (stream of labels with few examples per class, or stream of examples with few labels). This general goal requires scaling up graph models to millions of nodes, motivating graph hierarchical and coarsening approaches <ref type="bibr" target="#b5">Defferrard et al. (2016)</ref>.</p><p>Another future direction is to generalize the scope of Active Learning, to include e.g. the ability to ask questions <ref type="bibr" target="#b24">Rothe et al. (2017)</ref>, or in reinforcement learning setups, where few-shot learning is critical to adapt to non-stationary environments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>; Duvenaud et al. (2015); Gilmer et al. (2017), we Figure 1: Visual representation of One-Shot Learning setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>4.2 GRAPH NEURAL NETWORKS Graph Neural Networks, introduced in Gori et al. (2005); Scarselli et al. (2009) and further simplified in Li et al. (2015); Duvenaud et al. (2015); Sukhbaatar et al. (2016) are neural networks based on local operators of a graph G = (V, E), offering a powerful balance between expressivity and sample complexity; see Bronstein et al. (2017) for a recent survey on models and applications of deep learning on graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Graph Neural Network ilustration. The Adjacency matrix is computed before every Convolutional Layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>GNN model. Three blue blocks are used for Omniglot and Mini-Imagenet. (nf=96).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.96% ±0.20% 99.75% ±0.11% 97.64% ±0.30% 99.36% ±0.18%</figDesc><table><row><cell>Our GNN</cell><cell>99.2%</cell><cell>99.7%</cell><cell>97.4%</cell><cell>99.0%</cell></row><row><cell cols="5">Table 1: Few-Shot Learning -Omniglot accuracies. Siamese Net results are extracted from Vinyals</cell></row><row><cell>et al. (2016) reimplementation.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>40% of the samples are labeled. The labeled samples are balanced among classes in all experiments, in other words, all the classes have the same amount of labeled and unlabeled samples.</figDesc><table><row><cell>: Few-shot learning -Mini-Imagenet average accuracies with 95% confidence intervals.</cell></row><row><cell>6.3 SEMI-SUPERVISED</cell></row><row><cell>Semi-supervised experiments are performed on the 5-way 5-shot setting. Different results are pre-</cell></row><row><cell>sented when 20% and</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Semi-Supervised Learning -Omniglot accuracies. Trained only with labeled 50.33% ±0.36% 56.91% ±0.42% 66.41% ±0.63% GNN -Semi supervised 52.45% ±0.88% 58.76% ±0.86% 66.41% ±0.63%</figDesc><table><row><cell>5-Way 5-shot</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Semi-Supervised Learning -Mini-Imagenet average accuracies with 95% confidence intervals.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was partly supported by Samsung Electronics (Improving Deep Learning using Latent Structure).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08415</idno>
		<title level="m">Community detection with graph neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A compositional object-based approach to learning physical dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02185</idno>
		<title level="m">Towards a neural statistician</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="594" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<title level="m">Neural message passing for quantum chemistry</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03129</idno>
		<title level="m">Learning to remember rare events</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Generative adversarial residual pairwise networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambedkar</forename><surname>Dukkipati</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08033</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03141</idno>
		<title level="m">Meta-learning with temporal convolutions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00837</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Meta networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Question asking as program generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenden</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Gureckis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Metalearning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05175</idno>
		<title level="m">Prototypical networks for few-shot learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning multiagent communication with backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2244" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06391</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
