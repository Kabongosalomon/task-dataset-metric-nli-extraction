<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inverse Cooking: Recipe Generation from Food Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
							<email>amaia.salvador@upc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Politecnica de Catalunya</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
							<email>mdrozdzal@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Giro-I-Nieto</surname></persName>
							<email>xavier.giro@upc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Politecnica de Catalunya</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
							<email>adrianars@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Inverse Cooking: Recipe Generation from Food Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>People enjoy food photography because they appreciate food. Behind each meal there is a story described in a complex recipe and, unfortunately, by simply looking at a food image we do not have access to its preparation process. Therefore, in this paper we introduce an inverse cooking system that recreates cooking recipes given food images. Our system predicts ingredients as sets by means of a novel architecture, modeling their dependencies without imposing any order, and then generates cooking instructions by attending to both image and its inferred ingredients simultaneously. We extensively evaluate the whole system on the large-scale Recipe1M dataset and show that (1) we improve performance w.r.t. previous baselines for ingredient prediction; (2) we are able to obtain high quality recipes by leveraging both image and ingredients; (3) our system is able to produce more compelling recipes than retrieval-based approaches according to human judgment. We make code and models publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Food is fundamental to human existence. Not only does it provide us with energy-it also defines our identity and culture <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">34]</ref>. As the old saying goes, we are what we eat, and food related activities such as cooking, eating and talking about it take a significant portion of our daily life. Food culture has been spreading more than ever in the current digital era, with many people sharing pictures of food they are eating across social media <ref type="bibr" target="#b30">[31]</ref>. Querying Instagram for #food leads to at least 300M posts; similarly, searching for #foodie results in at least 100M posts, highlighting the unquestionable value that food has in our society. Moreover, eating patterns and cooking culture have been evolving over time. In the past, food was mostly prepared at home, but nowadays we frequently consume food prepared by thirdparties (e.g. takeaways, catering and restaurants). Thus, the access to detailed information about prepared food is * Work done during internship at Facebook AI Research 1 https://github.com/facebookresearch/inversecooking</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ingredients:</head><p>Flour, butter, sugar, egg, milk, salt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instructions:</head><p>-Preheat oven to 450 degrees.  limited and, as a consequence, it is hard to know precisely what we eat. Therefore, we argue that there is a need for inverse cooking systems, which are able to infer ingredients and cooking instructions from a prepared meal.</p><p>The last few years have witnessed outstanding improvements in visual recognition tasks such as natural image classification <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b13">14]</ref>, object detection <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b40">41]</ref> and semantic segmentation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b18">19]</ref>. However, when comparing to natural image understanding, food recognition poses additional challenges, since food and its components have high intraclass variability and present heavy deformations that occur during the cooking process. Ingredients are frequently occluded in a cooked dish and come in a variety of colors, forms and textures. Further, visual ingredient detection requires high level reasoning and prior knowledge (e.g. cake will likely contain sugar and not salt, while croissant will presumably include butter). Hence, food recognition challenges current computer vision systems to go beyond the merely visible, and to incorporate prior knowledge to enable high-quality structured food preparation descriptions.</p><p>Previous efforts on food understanding have mainly focused on food and ingredient categorization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b23">24]</ref>. However, a system for comprehensive visual food recognition should not only be able to recognize the type of meal or its ingredients, but also understand its preparation pro-cess. Traditionally, the image-to-recipe problem has been formulated as a retrieval task <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b44">45]</ref>, where a recipe is retrieved from a fixed dataset based on the image similarity score in an embedding space. The performance of such systems highly depends on the dataset size and diversity, as well as on the quality of the learned embedding. Not surprisingly, these systems fail when a matching recipe for the image query does not exist in the static dataset.</p><p>An alternative to overcome the dataset constraints of retrieval systems is to formulate the image-to-recipe problem as a conditional generation one. Therefore, in this paper, we present a system that generates a cooking recipe containing a title, ingredients and cooking instructions directly from an image. <ref type="figure" target="#fig_1">Figure 1</ref> shows an example of a generated recipe obtained with our method, which first predicts ingredients from an image and then conditions on both the image and the ingredients to generate the cooking instructions. To the best of our knowledge, our system is the first to generate cooking recipes directly from food images. We pose the instruction generation problem as a sequence generation one conditioned on two modalities simultaneously, namely an image and its predicted ingredients. We formulate the ingredient prediction problem as a set prediction, exploiting their underlying structure. We model ingredient dependencies while not penalizing for prediction order, thus revising the question of whether order matters <ref type="bibr" target="#b50">[51]</ref>. We extensively evaluate our system on the large-scale Recipe1M dataset <ref type="bibr" target="#b44">[45]</ref> that contains images, ingredients and cooking instructions, showing satisfactory results. More precisely, in a human evaluation study, we show that our inverse cooking system outperforms previously introduced image-to-recipe retrieval approaches by a large margin. Moreover, using a small set of images, we show that food image-to-ingredient prediction is a hard task for humans and that our approach is able to surpass them.</p><p>The contributions of this paper can be summarized as: -We present an inverse cooking system, which generates cooking instructions conditioned on an image and its ingredients, exploring different attention strategies to reason about both modalities simultaneously. -We exhaustively study ingredients as both a list and a set, and propose a new architecture for ingredient prediction that exploits co-dependencies among ingredients without imposing order. -By means of a user study we show that ingredient prediction is indeed a difficult task and demonstrate the superiority of our proposed system against image-torecipe retrieval approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Food Understanding. The introduction of large scale food datasets, such as Food-101 <ref type="bibr" target="#b0">[1]</ref> and Recipe1M <ref type="bibr" target="#b44">[45]</ref>, to-gether with a recently held iFood challenge 2 has enabled significant advancements in visual food recognition, by providing reference benchmarks to train and compare machine learning approaches. As a result, there is currently a vast literature in computer vision dealing with a variety of food related tasks, with special focus in image classification <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. Subsequent works tackle more challenging tasks such as estimating the number of calories given a food image <ref type="bibr" target="#b31">[32]</ref>, estimating food quantities <ref type="bibr" target="#b4">[5]</ref>, predicting the list of present ingredients <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> and finding the recipe for a given image <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b1">2]</ref>. Additionally, <ref type="bibr" target="#b33">[34]</ref> provides a detailed cross-region analysis of food recipes, considering images, attributes (e.g. style and course) and recipe ingredients. Food related tasks have also been considered in the natural language processing literature, where recipe generation has been studied in the context of generating procedural text from either flow graphs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35]</ref> or ingredients' checklists <ref type="bibr" target="#b20">[21]</ref>.</p><p>Multi-label classification. Significant effort has been devoted in the literature to leverage deep neural networks for multi-label classification, by designing models <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b52">53]</ref> and studying loss functions <ref type="bibr" target="#b11">[12]</ref> well suited for this task. Early attempts exploit single-label classification models coupled with binary logistic loss <ref type="bibr" target="#b2">[3]</ref>, assuming the independence among labels and dropping potentially relevant information. One way of capturing label dependencies is by relying on label powersets <ref type="bibr" target="#b48">[49]</ref>. Powersets consider all possible label combinations, which makes them intractable for large scale problems. Another expensive alternative consists in learning the joint probability of the labels. To overcome this issue, probabilistic classifier chains <ref type="bibr" target="#b7">[8]</ref> and their recurrent neural network-based <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b36">37]</ref> counterparts propose to decompose the joint distribution into conditionals, at the expense of introducing intrinsic ordering. Note that most of these models require to make a prediction for each of the potential labels. Moreover, joint input and label embeddings <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b60">61]</ref> have been introduced to preserve correlations and predict label sets. As an alternative, researchers have attempted to predict the cardinality of the set of labels <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>; however, assuming the independence of labels. When it comes to multi-label classification objectives, binary logistic loss <ref type="bibr" target="#b2">[3]</ref>, target distribution crossentropy <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29]</ref>, target distribution mean squared error <ref type="bibr" target="#b55">[56]</ref> and ranking-based losses <ref type="bibr" target="#b11">[12]</ref> have been investigated and compared. Recent results on large scale datasets outline the potential of the target distribution loss <ref type="bibr" target="#b28">[29]</ref>.</p><p>Conditional text generation. Conditional text generation with auto-regressive models has been widely studied in the literature using both text-based <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b8">9]</ref> as well as image-based conditionings <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b45">46]</ref>. In neural machine translation, where the goal is to predict the translation for a given source text into another language, dif-  <ref type="figure">Figure 2</ref>: Recipe generation model. We extract image features e I with the image encoder, parametrized by θ I . Ingredients are predicted by θ L , and encoded into ingredient embeddings e L with θ e . The cooking instruction decoder, parametrized by θ R generates a recipe title and a sequence of cooking steps by attending to image embeddings e I , ingredient embeddings e L , and previously predicted words (r 0 , ..., r t−1 ). ferent architecture designs have been studied, including recurrent neural networks <ref type="bibr" target="#b47">[48]</ref>, convolutional models <ref type="bibr" target="#b10">[11]</ref> and attention based approaches <ref type="bibr" target="#b49">[50]</ref>. More recently, sequenceto-sequence models have been applied to more open-ended generation tasks, such as poetry <ref type="bibr" target="#b54">[55]</ref> and story generation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b8">9]</ref>. Following neural machine translation trends, autoregressive models have exhibited promising performance in image captioning <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b45">46]</ref>, where the goal is to provide a short description of the image contents, opening the doors to less constrained problems such as generating descriptive paragraphs <ref type="bibr" target="#b22">[23]</ref> or visual storytelling <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Generating recipes from images</head><p>Generating a recipe (title, ingredients and instructions) from an image is a challenging task, which requires a simultaneous understanding of the ingredients composing the dish as well as the transformations they went through, e.g. slicing, blending or mixing with other ingredients. Instead of obtaining the recipe from an image directly, we argue that a recipe generation pipeline would benefit from an intermediate step predicting the ingredients list. The sequence of instructions would then be generated conditioned on both the image and its corresponding list of ingredients, where the interplay between image and ingredients could provide additional insights on how the latter were processed to produce the resulting dish. <ref type="figure">Figure 2</ref> illustrates our approach. Our recipe generation system takes a food image as an input and outputs a sequence of cooking instructions, which are generated by means of an instruction decoder that takes as input two embeddings. The first one represents visual features extracted from an image, while the second one encodes the ingredients extracted from the image. We start by introducing our transfomer-based instruction decoder in Subsection 3.1. This allows us to formally review the transformer, which we then study and modify to predict ingredients in an orderless manner in Subsection 3.2. Finally, we review the optimization details in Subsection 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Cooking Instruction Transformer</head><p>Given an input image with associated ingredients, we aim to produce a sequence of instructions R = (r 1 , ..., r T ) (where r t denotes a word in the sequence) by means of an instruction transformer <ref type="bibr" target="#b49">[50]</ref>. Note that the title is predicted as the first instruction. This transformer is conditioned jointly on two inputs: the image representation e I and the ingredient embedding e L . We extract the image representation with a ResNet-50 <ref type="bibr" target="#b14">[15]</ref> encoder and obtain the ingredient embedding e L by means of a decoder architecture to predict ingredients, followed by a single embedding layer mapping each ingredient into a fixed-size vector.</p><p>The instruction decoder is composed of transformer blocks, each of them containing two attention layers followed by a linear layer <ref type="bibr" target="#b49">[50]</ref>. The first attention layer applies self-attention over previously generated outputs, whereas the second one attends to the model conditioning in order to refine the self-attention output. The transformer model is composed of multiple transformer blocks followed by a linear layer and a softmax nonlinearity that provides a distribution over recipe words for each time step t. <ref type="figure" target="#fig_3">Figure 3a</ref> illustrates the transformer model, which traditionally is conditioned on a single modality. However, our recipe generator is conditioned on two sources: the image features e I ∈ R P ×de and ingredients embeddings e L ∈ R K×de (P and K denote the number of image and ingredient features, respectively, and d e is the embedding dimensionality). Thus, we want our attention to reason about both modalities simultaneously, guiding the instruction generation process. To that end, we explore three different fusion strategies (depicted in <ref type="figure" target="#fig_3">Figure 3</ref>  gredient embeddings e L . The output of both attention layers is combined via summation operation. -Sequential attention. This strategy sequentially attends over the two conditioning modalities. In our design, we consider two orderings: (1) image first where the attention is first computed over image embeddings e I and then over ingredient embeddings e L ; and (2) ingredients first where the order is flipped and we first attend over ingredient embeddings e L followed by image embeddings e I .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ingredient Decoder</head><p>Which is the best structure to represent ingredients? On the one hand, it seems clear that ingredients are a set, since permuting them does not alter the outcome of the cooking recipe. On the other hand, we colloquially refer to ingredients as a list (e.g. list of ingredients), implying some order. Moreover, it would be reasonable to think that there is some information in the order in which humans write down the ingredients in a recipe. Therefore, in this subsection we consider both scenarios and introduce models that work either with a list of ingredients or with a set of ingredients.</p><p>A list of ingredients is a variable sized, ordered collection of unique meal constituents. More precisely, let us define a dictionary of ingredients of size N as D = {d i } N i=0 , from which we can obtain a list of ingredients L by selecting K elements from D: L = [l i ] K i=0 . We encode L as a binary matrix L of dimensions K × N , with L i,j = 1 if d j ∈ D is selected and 0 otherwise (one-hot-code representation). Thus, our training data consists of M image and ingredient list pairs {(x (i) , L (i) )} M i=0 . In this scenario, the goal is to predictL from an image x by maximizing the following objective:</p><formula xml:id="formula_0">arg max θ I ,θ L M i=0 log p(L (i) = L (i) |x (i) ; θ I , θ L ),<label>(1)</label></formula><p>where θ I and θ L represent the learnable parameters of the image encoder and ingredient decoder, respectively. Since L denotes a list, we can factorize p(</p><formula xml:id="formula_1">L (i) = L (i) |x (i) ) into K conditionals: K k=0 log p(L (i) k = L (i) k |x (i) , L (i) &lt;k ) 3 and parametrize p(L (i) k |x (i) , L (i) &lt;k )</formula><p>as a categorical distribution. In the literature, these conditionals are usually modeled with auto-regressive (recurrent) models. In our experiments, we choose the transformer model as well. It is worth mentioning that a potential drawback of this formulation is that it inherently penalizes for order, which might not necessarily be relevant for ingredients.</p><p>A set of ingredients is a variable sized, unordered collection of unique meal constituents. We can obtain a set of ingredients S by selecting K ingredients from the dictionary D: S = {s i } K i=0 . We represent S as a binary vector s of dimension N , where s i = 1 if s i ∈ S and 0 otherwise. Thus, our training data consists of M image and ingredient set pairs:</p><formula xml:id="formula_2">{(x (i) , s (i) )} M i=0 .</formula><p>In this case, the goal is to predict s from an image x by maximizing the following objective:</p><formula xml:id="formula_3">arg max θ I ,θ L M i=0 log p(ŝ (i) = s (i) |x (i) ; θ I , θ L ).<label>(2)</label></formula><p>Assuming independence among elements, we can fac-</p><formula xml:id="formula_4">torize p(ŝ (i) = s (i) |x (i) ) as N j=0 log p(ŝ (i) j = s (i) j |x (i) ).</formula><p>However, the ingredients in the set are not necessarily independent, e.g. salt and pepper frequently appear together.</p><p>To account for element dependencies in the set, we model the set as a list, i.e. as a product of conditional probabilities, by means of an auto-regressive model such as the transformer. The transformer predicts ingredients in a listlike fashion p(L</p><formula xml:id="formula_5">(i) k |x (i) , L (i) &lt;k )</formula><p>, until the end of sequence eos token is encountered. As mentioned previously, the drawback of this approach is that such model design penalizes  for order. In order to remove the order in which ingredients are predicted, we propose to aggregate the outputs across different time-steps by means of a max pooling operation (see <ref type="figure" target="#fig_5">Figure 4</ref>). Moreover, to ensure that the ingredients inL (i) are selected without repetition, we force the pre-activation of p(L</p><formula xml:id="formula_6">(i) k |x (i) , L (i) &lt;k )</formula><p>to be −∞ for all previously selected ingredients at time-steps &lt; k. We train this model by minimizing the binary cross-entropy between the predicted ingredients (after pooling) and the ground truth. Including the eos in the pooling operation would result in loosing the information of where the token appears. Therefore, in order to learn the stopping criteria of the ingredient prediction, we introduce an additional loss accounting for it. The eos loss is defined as the binary cross-entropy loss between the predicted eos probability at all time-steps and the ground truth (represented as a unit step function, whose value is 0 for the time-steps corresponding to ingredients and 1 otherwise). In addition to that, we incorporate a cardinality 1 penalty, which we found empirically useful. At inference time, we directly sample from the transformer's output. We refer to this model as set transformer.</p><p>Alternatively, we could use target distribution p(s (i) |x (i) ) = s (i) / j s (i) j <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29]</ref> to model the joint distribution of set elements and train a model by minimizing the cross-entropy loss between p(s (i) |x (i) ) and the model's output distribution p(ŝ (i) |x (i) ). Nonetheless, it is not clear how to convert the target distribution back to the corresponding set of elements with variable cardinality. In this case, we build a feed forward network and train it with the target distribution cross-entropy loss. To recover the ingredient set, we propose to greedily sample elements from a cumulative distribution of sorted output probabilities p(ŝ (i) |x (i) ) and stop the sampling once the sum of probabilities of selected elements is above a threshold. We refer to this model as feed forward (target distribution).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimization</head><p>We train our recipe transfomer in two stages. In the first stage, we pre-train the image encoder and ingredients decoder as presented in Subsection 3.2. Then, in the second stage, we train the ingredient encoder and instruction decoder (following Subsection 3.1) by minimizing the negative log-likelihood and adjusting θ R and θ E . Note that, while training, the instruction decoder takes as input the ground truth ingredients. All transformer models are trained with teacher forcing <ref type="bibr" target="#b57">[58]</ref> except for the set transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section is devoted to the dataset and the description of implementation details, followed by an exhaustive analysis of the proposed attention strategies for the cooking instruction transformer. Further, we quantitatively compare the proposed ingredient prediction models to previously introduced baselines. Finally, a comparison of our inverse cooking system with retrieval-based models as well as a comprehensive user study is provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We train and evaluate our models on the Recipe1M dataset <ref type="bibr" target="#b44">[45]</ref>, composed of 1 029 720 recipes scraped from cooking websites. The dataset contains 720 639 training, 155 036 validation and 154 045 test recipes, containing a title, a list of ingredients, a list of cooking instructions and (optionally) an image. In our experiments, we use only the recipes containing images, and remove recipes with less than 2 ingredients or 2 instructions, resulting in 252 547 training, 54 255 validation and 54 506 test samples.</p><p>Since the dataset was obtained by scraping cooking websites, the resulting recipes are highly unstructured and contain frequently redundant or very narrowly defined cooking ingredients (e.g. olive oil, virgin olive oil and spanish olive oil are separate ingredients). Moreover, the ingredient vocabulary contains more than 400 different types of cheese, and more than 300 types of pepper. As a result, the original dataset contains 16 823 unique ingredients, which we preprocess to reduce its size and complexity. First, we merge ingredients if they share the first or last two words (e.g. bacon cheddar cheese is merged into cheddar cheese); then, we cluster the ingredients that have same word in the first or in the last position (e.g. gorgonzola cheese or cheese blend are clustered together into the cheese category); finally we remove plurals and discard ingredients that appear less than 10 times in the dataset. Altogether, we reduce the ingredient vocabulary from over 16k to 1 488 unique ingredients. For the cooking instructions, we tokenize the raw text and remove words that appear less than 10 times in the dataset, and replace them with unknown word token. Moreover, we add special tokens for the start and the end of recipe as well  as the end of instruction. This process results in a recipe vocabulary of 23 231 unique words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We resize images to 256 pixels in their shortest side and take random crops of 224 × 224 for training and we select central 224 × 224 pixels for evaluation. For the instruction decoder, we use a transformer with 16 blocks and 8 multi-head attentions, each one with dimensionality 64. For the ingredient decoder, we use a transformer with 4 blocks and 2 multi-head attentions, each one with dimensionality of 256. To obtain image embeddings we use the last convolutional layer of ResNet-50 model. Both image and ingredients embedings are of dimension 512. We keep a maximum of 20 ingredients per recipe and truncate instructions to a maximum of 150 words. The models are trained with Adam optimizer <ref type="bibr" target="#b21">[22]</ref> until early-stopping criteria is met (using patience of 50 and monitoring validation loss). All models are implemented with PyTorch 4 <ref type="bibr" target="#b39">[40]</ref>. Additional implementation details are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Recipe Generation</head><p>In this section, we compare the proposed multi-modal attention architectures described in Section 3.1. <ref type="table" target="#tab_2">Table 1</ref> (left) reports the results in terms of perplexity on the validation set. We observe that independent attention exhibits the lowest results, followed by both sequential attentions. While the latter have the capability to refine the output with either ingredient or image information consecutively, independent attention can only do it in one step. This is also the case of concatenated attention, which achieves the best performance. However, concatenated attention is flexible enough to decide whether to give more focus to one modality, at the expense of the other, whereas independent attention is forced to include information from both modalities. Therefore, we use the concatenated attention model to report results on the test set. We compare it to a system going directly from image-to-sequence of instructions without predicting ingredients (I2R). Moreover, to assess the in-fluence of visual features on recipe quality, we adapt our model by removing visual features and predicting instructions directly from ingredients (L2R). Our system achieves a test set perplexity of 8.51, improving both I2R and L2R baselines, and highlighting the benefits of using both image and ingredients when generating recipes. L2R surpasses I2R with a perplexity of 8.67 vs. 9.66, demonstrating the usefulness of having access to concepts (ingredients) that are essential to the cooking instructions. Finally, we greedily sample instructions from our model and analyze the results. We notice that generated instructions have an average of 9.21 sentences containing 9 words each, whereas real, ground truth instructions have an average of 9.08 sentences of length 12.79. See supplementary material for qualitative examples of generated recipes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ingredient Prediction</head><p>In this section, we compare the proposed ingredient prediction approaches to previously introduced models, with the goal of assessing whether ingredients should be treated as lists or sets. We consider models from the multilabel classification literature as baselines, and tune them for our purposes. On the one hand, we have models based on feed forward convolutional networks, which are trained to predict sets of ingredients. We experiment with several losses to train these models, namely binary cross-entropy, soft intersection over union as well as target distribution crossentropy. Note that binary cross-entropy is the only one not taking into account dependencies among elements in the set. On the other hand, we have sequential models that predict lists, imposing order and exploiting dependencies among elements. Finally, we consider recently proposed models which couple set prediction with cardinality prediction to determine which elements to include in the set <ref type="bibr" target="#b43">[44]</ref>. <ref type="table" target="#tab_2">Table 1</ref> (right) reports the results on the validation set for the state-of-the-art baselines as well as the proposed approaches. We evaluate the models in terms of Intersection over Union (IoU) and F1 score, computed for accumulated counts of T P , F N and F P over the entire dataset split (following Pascal VOC convention). As shown in the table, the feed forward model trained with binary crossentropy <ref type="bibr" target="#b2">[3]</ref> (FF BCE ) exhibits the lowest performance on both metrics, which could be explained by the assumed independence among ingredients. These results are already notably improved by the method that learns to predict the set cardinality (FF DC ). Similarly, the performance increases when training the model with structured losses such as soft IoU (FF IOU ). Our feed forward model trained with target distribution (FF T D ) and sampled by thresholding (th = 0.5) the sum of probabilities of selected ingredients outperforms all feed forward baselines, including recently proposed alternatives for set prediction such as <ref type="bibr" target="#b43">[44]</ref>   <ref type="table">Table 2</ref>: Ingredient Cardinality. <ref type="figure">Figure 5</ref>: Ingredient prediction results: P@K and F1 per ingredient. elements in a set and implicitly captures cardinality information. Following recent literature modeling sets as lists <ref type="bibr" target="#b36">[37]</ref>, we train a transformer network to predict ingredients given an image by minimizing the negative log-likelihood loss (TF list ). Moreover, we train the same transformer by randomly shuffling the ingredients (thus, removing order from the data). Both models exhibit competitive results when compared to feed forward models, highlighting the importance of modeling dependencies among ingredients. Finally, our proposed set transformer TF set , which models ingredient co-occurrences exploiting the auto-regressive nature of the model yet satisfying order invariance, achieves the best results, emphasizing the importance of modeling dependencies, while not penalizing for any given order. The average number of ingredients per sample in Recipe1M is 7.99 ± 3.21 after pre-processing. We report the cardinality prediction errors as well as the average number of predicted ingredients for each of the tested models in <ref type="table">Table 2</ref>. TF set is the third best method in terms of cardinality error (after FF IOU and TF list ), while being superior to all methods in terms of F1 and IoU. Further, <ref type="figure">Figure 5</ref> (left) shows the precision score at different values of K. As observed, the plot follows similar trends as <ref type="table" target="#tab_2">Table 1</ref> (right), with FF T D being among the most competitive models and TF set outperforming all previous baselines for most values of K. <ref type="figure">Figure 5</ref> (right) shows the F1 per ingredient, where the ingredients in the horizontal axes are sorted by score. Again, we see that models that exploit dependencies consistently improve ingredient's F1 scores, strengthening the importance of modeling ingredient co-occurrences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Generation vs Retrieval</head><p>In this section, we compare our proposed recipe generation system with retrieval baselines, which we use to search recipes in the entire test set for fair comparison.</p><p>Ingredient prediction evaluation. We use the retrieval model in <ref type="bibr" target="#b44">[45]</ref> as a baseline and compare it with our best ingredient predictions models, namely FF T D and FF set . The retrieval model, which we refer to as R I2LR , learns joint embeddings of images and recipes (title, ingredients and instructions). Therefore, for the ingredient prediction   <ref type="table">Table 4</ref>: User studies. Left: IoU &amp; F1 scores for ingredients obtained with retrieval <ref type="bibr" target="#b44">[45]</ref>, our approach and humans. Right: Recipe success rate according to human judgment.</p><p>task, we use the image embeddings to retrieve the closest recipe and report metrics for the ingredients of the retrieved recipe. We further consider an alternative retrieval architecture, which learns joint embeddings between images and ingredients list (ignoring title and instructions). We refer to this model as R I2L . <ref type="table" target="#tab_5">Table 3</ref> (left) reports the obtained results on the Recipe1M test set. The R I2LR model outperforms the R I2L one, which indicates that instructions contain complementary information that is useful when learning effective embeddings. Furthermore, both of our proposed methods outperform the retrieval-baselines by a large margin (e.g. TF set outperforms the R I2LR retrieval baseline by 12.26 IoU points and 15.48 F1 score points), which demonstrates the superiority of our models. Finally, <ref type="figure" target="#fig_6">Figure  6</ref> presents some qualitative results for image-to-ingredient prediction for our model as well as for the retrieval based system. We use blue to highlight the ingredients that are present in the ground truth annotation and red otherwise. Recipe generation evaluation. We compare our proposed instruction decoder (which generates instructions given an image and ingredients) with a retrieval variant. For a fair comparison, we retrain the retrieval system to find the cooking instructions given both image and ingredients. In our evaluation, we consider the ground truth ingredients as reference and compute recall and precision w.r.t. the ingredients that appear in the obtained instructions. Thus, recall computes the percentage of ingredients in the reference that appear in the output instructions, whereas precision measures the percentage of ingredients appearing in the instructions that also appear in the reference. <ref type="table" target="#tab_5">Table 3</ref> (right) displays comparison between our model and the retrieval system. Results show that ingredients appearing in generated instructions have better recall and precision scores than the ingredients in retrieved instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">User Studies</head><p>In this section, we quantify the quality of predicted ingredients and generated instructions with user studies. In the first study, we compare the performance of our model against human performance in the task of recipe generation (including ingredients and recipe instructions). We randomly select 15 images from the test set, and ask users to select up to 20 distinct ingredients as well as write a recipe that would correspond with the provided image. To reduce the complexity of the task for humans, we reduced the ingredient vocabulary from 1 488 to 323, by increasing the frequency threshold from 10 to 1k. We collected answers from 31 different users, altogether collecting an average of 5.5 answers for each image. For fair comparison, we re-train our best ingredient prediction model on the reduced vocabulary of ingredients. We compute IoU and F1 ingredient scores obtained by humans, the retrieval baseline and our method. Results are included in <ref type="table">Table 4</ref> (left), underlining the complexity of the task. As shown in the table, humans outperform the retrieval baseline (F1 of 35.20% vs 30.55%, respectively). Furthermore, our method outperforms both human baseline and retrieval based systems obtaining F1 of 49.08%. Qualitative comparisons between generated and human-written recipes (including recipes from average and expert users) are provided in the supplementary material.</p><p>The second study aims at quantifying the quality of the generated recipes (ingredients and instructions) with respect to (1) the real recipes in the dataset, and (2) the ones obtained with the retrieval baseline <ref type="bibr" target="#b44">[45]</ref>. With this purpose, we randomly select 150 recipes with their associated images from the test set and, for each image, we collect the corresponding real recipe, the top-1 retrieved recipe and our generated recipe. We present the users with 15 imagerecipe pairs (randomly chosen among the real, retrieved and generated ones) asking them to indicate whether the recipe matches the image. In the study, we collected answers from 105 different users, resulting in an average of 10 responses for each image. <ref type="table">Table 4</ref> (right) presents the results of this study, reporting the success rate of each recipe type. As it can be observed, the success rate of generated recipes is higher than the success rate of retrieved recipes, stressing the benefits of our approach w.r.t. retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduced an image-to-recipe generation system, which takes a food image and produces a recipe consisting of a title, ingredients and sequence of cooking instructions. We first predicted sets of ingredients from food images, showing that modeling dependencies matters. Then, we explored instruction generation conditioned on images and inferred ingredients, highlighting the importance of reasoning about both modalities at the same time. Finally, user study results confirm the difficulty of the task, and demonstrate the superiority of our system against stateof-the-art image-to-recipe retrieval approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>We are grateful to Nicolas Ballas, Lluis Castrejon, Zizhao Zhang and Pascal Vincent for their fruitful comments and suggestions. We also want to express our gratitude to Joelle Pineau for her unwavering support to this project. Finally, we wish to thank everyone who anonymously participated in the user studies.</p><p>This work has been partially developed in the framework of projects TEC2013-43935-R and TEC2016-75976-R, financed by the Spanish Ministerio de Economa y Competitividad and the European Regional Development Fund.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Supplementary Material</head><p>This supplementary material intends to provide further details as well as qualitative results. In Section 7.1, we describe additional implementation and training details. Section 7.2 presents an analysis of our ingredient vocabulary before and after its pre-processing. Examples of generated recipes, displayed together with real ones from the dataset, are presented in Section 7.3. Section 7.4 includes screenshots of the two forms that were used to collect data for the user studies. Section 7.5 includes examples of human written recipes compared to real and generated ones. Finally, in Section 7.6, we provide examples of generated recipes for out-of-dataset pictures taken by authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Training Details</head><p>Ingredient Prediction. Feed-forward models FF BCE , FF T D and FF IOU were trained with a mini-batch size of 300, whereas FF DC was trained with a mini-batch size of 256. All of them were trained with a learning rate of 0.001. The learning rate for pre-trained ResNet layers was scaled for each model as follows: 0.01× for FF BCE , FF IOU and FF DC and 0.1× for FF T D . Transformer list-based models TF list were trained with mini-batch size 300 and learning rate 0.001, scaling the learning rate of ResNet layers with a factor of 0.1×. Similarly, the set transformer TF set was trained with mini-batch size of 300 and a learning rate of 0.0001, scaling the learning rate of pre-trained ResNet layers with a factor of 1.0×. The optimization of TF set minimizes a cost function composed of three terms, namely the ingredient prediction loss L ingr and the end-of-sequence loss L eos and the cardinality penalty L card . We set the contribution of each term with weights 1000.0 and 1.0 and 1.0, respectively. We use a label smoothing factor of 0.1 for all models trained with BCE loss (FF BCE , FF DC , TF set ), which we found experimentally useful.</p><p>Instruction Generation. We use a batch size of 256 and learning rate of 0.001. Parameters of the image encoder module are taken from the ingredient prediction model and frozen during training for instruction generation.</p><p>All models are trained with Adam optimizer (β 1 = 0.9, β 1 = 0.99 and =1e-8), exponential decay of 0.99 after each epoch, dropout probability 0.3 and a maximum number of 400 epochs (if early stopping criterion is not met). During training we randomly flip (p = 0.5), rotate (±10 degrees) and translate images (±10% image size on each axis) for augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Ingredient Analysis</head><p>We provide visualizations of the ingredient vocabulary used to train our models. <ref type="figure">Figure 7</ref> displays each unique ingredient in the vocabulary before and after our preprocessing stage. The size of each ingredient word indicates its frequency in the dataset (e.g. butter and salt appear in many recipes). After filtering and clustering ingredients, the distribution slightly changes (e.g. pepper becomes the most frequent ingredient, and popular ingredients such as olive oil or vegetable oil are clustered into oil). Additionally, we illustrate the high ingredient overlap in the dataset with an example of the different types of cheese that appear as different ingredients before pre-processing. <ref type="figure">Figure 8</ref> shows additional examples of generated recipes obtained with our method. We also provide the real recipe for completeness. Although sometimes far from the real recipe, our system is able to generate plausible and structured recipes for the input images. Common mistakes include failures in ingredient recognition (e.g. stuffed tomatoes are confused with stuffed peppers in <ref type="figure">Figure 8b</ref>), inconsistencies between ingredients and instructions (e.g. cucumber is predicted as an ingredient but unused in <ref type="figure">Figure  8d</ref>, and meat is mentioned in the title and instructions but is not predicted as an ingredient in <ref type="figure">Figure 8e</ref>), and repetitions in ingredient enumeration (e.g. Stir in tomato sauce, tomato paste, tomato paste, ... in <ref type="figure">Figure 8c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Generated Recipes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">User Study Forms</head><p>We provide screenshots of the two forms used to collect data for user studies. <ref type="figure" target="#fig_7">Figure 9</ref> shows the interface used by users to select image ingredients (each ingredient was selected using a drop-down menu), and write recipes (as freeform text). <ref type="figure" target="#fig_1">Figure 10</ref> shows the form we used to assess whether a recipe matched the provided image according to human judgment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Human-written Recipes</head><p>In <ref type="figure" target="#fig_1">Figure 11</ref> we show examples of recipes written by humans, which were collected using the form in <ref type="figure" target="#fig_7">Figure 9</ref>. We also display the real and generated recipes for completeness. Recipes written by humans tend to be shorter, with an average of 5.29 instructions of 9.03 words each. In contrast, our model generates recipes that contain an average of instructions 9.21 of 9 words each, which closely matches the real distribution (9.08 sentences of length 12.79).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6.">Dine Out: A case study</head><p>We test the capabilities of our model to generalize for out-of-dataset food images. <ref type="figure" target="#fig_1">Figure 12</ref> shows recipes obtained for food images taken by authors at their homes or in restaurants during the weeks prior to the submission. <ref type="figure">Figure 7</ref>: Ingredient word clouds. The size of each ingredient word is proportional to the frequency of appearance in the dataset. We display word clouds for ingredients before (7a) and after (7b) our pre-processing step. In 7c we show the different types of cheese that are clustered together after pre-processing.</p><p>(a) Before pre-processing.</p><p>(b) After pre-processing.</p><p>(c) Types of cheese before pre-processing. <ref type="figure">Figure 8</ref>: Recipe examples. We show both real and generated recipes for different test images.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>-Cream butter and sugar. -Add egg and milk. -Sift flour and salt together. -Add to creamed mixture. -Roll out on floured board to 1/4 inch thickness. -Cut with biscuit cutter. -Place on ungreased cookie sheet. -Bake for 10 minutes.Title: Biscuits</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Example of a generated recipe, composed of a title, ingredients and cooking instructions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>): -Concatenated attention. This strategy first concatenates both image e I and ingredients e L embeddings over the first dimension e concat ∈ R (K+P )×de . Then, attention is applied over the combined embeddings. -Independent attention. This strategy incorporates two attention layers to deal with the bi-modal conditioning. In this case, one layer attends over the image embedding e I , whereas the other attends over the in-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Attention strategies for the instruction decoder. In our experiments, we replace the attention module in the transformer (a), with three different attention modules (b-d) for cooking instruction generation using multiple conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 L</head><label>3</label><figDesc>(i) k denotes the k-th row of L (i) and L (i) &lt;k represents all rows of L (i) up to, but not including, the k-th one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Set transformer (TF set ). Softmax probabilities are pooled across time to avoid penalizing for order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Ingredient prediction examples. We compare obtained ingredients with our method and the retrieval baseline. Ingredients are displayed in blue if they are present in the real sample and red otherwise. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>User Study 1. Interface for writing recipes and selecting ingredients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>User Study 2. Recipe quality assessment form.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Written Recipes. Real, generated and human written recipes collected with our user study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Model selection (val).</figDesc><table><row><cell>Left: Recipe perplexity</cell></row><row><cell>(ppl). Right: Global ingredient IoU &amp; F1.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>(FF DC ). Note that target distribution models dependencies among Card. error # pred. ingrs FFBCE 5.67 ± 3.10 2.37 ± 1.58 FFDC 2.68 ± 2.07 9.18 ± 2.06 FFIOU 2.46 ± 1.95 7.86 ± 1.72 FFT D 3.02 ± 2.50 8.02 ± 3.24 TFlist 2.49 ± 2.11 7.05 ± 2.77 TFlist + shuffle 3.24 ± 2.50 5.06 ± 1.85 TFset 2.56 ± 1.93 9.43 ± 2.35</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Test performance against retrieval.</figDesc><table><row><cell>Left: Global</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.kaggle.com/c/ifood2018</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://pytorch.org/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ingredients</head><p>hamburger, beans, tomato, soup, onion, macaroni, chili, sugar, ketchup, broth, butter, pepper Instructions -Combine all ingredients and cook over medium heat until potatoes are just tender.</p><p>-Turn down heat to low and simmer at least 1.5 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ingredients chuck, tomato, water, soup</head><p>Instructions -Brown ground chuck in a large dutch oven.</p><p>-Drain any grease and add the crushed tomatoes, onion soup mix and water. -Simmer 5 minutes.</p><p>-Add velveeta shells and cheese, mix well and serve with hot rolls.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ingredients</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cross-modal retrieval in the cooking context: Learning semantic text-image embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micael</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Cadène</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laure</forename><surname>Soulier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep-based ingredient recognition for cooking recipe retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Jing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cross-modal recipe retrieval with rich food attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Jing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic chinese food identification and quantity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei-Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Hsiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Ju</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Han</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane-Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che-Hua</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ouhyoung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2012 Technical Briefs</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Chinesefoodnet: A large-scale image dataset for chinese food recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Diao</surname></persName>
		</author>
		<idno>abs/1705.02743</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Towards diverse and natural image descriptions via a conditional gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bayes optimal multilabel classification via probabilistic classifier chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Dembczyński</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyke</forename><surname>Hüllermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical neural story generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Claude Fischler. Food, self and identity. Information (International Social Science Council)</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno>abs/1705.03122</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep convolutional ranking for multilabel image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<idno>abs/1312.4894</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Alexander Toshev, and Sergey Ioffe</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CHEF: A model of case-based planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><forename type="middle">J</forename><surname>Hammond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling restaurant context for food recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Personalized classifier for food image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shota</forename><surname>Horiguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sosuke</forename><surname>Amano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Hierarchically structured reinforcement learning for topically coherent visual story generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Asli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><forename type="middle">Oliver</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<idno>abs/1805.08191</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Globally coherent text generation with neural checklist models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A hierarchical approach for generating descriptive image paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cleannet: Transfer learning for scalable image classifier training with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-label classification via feature-aware implicit label space encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijia</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepfood: Deep learningbased food image recognition for computer-aided dietary assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Vokkarane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICOST</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Knowing when to look: Adaptive attention via a visual sentinel for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1805.00932</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Wide-slice residual networks for food recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Martinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gian</forename><forename type="middle">Luca</forename><surname>Foresti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Micheloni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Food Photo Frenzy: Inside the Instagram Craze and Travel Trend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Mcguire</surname></persName>
		</author>
		<ptr target="https://www.business.com/articles/food-photo-frenzy-inside-the-instagram-craze-and-travel-trend/" />
		<imprint>
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Im2calories: towards an automated mobile vision food diary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Nutrinet: A deep learning food and drink image recognition system for dietary assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Mezgec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">Koroui</forename><surname>Seljak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nutrients</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">You are what you eat: Exploring rich recipe information for cross-region food analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqing</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing-Kun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqiang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Flowgraph2text: Automatic sentence skeleton compilation for procedural text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokuni</forename><surname>Maeta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuro</forename><surname>Sasada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichiro</forename><surname>Yoshino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Funatomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoko</forename><surname>Yamakata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INLG. The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Flow graph corpus from recipe texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokuni</forename><surname>Maeta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoko</forename><surname>Yamakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuro</forename><surname>Sasada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC. European Language Resources Association (ELRA)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Maximizing subset accuracy with recurrent neural networks in multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinseok</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneldo</forename><surname>Loza Mencía</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hyunwoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fürnkranz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep learning for food recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SoICT</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Is saki# delicious?: The food perception gap on instagram and its relation to health</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferda</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raggi</forename><surname>Al Hammouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Hamid Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Abbasnejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reid</surname></persName>
		</author>
		<title level="m">Predicting sets with deep neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Joint learning of set cardinality and state distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Hamid Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning cross-modal embeddings for cooking recipes and food images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Hynes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferda</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Random klabelsets: An ensemble method for multilabel classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Vlahavas</surname></persName>
		</author>
		<editor>Joost N. Kok, Jacek Koronacki, Raomon Lopez de Mantaras, Stan Matwin, Dunja Mladenič, and Andrzej Skowron</editor>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">CNN-RNN: A unified framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Recipe recognition with large multimodal food dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devinder</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Precioso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICMEW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Chinese poetry generation with planning based neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1610.09889</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">CNN: single-label to multi-label. CoRR, abs/1406</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junshi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5726</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1989-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Geolocalized modeling for dish recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Learning deep latent spaces for multilabel classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Kuan</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chieh</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jen</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1707.00418</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
