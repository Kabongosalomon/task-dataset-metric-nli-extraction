<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EVALUATING THE CLINICAL REALISM OF SYNTHETIC CHEST X-RAYS GENERATED USING PROGRESSIVELY GROWING GANS A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="202110-03-12">March 12, 2021 10 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Segal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Engineering Research Group School of Electrical and Information Engineering</orgName>
								<orgName type="institution">University of the Witwatersrand</orgName>
								<address>
									<settlement>Johannesburg</settlement>
									<country key="ZA">South Africa</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Rubin</surname></persName>
							<email>david.rubin@wits.ac.za</email>
							<affiliation key="aff1">
								<orgName type="department">Biomedical Engineering Research Group School of Electrical and Information Engineering</orgName>
								<orgName type="institution">University of the Witwatersrand</orgName>
								<address>
									<settlement>Johannesburg</settlement>
									<country key="ZA">South Africa</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Rubin</surname></persName>
							<email>rubingrace@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiation Sciences Division of Radiology</orgName>
								<orgName type="institution">University of the Witwatersrand</orgName>
								<address>
									<settlement>Johannesburg</settlement>
									<country key="ZA">South Africa</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><forename type="middle">Joseph</forename><surname>Hospital</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiation Sciences Division of Radiology</orgName>
								<orgName type="institution">University of the Witwatersrand</orgName>
								<address>
									<settlement>Johannesburg</settlement>
									<country key="ZA">South Africa</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Pantanowitz</surname></persName>
							<email>adam.pantanowitz@wits.ac.za</email>
							<affiliation key="aff3">
								<orgName type="department">Biomedical Engineering Research Group School of Electrical and Information Engineering</orgName>
								<orgName type="institution">University of the Witwatersrand</orgName>
								<address>
									<settlement>Johannesburg</settlement>
									<country key="ZA">South Africa</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EVALUATING THE CLINICAL REALISM OF SYNTHETIC CHEST X-RAYS GENERATED USING PROGRESSIVELY GROWING GANS A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="202110-03-12">March 12, 2021 10 Mar 2021</date>
						</imprint>
					</monogr>
					<note>A PREPRINT -MARCH 12, 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>medical image generation · generative adversarial networks · x-rays · deep learning · medicine · radiology</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Chest x-rays are a vital diagnostic tool in the workup of many patients. Similar to most medical imaging modalities, they are profoundly multi-modal and are capable of visualising a variety of combinations of conditions. There is an ever pressing need for greater quantities of labelled images to drive forward the development of diagnostic tools, however this is in direct opposition to concerns regarding patient confidentiality which constrains access through permission requests and ethics approvals. Previous work has sought to address these concerns by creating class-specific generative adversarial networks (GANs) that synthesise images to augment training data. These approaches cannot be scaled as they introduce computational trade offs between model size and class number which places fixed limits on the quality that such generates can achieve. We address this concern by introducing latent class optimisation which enables efficient, multi-modal sampling from a GAN and with which we synthesise a large archive of labelled generates. We apply a Progressive Growing GAN (PGGAN) to the task of unsupervised x-ray synthesis and have radiologists evaluate the clinical realism of the resultant samples. We provide an in depth review of the properties of varying pathologies seen on generates as well as an overview of the extent of disease diversity captured by the model. We validate the application of the Fréchet Inception Distance (FID) to measure the quality of x-ray generates and find that they are similar to other high resolution tasks. We quantify x-ray clinical realism by asking radiologists to distinguish between real and fake scans and find that generates are more likely to be classed as real than by chance, but there is still progress required to achieve true realism. We confirm these findings by evaluating synthetic classification model performance on real scans. We conclude by discussing the limitations of PGGAN generates and how to achieve controllable, realistic generates going forward.</p><p>We release our source code, model weights, and an archive of labelled generates.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The chest radiograph (CXR) is the most common diagnostic radiological procedure <ref type="bibr" target="#b0">[1]</ref> and is commonly used to screen, diagnose or monitor conditions across a variety of clinical contexts. This ubiquity has resulted in substantial research interest in the automated diagnosis of such films <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>, with a recent surge in activity due to the COVID-19 pandemic <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. State-of-the-art (SOTA) models are capable of radiologist-level performance across a subset of pathologies in only a fraction of the time needed for human review <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b3">4]</ref>. This activity in CXRs represents a section of the development of Computer Aided Diagnostic (CAD) systems that aim to provide tangible benefit to clinicians and patients alike by reducing diagnostic turnaround times, minimising errors, and supporting the clinical decision making process <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Large scale, anonymised, public imaging datasets underscore these efforts by providing the necessary clinical data for the training of CAD models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. The development of these archives is both time consuming and costly, requiring extensive expert labelling and anonymisation of patient protected information prior to release. Radiology reports produced during clinical practice are typically used as surrogates for expert review and are mined for diagnostic labels <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b3">4]</ref>. Anonymisation of images involves the detection of annotations containing protected patient information such as names that may be contained within the pixel data, or the removal of metadata that may be contained within digital imaging and communications in medicine (DICOM) files <ref type="bibr" target="#b10">[11]</ref>. This process does not adjust the image data of a particular scan, which may result in patient re-identification if visual elements of the image are rare.</p><p>Medical imagery exhibits multi-modal long-tailed distributions with significant heterogeneity in the presentation of similar disease patterns due to equipment, scan technique, and patient variability <ref type="bibr" target="#b12">[13]</ref>. This issue is demonstrated by most large CXR archives as they are limited to a single clinical site which reduces the captured variability of the factors of heterogeneity <ref type="bibr" target="#b12">[13]</ref>. The resulting sparsity of training data has obvious implications for making the development of robust diagnostic models more challenging. In addition, alterations in protected patient information such as sex, age, ethnicity, and socioeconomic class have been demonstrated to produce biases in diagnostic performance not significantly explained by variations in disease prevalence with improvements only seen with usage of multi-source image datasets <ref type="bibr" target="#b13">[14]</ref>.</p><p>There is a clear need for methodologies capable of resolving this data disparity while retaining or even augmenting privacy. Generative adversarial networks (GANs) have received significant attention as a potential solution for their ability to synthesise medical images <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> without compromising patient confidentiality as they learn to replicate the source distribution without access to the underlying training data <ref type="bibr" target="#b17">[18]</ref>. GANs capable of high fidelity image synthesis would address data challenges in machine learning and in medical education and training by:</p><p>• Creating datasets that do not compromise patient confidentiality, but provide the same diagnostic outcomes;</p><p>• Providing class-balanced datasets;</p><p>• Generating augmented images by semantic variation in visual content without altering the diagnosis; and</p><p>• Synthesising images with specified target pathologies This study serves to comprehensively evaluate the medical plausibility of synthetic CXRs as well as their applications to diagnostic radiology <ref type="bibr" target="#b0">1</ref> . We evaluate the Progressively Growing GAN (PGGAN) <ref type="bibr" target="#b18">[19]</ref> methodology applied to generate multi-modal, megapixel resolution CXRs. We provide domain expert examination of the properties of generated images as well as a review of expert discrimination between real and generated samples. In addition, we provide the following contributions:</p><p>• An evaluation of the applicability of the Fréchet Inception Distance (FID) for automatically evaluating x-ray generates.</p><p>• A proposal for extracting images for a specific pathology through a modified latent space search.</p><p>• A proposal for generating patient image series through local pathology sampling.</p><p>• An evaluation of the performance of synthetic image datasets derived from multi-modal generators.</p><p>Source code is available <ref type="bibr" target="#b19">[20]</ref>, as well as model weights and a collection of labelled generated images <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generative Adversarial Networks</head><p>GANs are a form of implicit generative model rooted in game theory that learns to reproduce an unseen training distribution through competitive optimisation. The prototypical GAN consists of a pair of neural networks in opposition to one another: a generator (G) whose goal is to generate plausible samples, and a discriminator (D) whose goal is to distinguish such samples from real samples (x) drawn from a training distribution <ref type="bibr" target="#b21">[22]</ref>. The generator learns to sample from a latent vector (z) to produce a sample (G(z)) of the generated (P g ) distribution which is similar to a chosen reference distribution (P r ). The training signal is provided by how effectively such a generate is classified as real by the discriminator (D(G(z))). This configuration is optimal when the generator produces samples indistinguishable from the reference set and the discriminator can no longer learn to detect generates <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. The configuration can be interpreted as a two-player minimax game with the following value function:</p><formula xml:id="formula_0">min G max D V (D, G) = E x∼Pr [log D(x)] + E x∼Pg [log(1 − D(G(x)))]<label>(1)</label></formula><p>wherein the discriminator (D) attempts to maximise the value function by separating real from generated images and the generator (G) attempts to minimise it by producing samples that cannot be detected by the discriminator.</p><p>The GAN formulation has found utilisation for a variety of tasks, namely image-to-image translation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, image super-resolution <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, and semantic image editing <ref type="bibr" target="#b28">[29]</ref> to name just a few. Perhaps the most common usage is that of image synthesis, wherein a GAN attempts to produce images that mirror a reference image set. The Deep Convolutional GAN <ref type="bibr" target="#b29">[30]</ref> was an initial step towards improving resolutions by making use of convolutional layers as opposed to the fully connected approach of the original GAN.</p><p>This however, did not resolve fundamental problems with training instability that caused models to fail to converge or resulted in mode dropping, a phenomenon where image variability is sacrificed for image quality and at worst can result in a the generation of only a handful of images <ref type="bibr" target="#b30">[31]</ref>. These issues become more prominent as image resolution increases as the discriminator is able to more easily detect generated samples <ref type="bibr" target="#b18">[19]</ref>. This places significant constraints on the upper limit of resolutions that a DCGAN model can achieve.</p><p>Recent work has adapted the loss functions employed <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>, progressively scaled generate size <ref type="bibr" target="#b18">[19]</ref>, or employed style transfer techniques <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> to improve training stability. These techniques have enabled megapixel resolution generation while retaining much of the quality and diversity of reference images.</p><p>We focus on the Progressively Growing GAN (PGGAN) methodology employed by Karras et al. <ref type="bibr" target="#b18">[19]</ref>. This technique trains on progressively larger images as each resolution converges, stabilising the training process and enabling initial training to progress faster as batch sizes can be larger. It is an attractive option for high resolution image synthesis as it has far more moderate computational requirements compared to its later variants <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. The training methodology for PGGAN additionally combats the effects of generator modal collapse through the inclusion of minibatch discrimination, which operates by considering the variability of samples it is trained on. This promotes the generator to maintain sample diversity as a reduction would aid the discriminator in identifying samples as fake. Despite these benefits, there are important theoretical limitations for the PGGAN that must be considered. The later StyleGAN variant by Karras et al. <ref type="bibr" target="#b33">[34]</ref> discusses that the network's only source of stochastic variation arises from the initial latent space sample which results in the consumption of network synthetic capacity to preserve variation for later image scales. This results in repetitive patterns in images and a loss of variability at greater resolutions. In addition, the progressively growing technique itself is known to result in phase artifacts with sections of images becoming fixed in preferred locations from training at lower resolutions <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Synthetic Medical Data</head><p>There is a multitude of work that aims to adapt GANs for resolving data disparities via data generation while improving both patient confidentiality and model performance. In the pursuit of this, synthetic medical data aims to capture information of diagnostic utility while eliminating the possibility of patient re-identification <ref type="bibr" target="#b35">[36]</ref>. Park et al. utilise GANs to produce anonymised clinical data tables that are interoperable with model architectures applied to standard tables, yet can be shared without concern of violating patient privacy <ref type="bibr" target="#b35">[36]</ref>.</p><p>Other works have successfully applied DCGANs for dataset augmentation in various CXR classification tasks. Moradi et al. focus on generating both normal and cardiac abnormality images, they find that GAN-based augmentation outperforms traditional training augmentation of flipping, cropping, or scaling images <ref type="bibr" target="#b36">[37]</ref> . Salehinejad et al. expand upon this concept by producing a per-class DCGAN and utilising generates to balance classes for a more diverse performance improvement <ref type="bibr" target="#b15">[16]</ref>. Both approaches demonstrate the potential benefits GANs offer for dataset augmentation, however they are handicapped by the need for per-class models. Training a model per class is computationally prohibitive and fails to capture the interactions between pathologies that may be exploited to improve diagnostic accuracy. full-field mammograms and provide comparisons of generated and source images, noting that the images appear similar with the preservation of several common tissue artifacts <ref type="bibr" target="#b37">[38]</ref>. Bowles et al. augment CT and MRI data with synthetic slices and observe improvements in segmentation performance <ref type="bibr" target="#b38">[39]</ref>.</p><p>Despite significant variation in both domain and task, most medical image GAN implementations are constrained to produce individual classes or random samplings from the latent space with little control over generated content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Image Quality Metrics</head><p>The automated evaluation of the quality of medical imagery is a diverse field with a variety of available methods dependent on the evaluation task at hand. These methods are typically divided into full, reduced, and no reference assessment methods. Full reference methods typically evaluate degradation of an image against a source. This may take many forms, from directly comparing pixel values, signal to noise ratios, or to assessing structural or feature similarities. Reduced reference methods evaluate alterations in images against natural image statistics and are typically used in finding distortions in transmitted images. No reference methods only compute elements of a given image to produce a quality assessment and are similarly utilised for transmitted images where no reference may exist. Utilising standard full reference methods for synthetic image evaluation is difficult as while the images may closely resemble the source image distribution, the generator does not have direct access to the distribution and as such does not reproduce any particular image. This precludes the use of standard comparison techniques as even structural or feature similarity metrics are intended for comparison of matched images <ref type="bibr" target="#b39">[40]</ref>.</p><p>The Fréchet Inception Distance (FID) is a metric intended to provide a solution for evaluating the quality of generated images <ref type="bibr" target="#b40">[41]</ref>. It functions by embedding a set of real and synthetic images in the final average pooling layer of an Inception Net <ref type="bibr" target="#b41">[42]</ref> pre-trained on ImageNet <ref type="bibr" target="#b42">[43]</ref>. The sets are assumed to be multivariate Gaussian distributions with the average and covariance of each utilised to calculate the Fréchet distance (2), also known as the Wasserstein-2 distance.</p><p>(µ Pr , Pr ) and (µ Pg , Pg ) refer to the mean and covariance of the real and generated distributions respectively.</p><formula xml:id="formula_1">FID(P r , P g ) = µ Pr − µ Pg 2 2 + Tr ( Pr + Pg − 2( Pr Pg ) 1 2 )<label>(2)</label></formula><p>This distance reflects the difference in the average features extracted from each image set based on the learned kernels of the Inception Net model. This is broadly similar to the feature-based full reference quality assessment metric, yet is capable of application to unpaired images by considering the average combination of features in both sets. The distance has been demonstrated to be consistent with human judgement of visual quality and more resistant to noise than prior approaches <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b43">44]</ref>. The FID is sensitive to class mode dropping, with distances increasing with greater class discrepancies between the two sets as the average set of features begins to differ. Despite this sensitivity, the FID reports ideal results if a model reproduces the training samples perfectly <ref type="bibr" target="#b43">[44]</ref>. The use of the FID for medical images requires the underlying Inception model be capable of extracting features to adequately represent and compare sets against one other. This may be problematic given that x-rays are distinct from the classes found in ImageNet.</p><p>Human eYe Perceptual Evaluation (HYPE) <ref type="bibr" target="#b44">[45]</ref> in contrast to the aforementioned automated approaches, standardises the human evaluation of model generates by considering either the time necessary to discriminate between real and fake images (HYPE time ) or the average error rate given unlimited evaluation time (HYPE ∞ ). The evaluation methodology is demonstrated to produce reliable results that are capable of separating out differences in model performance.</p><p>In addition to the already mentioned quality measures for generated images, a significant element to examine is the performance of models developed using synthetic data applied to the original task of interest. Shmelkov et al. <ref type="bibr" target="#b45">[46]</ref> propose the GAN-train metric, which evaluates the classification performance of a model trained exclusively on synthetic data and then run on the original test set. This method quantifies the effective difference between synthetic and real datasets on a benchmark task of interest and provides important information regarding the extent to which the model captures significant features of the underlying data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">X-Ray Synthesis Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>The ChestX-ray14 dataset is an update to the ChestX-ray8 dataset <ref type="bibr">[</ref> These images were collected from a clinical archive and should broadly reflect the typical clinical prevalence of these conditions within the community served by the NIH. 75% of images are normal investigations. The remainder are made up of the various labels ranging from the most prevalent, infiltration (10%), to the least, hernia (0.5%). The diagnostic labels are accompanied by bounding boxes for feature localisation. These labels were created through natural language processing (NLP) of the associated radiology and were initially estimated to be over 90% accurate. This accuracy has been disputed, with the visual content reviewed to not adequately match the proposed labels for a number of investigations <ref type="bibr" target="#b46">[47]</ref>. A modified set of labels was made available by Rajpurkar et al. <ref type="bibr" target="#b8">[9]</ref> as part of their work on pathology detection, wherein a network was trained to classify images based on the original labels and then subsequently used to relabel the original dataset. These labels are available for the majority of images with a residual manually labelled test set which has not been released publicly.</p><p>The NIH dataset, license and publication can be found here: https://nihcc.app.box.com/v/ChestXray-NIHCC <ref type="figure">Figure 2</ref>: Examples of uncurated, random samples from the exponential moving average of the generator. Samples are drawn at random from a N (0, 1) distribution without use of the truncation trick.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Progressively Growing GAN</head><p>We implement a modified PGGAN model <ref type="bibr" target="#b18">[19]</ref> in Pytorch and perform training via Pytorch Lightning <ref type="bibr" target="#b47">[48]</ref> based on the open-source implementation produced by Facebook Research and available through the Pytorch Model Zoo 2 . We start with a randomly initialised model and initially generate 4x4 images with progressively doubling of the spatial resolution after convergence at each scale. We allow mixing of the prior trained layers with the newly added layers through upsampling with the proportion of the previous layer considered reducing in linearly as training progresses.</p><p>We utilise WGAN-GP loss <ref type="bibr" target="#b32">[33]</ref>, equalised learning rates, minibatch discrimination, and pixel normalisation <ref type="bibr" target="#b18">[19]</ref>. We also implement an exponential moving average of generator weights which we use for evaluation. We train up to a resolution of 1024x1024 with 800 000 images shown during each period of layer mixing and a further 800 000 for training of the added layers. All training was performed on an Amazon Web Services (AWS) p3.8xlarge instance with 4 V100 GPUs. Model training took 6 days.</p><p>Examples of images generated by the model can be seen in figure 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Image Classification</head><p>For labelling the various CXRs, we implement a Densenet-121 <ref type="bibr" target="#b48">[49]</ref> pre-trained on ImageNet <ref type="bibr" target="#b42">[43]</ref> and replace the final fully connected layer with the number of classes in the ChestX-ray14 dataset. The model is trained end-to-end in a multi-label configuration to predict all classes simultaneously while making use of the weighted cross entropy function (3) implemented by Guendel et al. <ref type="bibr" target="#b49">[50]</ref>. The modified loss balances the frequency of positive (N p ) and negative (N n ) labels per class (c) based on the overall frequency within the training dataset. This adjusts the loss to require the model to discriminate equally between all classes which improves performance for rarer classes.</p><formula xml:id="formula_2">D CE (y c ,ŷ c ) = w P · y c log(ŷ c ) + w N · (1 − y c ) log(1 −ŷ c ), w P = N p + N n N p , w N = N p + N n N n<label>(3)</label></formula><p>For training, we extract a subset of the full dataset to include only images with modified labels from Rajpurkar et al. <ref type="bibr" target="#b8">[9]</ref>, we group images at the patient level and average the labels across all images for an individual to produce a summary of the average set of conditions per patient. The dataset is then split into training, validation and test sets through iterative stratification of the average patient labels to maintain label proportions across sets while ensuring no patient overlap occurs. We augment images with a 10°rotation, random horizontal flip probability of 50%, and colour jitter for brightness, contrast, saturation, and hue of 0.1. The model is trained using an ADAM optimiser <ref type="bibr" target="#b50">[51]</ref> using the default settings with an initial learning rate of 10 −3 , which we reduce by a factor of ten if the validation macro-averaged Receiver Operating Characteristic (ROC) Area Under Curve (AUC) fails to improve for several epochs. Once model performance plateaus, it is evaluated on the test set. We repeat the same process for the final trained discriminator, replacing the final linear scoring layer with the number of classes and train the subsequent classification model end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Pathology Optimisation</head><p>Given the multi-modal nature of pathology contained within x-ray images, it is not possible to utilise standard conditional GANs <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref> for label-specific image synthesis, as these typically require independent classes that can be encoded and fed to the generator to control synthesis. Multi-modal constraints are more consistent with text-to-image generation <ref type="bibr" target="#b53">[54]</ref>, which considers a vector representation of the conditional text input that is semantically meaningful and capable of providing a useful distance based on model predictions <ref type="bibr" target="#b51">[52]</ref>. This enables the discriminator to evaluate both image quality and similarity to the conditional text input in scoring generates. These techniques are likely to be problematic in the progressively growing formulation as the disease patterns vary considerably in terms of scale of finding and influence over the image as a whole. As an example, Cardiomegaly may be evaluated simply by having the cardiac border enlarged relative to the overall width of the chest cavity, whereas Emphysema in contrast may have enlarged lung fields, flattened diaphragms, and a reduced apparent cardiac size. These features become apparent at differing resolutions and as such, attempting to condition this information throughout training is likely to hamper performance.</p><p>We sidestep these issues in multi-modal conditional training by instead drawing inspiration from work on the embedding of images into the latent space <ref type="bibr" target="#b54">[55]</ref>. Embedding images requires the inversion of the generator. This is typically performed either by training an encoder network that maps an image to a location in the latent space or by gradient descent optimisation of a random sample to minimise reconstruction loss <ref type="bibr" target="#b55">[56]</ref>. We opt to find a latent representation that maximises the corresponding image's classification score for a label of interest. We optimise using a single label at a time to allow for inclusion of related labels that would be consistent with the presentation of real x-rays. We base the implementation on the work by Creswell et al. <ref type="bibr" target="#b54">[55]</ref>. Pseudocode for such a method can be seen in Algorithm 1 with a visual representation of this technique in figure 3.</p><p>Algorithm 1: Algorithm for inferring z * ∈ Z , a latent representation for a generator G that produces an image x ∈ R m×n representative of class c ∈ C determined by a classifier F exceeding a class specific threshold T c  visual projections similar to included labels, although at times this is difficult to confirm. Some images do not match the included text, with some having multiple copies of a particular label. A white arrow typically used to demonstrate that a patient was upright for portable scan has also been reproduced, tends to co-occur with text denoting the scan type as portable and is often in similar positions to the reference images. Elements such as white borders, poor exposure, cropping, or rotation are all present in the source dataset and reproduced in some samples by the PGGAN model.</p><formula xml:id="formula_3">1 z * ∼ P z (Z) ; // Initialise z from prior distribution 2 S ← F c (G(z * )); 3 while S &lt; T c do 4 x ← G(z * ) ; // Generate image proposal 5 S ← F c (x) ; //</formula><p>The main distinction between distributions is that the generated images broadly lack certain smaller scale features of the source set. Jewelry, ECG leads, pacemakers, and IV catheters are largely absent with occasional partially formed objects in locations where these should appear. The elements of the missing structures form only a fraction of each overall image and are often fairly detailed objects themselves. A potential explanation for this phenomenon is the progressive growing technique itself, as these objects would only be generated near the end of the training process as the resolution of the images allowed for the details to be appreciated. Examining the generator in the final model without the moving weight average, supports this as a larger proportion of the images possess partially formed objects, implying the network was slowly incorporating these features. The appearance of these elements failed to improve despite training beyond the recommended total number of images. If this is the case, it is probably due to the reduced batch size at higher resolutions which would result in exceedingly slow convergence for rarer, more subtle features. An alternate explanation is the aforementioned capacity loss that prompted the shift to style-based networks as networks consume their own synthetic ability to retain variability in later layers. The effects of this are likely to be more pronounced in CXRs compared to other domains, like facial data, due to the presence of distinct small scale structures such as IV lines or ECG leads that are readily discernible on standard images.</p><p>To evaluate the applicability of the FID for automatic x-ray quality assessment, we first attempt to produce a meaningful zero measure by splitting the NIH dataset at the patient level and using the average labels from each patient to iteratively stratify the patients into two similar groups. The concept being that the groups should have similar disease distribution and be free of patient overlap. We calculate a low value of 0.53 for the FID, indicating that the underlying Inception network extracts similar features on average from both sets. These features may be sub-optimal given that the underlying network has not been trained on biomedical data, yet the small distance demonstrates that the images are embedded in a similar manner. We now need to demonstrate whether the metric can in fact distinguish between x-rays.</p><p>Given that the images are known to vary by pathology label, we argue that if the distance metric can reliably separate distinct classes, it must have filters that are capable of extracting features that are meaningful for evaluating the clinical plausibility of generates. If we were to re-train an Inception network purely to calculate the FID for medical imagery, the network would be trained to classify an image according to its class labels. If the current weights of the network are already able to provide such a separation, then there is no need to re-train and the metric can in fact be applied to the problem at hand.</p><p>To evaluate the extent to which the FID can discriminate between individual pathologies we utilise the No Finding label as a baseline as it principally should be absent of any significant changes while still varying similarly along protected patient parameters such as sex and age. We then evaluate the distance between it and the set of x-rays containing each other label, the results of which can be seen in table with the addition of a split along sex and the overall comparison between real and synthetic images 1. The results provide evidence that the FID is able to distinguish between the labelled pathologies and a baseline x-ray with the distance seemingly recapitulating the relative scale of the pathological change on the image. Very large distances are associated with the Edema and Consolidation labels, which are findings that may distort major segments of the lung fields, while Mass and Nodule labels produce significantly reduced distances as they generally occupy only small segments of the study. To provide more substantial evidence, we calculate a pairwise distance matrix between all classes and utilise multidimensional scaling to plot the relative 2D positions in <ref type="figure" target="#fig_4">figure 5</ref>. The various class locations show that the FID is able to cluster similar labels together. In addition, a hierarchical cluster based on the distance matrix, also seen in <ref type="figure" target="#fig_4">figure 5</ref>, successfully groups sets of related findings. The cluster demonstrates that the features extracted by the network underlying the FID are likely to be of a high enough quality to group sets of related findings and that the technique is likely applicable to CXRs despite not being explicitly trained to detect features of these images.  We as a result evaluate the quality of synthetic x-rays compared to the NIH source images. The FID between the full source dataset and 100 000 random PGGAN generates is 8.02. This value is comparable with FIDs reported for the PGGAN architecture on other non-biomedical high resolution image generation tasks <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pathological Variability</head><p>We estimate the pathological variability of the PGGAN generates in an effort to quantify the extent to which model capture the variability of the various disease entitites contained within the source CXRs. The labels were estimated by utilising a common classifier to label both the NIH and generated iamge sets with confidence intervals derived by bootstrapping the predictions 10 000 times. The proportion of the class labels from the NIH dataset and a random selection of 130 000 generated images can be seen in <ref type="figure">Figure 6</ref>. Overall it can be seen that the generates cover the range of labels with each represented in broadly similar proportions to the NIH set. Despite the similarity, there appears to be a degree of mode dropping with a majority of labels being significantly less prevalent in the generates compared to the reference set with the exception of Atelectasis, Cardiomegaly, Infiltration and No Finding. We hypothesize that this is likely to be due to a continuation of the phenomenon noted with the absence of certain smaller objects, the objects in this case being features of disease. This is supported by the label distribution, as more common conditions with larger disease features tend to be over represented, while diseases with progressively finer features are increasingly sparse in the generated samples. No Finding for example, is largely defined by the absence of smaller features and can be seen to be significantly in excess beyond the NIH set. Atelectasis and Cardiomegaly in comparison, define several characteristics that are present at a moderate resolution and see a smaller increase in prevalence. This is in stark contrast to Emphysema and Pleural Thickening, which may be quite fine on x-ray and similarly show the greatest reduction in prevalence. <ref type="figure">Figure 6</ref>: Label prevalence of the original NIH dataset and a random selection of 130 000 PGGAN generates. Point estimates are the average number of each label across all images. 95% Confidence intervals are provided by re-sampling the full set of labels with replacement 10 000 times. The network demonstrates a degree of mode dropping of classes with finer details that it finds difficult to reproduce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pathology Generation</head><p>We utilise class optimisation to generate pathological images based on the method described in Algorithm 1. We sample the initial latent code from a truncated normal distribution with a threshold of 0.7 <ref type="bibr" target="#b56">[57]</ref>. We attempt the method with both a Densenet-121 and a re-purposed Discriminator as classifiers. We generally found optimising the Densenet-121 model proved to be slower, more difficult and less reliable overall. The process frequently plateaus and subsequently fails to produce a sample for a particular class. Successes often rely on favourable sampling from the latent space, with such samples typically converging rapidly. The same process applied to the re-purposed discriminator proved more successful as samples converged more regularly with greater ease in achieving higher logit values for a particular class. This discrepancy is due to the input size of the different models. The Densenet as a result of being trained on a smaller resolution requires images to be resized prior to classification and as such results in a worse estimation of the gradient. Replication of the process with a Densenet trained on a larger input size produced significant improvements to both the reliability and rapidity of convergence. Examples of successful pathology generates can be seen in <ref type="figure">Figure 7</ref>. Many more examples of multi-modal pathology can be seen in the linked image archive.</p><p>Attempts to modify the optimisation process to produce examples of isolated labels often fails to converge, with optimisation by minimising other class scores alongside the maximisation of the class of interest typically resulting in a significant limitation to the score that can be achieved. This phenomenon is explained by the multi-modal nature of the disease classes and the underlying biomedical relationship between classes that results in label co-occurrence. These relationships can be seen with increases to the logit threshold when optimising for a particular class. Clearer examples of particular conditions tend to co-occur with their medical complications. Increasing class scores for Cardiomegaly begin to include a greater proportions of Pleural Effusion labels, this may be related to the complications that occur in heart failure that may cause images with these labels to co-occur. Similarly, increasing Emphysema class values produces associated Pneumothorax labels, while Nodule findings produce associated Mass labels. These findings reflect <ref type="figure">Figure 7</ref>: Example classes generated by maximising the classifier class logit. Images often have multiple findings, the finding optimised is the label given.</p><p>that the underlying unsupervised training methodology has captured the relationship between various x-ray features and optimising for the pathology enables us to tease out these properties.</p><p>As part of class optimisation, we're required to set a score threshold for determining when a proposed image has converged to be representative of a particular class. We utilise Youden's J statistic to determine the optimum cut-off based on the validation ROC curve per class when training the classifier. To simulate degrees of disease severity in produced images, we randomly increase this threshold by the absolute value of the distribution of N (1, 1) when optimising. It is quite seldom that a dataset includes only a single image of a patient. Typically there are series of patient images where individuals are re-scanned several times. There are a multitude of potential reasons for this, follow-up on patient condition, assessment of medical intervention, or routine imagery prior to surgery, are just a handful of potential reasons. The end result is that most datasets have a significant proportion of very similar images wherein position or disease severity may be altered slightly but the overall image is largely unchanged. We simulate this process by sampling in the vicinity of a particular pathology-optimised latent code. We treat the optimised location as a centre and sample the surrounding hypersphere within N (0, 0.2). We find that this enables the creation of images with the same set of diagnoses but minor changes in orientation, exposure, or disease severity. The degree of variability can be tuned by adjusting the standard deviation, however, we find a value of 0.2 tends to produce minor variations without compromising the diagnostic label.</p><p>The fact that the latent space may be optimised with regard to disease classification at all implies a degree of orientation with respect to pathology which may hold semantic value. We leave a full exploration of the properties of the latent space to future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Radiologist Review</head><p>We evaluate the plausibility of generated x-rays to domain experts through a series of image reviews. We ask a group of practising radiologists to identify a set of real images out of a grid of mixed x-rays as well as to select a real image out of a pair of real and generated images. We received feedback from 3 consultant (equivalent to board certified) radiologists, and 5 radiology registrars (equivalent to residents in North America). Image grids consist of six x-rays that are produced with a 50% probability of being real or generated, we evaluate six such grids. Real images were identified as such by radiologists 73% (95% CI: 63, 82) of the time, while generates were identified as real 61% (95% CI: 51, 70) of the time, with both groups more likely than chance to be identified as real, with reals p &lt; 0.0001 and generates p = 0.0155 for a one-sided t-test. Radiologist performance on x-ray discrimination equates to a HYPE ∞ score of 33% (95% CI: <ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b42">43)</ref>. Discrimination of image pairs is clearly in favour of real images, with radiologists correctly identifying the true image of a pair 71% (95% CI: 55, 86) of the time. We evaluate 20 such image pairs. Comments from respondents mentioned bony abnormalities as the main feature used to identify samples as being synthetic. Images were embedded as options within a Google Form and shown at a 260x260 resolution with unlimited time to review images prior to making a decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Synthetic Model Training</head><p>For evaluating the classification performance of a model trained only on synthetic data, we generate an archive of CXRs of comparable size and with labels in a similar proportion to the source NIH dataset. We uniformly sample between 3 to 6 related images per class optimisation operation and treat them as belonging to the same 'patient'. We similarly iteratively stratify the synthetic patients into training and validation sets in the same manner as during initial classifier training, retaining only the original test set of real images, which we use for all comparisons. We train a Densenet classifier in the same configuration as described in section 3.3. We evaluate performance relative to real images as well as images embedded into the generator's latent space. We consider embedding images as the current alternative to individual class generation, whereby a network predicts the latent encoding of a given image. To achieve this, we retrain the discriminator to predict such a representation given a random sample from the generator and minimising the mean squared error between the predicted and actual locations in the latent space. We train with the proposed embedded image alongside the original labels. A table of comparative ROC AUC values can be seen in <ref type="table" target="#tab_2">table 2</ref> The class generation results provide further evidence that the PGGAN model has learnt to replicate all the classes within the original NIH dataset, as the synthetic classifier is able to detect each pathology from the source to a degree significantly greater than expected by chance, despite never actually seeing an image from the original dataset. Furthermore, the generated classes demonstrate that the method reliably produces examples of each class, with a performance reduction similar to that seen with previous PGGAN generation tasks <ref type="bibr" target="#b16">[17]</ref>. These results should scale with improvements made to the quality of generated images, as augments to the underlying GAN will reduce the distinction between the synthetic and real sets, which alongside class generation, will allow for even better representations for training. The poor performance of embedded samples are indicative of such a method failing to preserve class-specific information, and show it to be less suitable of a method for producing anonymised clinical archives. A potential alternative to predictive embedding would be to optimise the latent space to minimise image reconstruction loss. Our experiments with this showed promise for more faithful representations, however, the performance was profoundly worse than alternatives, requiring days to convert an archive compared to only a handful of hours for competing approaches. We attempted comparisons with generating classes through a simpler DCGAN architecture similarly trained on the full NIH dataset. We found the generation process intractable as the model often failed to produce class scores sufficient to be considered examples of that class despite extensive optimisation. We attribute this to a combination of mode dropping and lower image quality which precludes the formation of certain classes. These results provide substantial evidence that class optimisation is effective at producing images that are representative of a particular disease label and can enable a single unsupervised GAN to produce a fully labelled cohort of classes from an anonymised dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Through both automated and expert review of the medical plausibility of generated x-rays as well an evaluation of synthetic model performance we have outlined the properties of the PGGAN architecture applied to CXRs. Expert review demonstrated global coherence of the imagery with reproduction of numerous features associated with varying disease classes. Deficiencies have been noted with a broad absence of detailed, small scale features that are typically easily discernible on x-ray films. Automated comparisons of image features by means of the FID provide evidence that the quality of generates is of a similar standard to typical high resolution tasks where PGGAN is applied.</p><p>These limitations are likely explained by stochastic bottlenecks and slower convergence of novel features at higher resolutions and are inherent to the design and training formulation employed by the baseline PGGAN methodology. Future work should follow improvements made to subsequent GAN architectures in the PGGAN lineage <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>, or alternatively examine alternate approaches such as Very Deep Variational Autoencoders (VAEs) <ref type="bibr" target="#b57">[58]</ref> which have similarly shown promise for high resolution image synthesis. Irrespective of the exact mechanism used to generate images, the implementation of multi-modal class optimisation allows for the extraction of disease-representative classes from the latent space that can be utilised to construct labelled synthetic datasets of arbitrary size.</p><p>We suspect this method will prove to be of even greater utility with GAN architectures that possess less entangled latent spaces and could allow for disease optimisation at varying resolutions by considering scale-specific elements of the extended latent space seen in StyleGAN and later works <ref type="bibr" target="#b33">[34]</ref>. We envision that such techniques should allow significant control over the severity of disease present in generates and may provide profound capabilities for dataset augmentation. This would allow for applications such as demonstrating potential progression or resolution of disease or retaining disease presentation but shifting patient particular parameters such as age or sex. Scale specific modifications may enable the synthesis of support devices such as IV lines that may be differed against the source image and used for augmentation in segmentation tasks. We envision that continued development along this route will enable task-specific image extraction from broad image generators, as unsupervised GANs trained on large corpora of diverse images should be able to extract a multitude of classes based on the capabilities of the associated classifier, even if such classes were not labelled in the original individual datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have applied a progressively growing GAN model to the task of synthesising high-resolution chest x-ray images for the evaluation of their suitability as a replacement for standard images for the tasks of model development and student education. The overarching goal of this investigation being to improve the protection of patient privacy without compromising data availability. We evaluate the applicability of the Fréchet Inception Distance to the evaluation of synthetic chest x-rays and find that the underlying network is capable of providing a meaningful metric for generate quality despite the difference in data distribution. We demonstrate that it is possible to produce realistic, clinically plausible images that capture much of the variation in standard x-rays, however, there remains a significant need for improvement in the reproduction of small-scale details to achieve truly indistinguishable samples. We demonstrate that the model is capable of reproducing all abnormalities of interest and in similar proportions to the source image distribution despite differences in small scale features. We describe a methodology for the extraction of class representative images from the generator's latent space by optimisation of a classifier score and demonstrate that such a method is capable of constructing a fully labelled synthetic dataset from an unsupervised generator. We describe potential avenues of improvement for generate quality and anticipate that such improvements will enable the production of high quality teaching and training images without the concern of breaching patient confidentiality. We make the source code, final model weights and a large archive of labelled generates used in this study available to the broader research community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compliance with Ethical Standards</head><p>Funding The authors declare that they received no funding for this work.</p><p>Conflicts of interest The authors declare that they have no conflict of interest. Ethics Approval An ethics waiver was issued for this work by the Human Research Ethics Committee (Medical) of the University of the Witwatersrand, Johannesburg on 11/08/2020.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Training configuration for PGGAN. The network trains until convergence and then doubles the spatial resolution. This process is repeated until the desired resolution is achieved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Beyond</head><label></label><figDesc>CXRs, PGGANs have been applied to generate other high resolution medical images. Beers et al. demonstrate the applicability of the method for creating high fidelity reproductions of the retinal fundus and MRI slices of gliomas [15], while Togo et al. produce patches of x-rays of gastritis [17]. Both implementations introduce domain-specific modifications. Beers et al. include segmentation maps as additional channels for fundal images to enhance the generation of features relevant to diagnosis of retinopathy, while Togo et al. introduce a conditional loss at higher resolutions to promote the distinction between gastritis and normal tissues. Korkinof et al. synthesise high resolution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Obtain class score from classifier 6 if S ≥ T c then 7 11 zFigure 3 :</head><label>67113</label><figDesc>* ← z * − α∇ z L ; Visual representation of class generation method shown in Algorithm 1.4 Results &amp; Analysis4.1 Image QualityThe generated images, as seen in figure 2, appear to broadly reflect the NIH source images with global features varying similarly across both sets. Sex, posture, exposure, and positioning (AP vs PA) are all represented within the generates. The images accurately reflect the standard anatomical features found in CXRs with realistic alterations in perspective seen with changes in patient posture or positioning of the x-ray detector. Soft tissues such as the heart, liver, and stomach are faithfully reproduced with normal variability in their relative positioning. Bony structures are correctly placed but suffer from inconsistent profiles, with the ribs in particular tending to reveal a degree of undulation. Closer inspection often reveals slight curves or alterations in calibre that are seldom explained by the perspective of the image.Beyond the x-ray itself, the model has learnt to include various markup elements included in the reference images. Most images have a symbol or tag demonstrating the left side of the image, in addition, projection descriptors such as 'PORTABLE' and 'AP' are included on numerous images. Images are typically sided correctly and tend to have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of sample quality between different GAN architectures and the source dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>NIH Class Label FIDs. Left: Classes plotted according to the multidimensional scaling of the pairwise distance matrix calculated by selecting each class as a baseline and determining the distance to all other classes in turn. Right: A hierarchical cluster based on the distance matrix showing the grouping of similar classes based on the features extracted by the Inception network when calculating the FID.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Example of local sampling after class generation. Images show similar anatomical and pathological features with variations in image contrast, text markup, and patient positioning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell cols="4">Fréchet Inception Distance (FID) per Dataset Split</cell></row><row><cell>Split</cell><cell>FID</cell><cell>Split</cell><cell>FID</cell></row><row><cell>Stratified</cell><cell cols="2">0.53 Hernia</cell><cell>15.96</cell></row><row><cell>Sex</cell><cell cols="2">7.87 Infiltration</cell><cell>20.05</cell></row><row><cell>No Finding</cell><cell cols="2">0.00 Mass</cell><cell>10.06</cell></row><row><cell>Atelectasis</cell><cell cols="2">19.90 Nodule</cell><cell>6.22</cell></row><row><cell cols="3">Cardiomegaly 14.23 Pleural Effusion</cell><cell>23.90</cell></row><row><cell cols="4">Consolidation 42.45 Pleural Thickening 13.05</cell></row><row><cell>Edema</cell><cell cols="2">59.40 Pneumonia</cell><cell>32.05</cell></row><row><cell>Emphysema</cell><cell cols="2">19.56 Pneumothorax</cell><cell>18.00</cell></row><row><cell>Fibrosis</cell><cell cols="2">9.72 Synthetic</cell><cell>8.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of Performance of Classification Models Trained on Synthetic vs Real Data</figDesc><table><row><cell>Class Label</cell><cell cols="3">Embedded Class Generated Real</cell></row><row><cell>Atelectasis</cell><cell>0.646</cell><cell>0.851</cell><cell>0.923</cell></row><row><cell>Cardiomegaly</cell><cell>0.602</cell><cell>0.907</cell><cell>0.968</cell></row><row><cell>Consolidation</cell><cell>0.675</cell><cell>0.928</cell><cell>0.963</cell></row><row><cell>Edema</cell><cell>0.746</cell><cell>0.941</cell><cell>0.985</cell></row><row><cell>Emphysema</cell><cell>0.51</cell><cell>0.901</cell><cell>0.972</cell></row><row><cell>Fibrosis</cell><cell>0.63</cell><cell>0.868</cell><cell>0.961</cell></row><row><cell>Hernia</cell><cell>0.611</cell><cell>0.951</cell><cell>0.986</cell></row><row><cell>Infiltration</cell><cell>0.596</cell><cell>0.796</cell><cell>0.847</cell></row><row><cell>Mass</cell><cell>0.516</cell><cell>0.825</cell><cell>0.936</cell></row><row><cell>No Finding</cell><cell>0.605</cell><cell>0.861</cell><cell>0.932</cell></row><row><cell>Nodule</cell><cell>0.543</cell><cell>0.748</cell><cell>0.909</cell></row><row><cell>Pleural Effusion</cell><cell>0.636</cell><cell>0.903</cell><cell>0.954</cell></row><row><cell>Pleural Thickening</cell><cell>0.583</cell><cell>0.881</cell><cell>0.954</cell></row><row><cell>Pneumonia</cell><cell>0.595</cell><cell>0.932</cell><cell>0.975</cell></row><row><cell>Pneumothorax</cell><cell>0.568</cell><cell>0.877</cell><cell>0.947</cell></row><row><cell>Average</cell><cell>0.604</cell><cell>0.878</cell><cell>0.947</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">An ethics waiver was issued for this work by the Human Research Ethics Committee (Medical) of the University of the Witwatersrand, Johannesburg on 11/08/2020.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/facebookresearch/pytorch_GAN_zoo</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Current Status of Thoracic Imaging&quot;. en. In: Grainger &amp; Allison&apos;s Diagnostic Radiology: A Textbook of Medical Imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Adam</surname></persName>
		</author>
		<idno>p. 3. ISBN: 978-0-7020-7562-9 978-0-7020-7561-2</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning&quot;. en</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Kermany</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cell.2018.02.010</idno>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Majkowska</surname></persName>
		</author>
		<idno type="DOI">10.1148/radiol.2019191293</idno>
	</analytic>
	<monogr>
		<title level="j">Chest Radiograph Interpretation with Deep Learning Models: Assessment with Radiologist-Adjudicated Reference Standards and Population-Adjusted Evaluation&quot;. en. In: Radiology</title>
		<imprint>
			<biblScope unit="volume">294</biblScope>
			<biblScope unit="page" from="33" to="8419" />
			<date type="published" when="2020-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05225</idno>
		<idno>arXiv: 1711.05225</idno>
	</analytic>
	<monogr>
		<title level="j">CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning&quot;. en</title>
		<imprint>
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
	<note>cs, stat. cs, stat</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automated Detection of COVID-19 Cases Using Deep Neural Networks with X-Ray Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tulin</forename><surname>Ozturk</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compbiomed.2020.103792</idno>
	</analytic>
	<monogr>
		<title level="j">Computers in Biology and Medicine</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An Artificial Intelligence System for Predicting the Deterioration of COVID-19 Patients in the Emergency Department</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farah</forename><forename type="middle">E</forename><surname>Shamout</surname></persName>
			<affiliation>
				<orgName type="collaboration">en</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:2008.01774</idno>
		<idno>arXiv: 2008.01774</idno>
		<imprint>
			<date type="published" when="2020-11" />
		</imprint>
	</monogr>
	<note>cs, eess. cs, eess</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">COVID-Net: A Tailored Deep Convolutional Neural Network Design for Detection of COVID-19 Cases from Chest X-Ray Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Wang</surname></persName>
			<affiliation>
				<orgName type="collaboration">en</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Wong</surname></persName>
			<affiliation>
				<orgName type="collaboration">en</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:2003.09871</idno>
		<idno>arXiv: 2003.09871</idno>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
	<note>cs, eess. cs, eess</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison&quot;. en</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Irvin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07031</idno>
		<idno>arXiv: 1901.07031</idno>
		<imprint>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
	<note>cs, eess. cs, eess</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Learning for Chest Radiograph Diagnosis: A Retrospective Comparison of the CheXNeXt Algorithm to Practicing Radiologists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pmed.1002686</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS Medicine</title>
		<editor>Aziz Sheikh</editor>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Computer-Aided Diagnosis, and Radiomics: Advances in Imaging towards to Precision Medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel Koenigkam</forename><surname>Santos</surname></persName>
		</author>
		<idno type="DOI">10.1590/0100-3984.2019.0049</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2019-12" />
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="387" to="396" />
		</imprint>
	</monogr>
	<note>Radiologia Brasileira</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MIMIC-CXR, a de-Identified Publicly Available Database of Chest Radiographs with Free-Text Reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41597-019-0322-0</idno>
		<idno>DOI: 10.1038/ s41597-019-0322-0</idno>
	</analytic>
	<monogr>
		<title level="m">Scientific Data</title>
		<imprint>
			<date type="published" when="2019-12" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2052" to="4463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ChestX-Ray8: Hospital-Scale Chest x-Ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.369</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="3462" to="3471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Review of Deep Learning in Medical Imaging: Imaging Traits, Technology Trends, Case Studies With Progress Highlights, and Future Promises</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1109/JPROC.2021.3054390</idno>
		<idno type="arXiv">arXiv:2008.09104</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE (2021)</title>
		<meeting>the IEEE (2021)</meeting>
		<imprint>
			<biblScope unit="page" from="18" to="9219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CheXclusion: Fairness Gaps in Deep Chest X-Ray Classifiers&quot;. en</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laleh</forename><surname>Seyyed-Kalantari</surname></persName>
		</author>
		<idno type="DOI">10.1142/9789811232701_0022</idno>
		<idno type="arXiv">arXiv:2003.00827</idno>
	</analytic>
	<monogr>
		<title level="m">Biocomputing 2021</title>
		<meeting><address><addrLine>Kohala Coast, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>WORLD SCIENTIFIC</publisher>
			<date type="published" when="2020-11" />
			<biblScope unit="page" from="232" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">High-Resolution Medical Image Synthesis Using Progressively Grown Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Beers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03144</idno>
		<idno>arXiv: 1805.03144</idno>
		<imprint>
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Synthesizing Chest X-Ray Pathology for Training Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hojjat</forename><surname>Salehinejad</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2018.2881415</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="278" to="0062" />
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Synthetic Gastritis Image Generation via Loss Function-Based Conditional PGGAN&quot;. en</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Togo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takahiro</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miki</forename><surname>Haseyama</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2019.2925863</idno>
		<idno>DOI: 10.1109/ ACCESS.2019.2925863</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2169" to="3536" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekta</forename><surname>Walia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Babyn</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2019.101552</idno>
	</analytic>
	<monogr>
		<title level="j">Generative Adversarial Network in Medical Imaging: A Review&quot;. en. In: Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Progressive Growing of GANs for Improved Quality, Stability, and Variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Segal</surname></persName>
		</author>
		<ptr target="https://github.com/BradSegal/CXR_PGGAN" />
	</analytic>
	<monogr>
		<title level="j">CXR PGGAN Code&quot;. In: GitHub. Note</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">CXR PGGAN Model and Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Segal</surname></persName>
		</author>
		<ptr target="https://kaggle.com/bradsegal/synthetic-pggan-chest-xrays" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems -Volume 2. NIPS&apos;14</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems -Volume 2. NIPS&apos;14<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Which Training Methods for GANs Do Actually Converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><forename type="middle">M</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In: ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved Techniques for Training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems. NIPS&apos;16</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems. NIPS&apos;16<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="978" to="979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image-to-Image Translation with Conditional Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.632</idno>
		<idno type="arXiv">arXiv:1611.07004</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<idno>arXiv: 1703.10593</idno>
		<imprint>
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.19</idno>
		<idno type="arXiv">arXiv:1609.04802</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CT Super-Resolution GAN Constrained by the Identical, Residual, and Cycle Learning Ensemble(GAN-CIRCLE)&quot;. en</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyu</forename><surname>You</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2019.2922960</idno>
		<idno type="arXiv">arXiv:1808.04256</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<date type="published" when="2020-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Interpreting the Latent Space of GANs for Semantic Face Editing&quot;. en</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00926</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="9240" to="9249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
			<affiliation>
				<orgName type="collaboration">en ; cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
			<affiliation>
				<orgName type="collaboration">en ; cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
			<affiliation>
				<orgName type="collaboration">en ; cs</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<idno>arXiv: 1511.06434</idno>
		<imprint>
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Towards Principled Methods for Training Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.04862</idno>
		<idno>arXiv: 1701.04862</idno>
		<imprint>
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
	<note>cs, stat. cs, stat</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Least Squares Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.304</idno>
		<idno type="arXiv">arXiv:1611.04076</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="2813" to="2821" />
		</imprint>
	</monogr>
	<note>Venice: IEEE</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improved Training of Wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems. NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems. NIPS&apos;17<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="978" to="979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Style-Based Generator Architecture for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00453</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="4396" to="4405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Analyzing and Improving the Image Quality of StyleGAN&quot;. en</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00813</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="8107" to="8116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Data Synthesis Based on Generative Adversarial Networks&quot;. en</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noseong</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.14778/3231751.3231757</idno>
		<idno type="arXiv">arXiv:1806.03384</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1071" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Chest X-Ray Generation and Data Augmentation for Cardiovascular Abnormality Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Moradi</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.2293971</idno>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging 2018: Image Processing</title>
		<editor>Elsa D. Angelini and Bennett A. Landman</editor>
		<meeting><address><addrLine>Houston, United States</addrLine></address></meeting>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mammo{GAN: High-Resolution Synthesis of Realistic Mammograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Korkinof</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03401</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning -Extended Abstract Track</title>
		<meeting><address><addrLine>London, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">GAN Augmentation: Augmenting Training Data Using Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bowles</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:1810.10863</idno>
		<idno>arXiv: 1810.10863</idno>
		<imprint>
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Medical Imaging and Its Objective Quality Assessment: An Introduction&quot;. en. In: Classification in BioApps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Thanki</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-65981-7_1</idno>
		<editor>Nilanjan Dey, Amira S. Ashour, and Surekha Borra</editor>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems. NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems. NIPS&apos;17<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="978" to="979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016)</title>
		<imprint>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Are GANs Created Equal? A Large-Scale Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems. NIPS&apos;18</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems. NIPS&apos;18<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="698" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01121</idno>
		<idno>arXiv: 1904.01121</idno>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">In: Computer Vision -ECCV 2018</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01216-8_14</idno>
		<idno type="arXiv">arXiv:1807.09499</idno>
		<editor>Vittorio Ferrari et al.</editor>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="volume">11206</biblScope>
			<biblScope unit="page" from="218" to="234" />
		</imprint>
	</monogr>
	<note>How Good Is My GAN?</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Exploring Large-Scale Public Medical Image Datasets&quot;. en</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Oakden-Rayner</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.acra.2019.10.006</idno>
	</analytic>
	<monogr>
		<title level="m">Academic Radiology</title>
		<imprint>
			<date type="published" when="2020-01" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">PyTorch Lightning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wa Falcon</surname></persName>
		</author>
		<ptr target="https://github.com/PyTorchLightning/pytorch-lightning3" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to Recognize Abnormalities in Chest X-Rays with Location-Aware Dense Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gündel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-13469-3_88</idno>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Conditional Generative Adversarial Nets&quot;. en</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<idno>arXiv: 1411.1784</idno>
		<imprint>
			<date type="published" when="2014-11" />
		</imprint>
	</monogr>
	<note>cs, stat. cs, stat</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Conditional Image Synthesis with Auxiliary Classifier GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09585</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research. International Convention Centre</title>
		<editor>Doina Precup and Yee Whye Teh</editor>
		<meeting>Machine Learning Research. International Convention Centre<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
	<note>Proceedings of the 34th International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Generative Adversarial Text to Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
		<editor>Maria Florina Balcan and Kilian Q. Weinberger</editor>
		<meeting>the 33rd International Conference on Machine Learning<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1060" to="1069" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Inverting the Generator of a Generative Adversarial Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonia</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil Anthony</forename><surname>Bharath</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2018.2875194</idno>
		<idno type="arXiv">arXiv:1611.05644</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2162" to="237" />
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00453</idno>
		<idno type="arXiv">arXiv:1904.03189</idno>
	</analytic>
	<monogr>
		<title level="m">In: 2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Seoul, Korea (South</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="4431" to="4440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Large Scale GAN Training for High Fidelity Natural Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Very Deep {VAEs Generalize Autoregressive Models and Can Outperform Them on Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10650</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations. 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
