<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AMR Parsing as Sequence-to-Graph Transduction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutai</forename><surname>Ma</surname></persName>
							<email>xutaima@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
							<email>kevinduh@cs.jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">Van</forename><surname>Durme</surname></persName>
							<email>vandurme@cs.jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AMR Parsing as Sequence-to-Graph Transduction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an attention-based model that treats AMR parsing as sequence-to-graph transduction. Unlike most AMR parsers that rely on pre-trained aligners, external semantic resources, or data augmentation, our proposed parser is aligner-free, and it can be effectively trained with limited amounts of labeled AMR data. Our experimental results outperform all previously reported SMATCH scores, on both AMR 2.0 (76.3% F1 on LDC2017T10) and AMR 1.0 (70.2% F1 on LDC2014T12).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Another View of Reentrancy</head><p>AMR is a rooted, directed, and usually acyclic graph where nodes represent concepts, and labeled directed edges represent the relationships between them (see <ref type="figure">Figure 1</ref> for an AMR example). The reason for AMR being a graph instead of a tree is that it allows reentrant semantic relations. For instance, in <ref type="figure">Figure 1</ref>(a) "victim" is both ARG0 and arXiv:1905.08704v2 [cs.CL]</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Abstract Meaning Representation (AMR, <ref type="bibr" target="#b4">Banarescu et al., 2013)</ref> parsing is the task of transducing natural language text into AMR, a graphbased formalism used for capturing sentence-level semantics. Challenges in AMR parsing include:</p><p>(1) its property of reentrancy -the same concept can participate in multiple relations -which leads to graphs in contrast to trees <ref type="bibr" target="#b61">(Wang et al., 2015)</ref>;</p><p>(2) the lack of gold alignments between nodes (concepts) in the graph and words in the text which limits attempts to rely on explicit alignments to generate training data <ref type="bibr" target="#b22">(Flanigan et al., 2014;</ref><ref type="bibr" target="#b61">Wang et al., 2015;</ref><ref type="bibr" target="#b15">Damonte et al., 2017;</ref><ref type="bibr" target="#b23">Foland and Martin, 2017;</ref><ref type="bibr" target="#b50">Peng et al., 2017b;</ref><ref type="bibr" target="#b25">Groschwitz et al., 2018;</ref><ref type="bibr" target="#b28">Guo and Lu, 2018)</ref>; and (3) relatively limited amounts of labeled data <ref type="bibr" target="#b33">(Konstas et al., 2017)</ref>.</p><p>Recent attempts to overcome these challenges include: modeling alignments as latent variables <ref type="bibr" target="#b38">(Lyu and Titov, 2018)</ref>; leveraging external semantic resources <ref type="bibr" target="#b1">(Artzi et al., 2015;</ref><ref type="bibr" target="#b6">Bjerva et al., 2016)</ref>; data augmentation <ref type="bibr" target="#b33">(Konstas et al., 2017;</ref><ref type="bibr" target="#b46">van Noord and Bos, 2017b)</ref>; and employing attention-based sequence-to-sequence models <ref type="bibr" target="#b5">(Barzdins and Gosko, 2016;</ref><ref type="bibr" target="#b33">Konstas et al., 2017;</ref><ref type="bibr" target="#b46">van Noord and Bos, 2017b)</ref>.</p><p>In this paper, we introduce a different way to handle reentrancy, and propose an attention-based model that treats AMR parsing as sequence-tograph transduction. The proposed model, supported by an extended pointer-generator network, is aligner-free and can be effectively trained with limited amount of labeled AMR data. Experiments on two publicly available AMR benchmarks demonstrate that our parser clearly outperforms the previous best parsers on both benchmarks. It achieves the best reported SMATCH scores: 76.3% F1 on LDC2017T10 and 70.2% F1 on LDC2014T12. We also provide extensive ablative and qualitative studies, quantifying the contributions from each component. Our model implementation is available at https://github. com/sheng-z/stog. ARG1 of "help-01". While efforts have gone into developing graph-based algorithms for AMR parsing <ref type="bibr" target="#b10">(Chiang et al., 2013;</ref><ref type="bibr" target="#b22">Flanigan et al., 2014)</ref>, it is more challenging to parse a sentence into an AMR graph rather than a tree as there are efficient off-the-shelf tree-based algorithms, e.g., <ref type="bibr" target="#b11">Chu and Liu (1965)</ref>; <ref type="bibr">Edmonds (1968)</ref>. To leverage these tree-based algorithms as well as other structured prediction paradigms <ref type="bibr" target="#b40">(McDonald et al., 2005)</ref>, we introduce another view of reentrancy. AMR reentrancy is employed when a node participates in multiple semantic relations. We convert an AMR graph into a tree by duplicating nodes that have reentrant relations; that is, whenever a node has a reentrant relation, we make a copy of that node and use the copy to participate in the relation, thereby resulting in a tree. Next, in order to preserve the reentrancy information, we add an extra layer of annotation by assigning an index to each node. Duplicated nodes are assigned the same index as the original node. <ref type="figure" target="#fig_0">Figure 1(b)</ref> shows a resultant AMR tree: subscripts of nodes are indices; two "victim" nodes have the same index as they refer to the same concept. The original AMR graph can be recovered by merging identically indexed nodes and unioning edges from/to these nodes. Similar ideas were used by <ref type="bibr" target="#b1">Artzi et al. (2015)</ref> who introduced Skolem IDs to represent anaphoric references in the transformation from CCG to AMR, and van Noord and Bos (2017a) who kept co-indexed AMR variables, and converted them to numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Formalization</head><p>If we consider the AMR tree with indexed nodes as the prediction target, then our approach to parsing is formalized as a two-stage process: node prediction and edge prediction. 1 An example of the parsing process is shown in <ref type="figure">Figure 2</ref>. Node Prediction Given a input sentence w = w 1 , ..., w n , each w i a word in the sentence, our approach sequentially decodes a list of nodes u = u 1 , ..., u m and deterministically assigns their indices d = d 1 , ..., d m .</p><formula xml:id="formula_0">P (u) = m i=1 P (u i | u &lt;i , d &lt;i , w)</formula><p>Note that we allow the same node to occur multi-1 The two-stage process is similar to "concept identification" and "relation identification" in <ref type="bibr" target="#b22">Flanigan et al. (2014)</ref>; ; <ref type="bibr" target="#b38">Lyu and Titov (2018)</ref>; inter alia.  <ref type="figure">Figure 2</ref>: A two-stage process of AMR parsing. We remove senses (i.e., -01, -02, etc.) as they will be assigned in the post-processing step.</p><p>ple times in the list; multiple occurrences of a node will be assigned the same index. We choose to predict nodes sequentially rather than simultaneously, because (1) we believe the current node generation is informative to the future node generation; (2) variants of efficient sequence-to-sequence models <ref type="bibr" target="#b2">(Bahdanau et al., 2014;</ref><ref type="bibr" target="#b58">Vinyals et al., 2015)</ref> can be employed to model this process. At the training time, we obtain the reference list of nodes and their indices using a pre-order traversal over the reference AMR tree. We also evaluate other traversal strategies, and will discuss their difference in Section 7.2. Edge Prediction Given a input sentence w, a node list u, and indices d, we look for the highest scoring parse tree y in the space Y(u) of valid trees over u with the constraint of d. A parse tree y is a set of directed head-modifier edges y = {(u i , u j ) | 1 ≤ i, j ≤ m}. In order to make the search tractable, we follow the arcfactored graph-based approach <ref type="bibr" target="#b40">(McDonald et al., 2005;</ref><ref type="bibr" target="#b32">Kiperwasser and Goldberg, 2016)</ref>, decomposing the score of a tree to the sum of the score of its head-modifier edges:</p><formula xml:id="formula_1">parse(u) = arg max y∈Y(u) (u i ,u j )∈y score(u i , u j )</formula><p>Based on the scores of the edges, the highest scoring parse tree (i.e., maximum spanning arborescence) can be efficiently found using the Chu-Liu-Edmonnds algorithm. We further incorporate indices as constraints in the algorithm, which is described in Section 4.4. After obtaining the parse tree, we merge identically indexed nodes to recover the standard AMR graph.      <ref type="figure">Figure 3</ref>: Extended pointer-generator network for node prediction. For each decoding time step, three probabilities p src , p tgt and p gen are calculated. The source and target attention distributions as well as the vocabulary distribution are weighted by these probabilities respectively, and then summed to obtain the final distribution, from which we make our prediction. Best viewed in color.</p><formula xml:id="formula_2">/ i z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V H J o 8 l r H u B M y A F B E 0 U a C E T q K B q U B C O x h f z / z 2 A 2 g j 4 u g O J w n 4 i g 0 j E Q r O 0 E r 3 S b + H 8 I R a Z T j E a b 9 c c a v u H H S V e D m p k B y N f v m r N 4 h 5 q i B C L p k x X c 9 N 0 M + Y R s E l T E u 9 1 E D C + J g N o W t p x B Q Y P 5 t f P a V n V h n Q M N a 2 I q R z 9 f d E x p Q x E x X Y T s V w Z J a 9 m f i f 1 0 0 x r P m Z i J I U I e K L R W E q K c Z 0 F g E d C A 0 c 5 c Q S x r W w t 1 I + Y p p x t E G V b A j</formula><formula xml:id="formula_3">/ i z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V H J o 8 l r H u B M y A F B E 0 U a C E T q K B q U B C O x h f z / z 2 A 2 g j 4 u g O J w n 4 i g 0 j E Q r O 0 E r 3 S b + H 8 I R a Z T j E a b 9 c c a v u H H S V e D m p k B y N f v m r N 4 h 5 q i B C L p k x X c 9 N 0 M + Y R s E l T E u 9 1 E D C + J g N o W t p x B Q Y P 5 t f P a V n V h n Q M N a 2 I q R z 9 f d E x p Q x E x X Y T s V w Z J a 9 m f i f 1 0 0 x r P m Z i J I U I e K L R W E q K c Z 0 F g E d C A 0 c 5 c Q S x r W w t 1 I + Y p p x t E G V b A j</formula><formula xml:id="formula_4">/ i z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V H J o 8 l r H u B M y A F B E 0 U a C E T q K B q U B C O x h f z / z 2 A 2 g j 4 u g O J w n 4 i g 0 j E Q r O 0 E r 3 S b + H 8 I R a Z T j E a b 9 c c a v u H H S V e D m p k B y N f v m r N 4 h 5 q i B C L p k x X c 9 N 0 M + Y R s E l T E u 9 1 E D C + J g N o W t p x B Q Y P 5 t f P a V n V h n Q M N a 2 I q R z 9 f d E x p Q x E x X Y T s V w Z J a 9 m f i f 1 0 0 x r P m Z i J I U I e K L R W E q K c Z 0 F g E d C A 0 c 5 c Q S x r W w t 1 I + Y p p x t E G V b A j</formula><formula xml:id="formula_5">/ i z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V H J o 8 l r H u B M y A F B E 0 U a C E T q K B q U B C O x h f z / z 2 A 2 g j 4 u g O J w n 4 i g 0 j E Q r O 0 E r 3 S b + H 8 I R a Z T j E a b 9 c c a v u H H S V e D m p k B y N f v m r N 4 h 5 q i B C L p k x X c 9 N 0 M + Y R s E l T E u 9 1 E D C + J g N o W t p x B Q Y P 5 t f P a V n V h n Q M N a 2 I q R z 9 f d E x p Q x E x X Y T s V w Z J a 9 m f i f 1 0 0 x r P m Z i J I U I e K L R W E q K c Z 0 F g E d C A 0 c 5 c Q S x r W w t 1 I + Y p p x t E G V b A j</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>Our model has two main modules: (1) an extended pointer-generator network for node prediction; and (2) a deep biaffine classifier for edge prediction. The two modules correspond to the two-stage process for AMR parsing, and they are jointly learned during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Extended Pointer-Generator Network</head><p>Inspired by the self-copy mechanism in <ref type="bibr" target="#b63">Zhang et al. (2018)</ref>, we extend the pointer-generator network <ref type="bibr" target="#b56">(See et al., 2017)</ref> for node prediction. The pointer-generator network was proposed for text summarization, which can copy words from the source text via pointing, while retaining the ability to produce novel words through the generator. The major difference of our extension is that it can copy nodes, not only from the source text, but also from the previously generated nodes on the target side. This target-side pointing is well-suited to our task as nodes we will predict can be copies of other nodes. While there are other pointer/copy networks <ref type="bibr" target="#b41">Merity et al., 2016;</ref><ref type="bibr" target="#b42">Miao and Blunsom, 2016;</ref>, we found the pointer-generator network very effective at reducing data sparsity in AMR parsing, which will be shown in Section 7.2.</p><p>As depicted in <ref type="figure">Figure 3</ref>, the extended pointergenerator network consists of four major components: an encoder embedding layer, an encoder, a decoder embedding layer, and a decoder. Encoder Embedding Layer This layer converts words in input sentences into vector representations. Each vector is the concatenation of embeddings of GloVe <ref type="bibr" target="#b51">(Pennington et al., 2014)</ref>, BERT <ref type="bibr" target="#b16">(Devlin et al., 2018)</ref>, POS (part-of-speech) tags and anonymization indicators, and features learned by a character-level convolutional neural network (CharCNN, <ref type="bibr" target="#b30">Kim et al., 2016)</ref>.</p><p>Anonymization indicators are binary indicators that tell the encoder whether the word is an anonymized word. In preprocessing, text spans of named entities in input sentences will be replaced by anonymized tokens (e.g. person, country) to reduce sparsity (see the Appendix for details).</p><p>Except BERT, all other embeddings are fetched from their corresponding learned embedding lookup tables. BERT takes subword units as input, which means that one word may correspond to multiple hidden states of BERT. In order to accurately use these hidden states to represent each word, we apply an average pooling function to the outputs of BERT. <ref type="figure" target="#fig_7">Figure 4</ref> illustrates the process of generating word-level embeddings from BERT.  Encoder The encoder is a multi-layer bidirectional RNN <ref type="bibr" target="#b55">(Schuster and Paliwal, 1997)</ref>:</p><formula xml:id="formula_6">h l i = [ − → f l (h l−1 i , h l i−1 ); ← − f l (h l−1 i , h l i+1 )],</formula><p>where − → f l and ← − f l are two LSTM cells (Hochreiter and Schmidhuber, 1997); h l i is the l-th layer encoder hidden state at the time step i; h 0 i is the encoder embedding layer output for word w i . Decoder Embedding Layer Similar to the encoder embedding layer, this layer outputs vector representations for AMR nodes. The difference is that each vector is the concatenation of embeddings of GloVe, POS tags and indices, and feature vectors from CharCNN.</p><p>POS tags of nodes are inferred at runtime: if a node is a copy from the input sentence, the POS tag of the corresponding word is used; if a node is a copy from the preceding nodes, the POS tag of its antecedent is used; if a node is a new node emitted from the vocabulary, an UNK tag is used.</p><p>We do not include BERT embeddings in this layer because AMR nodes, especially their order, are significantly different from natural language text (on which BERT was pre-trained). We tried to use "fixed" BERT in this layer, which did not lead to improvement. 2 Decoder At each step t, the decoder (an l-layer unidirectional LSTM) receives hidden state s l−1 t from the last layer and hidden state s l t−1 from the previous time step, and generates hidden state s l t :</p><formula xml:id="formula_7">s l t = f l (s l−1 t , s l t−1 ),</formula><p>where s 0 t is the concatenation (i.e., the inputfeeding approach, <ref type="bibr" target="#b37">Luong et al., 2015)</ref> of two vectors: the decoder embedding layer output for the previous node u t−1 (while training, u t−1 is the previous node of the reference node list; at test time it is the previous node emitted by the decoder), and the attentional vector s t−1 from the previous step (explained later in this section). s l 0 is the concatenation of last encoder hidden states from − → f l and ← − f l respectively. Source attention distribution a t src is calculated by additive attention <ref type="bibr" target="#b2">(Bahdanau et al., 2014)</ref>:</p><formula xml:id="formula_8">e t src = v src tanh(W src h l 1:n + U src s l t + b src ), a t src = softmax(e t src ),</formula><p>and it is then used to produce a weighted sum of encoder hidden states, i.e., the context vector c t . Attentional vector s t combines both source and target side information, and it is calculated by an MLP (shown in <ref type="figure">Figure 3</ref>):</p><formula xml:id="formula_9">s t = tanh(W c [c t ; s l t ] + b c )</formula><p>The attentional vector s t has 3 usages:</p><p>(1) it is fed through a linear layer and softmax to produce the vocabulary distribution:</p><formula xml:id="formula_10">P vocab = softmax(W vocab s t + b vocab )</formula><p>(2) it is used to calculate the target attention distribution a t tgt :</p><p>e t tgt = v tgt tanh(W tgt s 1:t−1 + U tgt s t + b tgt ), a t tgt = softmax(e t tgt ),</p><p>(3) it is used to calculate source-side copy probability p src , target-side copy probability p tgt , and generation probability p gen via a switch layer:</p><p>[p src , p tgt , p gen ] = softmax(W switch s t + b switch )</p><p>Note that p src + p tgt + p gen = 1. They act as a soft switch to choose between copying an existing node from the preceding nodes by sampling from the target attention distribution a t tgt , or emitting a new node in two ways: (1) generating a new node from the fixed vocabulary by sampling from P vocab , or (2) copying a word (as a new node) from the input sentence by sampling from the source attention distribution a t src . The final probability distribution P (node) (u t ) for node u t is defined as follows. If u t is a copy of existing nodes, then:</p><formula xml:id="formula_11">P (node) (u t ) = p tgt t−1 i:u i =ut a t tgt [i],</formula><p>otherwise:</p><formula xml:id="formula_12">P (node) (u t ) = p gen P vocab (u t ) + p src n i:w i =ut a t src [i],</formula><p>where a t [i] indexes the i-th element of a t . Note that a new node may have the same surface form as the existing node. We track their difference using indices. The index d t for node u t is assigned deterministically as below:</p><formula xml:id="formula_13">d t = t, if u t is a new node; d j ,</formula><p>if u t is a copy of its antecedent u j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Deep Biaffine Classifier</head><p>For the second stage (i.e., edge prediction), we employ a deep biaffine classifier, which was originally proposed for graph-based dependency parsing <ref type="bibr" target="#b17">(Dozat and Manning, 2016)</ref>, and recently has been applied to semantic parsing <ref type="bibr" target="#b48">(Peng et al., 2017a;</ref><ref type="bibr" target="#b18">Dozat and Manning, 2018)</ref>. As depicted in <ref type="figure" target="#fig_8">Figure 5</ref>, the major difference of our usage is that instead of re-encoding AMR nodes, we directly use decoder hidden states from the extended pointer-generator network as the input to deep biaffine classifier. We find two advantages of using decoder hidden states as input:</p><p>(1) through the input-feeding approach, decoder hidden states contain contextualized information from both the input sentence and the predicted nodes; (2) because decoder hidden states are used for both node prediction and edge prediction, we can jointly train the two modules in our model. Given decoder hidden states s 1 , ..., s m and a learnt vector representation s 0 of a dummy root, we follow <ref type="bibr" target="#b17">Dozat and Manning (2016)</ref>, factorizing edge prediction into two components: one that predicts whether or not a directed edge (u k , u t ) exists between two nodes u k and u t , and another that predicts the best label for each potential edge.</p><p>Edge and label scores are calculated as below: where MLP, Biaffine and Bilinear are defined as below:</p><formula xml:id="formula_14">s (edge-head) t = MLP (edge-head) (s t ) s (edge-dep) t = MLP (edge-dep) (s t ) s (label-head) t = MLP (label-head) (s t ) s (label-dep) t = MLP (label-dep) (s t ) score<label>(</label></formula><formula xml:id="formula_15">MLP(x) = ELU(W x + b) Biaffine(x 1 , x 2 ) = x 1 U x 2 + W [x 1 ; x 2 ] + b Bilinear(x 1 , x 2 ) = x 1 U x 2 + b</formula><p>Given a node u t , the probability of u k being the edge head of u t is defined as:</p><formula xml:id="formula_16">P (head) t (u k ) = exp(score (edge) k,t ) m j=1 exp(score (edge) j,t )</formula><p>The edge label probability for edge (u k , u t ) is defined as:</p><formula xml:id="formula_17">P (label) k,t (l) = exp(score (label) k,t [l]) l exp(score (label) k,t [l ])</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training</head><p>The training objective is to jointly minimize the loss of reference nodes and edges, which can be decomposed to the sum of the negative log likelihood at each time step t for (1) the reference node u t , (2) the reference edge head u k of node u t , and (3) the reference edge label l between u k and u t :</p><formula xml:id="formula_18">minimize − m t=1 [log P (node) (u t ) + log P (head) t (u k ) + log P (label) k,t (l) + λcovloss t ]</formula><p>covloss t is a coverage loss to penalize repetitive nodes: covloss t = i min(a t src [i], cov t [i]), where cov t is the sum of source attention distributions over all previous decoding time steps: cov t = t−1 t =0 a t src . See <ref type="bibr" target="#b56">See et al. (2017)</ref> for full details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Prediction</head><p>For node prediction, based on the final probability distribution P (node) (u t ) at each decoding time step, we implement both greedy search and beam search to sequentially decode a node list u and indices d. For edge prediction, given the predicted node list u, their indices d, and the edge scores S = {score (edge) i,j | 0 ≤ i, j ≤ m}, we apply the Chu-Liu-Edmonds algorithm with a simple adaption to find the maximum spanning tree (MST). As described in Algorithm 1, before calling the Chu-Liu-Edmonds algorithm, we first include a dummy root u 0 to ensure every node have a head, and then exclude edges whose source and destination nodes have the same indices, because these nodes will be merged into a single node to recover the standard AMR graph where self-loops are invalid. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>AMR parsing approaches can be categorized into alignment-based, transition-based, grammarbased, and attention-based approaches.</p><p>Alignment-based approaches were first explored by JAMR <ref type="bibr" target="#b22">(Flanigan et al., 2014)</ref>, a pipeline of concept and relation identification with a graphbased algorithm.  improved this by jointly learning concept and relation identification with an incremental model. Both approaches rely on features based on alignments. <ref type="bibr" target="#b38">Lyu and Titov (2018)</ref> treated alignments as latent variables in a joint probabilistic model, leading to a substantial reported improvement. Our approach re-quires no explicit alignments, but implicitly learns a source-side copy mechanism using attention.</p><p>Transition-based approaches began with <ref type="bibr" target="#b61">Wang et al. (2015</ref><ref type="bibr" target="#b59">Wang et al. ( , 2016</ref>, who incrementally transform dependency parses into AMRs using transitonbased models, which was followed by a line of research, such as <ref type="bibr" target="#b54">Puzikov et al. (2016);</ref><ref type="bibr" target="#b7">Brandt et al. (2016)</ref>; <ref type="bibr" target="#b24">Goodman et al. (2016)</ref>; <ref type="bibr" target="#b15">Damonte et al. (2017)</ref>; Ballesteros and Al-Onaizan (2017); <ref type="bibr" target="#b25">Groschwitz et al. (2018)</ref>. A pre-trained aligner, e.g. <ref type="bibr" target="#b52">Pourdamghani et al. (2014)</ref>; <ref type="bibr" target="#b36">Liu et al. (2018)</ref>, is needed for most parsers to generate training data (e.g., oracles for a transition-based parser). Our approach makes no significant use of external semantic resources, 3 and is aligner-free.</p><p>Grammar-based approaches are represented by <ref type="bibr" target="#b1">Artzi et al. (2015)</ref>; <ref type="bibr" target="#b49">Peng et al. (2015)</ref> who leveraged external semantic resources, and employed CCG-based or SHRG-based grammar induction approaches converting logical forms into AMRs. <ref type="bibr" target="#b53">Pust et al. (2015)</ref> recast AMR parsing as a machine translation problem, while also drawing features from external semantic resources.</p><p>Attention-based parsing with Seq2Seq-style models have been considered <ref type="bibr" target="#b5">(Barzdins and Gosko, 2016;</ref><ref type="bibr" target="#b50">Peng et al., 2017b)</ref>, but are limited by the relatively small amount of labeled AMR data. <ref type="bibr" target="#b33">Konstas et al. (2017)</ref> overcame this by making use of millions of unlabeled data through self-training, while van Noord and Bos (2017b) showed significant gains via a characterlevel Seq2Seq model and a large amount of silverstandard AMR training data. In contrast, our approach supported by extended pointer generator can be effectively trained on the limited amount of labeled AMR data, with no data augmentation.</p><p>(2018), and restore wiki links using the DBpedia Spotlight API <ref type="bibr" target="#b13">(Daiber et al., 2013)</ref> following <ref type="bibr" target="#b6">Bjerva et al. (2016);</ref><ref type="bibr" target="#b46">van Noord and Bos (2017b)</ref>. We add polarity attributes based on the rules observed from the training data. More details of preand post-processing are provided in the Appendix.  We conduct experiments on two AMR general releases (available to all LDC subscribers): AMR 2.0 (LDC2017T10) and AMR 1.0 (LDC2014T12). Our model is trained using ADAM (Kingma and Ba, 2014) for up to 120 epochs, with early stopping based on the development set. Full model training takes about 19 hours on AMR 2.0 and 7 hours on AMR 1.0, using two GeForce GTX TI-TAN X GPUs. At training, we have to fix BERT parameters due to the limited GPU memory. We leave fine-tuning BERT for future work. <ref type="table" target="#tab_2">Table 1</ref> lists the hyper-parameters used in our full model. Both encoder and decoder embedding layers have GloVe and POS tag embeddings as well as CharCNN, but their parameters are not tied. We apply dropout (dropout rate = 0.33) to the outputs of each module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Results</head><p>Corpus Parser F1(%) AMR 2.0 <ref type="bibr" target="#b8">Buys and Blunsom (2017)</ref> 61.9 van Noord and Bos (2017b) 71.0 * <ref type="bibr" target="#b25">Groschwitz et al. (2018)</ref> 71.0±0.5 <ref type="bibr" target="#b38">Lyu and Titov (2018)</ref> 74.4±0.2 <ref type="bibr" target="#b44">Naseem et al. (2019)</ref> 75.5</p><p>Ours 76.3±0.1 AMR 1.0 <ref type="bibr" target="#b21">Flanigan et al. (2016)</ref> 66.0 <ref type="bibr" target="#b53">Pust et al. (2015)</ref> 67.1 <ref type="bibr" target="#b60">Wang and Xue (2017)</ref> 68.1 <ref type="bibr" target="#b28">Guo and Lu (2018)</ref> 68.3±0.4</p><p>Ours 70.2±0.1 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head><p>We compare our approach against the previous best approaches and several recent competitors. <ref type="table" target="#tab_3">Table 2</ref> summarizes their SMATCH scores  on the test sets of two AMR general releases. On AMR 2.0, we outperform the latest push from <ref type="bibr" target="#b44">Naseem et al. (2019)</ref> by 0.8% F1, and significantly improves <ref type="bibr" target="#b38">Lyu and Titov (2018)</ref>'s results by 1.9% F1. Compared to the previous best attention-based approach (van Noord and Bos, 2017b), our approach shows a substantial gain of 5.3% F1, with no usage of any silver-standard training data. On AMR 1.0 where the traininng instances are only around 10k, we improve the best reported results by 1.9% F1. Fine-grained Results In <ref type="table" target="#tab_5">Table 3</ref>, we assess the quality of each subtask using the AMR-evaluation tools <ref type="bibr" target="#b15">(Damonte et al., 2017)</ref>. We see a notable increase on reentrancies, which we attribute to target-side copy (based on our ablation studies in the next section). Significant increases are also  shown on wikification and negation, indicating the benefits of using DBpedia Spotlight API and negation detection rules in post-processing. On all other subtasks except named entities, our approach achieves competitive results to the previous best approaches <ref type="bibr" target="#b38">(Lyu and Titov, 2018;</ref><ref type="bibr" target="#b44">Naseem et al., 2019)</ref>, and outperforms the previous best attention-based approach (van Noord and Bos, 2017b). The difference of scores on named entities is mainly caused by anonymization methods used in preprocessing, which suggests a potential improvement by adapting the anonymization method presented in <ref type="bibr" target="#b38">Lyu and Titov (2018)</ref> to our approach.  In the last row, we only evaluate model performance at the edge prediction stage by forcing our model to decode the reference nodes at the node prediction stage. The results mean if our model could make perfect prediction at the node prediction stage, the final SMATCH score will be substantially high, which identifies node prediction as the key to future improvement of our model.  There are three sources for node prediction: vocabulary generation, source-side copy, or targetside copy. Let all reference nodes from source z be N (z) ref , and all system predicted nodes from z be N (z) sys . we compute frequency, precision and recall of nodes from source z as below:</p><formula xml:id="formula_19">frequency (z) = |N (z) ref | z |N (z) ref | precision (z) = |N (z) ref ∩ N (z) sys | |N (z) sys | recall (z) = |N (z) ref ∩ N (z) sys | |N (z)</formula><p>ref | <ref type="figure" target="#fig_10">Figure 6</ref> shows the frequency of nodes from difference sources, and their corresponding precision and recall based on our model prediction. Among all reference nodes, 43.8% are from vocabulary generation, 47.6% from source-side copy, and only 8.6% from target-side copy. On one hand, the highest frequency of source-side copy helps address sparsity and results in the highest precision and recall. On the other hand, we see space for improvement, especially on the relatively low recall of target-side copy, which is probably due to its low frequency. Node Linearization As decribed in Section 3, we create the reference node list by a preorder traversal over the gold AMR tree. As for the children of each node, we sort them in alphanumerical order. This linearization strategy has two advantages: (1) pre-order traversal guarantees that a head node (predicate) always comes in front of its children (arguments); (2) alphanumerical sort orders according to role ID (i.e., ARG0&gt;ARG1&gt;...&gt;ARGn), following intuition from research in Thematic Hierarchies <ref type="bibr" target="#b20">(Fillmore, 1968;</ref><ref type="bibr" target="#b34">Levin and Hovav, 2005</ref>  <ref type="table">Table 5</ref>: SMATCH scores of full models trained and tested based on different node linearization strategies.</p><p>In <ref type="table">Table 5</ref>, we report SMATCH scores of full models trained and tested on data generated via our linearization strategy (Pre-order + Alphanum), as compared to two obvious alternates: the first alternate still runs a pre-order traversal, but it sorts the children of each node based on the their alignments to input words; the second one linearizes nodes purely based alignments. Alignments are created using the tool by <ref type="bibr" target="#b52">Pourdamghani et al. (2014)</ref>. Clearly, our linearization strategy leads to much better results than the two alternates. We also tried other traversal strategies such as combining in-order traversal with alphanumerical sorting or alignment-based sorting, but did not get scores even comparable to the two alternates. 5 Average Pooling vs. Max Pooling In <ref type="figure" target="#fig_7">Figure 4</ref>, we apply average pooling to the outputs (last-layer hidden states) of BERT in order to generate wordlevel embeddings for the input sentence. <ref type="table" target="#tab_10">Table 6</ref> shows scores of models using different pooling functions. Average pooling performs slightly better than max pooling.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We proposed an attention-based model for AMR parsing where we introduced a series of novel components into a transductive setting that extend beyond what a typical NMT system would do on this task. Our model achieves the best performance on two AMR corpora. For future work, we would like to extend our model to other semantic parsing tasks <ref type="bibr" target="#b47">(Oepen et al., 2014;</ref><ref type="bibr" target="#b0">Abend and Rappoport, 2013)</ref>. We are also interested in semantic parsing in cross-lingual settings <ref type="bibr" target="#b63">(Zhang et al., 2018;</ref><ref type="bibr" target="#b14">Damonte and Cohen, 2018)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Two views of reentrancy in AMR for an example sentence "The victim could help himself." (a) A standard AMR graph. (b) An AMR tree with node indices as an extra layer of annotation, where the corresponding graph can be recovered by merging nodes of the same index and unioning their incoming edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " f 9 9 M 4 0 Y q F / h o Q B M Q Q + I H U i / 3 v g 8 = " &gt; A A A B 9 X i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 s W y 2 k 3 b p Z h N 2 J 2 o J / R 9 e P C j i 1 f / i z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V H J o 8 l r H u B M y A F A q a K F B C J 9 H A o k B C O x h f z / z 2 A 2 g j Y n W H k w T 8 i A 2 V C A V n a K X 7 p N 9 D e E I d Z U N Q 0 3 6 5 4 l b d O e g q 8 X J S I T k a / f J X b x D z N A K F X D J j u p 6 b o J 8 x j Y J L m J Z 6 q Y G E 8 T E b Q t d S x S I w f j a / e k r P r D K g Y a x t K a R z 9 f d E x i J j J l F g O y O G I 7 P s z c T / v G 6 K Y c 3 P h E p S B M U X i 8 J U U o z p L A I 6 E B o 4 y o k l j G t h b 6 V 8 x D T j a I M q 2 R C 8 5 Z d X S e u i 6 r l V 7 / a y U q / l c R T J C T k l 5 8 Q j V 6 R O b k i D N A k n m j y T V / L m P D o v z r v z s W g t O P n M M f k D 5 / M H T e + S / Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f 9 9 M 4 0 Y q F / h o Q B M Q Q + I HU i / 3 v g 8 = " &gt; A A A B 9 X i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 s W y 2 k 3 b p Z h N 2 J 2 o J / R 9 e P C j i 1 f / i z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V H J o 8 l r H u B M y A F A q a K F B C J 9 H A o k B C O x h f z / z 2 A 2 g j Y n W H k w T 8 i A 2 V C A V n a K X 7 p N 9 D e E I d Z U N Q 0 3 6 5 4 l b d O e g q 8 X J S I T k a / f J X b x D z N A K F X D J j u p 6 b o J 8 x j Y J L m J Z 6 q Y G E 8 T E b Q t d S x S I w f j a / e k r P r D K g Y a x t K a R z 9 f d E x i J j J l F g O y O G I 7 P s z c T / v G 6 K Y c 3 P h E p S B M U X i 8 J U U o z p L A I 6 E Bo 4 y o k l j G t h b 6 V 8 x D T j a I M q 2 R C 8 5 Z d X S e u i 6 r l V 7 / a y U q / l c R T J C T k l 5 8 Q j V 6 R O b k i D N A k n m j y T V / L m P D o v z r v z s W g t O P n M M f k D 5 / M H T e + S / Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f 9 9 M 4 0 Y q F / h o Q B M Q Q + I H U i / 3 v g 8 = " &gt; A A A B 9 X i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 s W y 2 k 3 b p Z h N 2 J 2 o J / R 9 e P C j i 1 f / i z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V H J o 8 l r H u B M y A F A q a K F B C J 9 H A o k B C O x h f z / z 2 A 2 g j Y n W H k w T 8 i A 2 V C A V n a K X 7 p N 9 D e E I d Z U N Q 0 3 6 5 4 l b d O e g q 8 X J S I T k a / f J X b x D z N A K F X D J j u p 6 b o J 8 x j Y J L m J Z 6 q Y G E 8 T E b Q t d S x S I w f j a / e k r P r D K g Y a x t K a R z 9 f d E x i J j J l F g O y O G I 7 P s z c T / v G 6 K Y c 3 P h E p S B M U X i 8 J U U o z p L A I 6 E B o 4 y o k l j G t h b 6 V 8 x D T j a I M q 2 R C 8 5 Z d X S e u i 6 r l V 7 / a y U q / l c R T J C T k l 5 8 Q j V 6 R O b k i D N A k n m j y T V / L m P D o v z r v z s W g t O P n M M f k D 5 / M H T e + S / Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f 9 9 M 4 0 Y q F / h o Q B M Q Q + I H U i / 3 v g 8 = " &gt; A A A B 9 X i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 s W y 2 k 3 b p Z h N 2 J 2 o J / R 9 e P C j i 1 f / i z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V H J o 8 l r H u B M y A F A q a K F B C J 9 H A o k B C O x h f z / z 2 A 2 g j Y n W H k w T 8 i A 2 V C A V n a K X 7 p N 9 D e E I d Z U N Q 0 3 6 5 4 l b d O e g q 8 X J S I T k a / f J X b x D z N A K F X D J j u p 6 b o J 8 x j Y J L m J Z 6 q Y G E 8 T E b Q t d S x S I w f j a / e k r P r D K g Y a x t K a R z 9 f d E x i J j J l F g O y O G I 7 P s z c T / v G 6 K Y c 3 P h E p S B M U X i 8 J U U o z p L A I 6 E B o 4 y o k l j G t h b 6 V 8 x D T j a I M q 2 R C 8 5 Z d X S e u i 6 r l V 7 / a y U q / l c R T J C T k l 5 8 Q j V 6 R O b k i D N A k n m j y T V / L m P D o v z r v z s W g t O P n M M f k D 5 / M H T e + S / Q = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " R q f 2 5 5 u a S R 5 j 8 l n v l 9 a Z a 1 j G X z I = " &gt; A A A B 9 X i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 s W y 2 k 3 b p b h J 2 J 2 o J / R 9 e P C j i 1 f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>e 8 s u r p H V R 9 d y q d 3 t Z q d f y O I r k h J y S c + K R K 1 I n N 6 R B m o Q T T Z 7 J K 3 l z H p 0 X 5 9 3 5 W L Q W n H z m m P y B 8 / k D b f S T E g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R q f 2 5 5 u a S R 5 j 8 l n v l 9 a Z a 1 j G X z I = " &gt; A A A B 9 X i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 s W y 2 k 3 b p b h J 2 J 2 o J / R 9 e P C j i 1 f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>e 8 s u r p H V R 9 d y q d 3 t Z q d f y O I r k h J y S c + K R K 1 I n N 6 R B m o Q T T Z 7 J K 3 l z H p 0 X 5 9 3 5 W L Q W n H z m m P y B 8 / k D b f S T E g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R q f 2 5 5 u a S R 5 j 8 l n v l 9 a Z a 1 j G X z I = " &gt; A A A B 9 X i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 s W y 2 k 3 b p b h J 2 J 2 o J / R 9 e P C j i 1 f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>e 8 s u r p H V R 9 d y q d 3 t Z q d f y O I r k h J y S c + K R K 1 I n N 6 R B m o Q T T Z 7 J K 3 l z H p 0 X 5 9 3 5 W L Q W n H z m m P y B 8 / k D b f S T E g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R q f 2 5 5 u a S R 5 j 8 l n v l 9 a Z a 1 j G X z I = " &gt; A A A B 9 X i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 l E s M e C F 4 8 V 7 A e 0 s W y 2 k 3 b p b h J 2 J 2 o J / R 9 e P C j i 1 f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>e 8 s u r p H V R 9 d y q d 3 t Z q d f y O I r k h J y S c + K R K 1 I n N 6 R B m o Q T T Z 7 J K 3 l z H p 0 X 5 9 3 5 W L Q W n H z m m P y B 8 / k D b f S T E g = = &lt; / l a t e x i t &gt; p src &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L B J I G m i R a 8 y 7 f N w y I g z x Q 2 6 k q o Y = " &gt; A A A B 9 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S Q i 2 G P B i 8 c K 9 g P a W D b b T b t 0 d x N 2 J 2 o J / R 9 e P C j i 1 f / i z X / j t s 1 B W x 8 M P N 6 b Y W Z e m A h u 0 P O + n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V l D V p L G L d C Y l h g i v W R I 6 C d R L N i A w F a 4 f j 6 5 n f f m D a 8 F j d 4 S R h g S R D x S N O C V r p P u n 3 k D 2 h l p n R d N o v V 7 y q N 4 e 7 S v y c V C B H o 1 / + 6 g 1 i m k q m k A p i T N f 3 E g w y o p F T w a a l X m p Y Q u i Y D F n X U k U k M 0 E 2 v 3 r q n l l l 4 E a x t q X Q n a u / J z I i j Z n I 0 H Z K g i O z 7 M 3 E / 7 x u i l E t y L h K U m S K L h Z F q X A x d m c R u A O u G U U x s Y R Q z e 2 t L h 0 R T S j a o E o 2 B H / 5 5 V X S u q j 6 X t W / v a z U a 3 k c R T i B U z g H H 6 6 g D j f Q g C Z Q 0 P A M r / D m P D o v z r v z s W g t O P n M M f y B 8 / k D Y 1 q T C w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L B J I G m i R a 8 y 7 f N w y I g z x Q 2 6 k q o Y = " &gt; A A A B 9 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S Q i 2 G P B i 8 c K 9 g P a W D b b T b t 0 d x N 2 J 2 o J / R 9 e P C j i 1 f / i z X / j t s 1 B W x 8 M P N 6 b Y W Z e m A h u 0 P O + n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V l D V p L G L d C Y l h g i v W R I 6 C d R L N i A w F a 4 f j 6 5 n f f m D a 8 F j d 4 S R h g S R D x S N O C V r p P u n 3 k D 2 h l p n R d N o v V 7 y q N 4 e 7 S v y c V C B H o 1 / + 6 g 1 i m k q m k A p i T N f 3 E g w y o p F T w a a l X m p Y Q u i Y D F n X U k U k M 0 E 2 v 3 r q n l l l 4 E a x t q X Q n a u / J z I i j Z n I 0 H Z K g i O z 7 M 3 E / 7 x u i l E t y L h K U m S K L h Z F q X A x d m c R u A O u G U U x s Y R Q z e 2 t L h 0 R T S j a o E o 2 B H / 5 5 V X S u q j 6 X t W / v a z U a 3 k c R T i B U z g H H 6 6 g D j f Q g C Z Q 0 P A M r / D m P D o v z r v z s W g t O P n M M f y B 8 / k D Y 1 q T C w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L B J I G m i R a 8 y 7 f N w y I g z x Q 2 6 k q o Y = " &gt; A A A B 9 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S Q i 2 G P B i 8 c K 9 g P a W D b b T b t 0 d x N 2 J 2 o J / R 9 e P C j i 1 f / i z X / j t s 1 B W x 8 M P N 6 b Y W Z e m A h u 0 P O + n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V l D V p L G L d C Y l h g i v W R I 6 C d R L N i A w F a 4 f j 6 5 n f f m D a 8 F j d 4 S R h g S R D x S N O C V r p P u n 3 k D 2 h l p n R d N o v V 7 y q N 4 e 7 S v y c V C B H o 1 / + 6 g 1 i m k q m k A p i T N f 3 E g w y o p F T w a a l X m p Y Q u i Y D F n X U k U k M 0 E 2 v 3 r q n l l l 4 E a x t q X Q n a u / J z I i j Z n I 0 H Z K g i O z 7 M 3 E / 7 x u i l E t y L h K U m S K L h Z F q X A x d m c R u A O u G U U x s Y R Q z e 2 t L h 0 R T S j a o E o 2 B H / 5 5 V X S u q j 6 X t W / v a z U a 3 k c R T i B U z g H H 6 6 g D j f Q g C Z Q 0 P A M r / D m P D o v z r v z s W g t O P n M M f y B 8 / k D Y 1 q T C w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L B J I G m i R a 8 y 7 f N w y I g z x Q 2 6 k q o Y = " &gt; A A A B 9 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S Q i 2 G P B i 8 c K 9 g P a W D b b T b t 0 d x N 2 J 2 o J / R 9 e P C j i 1 f / i z X / j t s 1 B W x 8 M P N 6 b Y W Z e m A h u 0 P O + n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V l D V p L G L d C Y l h g i v W R I 6 C d R L N i A w F a 4 f j 6 5 n f f m D a 8 F j d 4 S R h g S R D x S N O C V r p P u n 3 k D 2 h l p n R d N o v V 7 y q N 4 e 7 S v y c V C B H o 1 / + 6 g 1 i m k q m k A p i T N f 3 E g w y o p F T w a a l X m p Y Q u i Y D F n X U k U k M 0 E 2 v 3 r q n l l l 4 E a x t q X Q n a u / J z I i j Z n I 0 H Z K g i O z 7 M 3 E / 7 x u i l E t y L h K U m S K L h Z F q X A x d m c R u A O u G U U x s Y R Q z e 2 t L h 0 R T S j a o E o 2 B H / 5 5 V X S u q j 6 X t W / v a z U a 3 k c R T i B U z g H H 6 6 g D j f Q g C Z Q 0 P A M r / D m P D o v z r v z s W g t O P n M M f y B 8 / k D Y 1 q T C w = = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Word-level embeddings from BERT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Deep biaffine classifier for edge prediction. Edge label prediction is not depicted in thefigure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Algorithm 1 :</head><label>1</label><figDesc>Chu-Liu-Edmonds algo. w/ Adaption Input : Nodes u = u1, ..., um , Indices d = d1, ...dm , Edge scores S = {score (edge) i,j | 0 ≤ i, j ≤ m} Output: A maximum spanning tree. // Include the dummy root u0. V ← {u0} ∪ u; d0 ← 0; // Exclude invalid edges. // di is the node index for node ui. E ← {(ui, uj) | 0 ≤ i, j ≤ m; di = dj}; // Chu-Liu-Edmonds algorithm return MST(V, E, S, u0);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Frequency, precision and recall of nodes from different sources, based on the AMR 2.0 test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The victim could help himself.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Node Prediction</cell><cell></cell></row><row><cell>possible</cell><cell>1</cell><cell>help</cell><cell>2</cell><cell>victim</cell><cell>3</cell><cell>victim</cell><cell>3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Edge Prediction</cell><cell></cell></row><row><cell cols="2">ARG1</cell><cell></cell><cell></cell><cell>ARG1 ARG0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>possible</cell><cell>1</cell><cell>help</cell><cell>2</cell><cell>victim</cell><cell>3</cell><cell>victim</cell><cell>3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Hyper-parameter settings</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: SMATCH scores on the test sets of AMR 2.0</cell></row><row><cell>and 1.0. Standard deviation is computed over 3 runs</cell></row><row><cell>with different random seeds.</cell></row></table><note>* indicates the previous best score from attention-based models.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Fine-grained F1 scores on the AMR 2.0 test</cell></row><row><cell>set. vN'17 is van Noord and Bos (2017b); L'18 is Lyu</cell></row><row><cell>and Titov (2018); N'19 is Naseem et al. (2019).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies on components of our model. (Scores are sorted by the delta from the full model.)Ablation Study We consider the contributions of several model components inTable 4. The largest performance drop is from removing source-side copy, 4 showing its efficiency at reducing sparsity from open-class vocabulary entries. Removing target-side copy also leads to a large drop. Specifically, the subtask score of reentrancies drops down to 38.4% when target-side copy is disabled. Coverage loss is useful with regard to discouraging unnecessary repetitive nodes.</figDesc><table><row><cell>In addition, our</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>SMATCH scores based different pooling functions. Standard deviation is over 3 runs on the test data.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Limited by the GPU memory, we do not fine-tune BERT on this task and leave it for future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">AMR Pre-and Post-processing Anonymization is often used in AMR preprocessing to reduce sparsity<ref type="bibr" target="#b62">(Werling et al., 2015;</ref><ref type="bibr" target="#b50">Peng et al., 2017b;</ref> Guo and Lu, 2018, inter alia). Similar to<ref type="bibr" target="#b33">Konstas et al. (2017)</ref>, we anonymize sub-graphs of named entities and other entities. Like<ref type="bibr" target="#b38">Lyu and Titov (2018)</ref>, we remove senses, and use Stanford CoreNLP to lemmatize input sentences and add POS tags.In post-processing, we assign the most frequent sense for nodes (-01, if unseen) likeLyu and Titov   3  We only use POS tags in the core parsing task. In postprocessing, we use an entity linker as a common move for wikification like van Noord and Bos (2017b).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">All other hyper-parameter settings remain the same.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5"><ref type="bibr" target="#b46">van Noord and Bos (2017b)</ref> also investigated linearization order, and found that alignment-based ordering yielded the best results under their setup where AMR parsing is treated as a sequence-to-sequence learning problem.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/goodmami/penman/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their valuable feedback. This work was supported in part by the JHU Human Language Technology Center of Excellence (HLTCOE), and DARPA LORELEI and AIDA. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA or the U.S. Government.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head><p>A.1 AMR Pre-and Post-processing Firstly, we to run Standford CoreNLP like <ref type="bibr" target="#b38">Lyu and Titov (2018)</ref>, lemmatizing input sentences and adding POS tags to each token. Secondly, we remove senses, wiki links and polarity attributes in AMR. Thirdly, we anonymize sub-graphs of named entities and * -entity in a way similar to <ref type="bibr" target="#b33">Konstas et al. (2017)</ref>. <ref type="figure">Figure 7</ref> shows an example before and after preprocessing. Subgraphs of named entities are headed by one of AMR's fine-grained entity types (e.g., highway, country region in <ref type="figure">Figure 7</ref>) that contain a :name role. Sub-graphs of other entities are headed by their corresponding entity type name (e.g., date-entity in <ref type="figure">Figure 7</ref>). We replace these sub-graphs with a token of a special pattern "TYPE i" (e.g. HIGHWAY 0, DATE 0 in <ref type="figure">Figure 7)</ref>, where "TYPE" indicates the AMR entity type of the corresponding sub-graph, and "i" indicates that it is the i-th occurrence of that type. On the training set, we use simple rules to find mappings between anonymized sub-graphs and spans of text, and then replace mapped text with the anonymized token we inserted into the AMR graph. Additionally, we build a mapping of Standford CoreNLP NER tags to AMR's fine-grained types based on the training set, which will be used in prediction. At test time, we normalize sentences to match our anonymized training data. For any entity span identified by Stanford CoreNLP, we replace it with a AMR entity type based on the mapping built during training. If no entry is found in the mapping, we replace entity spans with the coarse-grained NER tags from Stanford CoreNLP, which are also entity types in AMR.</p><p>In post-processing, we deterministically generate AMR sub-graphs for anonymizations using the corresponding text span. We assign the most frequent sense for nodes (-01, if unseen) like <ref type="bibr" target="#b38">Lyu and Titov (2018)</ref>. We add wiki links to named entities using the DBpedia Spotlight API <ref type="bibr" target="#b13">(Daiber et al., 2013)</ref> following <ref type="bibr" target="#b6">Bjerva et al. (2016)</ref>; van Noord and Bos (2017b) with the confidence threshod at 0.5. We add polarity attributes based on Algorithm 2 where the four functions isNegation, modifiedWord, mappedNode, and addPolarity consists of simple rules observed from the training set. We use the PENMANCodec 6 to encode and decode both intermediate and final AMRs. Anonymized Sentence: HIGHWAY_0 , the circumferential highway running around the south -western quadrant of the COUNTRY_REGION_0 , opened in late DATE_0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Before preprocessing</head><p>(o / open-01 :ARG1 (h / highway :wiki "Virginia_State_Route_288" :name (r / name :op1 "Route" :op2 288) :ARG1-of (r3 / run-04 :direction (a / around :op1 (q / quadrant :part-of (c / country-region :wiki -:name (r2 / name :op1 "Richmond" :op2 "New" :op3 "Urban" :op4 "Region")) :mod (s / southwest)))) :mod (c2 / circumference)) :time (l / late :op1 (d / date-entity :year 2004)))</p><p>After preprocessing</p><p>(o / open :ARG1 (h / HIGHWAY_0 :ARG1-of (r3 / run :direction (a / around :op1 (q / quadrant :part-of (c / COUNTRY_REGION_0) :mod (s / southwest)))) :mod (c2 / circumference)) :time (l / late :op1 (d / DATE_0))) <ref type="figure">Figure 7</ref>: An example AMR and the corresponding sentence before and after preprocessing. Senses are removed. The first named entity is replaced by "HIGHWAY 0"; the second named entity is replaced by "COUN-TRY REGION 0"; the first date entity replaced by "DATE 0".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Side-by-Side Examples</head><p>In the next page, we provide examples from the test set, with side-by-side comparisons between the full model prediction and the model prediction after ablation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence:</head><p>Smoke and clouds chase the flying waves Lemmas: ["smoke", "and", "cloud", "chase", "the", "fly", "wave"] Full Model</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Universal conceptual cognitive annotation (ucca)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="228" to="238" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Broad-coverage ccg semantic parsing with amr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1198</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1699" to="1710" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">AMR parsing using stack-LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1130</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1269" to="1275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Riga at semeval-2016 task 8: Impact of smatch extensions and character-level neural translation on amr parsing accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guntis</forename><surname>Barzdins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didzis</forename><surname>Gosko</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S16-1176</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1143" to="1147" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The meaning factory at semeval-2016 task 8: Producing amrs with boxer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Bjerva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hessel</forename><surname>Haagsma</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S16-1182</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1179" to="1184" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Icl-hd at semeval-2016 task 8: Meaning representation parsing -augmenting amr parsing with a preposition semantic role labeling neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lauritz</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Versley</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S16-1179</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1160" to="1166" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Oxford at semeval-2017 task 9: Neural amr parsing with pointeraugmented attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S17-2157</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</title>
		<meeting>the 11th International Workshop on Semantic Evaluation (SemEval-2017)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="914" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Smatch: an evaluation metric for semantic feature structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="748" to="752" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Parsing graphs with hyperedge replacement grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bevan</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="924" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the shortest arborescence of a directed graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Sinica</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What you can cram into a single $&amp;!#* vector: Probing sentence embeddings for linguistic properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2126" to="2136" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving efficiency and accuracy in multilingual entity extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Daiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hokamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Semantic Systems (I-Semantics)</title>
		<meeting>the 9th International Conference on Semantic Systems (I-Semantics)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Crosslingual abstract meaning representation parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Damonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1104</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1146" to="1155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An incremental parser for abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Damonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="536" to="546" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01734</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simpler but more accurate semantic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="484" to="490" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<title level="m">Optimum branchings. Mathematics and the Decision Sciences, Part</title>
		<imprint>
			<date type="published" when="1968" />
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The case for case</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
			<pubPlace>Holt, Rinehart &amp; Winston, New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cmu at semeval-2016 task 8: Graph-based amr parsing with infinite ramp loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S16-1186</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1202" to="1206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A discriminative graph-based parser for the abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1134</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1426" to="1436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Abstract meaning representation parsing using lstm recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Foland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1043</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="463" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ucl+sheffield at semeval-2016 task 8: Imitation learning for amr parsing with an alphabound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S16-1180</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1167" to="1172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Amr dependency parsing with a typed semantic algebra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Groschwitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Lindemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meaghan</forename><surname>Fowlie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1831" to="1841" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1154</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointing the unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1014</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Better transitionbased amr parsing with a refined search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1712" to="1722" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI&apos;16</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence, AAAI&apos;16</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simple and accurate dependency parsing using bidirectional lstm feature representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="313" to="327" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural amr: Sequence-to-sequence models for parsing and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1014</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Argument realization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beth</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malka</forename><forename type="middle">Rappaport</forename><surname>Hovav</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Assessing the ability of LSTMs to learn syntax-sensitive dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="521" to="535" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An AMR aligner tuned by transition-based parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2422" to="2430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1166</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Amr parsing as graph prediction with latent alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunchuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="397" to="407" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mc-Closky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Online large-margin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<title level="m">Pointer sentinel mixture models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Language as a latent variable: Discrete generative models for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1031</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="319" to="328" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-tosequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K16-1028</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Rewarding smatch: Transition-based amr parsing with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13370</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dealing with co-reference in neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Semantic Deep Learning (SemDeep-2)</title>
		<meeting>the 2nd Workshop on Semantic Deep Learning (SemDeep-2)<address><addrLine>Montpellier, France</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="41" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Neural semantic parsing by character-based translation: Experiments with abstract meaning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics in the Netherlands Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="93" to="108" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">SemEval 2014 task 8: Broad-coverage semantic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Oepen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Kuhlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Flickinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelina</forename><surname>Ivanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/S14-2008</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="63" to="72" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep multitask learning for semantic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1186</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2037" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A synchronous hyperedge replacement grammar based approach for AMR parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K15-1004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Nineteenth Conference on Computational Natural Language Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="32" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Addressing the data sparsity issue in neural amr parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="366" to="375" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Aligning english strings with abstract meaning representation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Pourdamghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1048</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="425" to="429" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Parsing english into abstract meaning representation using syntaxbased machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1136</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="1143" to="1154" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">M2l at semeval-2016 task 8: Amr parsing with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgeniy</forename><surname>Puzikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S16-1178</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1154" to="1159" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1099</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">What do you learn from context? probing for sentence structure in contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Camr at semeval-2016 task 8: An extended transition-based amr parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S16-1181</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1173" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Getting the most out of amr parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1129</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1257" to="1268" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A transition-based algorithm for amr parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/N15-1040</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="366" to="375" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Robust subgraph generation improves abstract meaning representation parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keenon</forename><surname>Werling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1095</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="982" to="991" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Cross-lingual decompositional semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1664" to="1675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Amr parsing with an incremental joint model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">U</forename><surname>Weiguang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhui</forename><surname>Gu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1065</idno>
		<idno>vv1 / chase-01 :ARG0 (vv2 / and :op1 (vv3 / smoke) :op2 (vv4 / cloud-01</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="680" to="689" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Without source-side copy, the prediction becomes totally different and inaccurate in this example. Sentence: Now we already have no cohesion! China needs to start a war! Full Model</title>
		<idno>vv6 / need-01 :ARG0 (vv7 / country :name (vv8 / name :op1 &quot;China&quot;) :wiki &quot;China&quot;) :ARG1 (vv9 / start-01 :ARG0</idno>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note>Full model prediction vs. no source-side copy prediction. vv4 / cohere-01) :polarity -:time (vv5 / already. vv7 :ARG1 (vv11 / war</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">China&quot;). The full model correctly copies the first node (&quot;vv7 / country&quot;) as ARG0 of &quot;start-01&quot;. Without target-side copy, the model has to generate a new node with a different index, i.e., &quot;vv10 / country&quot;. Sentence: The solemn and magnificent posture represents a sacred expectation for peace</title>
		<idno>Model (vv1 / represent-01 :ARG0 (vv2 / posture-01 :mod (vv3 / magnificent) :mod (vv4 / solemn)) :ARG1 (vv5 / expect-01 :ARG1</idno>
	</analytic>
	<monogr>
		<title level="m">Full model prediction vs. no target-side copy prediction. Nodes in blue denote the same concept (i.e., the country</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note>vv6 / peace) :mod (vv7 / sacred</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<idno>vv1 / represent-01 :ARG0 (vv2 / posture-01 :mod (vv3 / magnificent) :mod (vv4 / magnificent</idno>
		<title level="m">No Coverage Loss</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Without coverage loss, the model generates a repetitive modifier &quot;magnificent&quot;. Sentence: Do it gradually if it&apos;s not something you&apos;re particularly comfortable with. Full Model (vv1 / have</title>
		<idno>condition-91 :ARG1 (vv2 / do-02 :ARG0</idno>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
	<note>The full model correctly predicts the second modifier &quot;solemn. vv3 / you) :ARG1 (vv4 / it) :manner (vv5 / gradual</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>No</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Embeddings</surname></persName>
		</author>
		<idno>vv1 / have-concession-91 :ARG1 (vv2 / do-02 :ARG0 (vv3 / it) :ARG1 (vv4 / something :ARG0-of (vv5 / comfortable-02 :ARG0 vv3 :mod (vv7 / particular</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Full model prediction vs. no BERT embeddings prediction</title>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
