<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-Tuning DARTS for Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Suhaib Tanveer</surname></persName>
							<email>suhaibtanveer@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering Korea Advanced Institute of Science and Technology (KAIST) Daejeon</orgName>
								<address>
									<postCode>34141</postCode>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Umar</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centre of Integrated Smart Sensors (CISS)</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<postCode>34141</postCode>
									<settlement>Daejeon</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Khan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centre of Integrated Smart Sensors (CISS)</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<postCode>34141</postCode>
									<settlement>Daejeon</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Min</forename><surname>Kyung</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Electrical Engineering Korea Advanced Institute of Science and Technology (KAIST) Daejeon</orgName>
								<address>
									<postCode>34141</postCode>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fine-Tuning DARTS for Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural Architecture Search (NAS) has gained attraction due to superior classification performance. Differential Architecture Search (DARTS) is a computationally light method. To limit computational resources DARTS makes numerous approximations. These approximations result in inferior performance. We propose to fine-tune DARTS using fixed operations as they are independent of these approximations. Our method offers a good trade-off between the number of parameters and classification accuracy. Our approach improves the top-1 accuracy on Fashion-MNIST, CompCars, and MIO-TCD datasets by 0.56%, 0.50%, and 0.39%, respectively compared to the state-of-theart approaches. Our approach performs better than DARTS, improving the accuracy by 0.28%, 1.64%, 0.34%, 4.5%, and 3.27% compared to DARTS, on CIFAR-10, CIFAR-100, Fashion-MNIST, CompCars, and MIO-TCD datasets, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Image classification is a fundamental computer vision task. Image classification is employed in a number of different industries. Some popular industries that employ image classification are automobile, retail, security and healthcare industry.</p><p>Several approaches have been employed for image classification. Since the advent of deep learning, handcrafted deep neural networks are being used to achieve the state-of-the-art accuracy on the image classification task. Discovering state-ofthe-art neural network architectures requires significant effort of human experts. Therefore, Neural Architecture Search (NAS), has been widely adopted, allowing automatic design of neural networks. NAS methods have achieved astonishing results on the image classification task, surpassing the manual methods on many popular datasets.</p><p>Various approaches have been employed for NAS. Non-Stochastic NAS methods are able to find good architectures but at a very high search cost. These methods use reinforcement learning <ref type="bibr" target="#b0">[1]</ref>, evolutionary algorithms <ref type="bibr" target="#b1">[2]</ref> and sequential modelbased optimization <ref type="bibr" target="#b2">[3]</ref> techniques. On the other hand, stochastic methods are able to give competitive performance in a significantly shorter time.</p><p>DARTS <ref type="bibr" target="#b3">[4]</ref> is a widely used stochastic method but it makes numerous approximations to speedup. Approximations such as using finite difference approximation and using weights of one forward step instead of the optimal weights lead to inferior performance.</p><p>Fine-tuning is a proven method for improving performance of a neural network. It is a process that uses an already trained neural network on a given task and makes it perform another downstream similar task. Inspired by fine-tuning, we propose incorporating fixed-operations to fine-tune DARTS. We use attention modules as fixed operations in our approach owing to the proven success of attention modules in improving classification accuracy <ref type="bibr" target="#b4">[5]</ref>. Although our method performs automatic architecture search, incorporating manually-designed operations improves classification performance allowing us to search better architectures in the same amount of time compared to DARTS.</p><p>The rest of the paper is structured as follows. Previous work is detailed in Section 2. In Section 3, we provide detailed insights about our proposed method. Section 4 presents the experimental results. The paper is concluded in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Neural Architecture search (NAS) has been successfully applied to design model architectures for image classification. Reinforcement learning, evolutionary algorithms, sequential model-based and stochastic gradient-based optimization approaches are usually used for NAS. <ref type="bibr" target="#b0">[1]</ref> uses reinforcement learning with a controller RNN to design a cell. NAS-Net shows remarkable results but it requires a search cost of 2000 GPU days. Amoeba-Net <ref type="bibr" target="#b1">[2]</ref> uses an evolutionary method to search for the optimal architecture. The results are superior compared to NAS-Net but search time is 3150 GPU days. Progressive Neural Architecture Search (PNAS) <ref type="bibr" target="#b2">[3]</ref> uses a sequential model-based optimization strategy to guide the search through the search space. PNAS drastically reduces the search time to 225 GPU days while achieving competitive accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Non-Stochastic NAS Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NAS-Net</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Stochastic NAS methods</head><p>Neural Architecture Optimization Network (NAO-Net) <ref type="bibr" target="#b5">[6]</ref> uses an encoder, predictor and a decoder. The encoder maps neural network architectures to a continuous space. The predictor uses that continuous representation as input and predicts the accuracy. The decoder maps that continuous representation back to the neural network architecture. NAO-Net reduces the search time to 200 GPU Days and produces good results. Self-Evaluated Template Network (SETN) <ref type="bibr" target="#b6">[7]</ref> arXiv:2006.09042v1 [cs.CV] 16 Jun 2020 proposes to use an evaluator and a template network. SETN achieves impressive results at a very low search cost. SMASH <ref type="bibr" target="#b7">[8]</ref> proposes to accelerate architecture selection by learning an auxiliary Hyper-Net. The auxiliary Hyper-Net generates the weights of the main model conditioned on that models architecture. By comparing the relative validation performance of networks with Hyper-Net-generated weights, they effectively search over a wide range of architectures at the cost of a single training run.</p><p>In Stochastic Neural Architecture Search (SNAS) <ref type="bibr" target="#b8">[9]</ref> the search space is represented with a set of one-hot random variables from a fully factorizable joint distribution, multiplied to mask operations in the graph. They reduce the search cost significantly while giving competitive performance.</p><p>Gradient-based search using Differentiable Architecture Sampler (GDAS) <ref type="bibr" target="#b9">[10]</ref> develops a differentiable sampler over the Directed acyclic graph (DAG) to avoid traversing all the possibilities of the sub-graphs. GDAS is quite fast but lacks accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. DARTS and its derivatives</head><p>Differentiable Architecture Search (DARTS) <ref type="bibr" target="#b3">[4]</ref> introduces a differentiable and continuous search space instead of a discrete search space and achieves remarkable efficiency, incurring a low search cost. Several methods have been proposed to improve DARTS. Progressive Differentiable Architecture Search (PDARTS) <ref type="bibr" target="#b10">[11]</ref> proposes to progressively increase the depth of the network during search while decreasing the search space to cater to the resource constraint. Partial Channel Connections for Memory-Efficient Differentiable Architecture Search (PC-DARTS) <ref type="bibr" target="#b11">[12]</ref> leverages the redundancy in network space and samples a small portion of a super-net only via partial channel connections.</p><p>Prune and Replace DARTS (PR-DARTS) <ref type="bibr" target="#b12">[13]</ref> uses a small candidate operation pool from which candidates are progressively pruned and replaced with better performing ones. Amended-DARTS <ref type="bibr" target="#b13">[14]</ref> proposes an amending term for computing architectural gradients by making use of a direct property of the optimality of network parameter optimization. DARTS+ <ref type="bibr" target="#b14">[15]</ref> proposes to use an early stopping criterion to improve efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. FINE-TUNING DARTS</head><p>DARTS proposes a method where the architecture is updated by minimizing the validation loss using the delta rule. In other words, the neural architecture itself is made differentiable with the validation loss. In DARTS, a predefined number of cells are stacked together to form the neural network. A cell is a directed acyclic graph (DAG) consisting of a pre-defined number of nodes. A node is a feature map in convolution networks. Each cell has seven nodes, two input nodes, four intermediate nodes and one output node. Input nodes are the output nodes of previous two cells. Intermediate nodes have two inputs and one output. There are two types of cells: normal and reduction. The reduction cells reduce the spatial resolution of the features by half whereas the normal cells leave the feature resolution unchanged. The process is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The features in a cell are represented by nodes. The number of nodes are fixed from the beginning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Approximations in DARTS</head><p>Multiple approximations have been used in DARTS, leading to reduced performance. First, the architecture α is updated after a single update of the weights w rather than the optimal weights w * (α). More specifically</p><formula xml:id="formula_0">∇ α L val (w * (α), α) (1) ≈∇ α L val (w − ξ∇ w L train (w, α), α),<label>(2)</label></formula><p>where L train , L val and ξ represent the training loss, validation loss and learning rate for weight update, respectively. Second, a finite difference approximation has been used to reduce the computational complexity of obtaining the gradient of the architecture α with the validation data. With these approximations, the convergence of DARTS to the optimal architecture is yet to be theoretically proven. These approximation are crucial to DARTS as these provide the significant speedup. However, as common to approximations, these are expected to lead to inferior performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proposed Solution</head><p>Fine-tuning is a time-tested method for improving performance of a neural network. The parameters of a neural network pre-trained with a different dataset are adopted for the current task. In other words, few or all parameters of the neural network are initialized with the parameters of a pre-trained network and trained over the current data. Fine-tuning provides the easiest way of transferring information across datasets, thereby, allowing neural networks to learn quickly as well as improve performance. Different variants of fine-tuning have been adopted where some layers of the neural network are frozen to the pre-trained values while training.</p><p>Before the recent advancements of NAS methods, manually designed neural architectures have achieved a great degree of success. In the seminal work of AlexNet <ref type="bibr" target="#b15">[16]</ref>, the authors proposed a neural network, which significantly outperformed its predecessors. Subsequently, the method was further improved. ResNet <ref type="bibr" target="#b16">[17]</ref> introduces residual blocks with skip connections, which allow the gradients to back propagate through to the initial layers without vanishing. More recently, Attention Modules <ref type="bibr" target="#b4">[5]</ref> have been introduced, which allow the neural networks to focus on regions of interest before making a decision.</p><p>NAS is an interesting theoretical problem as it introduces algorithms to obtain neural architecture from scratch. However, NAS methods do not make use of the years of effort towards manually designing neural architecture. Our idea is to fine-tune neural architecture search to obtain better performance. By this we utilize manually-designed architectures in the DARTS method.</p><p>In original DARTS, there are M possible connections and N possible operations. Thus, the cell architecture represented by a matrix α ∈ R M ×N . We propose to extend α as α ∈ R M ×N , where M &gt; M and N ≥ N . Mathematically, this extension can be represented as</p><formula xml:id="formula_1">α = [α; α F ],<label>(3)</label></formula><p>where α F represents fixed operations and ; represents concatenation. Columns of zeros are included if N &gt; N . The value of N is determined as</p><formula xml:id="formula_2">N = N + |O(α F ) − O(α)|,<label>(4)</label></formula><p>where O(α) is the set of operations of α and |.| represents the cardinality of the set. Note that N = N if all the operations in α F exist in α. When the architecture α is updated, only the architecture parameters from α are updated and α F remains fixed. More specifically</p><formula xml:id="formula_3">α (i,j) := α (i,j) − γ∆ (i,j) α L val (w , α) if i ≤ M, j ≤ N α (i,j) otherwise,<label>(5)</label></formula><p>where γ is the learning rate for architecture update, i ∈ {1, 2, ..., M }, j ∈ 1, 2, ..., N and</p><formula xml:id="formula_4">w = w − ξ∆ w L train (w, α).<label>(6)</label></formula><p>By introducing α F , we expect two distinct advantages. First, α F can be based on any recent manually-designed architecture, thus making use of past efforts in architecture design. Second, part of the architecture is somewhat independent of the DARTS method. In other words, the approximations of the DARTS method do not carry their influence to α F resulting in robust performance. In <ref type="figure" target="#fig_1">Fig. 2</ref> we show the difference between our approach and original DARTS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Architecture</head><p>We use the attention modules described in <ref type="bibr" target="#b4">[5]</ref> as α F as it has been recently proposed, highly intuitive and shows good performance. The attention module adaptively recalibrates channelwise feature responses by explicitly modeling interdependencies between channels. <ref type="figure" target="#fig_2">Fig. 3</ref> depicts the architecture of our attention module. In the squeeze phase, each attention module makes use of a global average pooling operation. The squeeze phase is followed by the excitation phase which makes use of two small fully-connected layers. The excitation phase is followed by an inexpensive channel-wise scaling/reshaping operation. Reduction ratio r is a hyperparameter that allows us to vary <ref type="figure">Fig. 4</ref>. Comparison of single-stem and dual-stem approach. Blocks marked in gray indicate the difference between the two approaches. the capacity and computational cost of the attention module in the network. Following <ref type="bibr" target="#b4">[5]</ref> we set the reduction ratio r as 16 in our experiments. Attention module has 2 nodes (3 connections) and 3 operations, so α ∈ R 17×11 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this section, we discuss applying our method to numerous public image classification datasets. For our experiments, we used an Nvidia GTX 1080 GPU with 8GB of memory, which is a relatively small GPU. The computer used for running the experiments had an Intel(R) Core(TM) i7-3770K CPU and  <ref type="bibr" target="#b1">[2]</ref> 97.87 34.9 Evolution 3150 PNAS <ref type="bibr" target="#b2">[3]</ref> 96.59 ± 0.09 3.2 SMBO 225 NAONet <ref type="bibr" target="#b5">[6]</ref> 96.82 10.6 NAO 200 SMASHv2 <ref type="bibr" target="#b7">[8]</ref> 95.97 16 GB 1.5 SETN + cutout <ref type="bibr" target="#b6">[7]</ref> 97.31 4.6 GB 1.8 GDAS + cutout <ref type="bibr" target="#b9">[10]</ref> 96.25 2.5 GB 0.17 DARTS(2nd order) + cutout <ref type="bibr" target="#b3">[4]</ref> 97.24 ± 0.09 3.3 GB 1 SNAS (mild) + cutout <ref type="bibr" target="#b8">[9]</ref> 97.02 2.9 GB 1.5 PR-DARTS DL1 + cutout <ref type="bibr" target="#b12">[13]</ref> 97.26 ± 0.12 3.2 GB 0.82 PC-DARTS +cutout <ref type="bibr" target="#b11">[12]</ref> 97.43 3.6 GB 0.1 PDARTS + cutout <ref type="bibr" target="#b10">[11]</ref> 97.50 3.4 GB 0.3 Amended-DARTS S1 + cutout <ref type="bibr" target="#b13">[14]</ref> 97. <ref type="bibr" target="#b18">19</ref>      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8GB of RAM. For experiments on CompCars and MIO-TCD</head><p>we use a dual-stem approach instead of a one-stem approach because these datasets have images of size larger than 32×32. Rather than passing redundant information to the first and the second cells of the neural network, we propose applying two different transformations to the input image, and passing the first transformed image to the first cell and the second transformed image to the second cell. In this way, different information is passed to the first two cells of the neural network. A comparison of both approaches is shown in <ref type="figure">Fig. 4</ref>. Results  <ref type="bibr" target="#b25">[26]</ref> Manual 97.61% Theagarajan et al. <ref type="bibr" target="#b26">[27]</ref> Manual 97.80% Kim et al. <ref type="bibr" target="#b27">[28]</ref> Manual 97.86% Lee et al. <ref type="bibr" target="#b28">[29]</ref> Manual 97.92% Jung et al. <ref type="bibr" target="#b29">[30]</ref> Manual 97.95% DARTS <ref type="bibr" target="#b3">[4]</ref> NAS-based 95.07% SNAS <ref type="bibr" target="#b8">[9]</ref> NAS-based 95.50% Ours NAS-based 98.34%</p><p>show that our dual-stem approach works better on these datasets compared to single-stem approach of DARTS.</p><p>A. CIFAR-10 Dataset CIFAR-10 [31] is a dataset for image classification. The CIFAR-10 dataset consists of 60,000 32 × 32 color images divided in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.</p><p>For the CIFAR-10 dataset, we used an initial learning rate of 0.025 and the learning rate was updated using the strategy in <ref type="bibr" target="#b31">[32]</ref> to learn the network parameters. The momentum and weight decay parameters of stochastic gradient descent (SGD) optimizer were set to 0.9 and 3 × 10 −4 . For the cell search, we used 3 × 10 −4 , 0.5, 0.999 and 10 −3 as the learning rate, β 1 , β 2 and weight decay values of Adam optimizer, respectively. 50% of the training data was used as validation data during the architecture search. For the final training, the standard training/testing split is used.We stacked 8 and 20 cells during architecture search and final training after architecture search respectively. A batch size of 32 and 56 were used during architecture search and final training, respectively. The initial number of channels during architecture search and final training were set to 16 and 36, respectively. We performed cell search and final training of the neural network over 50 and 600 epochs of the training data, respectively. Additional enhancements during final training include cutout <ref type="bibr" target="#b32">[33]</ref>, path dropout of probability 0.2 and auxiliary towers with weight 0.4.</p><p>We give a detailed comparison of our approach with all the approaches mentioned in Section 2 in <ref type="table" target="#tab_0">Table I</ref>. Although AmoebaNet <ref type="bibr" target="#b33">[34]</ref>, gives better accuracy but the number of parameters and search cost are too high. NAONet <ref type="bibr" target="#b5">[6]</ref>, gives the same accuracy as compared to our method but the number of parameters and search cost is higher. Although some of the methods ( GDAS, PR-DARTS, PC-DARTS, PDARTS, and DARTS+ ) search faster compared to our method. However, the GPU days metric depends upon the type of the GPU used, as some of these methods use a different GPU so it's not a very accurate metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CIFAR-100 Dataset</head><p>CIFAR-100 <ref type="bibr" target="#b30">[31]</ref>, is a large-scale dataset for image classification. The CIFAR-100 dataset consists of 60,000 32×32 color images in 100 classes, with 600 images per class. There are 500 training images and 100 test images per class.</p><p>We directly apply our best searched architecture from CIFAR-10 experiments to CIFAR-100. We follow the same training settings as we did for the final training stage of CIFAR-10. All the architecture search methods reported in <ref type="table" target="#tab_0">Table II</ref> use the architecture searched on the CIFAR-10 dataset to provide a fair comparison. Although NAONet <ref type="bibr" target="#b5">[6]</ref> and AmoebaNet <ref type="bibr" target="#b1">[2]</ref>, give more accuracy but the number of parameters are quite high compared to our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Fashion-MNIST Dataset</head><p>Fashion-MNIST <ref type="bibr" target="#b34">[35]</ref> is a dataset consisting of images related to clothe-ware, shoes, and bag. The Fashion-MNIST dataset has a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 gray-scale image, associated with a label from 10 classes.</p><p>We used an initial learning rate of 0.025 and the learning rate was updated using the strategy in <ref type="bibr" target="#b31">[32]</ref> to learn the network parameters. The momentum and weight decay parameters of stochastic gradient descent (SGD) optimizer were set to 0.9 and 3 × 10 −4 . For the cell search, we used 3 × 10 −4 , 0.5, 0.999 and 10 −3 as the learning rate, β 1 , β 2 and weight decay values of Adam optimizer, respectively. 40% of the training data was used as validation data during the architecture search. 15% of the training data was used as validation data during final training. We stacked 8 and 20 cells during the architecture search and final training after architecture search respectively. A batch size of 32 and 72 were used during architecture search and final training, respectively. The initial number of channels during architecture search and final training were set to 16 and 36, respectively. We performed cell search and final training of the neural network over 50 and 600 epochs of the training data, respectively. Additional enhancements during final training include cutout <ref type="bibr" target="#b32">[33]</ref>, path dropout of probability 0.2 and random erasing <ref type="bibr" target="#b18">[19]</ref>.</p><p>We give a comparison of our approach with other state-of-theart approaches on Fashion-MNIST dataset in <ref type="table" target="#tab_0">Table III</ref>. For a fair comparison, we use similar training settings to report results of DARTS on the Fashion-MNIST dataset. In accordance with our theory, our method performs better compared to DARTS. Although the model proposed by Neupde <ref type="bibr" target="#b21">[22]</ref> uses only 0.4M parameters, its error rate is higher compared to other methods. Results clearly show the superiority of our method compared to other approaches as we are able to achieve state-of-the-art accuracy on Fashion-MNIST dataset. The reduction cell learned on the Fashion-MNIST dataset using DARTS <ref type="bibr" target="#b3">[4]</ref> is shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. Reduction cell learned on Fashion-MNIST dataset using our method is shown in <ref type="figure" target="#fig_4">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. CompCars Dataset</head><p>CompCars <ref type="bibr" target="#b22">[23]</ref> is a large-scale dataset for fine-grained vehicle classification. CompCars dataset is further divided into two groups. The first group contains images of cars taken from the internet while the second group contains images taken from surveillance cameras. These images are challenging as they were taken in different weather and illumination conditions. The web-natured subset has 431 car models, with 35,456 and 15,627 images for training and testing, respectively. The surveillancenatured subset has 281 car models, with 31,146 and 13,333 training and testing images, respectively.</p><p>For the CompCars dataset, we used an initial learning rate of 0.1 and the learning rate was updated using the strategy in <ref type="bibr" target="#b31">[32]</ref> to learn the network parameters. The momentum and weight decay parameters of stochastic gradient descent (SGD) optimizer were set to 0.9 and 3 × 10 −4 . For the cell search, we used 3 × 10 −4 , 0.5, 0.999 and 10 −3 as the learning rate, β 1 , β 2 and weight decay values of Adam optimizer, respectively. Training images were further divided into training and validation subsets with a ratio of 60:40 and 70:30 respectively during architecture search and final training for web-nature data . 40% of the training data was used as validation data during the architecture search. For the final training, we used 30% (web-natured) and 20% (surveillancenatured) of the training data for validation. We stacked 6 and 16 cells during architecture search and final training after architecture search respectively. A batch size of 32 and 72 were used during architecture search and final training, respectively. The initial number of channels during architecture search and final training were set to 16 and 48, respectively. For architecture search, we performed 80 and 50 epochs over the training data for the web and surveillance subsets, respectively whereas we performed 250 epochs for final training with both the subsets.</p><p>In <ref type="table" target="#tab_0">Table IV</ref>, we compare our approach with DARTS[4], SNAS <ref type="bibr" target="#b8">[9]</ref>, manually designed CNNs that include AlexNet <ref type="bibr" target="#b22">[23]</ref> , Overfeat <ref type="bibr" target="#b22">[23]</ref>, GoogLeNet <ref type="bibr" target="#b22">[23]</ref>, and with state-of-theart approaches on the web (Han et al. <ref type="bibr" target="#b23">[24]</ref>) and surveillance subsets <ref type="bibr" target="#b24">[25]</ref>. As evident from the table, our approach gives the best accuracy compared to all other competing methods. <ref type="bibr" target="#b35">[36]</ref> is the largest dataset for motorized-traffic analysis to date. It is a challenging dataset because of the diversity of pose, lighting, inter-class similarity, and image resolution. MIO-TCD dataset contains 11 traffic object classes such as single-unit-truck, pickup-truck, and articulated-truck. The classification dataset consists of 648,959 images. The dataset is split into 80% training (519,164) and 20% testing(129,795) images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. MIO-TCD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIO-vision Traffic Camera Dataset (MIO-TCD)</head><p>We used the same values of β 1 , β 2 , momentum and the learning rate strategies as for the CompCars dataset. An initial learning rate of 0.025 was used. A batch size of 32 and 72 were used during architecture search and final training, respectively. The initial number of channels during architecture search and final training were set to 16 and 36, respectively. 35% of the training data was used as validation data during the architecture search. For the final training, we used 20% of the training data for validation. Again for time efficiency, we stacked 6 cells at architecture search and 14 cells during final training.</p><p>In <ref type="table" target="#tab_5">Table V</ref>, we compare the top-1 accuracy on the MIO-TCD <ref type="bibr" target="#b35">[36]</ref> dataset of our approach with DARTS <ref type="bibr" target="#b3">[4]</ref>, SNAS <ref type="bibr" target="#b8">[9]</ref>, and with manual state-of-the-art approaches, which include <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, and <ref type="bibr" target="#b29">[30]</ref>. Results show that our approach outperforms all other competing approaches.</p><p>V. CONCLUSION Based on the proven success of fine-tuning in manually designed architectures we propose to fine-tune DARTS by adding fixed operations. We add attention modules after each cell. These operations are independent of the approximations used in DARTS. We conduct experiments on CIFAR-10, CIFAR-100, Fashion-MNIST, CompCars and MIO-TCD, and our results show the validity of our claim. We were able to obtain state-of-the-art results on Fashion-MNIST, CompCars</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Step-wise depiction of DARTS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Comparison of DARTS<ref type="bibr" target="#b3">[4]</ref> and our approach for a four cell network. Block marked in grey indicate the difference between the two approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The schema of attention module. 'FC' represents fully-connected layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Reduction cell found on Fashion-MNIST dataset using DARTS. Cell Out represents the output node of the cell, Cell P 1 and Cell P 2 represent outputs of previous two cells respectively, and 0, 1, 2 and 3 are intermediate nodes of the cell. A: Average Pooling 3 × 3, B: Skip Connection and C: Concatenation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Reduction cell found on Fashion-MNIST dataset using our method. Cell Out represents the output node of the cell, Cell P 1 and Cell P 2 represent outputs of previous two cells respectively, and 0, 1, 2 and 3 are intermediate nodes of the cell. C: Concatenation, D: Separable Convolution 5 × 5, E: Dilated Separable Convolution 5 × 5 and F: Maximum Pooling 3 × 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Top-1 Accuracy on CIFAR-10 dataset. GB:Gradient-Based</figDesc><table><row><cell>Architecture</cell><cell cols="2">Accuracy(%) Params(M)</cell><cell>Search Method</cell><cell>Search Cost GPU days</cell></row><row><cell>DenseNet-BC [18]</cell><cell>96.54</cell><cell>25.6</cell><cell>Manual</cell><cell>-</cell></row><row><cell>NASNetA + cutout [1]</cell><cell>97.35</cell><cell>3.3</cell><cell>RL</cell><cell>2000</cell></row><row><cell>AmoebaNet-B + cutout</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell>Architecture</cell><cell cols="2">Accuracy (%) Params(M)</cell><cell>Search Method</cell><cell>*Search Cost GPU days</cell></row><row><cell>DenseNet-BC [18]</cell><cell>82.82</cell><cell>25.6</cell><cell>Manual</cell><cell>-</cell></row><row><cell>NASNetA + cutout [1]</cell><cell>82.19</cell><cell>3.3</cell><cell>RL</cell><cell>2000</cell></row><row><cell>AmoebaNet-B + cutout [2]</cell><cell>84.20</cell><cell>34.9</cell><cell>Evolution</cell><cell>3150</cell></row><row><cell>PNAS + cutout [3]</cell><cell>82.37</cell><cell>3.2</cell><cell>SMBO</cell><cell>225</cell></row><row><cell>NAONet [6]</cell><cell>84.33</cell><cell>10.6</cell><cell>NAO</cell><cell>200</cell></row><row><cell>SETN + cutout [7]</cell><cell>82.75</cell><cell>4.6</cell><cell>Gradient-based</cell><cell>1.8</cell></row><row><cell>GDAS + cutout [10]</cell><cell>80.91</cell><cell>2.5</cell><cell>Gradient-based</cell><cell>0.17</cell></row><row><cell>DARTS(2nd order) + cutout [4]</cell><cell>82.46</cell><cell>3.3</cell><cell>Gradient-based</cell><cell>1</cell></row><row><cell>PDARTS + cutout [11]</cell><cell>82.80</cell><cell>3.4</cell><cell>Gradient-based</cell><cell>0.3</cell></row><row><cell>DARTS+ with cutout [15]</cell><cell>83.72</cell><cell>3.7</cell><cell>Gradient-based</cell><cell>0.4</cell></row><row><cell>Ours + cutout</cell><cell>84.10</cell><cell>3.9</cell><cell>Gradient-based</cell><cell>1</cell></row></table><note>: Top-1 Accuracy on CIFAR-100 dataset.*: Search cost on CIFAR-10.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Top-1 Accuracy on Fashion-MNIST dataset. GB:Gradient-Based.</figDesc><table><row><cell>Architecture</cell><cell cols="2">Accuracy (%) Params(M)</cell><cell>Search Method</cell></row><row><cell>ResNet-110 + random erasing [19]</cell><cell>95.99 ± 0.13</cell><cell>1.7</cell><cell>Manual</cell></row><row><cell>ResNeXt-8-64 + random erasing [19]</cell><cell>96.21 ± 0.06</cell><cell>34.4</cell><cell>Manual</cell></row><row><cell>WRN-28-10 + random erasing [19]</cell><cell>96.35 ± 0.03</cell><cell>36.5</cell><cell>Manual</cell></row><row><cell>VGG8B [20]</cell><cell>95.47</cell><cell>7.3</cell><cell>Manual</cell></row><row><cell>DeepCaps [21]</cell><cell>94.46</cell><cell>7.2</cell><cell>Manual</cell></row><row><cell>Neupde [22]</cell><cell>92.40</cell><cell>0.4</cell><cell>Manual</cell></row><row><cell>DARTS(2nd order) + cutout + random erasing [4]</cell><cell>96.57</cell><cell>2.6</cell><cell>GB</cell></row><row><cell>Ours + cutout + random erasing</cell><cell>96.91</cell><cell>3.2</cell><cell>GB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell cols="4">: Top-1 Accuracy on CompCars dataset.</cell></row><row><cell>Method</cell><cell cols="3">Nature of Data CNN Type Accuracy</cell></row><row><cell>AlexNet[23]</cell><cell>Web</cell><cell>Manual</cell><cell>81.9%</cell></row><row><cell>Overfeat[23]</cell><cell>Web</cell><cell>Manual</cell><cell>87.9%</cell></row><row><cell>GoogLeNet[23]</cell><cell>Web</cell><cell>Manual</cell><cell>91.2%</cell></row><row><cell>Han et al.[24]</cell><cell>Web</cell><cell>Manual</cell><cell>95.4%</cell></row><row><cell>DARTS [4]</cell><cell>Web</cell><cell>NAS-based</cell><cell>91.4%</cell></row><row><cell>SNAS [9]</cell><cell>Web</cell><cell>NAS-based</cell><cell>91.4%</cell></row><row><cell>Ours</cell><cell>Web</cell><cell>NAS-based</cell><cell>95.9%</cell></row><row><cell>AlexNet[23]</cell><cell>Surveillance</cell><cell>Manual</cell><cell>98.0%</cell></row><row><cell>Overfeat[23]</cell><cell>Surveillance</cell><cell>Manual</cell><cell>98.3%</cell></row><row><cell>GoogLeNet[23]</cell><cell>Surveillance</cell><cell>Manual</cell><cell>98.4%</cell></row><row><cell>Fang et al.[25]</cell><cell>Surveillance</cell><cell>Manual</cell><cell>98.6%</cell></row><row><cell>DARTS [4]</cell><cell>Surveillance</cell><cell>NAS-based</cell><cell>98.1%</cell></row><row><cell>SNAS [9]</cell><cell>Surveillance</cell><cell>NAS-based</cell><cell>98.4%</cell></row><row><cell>Ours</cell><cell>Surveillance</cell><cell>NAS-based</cell><cell>99.2%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Top-1 Accuracy on MIO-TCD dataset.</figDesc><table><row><cell>Method</cell><cell>CNN Type Accuracy</cell></row><row><cell>Xception</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>and MIO-TCD datasets while our results on other datasets were also competitive.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page">1935</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural architecture optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7816" to="7827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">One-shot neural architecture search via selfevaluated template network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3681" to="3690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Smash: oneshot model architecture search through hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05344</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SNAS: stochastic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Progressive differentiable architecture search: Bridging the depth gap between search and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12760</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Pc-darts: Partial channel connections for memory-efficient differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05737</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Prune and replace nas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Laube</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07528</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Stabilizing darts with amended gradient estimation on architectural parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11831</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06035</idno>
		<title level="m">Darts+: Improved differentiable architecture search with early stopping</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Training neural networks with local error signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nøkland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Eidnes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06656</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deepcaps: Going deeper with capsule networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rajasegaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jayasundara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasekara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jayasekara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seneviratne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Neupde: Neural network based ordinary and partial differential equations for modeling time-dependent data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schaeffer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03190</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A large-scale car dataset for fine-grained categorization and verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/1506.08959</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attribute-aware attention model for fine-grained representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Multimedia, MM &apos;18</title>
		<meeting>the 26th ACM International Conference on Multimedia, MM &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2040" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fine-grained vehicle model recognition using a coarse-to-fine convolutional neural network architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1782" to="1792" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Eden: Ensemble of deep networks for vehicle classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Theagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vehicle type classification using bagging and convolutional neural network on multi view surveillance image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="41" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep learning-based vehicle classification using an ensemble of local expert and global networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="47" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Resnet-based vehicle classification and localization in traffic surveillance systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W. Young</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="61" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">tech. rep</title>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the aaai conference on artificial intelligence</title>
		<meeting>the aaai conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mio-tcd: A new benchmark dataset for vehicle classification and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Branchaud-Charron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lemaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Achkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eichel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="5129" to="5141" />
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
