<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Stacked Hierarchical Multi-patch Network for Image Deblurring</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Data61/CSIRO</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
							<email>daiyuchao@nwpu.edu.cn3</email>
							<affiliation key="aff2">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Data61/CSIRO</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Stacked Hierarchical Multi-patch Network for Image Deblurring</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite deep end-to-end learning methods have shown their superiority in removing non-uniform motion blur, there still exist major challenges with the current multi-scale and scale-recurrent models: 1) Deconvolution/upsampling operations in the coarse-to-fine scheme result in expensive runtime; 2) Simply increasing the model depth with finer-scale levels cannot improve the quality of deblurring. To tackle the above problems, we present a deep hierarchical multi-patch network inspired by Spatial Pyramid Matching to deal with blurry images via a fine-tocoarse hierarchical representation. To deal with the performance saturation w.r.t. depth, we propose a stacked version of our multi-patch model. Our proposed basic multi-patch model achieves the state-of-the-art performance on the Go-Pro dataset while enjoying a 40× faster runtime compared to current multi-scale methods. With 30ms to process an image at 1280×720 resolution, it is the first real-time deep motion deblurring model for 720p images at 30fps. For stacked networks, significant improvements (over 1.2dB) are achieved on the GoPro dataset by increasing the network depth. Moreover, by varying the depth of the stacked model, one can adapt the performance and runtime of the same network for different application scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of non-uniform blind image deblurring is to remove the undesired blur caused by the camera motion and the scene dynamics <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">26</ref>]. Prior to the success of deep learning, conventional deblurring methods used to employ a variety of constraints or regularizations to approximate the motion blur filters, involving an expensive non-convex nonlinear optimization. Moreover, the commonly used assumption of spatially-uniform blur kernel is overly restrictive, resulting in a poor deblurring of complex blur patterns.</p><p>Deblurring methods based on Deep Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23]</ref> learn the regression between a blurry input image and the corresponding sharp image in an Real-Time <ref type="bibr">Nah</ref>  <ref type="figure">Figure 1</ref>. The PSNR vs. runtime of state-of-the-art deep learning motion deblurring methods and our method on the GoPro dataset <ref type="bibr" target="#b16">[17]</ref>. The blue region indicates real-time inference, while the red region represents high performance motion deblurring (over 30 dB). Clearly, our method achieves the best performance at 30 fps for 1280 × 720 images, which is 40× faster than the very recent method <ref type="bibr" target="#b26">[26]</ref>. A stacked version of our model further improves the performance at a cost of somewhat increased runtime.</p><p>end-to-end manner <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">26]</ref>. To exploit the deblurring cues at different processing levels, the "coarse-to-fine" scheme has been extended to deep CNN scenarios by a multi-scale network architecture <ref type="bibr" target="#b16">[17]</ref> and a scale-recurrent architecture <ref type="bibr" target="#b26">[26]</ref>. Under the "coarse-to-fine" scheme, a sharp image is gradually restored at different resolutions in a pyramid. Nah et al. <ref type="bibr" target="#b16">[17]</ref> demonstrated the ability of CNN models to remove motion blur from multi-scale blurry images, where a multi-scale loss function is devised to mimic conventional coarse-to-fine approaches. Following a similar pipeline, Tao et al. <ref type="bibr" target="#b26">[26]</ref> share network weights across scales to improve training and model stability, thus achieving highly effective deblurring compared with <ref type="bibr" target="#b16">[17]</ref>. However, there still exist major challenges in deep deblurring:</p><p>• Under the coarse-to-fine scheme, most networks use a large number of training parameters due to large filter sizes. Thus, the multi-scale and scale-recurrent methods result in an expensive runtime (see <ref type="figure">Fig. 1</ref>) and struggle to improve deblurring quality.</p><p>• Increasing the network depth for very low-resolution input in multi-scale approaches does not seem to improve the deblurring performance <ref type="bibr" target="#b16">[17]</ref>. In this paper, we address the above challenges with the multi-scale and scale-recurrent architectures. We investigate a new scheme which exploits the deblurring cues at different scales via a hierarchical multi-patch model. Specifically, we propose a simple yet effective multi-level CNN model called Deep Multi-Patch Hierarchical Network (DMPHN) which uses multi-patch hierarchy as input. In this way, the residual cues from deblurring local regions are passed via residual-like links to the next level of network dealing with coarser regions. Feature aggregation over multiple patches has been used in image classification <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref>. For example, <ref type="bibr" target="#b12">[13]</ref> proposes Spatial Pyramid Matching (SPM) which divides images into coarse-to-fine grids in which histograms of features are computed. In <ref type="bibr" target="#b9">[10]</ref>, a second-order fine-grained image classification model uses overlapping patches for aggregation. Sun et al. <ref type="bibr" target="#b24">[25]</ref> learned a patch-wise motion blur kernel through a CNN for restoration via an expensive energy optimization.</p><p>The advantages of our network are twofold: 1) As the inputs at different levels have the same spatial resolution, we can apply residual-like learning which requires small filter sizes and leads to a fast inference; 2) We use an SPM-like model which is exposed to more training data at the finest level due to relatively more patches available for that level.</p><p>In addition, we have observed a limitation to stacking depth on multi-scale and multi-patch models, thus increasing the model depth by introducing additional coarser or finer grids cannot improve the overall deblurring performance of known models. To address this issue, we present two stacked versions of our DMPHN, whose performance is higher compared to current state-of-the-art deblurring methods. Our contributions are summarized below:</p><p>I. We propose an end-to-end CNN hierarchical model akin to Spatial Pyramid Matching (SPM) that performs deblurring in the fine-to-coarse grids thus exploiting multi-patch localized-to-coarse operations. Each finer level acts in the residual manner by contributing its residual image to the coarser level thus allowing each level of network focus on different scales of blur.</p><p>II. We identify the limitation to stacking depth of current deep deblurring models and introduce novel stacking approaches which overcome this limitation.</p><p>III. We perform baseline comparisons in the common testbed (where possible) for fair comparisons. IV. We investigate the influence of weight sharing between the encoder-decoder pairs across hierarchy levels, and we propose a memory-friendly variant of DMPHN. Our experiments will demonstrate clear benefits of our SPM-like model in motion deblurring. To the best of our knowledge, our CNN model is the first multi-patch take on blind motion deblurring and DMPHN is the first model that supports deblurring of 720p images real-time (at 30fps).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Conventional image deblurring methods <ref type="bibr">[1, 4-6, 14, 20, 22, 27]</ref> fail to remove non-uniform motion blur due to the use of spatially-invariant deblurring kernel. Moreover, their complex computational inference leads to long processing times, which cannot satisfy the ever-growing needs for realtime deblurring.</p><p>Deep Deblurring. Recently, CNNs have been used in nonuniform image deblurring to deal with the complex motion blur in a time-efficient manner <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">28]</ref>. Xu et al. <ref type="bibr" target="#b28">[28]</ref> proposed a deconvolutional CNN which removes blur in non-blind setting by recovering a sharp image given the estimated blur kernel. Their network uses separable kernels which can be decomposed into a small set of filters. Sun et al. <ref type="bibr" target="#b24">[25]</ref> estimated and removed a non-uniform motion blur from an image by learning the regression between 30×30 image patches and their corresponding kernels. Subsequently, the conventional energy-based optimization is employed to estimate the latent sharp image.</p><p>Su et al. <ref type="bibr" target="#b23">[24]</ref> presented a deep learning framework to process blurry video sequences and accumulate information across frames. This method does not require spatiallyaligned pairs of samples. Nah et al. <ref type="bibr" target="#b16">[17]</ref> exploited a multiscale CNN to restore sharp images in an end-to-end fashion from images whose blur is caused by various factors. A multi-scale loss function is employed to mimic the coarseto-fine pipeline in conventional deblurring approaches.</p><p>Recurrent Neural Network (RNN) is a popular tool employed in deblurring due to its advantage in sequential information processing. A network consisting of three deep CNNs and one RNN, proposed by <ref type="bibr" target="#b29">[29]</ref>, is a prominent example. The RNN is applied as a deconvolutional decoder on feature maps extracted by the first CNN module. Another CNN module learns weights for each layer of RNN. The last CNN module reconstructs the sharp image from deblurred feature maps. Scale-Recurrent Network (SRN-DeblurNet) <ref type="bibr" target="#b26">[26]</ref> uses ConvLSTM cells to aggregate feature maps from coarse-to-fine scales. This shows the advantage of RNN units in non-uniform image deblurring task.</p><p>Generative Adversarial Nets (GANs) have also been employed in deblurring due to their advantage in preserving texture details and generating photorealistic images. Kupyn et al. <ref type="bibr" target="#b11">[12]</ref> presented a conditional GAN which produces high-quality delburred images via the Wasserstein loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Framework</head><p>In this paper, we propose to exploit the multi-patch hierarchy for efficient and effective blind motion deblurring. The overall architecture of our proposed DMPHN network is shown in <ref type="figure" target="#fig_1">Fig. 2</ref> fro which we use the (1-2-4-8) model (explained in Sec. 3.2) as an example. Our network is inspired by coarse-to-fine Spatial Pyramid Matching <ref type="bibr" target="#b12">[13]</ref>, which has been used for scene recognition <ref type="bibr" target="#b9">[10]</ref> to aggregate multiple image patches for better performance. In contrast to the expensive inference in multi-scale and scale-recurrent network models, our approach uses residual-like architecture, thus requiring small-size filters which result in fast processing. The differences between <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">26]</ref> and our network architecture are illustrated in <ref type="figure">Fig. 3</ref>. Despite our model uses a very simple architecture (skip and recurrent connections have been removed), it is very effective. In contrast to <ref type="bibr" target="#b16">[17]</ref> which uses deconvolution/upsampling links, we use operations such as feature map concatenations, which are possible due to the multi-patch setup we propose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Encoder-decoder Architecture</head><p>Each level of our DMPHN network consists of one encoder and one decoder whose architecture is illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref>. Our encoder consists of 15 convolutional layers, 6 residual links and 6 ReLU units. The layers of decoder and encoder are identical except that two convolutional layers are replaced by deconvolutional layers to generate images.</p><p>The parameters of our encoder and decoder amount to 3.6 MB due to residual nature of our model which contributes significantly to the fast deblurring runtime. By contrast, the multi-scale deblurring network in <ref type="bibr" target="#b16">[17]</ref> has 303.6 Mb parameters which results in the expensive inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Architecture</head><p>The overall architecture of our DMPHN network is depicted in <ref type="figure" target="#fig_1">Fig. 2</ref>, in which we use the (1-2-4-8) model for illustration purposes. Notation (1-2-4-8) indicates the numbers of image patches from the coarsest to the finniest level i.e., a vertical split at the second level, 2 × 2 = 4 splits at the third level, and 2 × 4 = 8 splits at the fourth level.</p><p>We denote the initial blurry image input as B 1 , while B ij is the j-th patch at the i-th level. Moreover, F i and G i are the encoder and decoder at level i, C ij is the output of G i for B ij , and S ij represents the output patches from G i .</p><p>Each level of our network consists of an encoder-decoder pair. The input for each level is generated by dividing the original blurry image input B 1 into multiple nonoverlapping patches. The output of both encoder and decoder from lower level (corresponds to finer grid) will be added to the upper level (one level above) so that the top level contains all information inferred in the finer levels. Note that the numbers of input and output patches at each level are different as the main idea of our work is to make the lower level focus on local information (finer grid) to produce residual information for the coarser gird (obtained by concatenating convolutional features). Consider the (1-2-4-8) variant as an example. The deblurring process of DMPHN starts at the bottom level 4. B 1 is sliced into 8 non-overlapping patches B 4,j , j = 1, · · · , 8, which are fed into the encoder F 4 to produce the following convolutional feature representation:</p><formula xml:id="formula_0">C 4,j = F 4 (B 4,j ), j ∈ {1...8}.<label>(1)</label></formula><p>Then, we concatenate adjacent features (in the spatial sense) to obtain a new feat. representation C * 4,j , which is of the same size as the conv. feat. representation at level 3:</p><formula xml:id="formula_1">C * 4,j = C 4,2j−1 ⊕ C 4,2j , j ∈ {1...4},<label>(2)</label></formula><p>where ⊕ denotes the concatenation operator. The concatenated feature representation C * 4,j is passed through the encoder G 4 to produce S 4,j = G 4 (C * 4,j ). Next, we move one level up to level 3. The input of F 3 is formed by summing up S 4,j with the sliced patches B 3,j . Once the output of F 3 is produced, we add to it C * 4,j :</p><formula xml:id="formula_2">C 3,j = F 3 (B 3,j + S 4,j ) + C * 4,j , j ∈ {1...4}.<label>(3)</label></formula><p>At level 3, we concatenate the feature representation of level 3 to obtain C * 3,j and pass it through G 3 to obtain S 3,j :</p><formula xml:id="formula_3">C * 3,j = C 3,2j−1 ⊕ C 3,2j , j ∈ {1, 2},<label>(4)</label></formula><formula xml:id="formula_4">S 3,j = G 3 (C * 3,j ), j ∈ {1, 2}.<label>(5)</label></formula><p>Note that features at all levels are concatenated along spatial dimension: imagine neighboring patches being concatenated to form a larger "image". At level 2, our network takes two image patches B 2,1 and B 2,2 as input. We update B 2,j so that B 2,j := B 2,j + S 3,j and pass it through F 2 :</p><formula xml:id="formula_5">C 2,j = F 2 (B 2,j + S 3,j ) + C * 3,j , j ∈ {1, 2}, (6) C * 2 = C 2,1 ⊕ C 2,2 .<label>(7)</label></formula><p>The residual map at level 2 is given by:</p><formula xml:id="formula_6">S 2 = G 2 (C * 2 ).<label>(8)</label></formula><p>At level 1, the final deblurred output S 1 is given by:</p><formula xml:id="formula_7">C 1 = F 1 (B 1 + S 2 ) + C * 2 , S 1 = G 1 (C 1 ).<label>(9)</label></formula><p>Different from approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">26]</ref> that evaluate the Mean Square Error (MSE) loss at each level, we evaluate the MSE loss only at the output of level 1 (which resembles res. network). The loss function of DMPHN is given as:</p><formula xml:id="formula_8">L = 1 2 S 1 − G 2 F ,<label>(10)</label></formula><p>where G denotes the ground-truth sharp image. Due to the hierarchical multi-patch architecture, our network follows the principle of residual learning: the intermediate outputs at different levels S i capture image statistics at different scales. Thus, we evaluate the loss function only at level 1.</p><p>We have investigated the use of multi-level MSE loss, which forces the outputs at each level to be close to the ground truth image. However, as expected, there is no visible performance gain achieved by using the multi-scale loss. <ref type="figure">Figure 6</ref>. Deblurring results. Red block contains the blurred subject, blue and green are the results for <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b26">[26]</ref>, respectively, yellow block indicates our result. As can be seen, our method produces the sharpest and most realistic facial details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Stacked Multi-Patch Network</head><p>As reported by Nah et al. <ref type="bibr" target="#b16">[17]</ref> and Tao et al. <ref type="bibr" target="#b26">[26]</ref>, adding finer network levels cannot improve the deblurring performance of the multi-scale and scale-recurrent architectures. For our multi-patch network, we have also observed that dividing the blurred image into ever smaller grids does not further improve the deblurring performance. This is mainly due to coarser levels attaining low empirical loss on the training data fast thus excluding the finest levels from contributing their residuals.</p><p>In this section, we propose a novel stacking paradigm for deblurring. Instead of making the network deeper vertically (adding finer levels into the network model, which increases the difficulty for a single worker), we propose to increase the depth horizontally (stacking multiple network models), which employs multiple workers (DMPHN) horizontally to perform deblurring.</p><p>Network models can be cascaded in numerous ways. In <ref type="figure" target="#fig_3">Fig. 5</ref>, we provide two diagrams to demonstrate the proposed models. The first model, called Stack-DMPHN, stacks multiple "bottom-top" DMPHNs as shown in <ref type="figure" target="#fig_3">Fig. 5</ref> (top). Note that the output of sub-model i − 1 and the input of sub-model i are connected, which means that for the optimization of sub-model i, output from the sub-model i − 1 is required. All intermediate features of sub-model i − 1 are passed to sub-model i. The MSE loss is evaluated at the output of every sub-model i.</p><p>Moreover, we investigate a reversed direction of information flow, and propose a Stacked v-shape "top-bottomtop" multi-patch hierarchical network (Stack-VMPHN). We will show in our experiments that the Stack-VMPHN outperforms DMPHN. The architecture of Stack-VMPHN is shown in <ref type="figure" target="#fig_3">Fig. 5 (bottom)</ref>. We evaluate the MSE loss at the output of each sub-model of Stack-VMPHN.</p><p>The Stack-VMPHN is built from our basic DMPHN units and it can be regarded as a reversed version of Stack(2)-DMPHN (2 stands for stacking of two submodels). In Stack-DMPHN, processing starts from the bottom level and ends at the top-level, then the output of the top-level is forwarded to the bottom level of next model. However, VMPHN begins from the top level, reaches the bottom level, and then it proceeds back to the top level.</p><p>The objective to minimize for both Stack-DMPHN and Stack-VMPHN is simply given as:</p><formula xml:id="formula_9">L = 1 2 N i=1 S i − G 2 F ,<label>(11)</label></formula><p>where N is the number of sub-models used, S i is the output of sub-model i, and G is the ground-truth sharp image. Our experiments will illustrate that these two stacked networks improve the deblurring performance. Although our stacked architectures use DMPHN units, we believe they are generic frameworks-other deep deblurring methods can be stacked in the similar manner to improve their performance. However, the total processing time may be unacceptable if a costly deblurring model is employed for the basic unit. Thanks to fast and efficient DMPHN units, we can control the runtime and size of stacking networks within a reasonable range to address various applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Network Visualization</head><p>We visualize the outputs of our DMPHN unit in <ref type="figure" target="#fig_4">Fig. 7</ref> to analyze its intermediate contributions. As previously explained, DMPHN uses the residual design. Thus, finer levels contain finer but visually less important information compared to the coarser levels. In <ref type="figure" target="#fig_4">Fig. 7</ref>, we illustrate outputs S i of each level <ref type="figure" target="#fig_1">of DMPHN (1-2-4-8</ref>). The information contained in S 4 is the finest and most sparse. The outputs become less sparse, sharper and richer in color as we move up level-by-level. For the stacked model, the output of every sub-model is optimized level-by-level, which means the first output has the poorest quality and the last output achieves the best performance. <ref type="figure" target="#fig_5">Fig. 8</ref> presents the outputs of Stack(3)-DMPHN (3 sub-models stacked together) to demonstrate that each sub-model gradually improves the quality of deblurring. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation Details</head><p>All our experiments are implemented in PyTorch and evaluated on a single NVIDIA Tesla P100 GPU. To train DMPHN, we randomly crop images to 256×256 pixel size. Subsequently, we extract patches from the cropped images and forward them to the inputs of each level. The batch size is set to 6 during training. The Adam solver <ref type="bibr" target="#b8">[9]</ref> is used to train our models for 3000 epochs. The initial learning rate is set to 0.0001 and the decay rate to 0.1. We normalize image to range [0, 1] and subtract 0.5. <ref type="table">Table 1</ref>. Quantitative analysis of our model on the GoPro dataset <ref type="bibr" target="#b16">[17]</ref>. Size and Runtime are expressed in MB and seconds. The reported time is the CNN runtime (writing generated images to disk is not considered). Note that we employ (1-2-4) hierarchical unit for both Stack-DMPHN and Stack-VMPHN. We did not investigate deeper stacking networks due to the GPU memory limits and long training times. We train/evaluate our methods on several versions of the GoPro dataset <ref type="bibr" target="#b16">[17]</ref> and the VideoDeblurring dataset <ref type="bibr" target="#b23">[24]</ref>. GoPro dataset <ref type="bibr" target="#b16">[17]</ref> consists of 3214 pairs of blurred and clean images extracted from 33 sequences captured at 720×1280 resolution. The blurred images are generated by averaging varying number (7-13) of successive latent frames to produce varied blur. For a fair comparison, we follow the protocol in <ref type="bibr" target="#b16">[17]</ref>, which uses 2103 image pairs for training and the remaining 1111 pairs for testing. VideoDeblurring dataset <ref type="bibr" target="#b23">[24]</ref> contains videos captured by various devices, such as iPhone, GoPro and Nexus. The quantitative part has 71 videos. Every video consists of 100 frames at 720×1280 resolution. Following the setup in <ref type="bibr" target="#b23">[24]</ref>, we use 61 videos for training and the remaining 10 videos for testing. In addition, we evaluate the model trained on the GoPro dataset <ref type="bibr" target="#b16">[17]</ref> on the VideoDeblurring dataset to evaluate the generalization ability of our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Setup and Results</head><p>We feed the original high-resolution 720×1280 pixel images into DMPHN for performance analysis. The PSNR, SSIM, model size and runtime are reported in <ref type="table">Table 1</ref> for an in-depth comparison with competing state-of-the-art motion deblurring models. For stacking networks, we employ (1-2-4) multi-patch hierarchy in every model unit considering the runtime and difficulty of training. Performance. As illustrated in <ref type="table">Table 1</ref>, our proposed DM-PHN outperforms other competing methods according to PSNR and SSIM, which demonstrates the superiority of non-uniform blur removal via the localized information our model uses. The deepest DMPHN we trained and evaluated is <ref type="bibr">(1-2-4-8-16)</ref> due to the GPU memory limitation. The best performance is obtained with (1-2-4-8) model, for which PSNR and SSIM are higher compared to all current stateof-the-art models. Note that our model is simpler than other competing approaches e.g., we do not use recurrent units. We note that patches that are overly small (below 1/16 size) are not helpful in removing the motion blur.  <ref type="bibr" target="#b23">[24]</ref> for models trained on GoPro dataset. PSDeblur means using Photoshop CC 2015. We select the "single frame" version of approach <ref type="bibr" target="#b23">[24]</ref>  The deblurred images from the GoPro dataset are shown in <ref type="figure">Fig. 6 and 9</ref>. In <ref type="figure">Fig. 6</ref>, we show the deblurring performance of different models for an image containing heavy motion blur. We zoom in the main object for clarity. In <ref type="figure">Fig. 9</ref>, we select the images of different scenes to demonstrate the advantages of our model. As can be seen, our DMPHN produces the sharpest details in all cases. Runtime. In addition to the superior PSNR and SSIM of our model, to the best of our knowledge, DMPHN is also the first deep deblurring model that can work in real-time. For example, DMPHN (1-2-4-8) takes 30ms to process a 720×1280 image, which means it supports real-time 720p image deblurring at 30fps. However, there are runtime overheads related to I/O operations, so the real-time deblurring application requires fast transfers from a video grabber to GPU, larger GPU memory and/or an SSD drive, etc.</p><p>The following factors contribute to our fast runtime: i) shallower encoder-decoder with smal-size convolutional filters; ii) removal of unnecessary links e.g., skip or recurrent connections; iii) reduced number of upsampling/deconvolution between convolutional features of different levels. Baseline Comparisons. Although our model has a much better performance than the multi-scale model <ref type="bibr" target="#b16">[17]</ref>, it is an unfair comparison as network architectures of our proposed model and <ref type="bibr" target="#b16">[17]</ref> differ significantly. Compared with <ref type="bibr" target="#b16">[17]</ref>, which uses over 303.6MB parameters, we apply much shallower CNN encoders and decoders with the model size 10× smaller. Thus, we create a Deep Multi-Scale Network (DMSN) that uses our encoder-decoder following the setup in <ref type="bibr" target="#b16">[17]</ref> for the baseline comparison (sanity check) between multi-patch and multi-scale methods. As shown in <ref type="table" target="#tab_2">Table 2</ref>, the PSNR of DMSN is worse than <ref type="bibr" target="#b16">[17]</ref>, which is expected due to our simplified CNN architecture. Compared with our DMPHN, the best result obtained with DMSN is worse than the DMPHN(1-2) model. Due to the common testbed, we argue that the performance of DMSN and DMHPN reported by us is the fair comparison of the multi-patch hierarchical and multi-scale models <ref type="bibr" target="#b16">[17]</ref>. Weight Sharing. Below, we investigate weight sharing between the encoder-decoder pairs of all levels of our network to reduce the number of parameters in our model. <ref type="table" target="#tab_5">Table 5</ref> shows that weight sharing results in a slight loss of performance but reduces the number of parameters significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we address the challenging problem of nonuniform motion deblurring by exploiting the multi-patch SPM-and residual-like model as opposed to the widely used multi-scale and scale-recurrent architectures. Based on oru proposition, we devised an end-to-end deep multi-patch hierarchical deblurring network. Compared against existing deep deblurring frameworks, our model achieves the stateof-the-art performance (according to PSNR and SSIM) and is able to run at 30fps for 720p images. Our work provides an insight for subsequent deep deblurring works regarding efficient deblurring. Our stacked variants Stack(4)-DMPHN and Stack(2)-VMPHN further improved results over both shallower DMPHN and competing approaches while being ∼4× faster than the latter methods. Our stacking architecture appears to have overcome the limitation to stacking depth which other competing approaches exhibit. <ref type="figure">Figure 9</ref>. Deblurring performance on the blurry images from the GoPro and the VideoDeblurring datasets. The first column contains the original blurry images, the second column is the result of <ref type="bibr" target="#b16">[17]</ref>, the third column is the result of <ref type="bibr" target="#b26">[26]</ref>. Our results are presented in the last column. As can be seen, our model achieves the best performance across different scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Outputs of Stacked Network</head><p>Below we present the intermediate outputs of our Stack-VMPHN. <ref type="figure" target="#fig_6">Figure 10</ref> shows that the performance is optimized level by level, which is consistent with the behaviour of Stack-DMPHN. We also provide more instances for Stack-DMPHN to demonstrate its process in <ref type="figure" target="#fig_7">Figure 11</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Extension to Saliency Detection</head><p>We perform saliency detection with our proposed model to investigate the generalization ability on different tasks. Our proposed model is evaluated on the MSRA-B dataset. This dataset consists of 3000 images for training and 2000 images for testing. Note that all current deep methods of saliency detection highly depend on VGG or ResNet pretrained on ImageNet and these methods often will not converge without pre-training on ImageNet. By contrast, our network can be easily trained from scratch. It outperforms all conventional methods and it is real-time. We evaluated single VMPHN for quantitative analysis. To make our network compatible with the saliency detection task, the output channel is modified to 1 for gray image generation, and the residual connection between input and output at level 1 is disabled in VMPHN. <ref type="figure" target="#fig_1">Figure 12</ref> and <ref type="table" target="#tab_5">Table 5</ref> show our results.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Our proposed Deep Multi-Patch Hierarchical Network (DMPHN). As the patches do not overlap with each other, they may cause boundary artifacts which are removed by the consecutive upper levels of our model. Symbol + is a summation akin to residual networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>The architecture and layer configurations of our (a) decoder and (b) encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>The architecture of stacking network. (a) Stack-DMPHN. (b) Stack-VMPHN. (c) The information flow for two different stacking approaches. Note that the units in both stacking networks have (1-2-4) multi-patch hierarchical architecture. The model size of VMPHN unit is 2× as large as DMPHN unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Outputs Si for different levels of DMHPN(1-2-4-8). Images from right to left visualize bottom level S4 to top level S1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Outputs of different sub-models of Stack(3)-DMHPN. From left to right are the outputs of M1 to M3. The clarity of results improves level-by-level. We observed the similar behavior for Stack-VMPHN (not shown for brevity).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .</head><label>10</label><figDesc>The outputs for different sub-models of Stack(3)-VMHPN. From left to right are the outputs of M1 to M3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 .</head><label>11</label><figDesc>The outputs for different sub-models of Stack(3)-DMHPN. From left to right are the outputs of M1 to M3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 .</head><label>12</label><figDesc>Instances of saliency detection on the MSRA-B dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The baseline performance of multi-scale and multi-patch methods on the GoPro dataset<ref type="bibr" target="#b16">[17]</ref>. Note that DMSN(1) and DM-PHN(1) are in fact the same model.</figDesc><table><row><cell></cell><cell>PSNR</cell><cell>SSIM</cell><cell>Size</cell><cell>Runtime</cell></row><row><cell>Sun et al. [25]</cell><cell cols="3">24.64 0.8429 54.1</cell><cell>12000</cell></row><row><cell>Nah et al. [17]</cell><cell cols="3">29.23 0.9162 303.6</cell><cell>4300</cell></row><row><cell>Zhang et al. [29]</cell><cell cols="3">29.19 0.9306 37.1</cell><cell>1400</cell></row><row><cell>Tao et al. [26]</cell><cell cols="3">30.10 0.9323 33.6</cell><cell>1600</cell></row><row><cell>DMPHN(1)</cell><cell cols="2">28.70 0.9131</cell><cell>7.2</cell><cell>5</cell></row><row><cell>DMPHN(1-2)</cell><cell cols="3">29.77 0.9286 14.5</cell><cell>9</cell></row><row><cell>DMPHN(1-1-1)</cell><cell cols="3">28.11 0.9041 21.7</cell><cell>12</cell></row><row><cell>DMPHN(1-2-4)</cell><cell cols="3">30.21 0.9345 21.7</cell><cell>17</cell></row><row><cell>DMPHN(1-4-16)</cell><cell cols="3">29.15 0.9217 21.7</cell><cell>92</cell></row><row><cell>DMPHN(1-2-4-8)</cell><cell cols="3">30.25 0.9351 29.0</cell><cell>30</cell></row><row><cell cols="4">DMPHN(1-2-4-8-16) 29.87 0.9305 36.2</cell><cell>101</cell></row><row><cell>DMPHN</cell><cell cols="3">30.21 0.9345 21.7</cell><cell>17</cell></row><row><cell>Stack(2)-DMPHN</cell><cell cols="3">30.71 0.9403 43.4</cell><cell>37</cell></row><row><cell>Stack(3)-DMPHN</cell><cell cols="3">31.16 0,9451 65.1</cell><cell>233</cell></row><row><cell>Stack(4)-DMPHN</cell><cell cols="3">31.20 0.9453 86.8</cell><cell>424</cell></row><row><cell>VMPHN</cell><cell cols="3">30.90 0.9419 43.4</cell><cell>161</cell></row><row><cell>Stack(2)-VMPHN</cell><cell cols="3">31.50 0.9483 86.8</cell><cell>552</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Quantitative analysis (PSNR) on the VideoDeblurring dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>for fair comparisons. Input 24.14 30.52 28.38 27.31 22.60 29.31 27.74 23.86 30.59 26.98 27.14 PSDeblur 24.42 28.77 25.15 27.77 22.02 25.74 26.11 19.75 26.48 24.62 25.08 WFA [2] 25.89 32.33 28.97 28.36 23.99 31.09 28.58 24.78 31.30 28.20 28.35 Su et al. [24] 25.75 31.15 29.30 28.38 23.63 30.70 29.23 25.62 31.92 28.06 28.37 DMPHN 29.89 33.35 31.82 31.32 26.35 32.49 30.51 27.11 34.77 30.02 30.76 Stack(2)-DMPHN 30.19 33.98 32.16 31.82 26.57 32.94 30.73 27.45 35.11 30.41 31.22 Stack(3)-DMPHN 30.48 34.31 32.24 32.09 26.77 33.08 30.84 27.51 35.24 30.57 31.39 Stack(4)-DMPHN 30.48 34.41 32.25 32.10 26.87 33.12 30.86 27.55 35.25 30.60 31.43 VMPHN outperformed shallower DMPHN by ∼1.3% PSNR. SSIM scores indicate the same trend.</figDesc><table><row><cell>Methods</cell><cell>#1</cell><cell>#2</cell><cell>#3</cell><cell>#4</cell><cell>#5</cell><cell>#6</cell><cell>#7</cell><cell>#8</cell><cell>#9</cell><cell>#10</cell></row><row><cell cols="5">Table 4. Quantitative results for the weight sharing on GoPro [17].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Models</cell><cell cols="4">PSNR SSIM Size (MB)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DMPHN(1-2)</cell><cell cols="2">29.77 0.9286</cell><cell></cell><cell>14.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DMPHN(1-2)-WS</cell><cell cols="2">29.22 0.9210</cell><cell></cell><cell>7.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DMPHN(1-2-4)</cell><cell cols="2">30.21 0.9343</cell><cell></cell><cell>21.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DMPHN(1-2-4)-WS</cell><cell cols="2">29.56 0.9257</cell><cell></cell><cell>7.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DMPHN(1-2-4-8)</cell><cell cols="2">30.25 0.9351</cell><cell></cell><cell>29.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">DMPHN(1-2-4-8)-WS 30.04 0.9318</cell><cell></cell><cell>7.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Moreover, stacked variant Stack(4)-DMPHN outper-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">formed shallower model DMPHN by 1% PSNR, VMPHN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">outperformed DMPHN by 0.7% PSNR while stacked vari-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ant Stack(2)-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Average</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Quantitative analysis of saliency detection on MSRA-B. For F β , higher scores are better. For MAE, lower scores are better.</figDesc><table><row><cell>Model</cell><cell>[8]</cell><cell>[30]</cell><cell>[15]</cell><cell>[7]</cell><cell cols="2">[31] OURS</cell></row><row><cell>F β</cell><cell cols="5">.728 .751 .723 .717 .713</cell><cell>.768</cell></row></table><note>MAE .123 .117 .121 .144 .161 .107</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This research is supported in part by the Australian Research Council through Australian Centre for Robotic Vision (CE140100016), Australian Research Council grants (DE140100180), the China Scholarship Council (CSC Student ID 201603170283). Y. Dai is also funded in part by the Natural Science Foundation of China (61871325, 61420106007). Hongdong Li is also funded in part by ARC-DP (190102261) and ARC-LE (190100080). We also thank for the support of CSIRO Scientific Computing, NVIDIA (GPU grant) and National University of Defense Technology.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyong</forename><surname>Lee</surname></persName>
		</author>
		<idno>145:1-145:8</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hand-held video deblurring via efficient fourier aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="270" to="283" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generalized video deblurring for dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5426" to="5434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Single image motion deblurring using transparency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mathematical models and practical solvers for uniform motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Saliency detection via absorbing markov chain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1665" to="1672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2083" to="2090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A deeper look at power normalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deblurgan: Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykola</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07064</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Blind motion deblurring using image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="841" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Saliency detection via dense and sparse reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2976" to="2983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep multi-patch aggregation network for image style, aesthetics, and quality estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radomir</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="257" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Blur-invariant deep learning for blind-deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><forename type="middle">Kumar</forename><surname>Thekke Madam Nimisha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4762" to="4770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simultaneous stereo video deblurring and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6987" to="6996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Motion Deblurring: Algorithms and Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to deblur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1439" to="1451" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stereo video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anita</forename><surname>Sellent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="558" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep video deblurring for hand-held cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuochen</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning a convolutional neural network for nonuniform motion blur removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenfei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf</title>
		<meeting>IEEE Conf</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Comp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt. Recogn</title>
		<imprint>
			<biblScope unit="page" from="769" to="777" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scale-recurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8174" to="8182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Two-phase kernel estimation for robust motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="157" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Sj Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1790" to="1798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring using spatially variant recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Rynson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf</title>
		<meeting>IEEE Conf</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Comp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt. Recogn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Saliency optimization from robust background detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangjiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2814" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Harf: Hierarchyassociated rich features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
