<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multicolumn Networks for Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multicolumn Networks for Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>XIE AND ZISSERMAN: MULTICOLUMN NETWORKS FOR FACE RECOGNITION 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The objective of this work is set-based face recognition, i.e. to decide if two sets of images of a face are of the same person or not. Conventionally, the set-wise feature descriptor is computed as an average of the descriptors from individual face images within the set. In this paper, we design a neural network architecture that learns to aggregate based on both "visual" quality (resolution, illumination), and "content" quality (relative importance for discriminative classification).</p><p>To this end, we propose a Multicolumn Network (MN) that takes a set of images (the number in the set can vary) as input, and learns to compute a fix-sized feature descriptor for the entire set. To encourage high-quality representations, each individual input image is first weighted by its "visual" quality, determined by a self-quality assessment module, and followed by a dynamic recalibration based on "content" qualities relative to the other images within the set. Both of these qualities are learnt implicitly during training for setwise classification. Comparing with the previous state-of-the-art architectures trained with the same dataset (VGGFace2), our Multicolumn Networks show an improvement of between 2-6% on the IARPA IJB face recognition benchmarks, and exceed the state of the art for all methods on these benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Set-based recognition is commonly used in applications where the task is to determine if the two sets belong to the same person or not, for instance, for face verification or person re-identification. In the paper we will focus on the example of human faces, where each set could contain face images of the same person from multiple sources (e.g. still images, video from survillience cameras, or a mixture of both). With the great success of deep learning for image classification <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref>, the popular approach to set-based face verification is to first train a deep convolutional neural network (CNN) on face image classification, and a set-wise descriptor is then obtained by simply averaging over the CNN feature vectors of individual images in the set <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref>. Verification then proceeds by comparing the setbased vectors. With the help of a large, diverse dataset, this approach has already achieved very impressive results on challenging benchmarks, such as the IARPA IJB-A, IJB-B and IJB-C face recognition datasets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>The simple averaging combination rule suffers from two principal deficiencies: First, visual quality assessment-the naïve average pooling ignores the difference in the amount of information provided by each face image within the set. For instance, an aberrant c 2018. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. image (blurry, extreme illumination or low resolution) will not contribute as much information as an 'ideal' image (sharp focus, well lit and high resolution). Thus, as well as producing a face descriptor for a face image, a "self-aware" quality measure should also be produced reflecting the visual quality of each input image. This quality measure should be taken into account when computing the set-based descriptor. Second, content-aware quality assessment-while computing the set-based feature descriptor, the contribution from each individual image should be recalibrated based on its relative importance to the others present in the set. In other words, each face image should be aware of the existence of the other input images. For instance, suppose all but one of the faces is frontal, then the non-frontal face could contribute more information relative to an individual frontal face. Thus the combination mechanism should also have the capability to compute the relative content quality of each face when determining its contribution to the final set-wise representation. These two quality measures are complementary: one is absolute, depending only on the individual image; the other is relative, depending on the set of images.</p><p>In this paper, we propose a Multicolumn Network (MN), that can be trained to aggregate the individual feature vectors into a set-wise descriptor and addresses both these deficiencies. The model is composed of a standard ResNet50 network with two extra quality control blocks, namely "visual" quality, and relative "content" quality, where the former block is able to down-weight the aberrant images, whilst the latter could highlight the most discriminative images when all images are of good "visual" quality. For efficient training, we first pre-train a ResNet50 on a large scale dataset-VGGFace2 <ref type="bibr" target="#b2">[3]</ref> with standard image-wise classification loss, then use this ResNet as the backbone of the MN, and finetune the entire architecture end-to-end with set-wise classification. Overall, the proposed MN is shown to improve the performance on all of the IARPA IJB face recognition benchmarks over the previous stateof-the-art architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>In this section, we review the work that has influenced the Multicolumn Network design.</p><p>Attention-based feature aggregation. Two recent papers have proposed architectures using multiple columns for face descriptor aggregation. They are Neural Aggregation Network (NAN) <ref type="bibr" target="#b18">[19]</ref> and DR-GAN <ref type="bibr" target="#b16">[17]</ref>. NAN uses within-set attention with a softmax for normalization, and aggregate from multiple inputs based on the softmax scores. The model also incorporates a second attention stage which inputs the aggregated feature descriptor (with the attention scores from the first stage) and outputs a filter kernel that is applied to each face descriptor in the set as a form of channel-wise modulation. DR-GAN generates a weight for each input using a (sigmoid) gating function, and aggregates from the multiple inputs using a weighted average. It does not have a second stage.</p><p>In a similar manner to these previous works, the proposed MN has a first stage that aggregates by weighted averaging (on visual "quality" for each input face image). In contrast, for the second stage, the MN models the relation between each input face and the aggregated feature, and produces different relative "content" quality scores for each face within the set (rather than a common modulation as in NAN). In this way the MN is designed to handle the challenges from both visual and content quality; and indeed, the proposed MN demonstrates superior performance over NAN and DR-GAN on the public benchmarks, as shown in the IJB-A results of section 4.3. In order to obtain the feature descriptor for the entire set, all images are passed through an embedding module (parametrized as a ResNet50) until the last residual block. For "visual" quality control, scalar outputs (α) are implicitly predicted with a fully connected (FC) layer for each image. For "content" quality control, the feature representation of each face (V ) is concatenated with the mean face (V m ), and β is predicted by a fully connected (FC) layer, therefore, the relative contribution of each face image is modelled explicitly. Eventually, the set descriptor is computed by incorporating both factors, i.e. "visual" quality and "content" quality. Note, although only three images are shown here, the architecture can ingest a variable number of images at test time.</p><p>Relation/metric learning. In <ref type="bibr" target="#b11">[12]</ref>, the authors propose a simple Relation Network (RN) for solving the problem of relational reasoning. In order to model the relations among objects, feature descriptors at every spatial location are concatenated with those at every other locations, and the RN module is parametrized with fully connected (fc) layers, which are explicitly trained to infer the existence and implications of object relations. In <ref type="bibr" target="#b14">[15]</ref>, the authors propose similar idea for low-shot learning. In order to model the relations (similarity) between images from a support set and test set, feature maps of the images are concatenated, and passed to a relation module (also parametrized as fc layers) for similarity learning. In the proposed Multicolumn Network, we take inspiration from both, and aim to model the relations between each image and every other image within the set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multicolumn Networks</head><p>We consider the task of set-based face verification, where the objective is to verify if two given sets are of the same person or not. We propose a Multicolumn Network (MN) that can ingest a variable number of images as input, and produce a fixed-size feature descriptor for the entire template. The descriptor is also permutation invariant to the order of the images. The MN architecture is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. It consists of three sequential blocks: First, a common embedding module (i.e. shared weights) is used to generate the per-image feature descriptor. This module is based on the standard ResNet50. Having a common module ensures that this stage of the architecture is permutation-invariant. Second, the compact feature descriptor for each input image is passed to a quality assessment block, and a scalar inferred as a "visual" quality indicator. This ensures that the visual quality is computed independently for each each input image, and completes the task of the column per image. The outcome of the column is thus two-fold: the descriptor, and a weighting generated by a gating (sigmoid) function which can downweight aberrant images (e.g. low quality, extreme illumination, nonface). Third, to compute the relative contribution of each image, the relations of each input image should ideally be modelled together with every other one within the set. However, this would introduce n 2 different combinations (n refers to the number of images within the set). To tackle this problem, we use a "mean face" as an anchor, and each face descriptor is only compared to this anchor to get their relative contribution to the final set descriptor.</p><p>In the following sections we give more implementation details, and the architecture is fully specified in <ref type="table" target="#tab_1">Table 1</ref>. In the following, we say 'fully connected', but actually this is only a 1D weight vector (e.g. 2048 × 1) of the same size as the input, rather than the fc layers in AlexNet, which are huge (e.g. 4096 × 4096). Thus, the proposed MN actually only introduce a very small number of extra parameters, and can be used along with any popular architectures, e.g. VGGNets, ResNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Embedding Module</head><p>Formally, we denote the input image as I 1 , I 2 , ..., I n , and the shared embedding is parameterized as a standard ResNet50 ψ(·, θ 1 ), the outputs from the embedding module is therefore:</p><formula xml:id="formula_0">[V 1 ,V 2 , ...,V n ] = [ψ(I 1 ; θ 1 ), ψ(I 2 ; θ 1 ), ..., ψ(I n ; θ 1 )]<label>(1)</label></formula><p>where each input image is of size I n ∈ R 224×224×3 , and the outputed compact feature representation V n ∈ R 1×2048 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Self-aware Visual Quality Assessment</head><p>In this part, we parametrize a quality control block with a simple fully conneted layer. Formally, we have:</p><formula xml:id="formula_1">[α 1 , α 2 , ..., α n ] = [ f (V 1 ; θ 2 ), f (V 2 ; θ 2 ), ..., f (V n ; θ 2 )]<label>(2)</label></formula><p>where θ 2 ∈ R 2048×1 . In our case, we apply a sigmoid function: f (·, θ 2 ) = σ (·, θ 2 ). Therefore, the block takes the compact feature vectors as inputs, and produce a scalar score ([0-1]) indicating low to high visual quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Content-aware Content Quality Assessment</head><p>To estimate the relative contribution of each face image, we first approximate the feature descriptor for the anchor face as :</p><formula xml:id="formula_2">V m = ∑ n i=1 α i V i ∑ n i α i ,<label>(3)</label></formula><p>the "content" quality is then estimated by modelling the relation between the individual inputs and the mean face, parametrized as g(·; θ 3 ):</p><formula xml:id="formula_3">[β 1 , β 2 , ..., β n ] = [g([V m :V 1 ]; θ 3 ), g([V m :V 2 ]; θ 3 ), ..., g(V m :V n ]; θ 3 )]<label>(4)</label></formula><p>where [V m :V n ] refers to the concatenation of the feature descriptor from the mean face and input image, θ 3 ∈ R 4096×1 . Again, the function g is parametrized with a fully connected layer, and a sigmoid function is used: g(·, θ 3 ) = σ (·, θ 3 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Set-based Descriptor</head><p>As a result, the two extra quality control blocks introduce only about 6K more parameters to the standard ResNet50, and the set descriptor is then calculated as a combination of both "visual" and "content" qualities:</p><formula xml:id="formula_4">V d = ∑ n i=1 α i β i V i ∑ n i α i β i ,<label>(5)</label></formula><p>4 Experimental Details</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">VGGFace2 Dataset</head><p>In this paper, all models are trained with the large-scale VGGFace2 dataset <ref type="bibr" target="#b2">[3]</ref>. In total, the dataset contains about 3.31 million images with large variations in pose, age, illumination, ethnicity and profession (e.g. actors, athletes, politicians). Approximately, 362.6 images exist for each of the 9131 identities on average. The entire dataset is divided into the training set (8631 identities) and validation set (500 identities). In order to be comparable with existing models, we follow the same dataset split and train the models with only 8631 identities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Details</head><p>We train the Multicolumn Network (Table 1) in two phases: First, we pretrain a ResNet50 with image-wise classification, at this step, a region of 224 × 224 pixels is randomly cropped from each sample, and the shorter side resized to 256. The mean value of each channel is subtracted for each pixel. Stochastic gradient descent is used with mini-batches of size 256, with a balancing-sampling strategy for each mini-batch due to the unbalanced training distributions. The initial learning rate is 0.1 for the models learned from scratch, and this is decreased twice with a factor of 10 when errors plateau. The weights of the models are initialised as described in <ref type="bibr" target="#b5">[6]</ref>. This training process exactly follows the one described in <ref type="bibr" target="#b2">[3]</ref>.</p><p>Second, we further use the pre-trained networks in the Multicolumn Network, and finetune this model with set-based classification, each set contains 3 images (we also tried to train with 5 or 7 images as a set, however, the error signals become too weak for efficient training), random transformations are used with a probability of 20% for each image, e.g. monochrome augmentation, horizontal flipping, geometric transformation, gaussian blur, motion blur and jpeg compression.  In order to obtain the feature descriptor for the N input images (N=3 in our case), all images are passed through an embedding module (parametrized as a ResNet50) until the last residual block. For "visual" quality control, scalar outputs (α) are implicitly predicted with a fully connected (FC) layer for each image. For "content" quality control, the feature representation of each face (V ) is concatenated with the mean face (V m ), and β is predicted by a fully connected (FC) layer, enabling the relative contribution of each face image to be modelled explicitly. Eventually, the set descriptor is computed by incorporating both factors, i.e. "visual" quality and "content" quality.</p><formula xml:id="formula_5">  × 3 N × 7 × 7 × 2048 global average pool N × 1 × 1 × 2048 (V 1 ...V N ) Visual Quality fc, 2048 × 1 N × 1 (α 1 ...α N ) weighted average 1 × 1 × 1 × 2048 (V m = ∑ n i=1 α i V i ∑ n i α i ) Content Quality feature concatenation N × 1 × 1 × 4096 (V = [V N : V m ]) fc, 4096 × 1 N × 1 (β 1 ...β N ) weighted average 1 × 1 × 1 × 2048 (V d = ∑ n i=1 α i β i V i ∑ n i α i β i )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We evaluate all models on the challenging IARPA Janus Benchmarks, where all images and videos are captured from unconstrained environments with large variations in viewpoints and image quality. We evaluate the models on the standard 1:1 verification protocol, where a set consists of a variable number of face images and video frames from different sources. During testing, set descriptor is computed and compared with cosine similarity. The performance is reported as the true accept rates (TAR) vs. false positive rates (FAR) (i.e. receiver operating characteristics (ROC) curve). To evaluate the effect of different quality assessment blocks, two versions of the architecture are compared, namely MN-v ("visual" quality only) and MN-vc ("visual" + "content"), as described in equations <ref type="formula" target="#formula_2">(3)</ref> and <ref type="formula" target="#formula_4">(5)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IJB-A Dataset [8]</head><p>The IJB-A dataset contains 5712 images and 2085 videos from 500 subjects, with an average of 11.4 images and 4.2 videos per subject. <ref type="bibr" target="#b16">[17]</ref> CASIA-Net <ref type="bibr" target="#b19">[20]</ref> Multi-PIE+CASIA-WebFace 0.539 ± 0.043 0.774 ± 0.027 -Bansalit et al. <ref type="bibr">[</ref>   Note that the result of Navaneeth et al. <ref type="bibr" target="#b1">[2]</ref> is on the Janus CS3 dataset.</p><formula xml:id="formula_6">1:1 Verification TAR Architecture Dataset FAR=1E − 3 FAR=1E − 2 FAR=1E − 1 DR-GAN</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IJB-C Dataset [10]</head><p>The IJB-C dataset is a further extension of IJB-B, having 3, 531 subjects with 31.3K still images and 117.5K frames from 11, 779 videos. In total, there are 23124 templates with 19557 genuine matches and 15639K impostor matches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion</head><p>The IJB-A benchmark only contains a very small number of testing faces (about 10K pairs), and this benchmark has already become saturated. In this section, we will mainly focus on discussing the results from the more challenging IJB-B and IJB-C benchmarks. The following phenomena can be observed from the evaluation results: First, comparing with the baseline ResNet50, the proposed MN architecture (using the same backbone ResNet50) only introduces an extra 6K parameters, but the performance is consistently boosted by about 2-6% on both IJB-B and IJB-C.  <ref type="table">Table 4</ref>: Evaluation on 1:1 verification protocol on IJB-C dataset. Higher is better.</p><p>The results marked with † are read from the ROC curve in <ref type="bibr" target="#b9">[10]</ref>. Second, MN-v (visual only) already beats the state-of-the-art architecture (SENet50), showing the importance of suppressing the aberrant images within the set. This architecture is similar to that proposed in DR-GAN <ref type="bibr" target="#b16">[17]</ref>. However, as shown in the IJB-A results <ref type="table" target="#tab_3">(Table 2)</ref>, the MN-v results are significantly better than those of DR-GAN; this shows the benefit of using a stronger backbone architecture (i.e. ResNet50 compared to CASIA-Net). Third, when adding the "content" assessment (MN-vc), the performance is further improved. This indicates that both "visual" and "content" assessments actually play a role in boosting the verification results. Fourth, performance improvements are most substantial at FAR = 10 −5 , FAR = 10 −4 and FAR = 10 −3 (left starting part of both ROC curves in <ref type="figure" target="#fig_2">Figure 2</ref>). This is as expected due to the fact that the main issue of average feature aggregation (as used in previous results) is that the set-based descriptor can be distracted by low-quality images (either "visual" or "content"). The consequence is that they can then match to any other set, e.g. blurry images of different people can often look very similar, extreme pose faces or covered faces can look similar. Consequently, the matching scores for the false positives pairs can be higher than those of true positives. By suppressing the aberrant images and highlighting the discriminative faces, the most dramatic improvement that MN-vc brings to producing high quality set descriptors is in reducing the false positive rate. Moreover, we would also expect to see a further performance boost by using a more powerful backbone architecture. As a proof of concept, we take the publicly available SENets_ft from Cao et al. <ref type="bibr" target="#b2">[3]</ref> (pretrained on both MS1M <ref type="bibr" target="#b4">[5]</ref> and VGGFace2), extract features for the images in the IJB-C benchmark, and simply use the "visual" quality scores from the Multicolumn Network (trained with ResNet50) to generate the set descriptors. As shown in <ref type="table" target="#tab_7">Table 5</ref>, compared with the naïve feature aggregation this already shows a significant improvement of over 5% and 2% at FAR = 10 −5 and FAR = 10 −4 , respectively, on the very strong baseline. This supports once again the false positive discussion above, and shows the potential benefit of using a stronger backbone network for future end-to-end training of the Multicolumn Network.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Visualization</head><p>In <ref type="figure" target="#fig_3">Figure 3</ref>, we first show the sorted images in ascending order based on the scores implicitly inferred from the self-aware "visual" quality assessment block. As expected, the self-aware "visual" quality scores for aberrant images are highly correlated with human intuition, i.e. blurry, nonface, extreme poses. Note, the images of medium and high quality are not so well separated, though high quality ones are often near frontal. Following the arrow, the sigmoid scores (α) get higher. From the perspective of the Multicolumn Network, those bottom images are treated as of higher "visual" quality than the top images. As expected, this is highly correlated with the way we define good face images.</p><p>In <ref type="figure">Figure 4</ref>, we show the images and "content" scores (γ) based on the content-aware quality assessment block. Two observations can be drawn to show the benefit of having double quality assessment blocks in MNs: First, images with relatively lower "visual" quality may possess more importance than the higher ones, e.g. image sets in the first row. Second, images with similar "visual" quality can be of very different importances, e.g. image sets in the 3rd row. <ref type="figure">Figure 4</ref>: Visualization of the image sets after within-set recalibration. We use three images (from VGGFace2) as a set, and α and γ refer to the "visual" quality and the recalibrated importance, i.e. γ i = α i β i ∑ j α j β j . The images within the set are ordered based on γ. As can be seen, the images with relatively higher "visual" quality do not necessarily indicate higher importance while calculating the final set-wise feature, e.g. images sets in the first row. Images with similar "visual" quality can be of very different importances, e.g. image sets in the 3rd row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have demonstrated that the addition of two intuitive weightings (visual and content) can significantly improve the verification performance of set based descriptors, compared to simple averaging. This addition only adds around 6K more parameters to the 25.6M parameters of the ResNet50 architecture. As future works, there are several promising directions: First, a more powerful backbone network can be used for end-to-end training of Multicolumn Networks, which can further boost the performance. Second, to further improve the performance, quality assessment should not only be based on the feature descriptor from the whole face, but also be focusing on more detailed facial parts, e.g. eyes, noses. Therefore, for each input set, several feature descriptors can be computed, weighted by visual and content qualities, and trained with set-wise classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of the Multicolumn Network (MN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(i.e. each set can be image-only, video-frame-only, or a mixture of still images and frames). Note, in contrast to the traditional closed-world classification tasks (where the identities are the same during training and testing), face verification is treated as an open-world problem (i.e. the label spaces of the training and test set are disjoint), and thus challenges the capacity and generalization of the feature representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Visual) Multicolumn Networks (Visual+ Content) (a) ROC for IJB-B (b) ROC for IJB-C ROC curve of 1:1 verification protocol on IJB-B &amp; IJB-C dataset. Comparing with the baseline (ResNet50) and the state-of-the-art model (SENet-50), the Multicolumn Networks consistently show an improvement on the benchmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of the "visual" quality for the images in the IJB benchmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Multicolumn Networks (MN).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>A [8], having 1, 845 subjects</cell></row></table><note>Evaluation on 1:1 verification protocol on IJB-A dataset. Higher is better. Results from Cao et al. are computed by the commonly used feature aggregation: averaging. The values with † are read from the ROC curve in [1].IJB-B Dataset [18] The IJB-B dataset is an extension of IJB-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Evaluation on 1:1 verification protocol on IJB-B dataset. Higher is better.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Evaluation on 1:1 verification protocol on IJB-C dataset. Higher is better.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We benefited from insightful discussions with Lars Ericson, Jeffrey Byrne, Chris Boehnen, Patrick Grother. This research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract number 2014-14071600010. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purpose notwithstanding any copyright annotation thereon.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Umdfaces: An annotated face dataset for training deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nanduri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01484</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep heterogeneous feature fusion for template-based face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision, WACV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">VGGFace2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/" />
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Template adaptation for face verification and identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Crosswhite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08221</idno>
		<title level="m">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">IARPA janus benchmark-c: Face dataset and protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Niggel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IAPR International Conference on Biometrics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to humanlevel performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Disentangled representation learning gan for pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">IARPA janus benchmark-b face dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Whitelam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop on Biometrics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural aggregation network for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<title level="m">Learning Face Representation from Scratch</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
