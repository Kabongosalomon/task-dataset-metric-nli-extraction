<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Light-weight calibrator: a separable component for unsupervised domain adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaokai</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Institute for interdisciplinary information core technology(IIISCT)</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailu</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Zhou</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Tsukuba</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfei</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sia</forename><surname>Huat Tan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Xu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Song</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglong</forename><surname>Bao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Light-weight calibrator: a separable component for unsupervised domain adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing domain adaptation methods aim at learning features that can be generalized among domains. These methods commonly require to update source classifier to adapt to the target domain and do not properly handle the trade off between the source domain and the target domain. In this work, instead of training a classifier to adapt to the target domain, we use a separable component called data calibrator to help the fixed source classifier recover discrimination power in the target domain, while preserving the source domain's performance. When the difference between two domains is small, the source classifier's representation is sufficient to perform well in the target domain and outperforms GAN-based methods in digits. Otherwise, the proposed method can leverage synthetic images generated by GANs to boost performance and achieve state-of-the-art performance in digits datasets and driving scene semantic segmentation. Our method empirically reveals that certain intriguing hints, which can be mitigated by benigh noise similar to adversarial attack to domain discriminators, are one of the sources for performance degradation under the domain shift.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks have achieved great performance in solving diverse machine learning problems. However, solving the so called domain shift problem is challenging when neural networks are trying to generalize across domains <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b23">24]</ref>. Extensive efforts have been made on unsupervised domain adaptation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29]</ref>. Early domain adaptation methods use different distance metrics or statistics data to align neural networks' feature distribution of source domain with their feature distribution of target domain. Adversarial domain adaptation methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35]</ref> leverage a two players adversarial game to * Corresponding Authors achieve domain adaptation: A domain classifier is encouraged to learn the difference between the feature distribution of two domains while the classification model is encouraged to maximize the classification loss of the domain classifier by learning domain invariant representation that is indistinguishable to the domain classifier. In addition to featurelevel adversarial game, there is another line of works that use Generative Adversarial Networks(GANs) <ref type="bibr" target="#b7">[8]</ref> to generate source domain images with target domain styles, playing a pixel level adversarial game.</p><p>However, there are issues that have been rarely discussed. Consider a neural network that is deployed in a device and the device needs to move between different domains. It moves from a domain that is close to its trained source domain to another domain that has no labeled data. <ref type="bibr">Figure 2</ref>. Performance trade off between source and target domain. Some existing methods improve target performance at the expense of source domain performance. On contrast, the proposed method keeps good source domain performance and outperforms these methods in target domain performance.</p><p>Traditional unsupervised domain adaptation suffices to handle this simple case. However, the devices can freely move to other domains, which include the source domain. This sample but more realistic scenario brings issues to existing methods. The issues are two folds: (1) Existing methods commonly require to finetune or train a new classifier during domain adaptation. It is not flexible if models are compressed and deployed <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b45">46]</ref>. <ref type="bibr" target="#b1">(2)</ref> Previous methods omit to show the trade off between source domain performance and target domain performance. Some of them have poor performance trade off as indicated in <ref type="figure">Figure 2</ref>. Therefore, when the environments are constantly changing, existing methods are likely to have performance degradation and are not able to adapt to new environments in a flexible way.</p><p>Some prior works try to work on changing domains <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b0">1]</ref>. Bobu et al. <ref type="bibr" target="#b0">[1]</ref> proposes to adapt to continuously changing target domains and Wulfmeier et al. <ref type="bibr" target="#b36">[37]</ref> proposes to incrementally adapt to changing domains. However their methods require to finetune the model, and after the model is deployed, the method cannot work properly for unanticipated new domains. We thereby propose two properties a domain adaptation method should have for changing target domains with deployed models.</p><p>(1) Good trade-off between source and target domain. Given the complexity of the real world, it is unrealistic to assume that the one chosen target domain is the ultimate application domain. Existing methods assume that the source domain only consists of synthetic images and omit to show the source domain performance after domain adaptation, mostly because that it is assumed the source domain will not be encountered again. A counter example is that both source domain and target domain consist of real world images and source domain will also be encountered. In this case, sacrificing source domain performance is not acceptable.</p><p>(2) Flexibility to adapt to arbitrary new domains after being deployed. Deep neural networks are widely deployed in specialized devices <ref type="bibr" target="#b9">[10]</ref>. Usually, they are compressed via model compression methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b24">25]</ref> before being deployed and they are not expected to be updated after being deployed. As far as we know, all existing domain adaptation methods will require finetune the models, which contradicts with model compression methods. It is natural to expect that collecting more data will make a neural network learn universal representation and tremendous investment is made for collecting bigger datasets <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref>. However, datasets are found to contain database bias <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b23">24]</ref>. Training against large datasets does not guarantee the performance of models under changing environments. Therefore, adapting to unanticipated new environments will be necessary and lacking of the flexibility will be an issue.</p><p>In this work, we take the first step to mitigate both limitations and formulate unsupervised domain adaptation in a novel way. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the difference between previous methods and our method in the conceptual level. Previous methods commonly update the source classifier's weights when domain adaptation is needed while ours modifies inputs to achieve domain adaptation.</p><p>We refer existing methods that attempt to learn crossdomain models as monolithic domain adaptation approach. On contrast, we propose a separable component called data calibrator to achieve domain adaptation, which can be seen as a distributed domain adaptation approach. In our framework, the source classifier is responsible for learning representation under supervised training and the data calibrator is responsible for achieving domain adaptation via unsupervised training.</p><p>Our core observation is that the learnt representation from the source domain is not as bad as we thought as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. The performance degradation brought by domain shift can be mitigated by slightly modifying the target domain images by adding perturbation , which we refer as calibration, to the images. By applying calibration to target domain images, these images fit the source classifier's learnt representation significantly better. We show that we can train a light-weight data calibrator whose number of parameters is only 0.25% to 5.8% of the deployed model and we can use it to adapt the deployed model to arbitrary target domains.</p><p>We also want to emphasize that our study focus on the setting that the source domain and the target domain share the common label space otherwise the source classifier will not work properly in the target domain.</p><p>To summarize our contributions:</p><p>• We propose a data calibrator to calibrate target domain images to better fit source classifier's representation while maintaining the source domain performance. We improve previous state-of-the-art average accuracy from 95.1% to 97.6% in digits experiments and frequency weighted IoU from 72.4% to 75.1% in GTA5 to CityScapes adaptation.</p><p>• The proposed data calibrator is light weight and can be less than 1% in terms of number of parameters compared to the deployed model in GTA5 to CityScapes adaptation and it is a separable domain adaptation approach for it does not need to update the source classifier's weights, thus very convenient for deployment.</p><p>• We give new insights on what causes the performance degradation under domain shift and show how to counter it correspondingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unsupervised Domain Adaptation Visual domain adaptation can trace back to <ref type="bibr" target="#b27">[28]</ref>. Early domain adaptation methods focus on aligning deep representation between two domains by using Maximum Mean Discrepancy(MMD) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b18">19]</ref> whereas deep Correlation Align-ment (CORAL) <ref type="bibr" target="#b30">[31]</ref> used statistics such as mean and covariance to achieve feature alignment.</p><p>Another line of works leverage the idea of domain classifiers. Torralba et al. <ref type="bibr" target="#b32">[33]</ref> used "name the database" to demonstrate that databases are commonly biased and it is even possible to train a domain classifier to correctly classify images to databases they come from. Intuitively, if a domain classifier can learn the difference between source domain and target domain from pixels, then it is also possible for a domain classifier to learn the difference between deep representation of source domain images and target domain images. A line of works explore the idea of training a classifier that confuses the domain classifier by maximizing the domain confusion loss <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36]</ref>. In addition to the attempt of confusing a domain classifier in the feature level, pixel level adaptation is also explored. Hoffman et al. <ref type="bibr" target="#b12">[13]</ref> achieves pixel level adaptation for segmentation task, but it uses neural networks' hidden layer output for pixel level adaptation. Our method incorporates both pixel level domain classifier and feature level domain classifier. The pixel level classifier we use directly takes the pixels as inputs, closer to the spirit of "name the dataset" <ref type="bibr" target="#b32">[33]</ref>.</p><p>Generative Adversarial Networks Another line of works leverages the power of Generative Adversarial Networks (GANs) <ref type="bibr" target="#b7">[8]</ref> to generate source images with target images' style. The first of this kind is CoGANs <ref type="bibr" target="#b17">[18]</ref> that jointly learns the source domain representation and the target domain representation by forcing the weight sharing between two GANs. Bousmalis et al. <ref type="bibr" target="#b1">[2]</ref> used GANs to produce images that have similar styles to target domain and makes the target task classifier to train images of both. Hoffman et al. <ref type="bibr" target="#b11">[12]</ref> proposes to use semantic consistency loss and cycle consistency loss and achieve significantly better domain adaptation performance. As a comparison, our method can outperform those methods without requiring high-resources to train GANs.</p><p>Adversarial Attack Neural networks are known for suffering from adversarial attacks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b44">45]</ref>. The simplest form of adversarial attack is FGSM <ref type="bibr" target="#b8">[9]</ref>, which adds a calculated perturbation on the original image, making neural networks misclassify with high confidence. Interestingly, the proposed data calibrator also uses an additive perturbation on images to achieve domain adaptation. The connection between adversarial attack and domain adaptation will be revealed at the objective function in our framework. Essentially, our data calibrator learns to generate adversarial examples that maximize classification loss of domain classifiers.Recently, Ilyas et al. <ref type="bibr" target="#b13">[14]</ref> demonstrates that adversarial attack might leverage "non-robust features" to control classifiers' prediction. We believe that "non-robust features" play an important role in performance degradation brought by domain shift. We will provide more analysis about the connection between our method and adversarial attack in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A Separable Calibrator For Unsupervised</head><p>Domain Adaptation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The overview of the method</head><p>In unsupervised domain adaptation, we have access to source domain images X s and labels Y s drawn from the source domain distribution p s (x, y), and target domain images X t drawn from a target domain distribution p t (x, y), where there are no labels. Let F s be the learned classifier for source domain images. The goal of our work is to design a data calibrator G c such that F s • G c achieves high accuracy on both source and target domain data. As the classifier F s is only trained on source domain and there is no information related to the target, the data calibrator G c has to satisfy:</p><formula xml:id="formula_0">F s (G c (X t )) ∼ F s (X s ), F s (G c (X s )) ∼ F s (X s )<label>(1)</label></formula><p>where X t and X s are from target and source domain respectively.</p><formula xml:id="formula_1">Let F s = C s • M s where M s the feature extractor and C s is the final classifier. A relaxed condition for achieving (1) is to impose the Lipschitz condition on F s • G c , i.e. F s • G c (x) − F s • G c (y) ≤ L x − y ,</formula><p>for some constant L &gt; 0 which is a stability condition. Therefore, the following two constraints are imposed on the data calibrator:</p><formula xml:id="formula_2">G c (X t ) ∼ X s , G c (X s ) ∼ X s M s (G c (X t )) ∼ M s (X s ), M s (G c (X s )) ∼ M s (X s )<label>(2)</label></formula><p>It is noted that G c (x) denotes the input of F s and M s denotes the feature map which implies the alignment on both pixel and feature level for source and target domain data. This motivates the following loss function:</p><formula xml:id="formula_3">min Gc H(X s ||G c (X t )) + H(M s (X s )||M s (G c (X t ))) H(X s ||G c (X s )) + H(M s (X s )||M s (G c (X s ))),<label>(3)</label></formula><p>where H denotes the Cross entropy. The loss function in <ref type="formula" target="#formula_3">(3)</ref> encourages the data calibrator for domain adaption while keeping the performance in source domain. In this work, the data calibrator is set as G c = I + G c , i.e. only the perturbation is learned by the calibrator. However, as the target information is blind, minimizing (3) is difficult and another method is needed for training the calibrator G c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Adversarial Domain Adaptation with Proposed Calibrator</head><p>In this work, we extend the traditional adversarial domain adaption methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b34">35]</ref> and train the proposed calibrator via adversarial learning instead of minimizing <ref type="bibr" target="#b2">(3)</ref>.</p><p>Traditional adversarial domain adaptation methods play a adversarial game between the target classifier F t and feature discriminator D f eat . Because they update weight parameters of F t to maximize the confusion loss of domain discriminators, the resulted adapted models lack the flexibility of adjusting to new domains after being deployed and are under the risk of sacrificing source domain performance.</p><p>On contrast, the basic idea of our extended adversarial domain adaption method is that let there be pixel level domain discriminator D pixel and feature level domain discriminator D f eat . And let a data calibrator modify images such that domain discriminators D pixel can no longer distinguish between G c (X t ) and X s nor between G c (X s ) and X s . Meanwhile, the corresponding features of calibrated images are also confusing D f eat such that the feature level discriminator can no longer distinguish between M s (G c (X s )) and M s (X s ) nor between M s (G c (X t )) and M s (X s ). After the calibrator is trained, inputs are fed to the calibrator before fed to the model, as shown in the testing phase at <ref type="figure">Figure 4</ref>.</p><p>As shown in <ref type="figure">Figure 4</ref>, the training of the proposed method needs a trained source classifier F s . Let the source classifier F s be trained by the following loss function:</p><formula xml:id="formula_4">L source (f S , X S , Y S ) = −E (xs,ys)∼(X S ,Y S ) K k=1 1 [k=ys] log σ f (k) S (x s ) .<label>(4)</label></formula><p>Based on the learned classifier F s , the pixel level domain discriminator D pixel and feature level domain discriminator D f eat are proposed for training the calibrator such that the pixel and feature level alignment conditions (2) is satisfied. Furthermore, in order to have a finer discrimination power among images and features from source domain and target domain, we divide the inputs of the domain discriminators into 4 groups inspired by the few shot domain adaptation <ref type="bibr" target="#b20">[21]</ref>.</p><p>These four groups (G i ,i=1,2,3,4) are defined as as follows: G 1 represents source domain images X s , G 2 represents target domain images X t . Therefore, learning to distinguish images and features from G 1 and G 2 encourages the domain discriminators to learn the distributions of source domain and target domain. Additionally, calibrated source images G c (X s ) are defined to belong to G 3 and calibrated target images G c (X t ) are defined to belong to G 4 as to provide learning signal for the adversarial game. Let y Gi , i = 1, 2, 3, 4 be the group labels for each group.</p><p>Feature Level Discriminator. The feature level discriminator aims to discriminate feature level distribution M s (G i ). Its objective is to minimize categorical cross entropy loss as following:  <ref type="figure">Figure 4</ref>. Training, testing phase and data calibrator architecture. In the training phase, the pixel level discriminators and the feature space discriminator try to discriminate images to 4 groups while the data calibrator tries to fool both discriminators to treat calibrated images as the source images. In the testing phase, the deployed model takes calibrated images as inputs. The architecture for the data calibrator consists of down sampling layers, up sampling layers and skip connections.</p><formula xml:id="formula_5">L f eat−D = −E 4 i=1 y Gi log(D f eat (M (G i ))) ,<label>(5)</label></formula><p>In our work, the feature level discriminator D f eat is a simple neural network with only two fully connected layers. During training, the feature level discriminator learns to discriminate features distribution of M s (G i ). Pixel Level Discriminator. The limitation of using only feature level discriminator is that feature level discriminator cannot fully capture the information in the pixel level after images are transformed via pooling layers and strided convolutional layers of the model. Thus, following the original idea of <ref type="bibr" target="#b32">[33]</ref>, a pixel level discriminator D pixel is added to learn pixel level distribution of G i by following objective function:</p><formula xml:id="formula_6">L pixel−D = −E 4 i=1 y Gi log(D pixel (G i )) .<label>(6)</label></formula><p>The pixel level discriminator D pixel shares the same architecture as the feature level discriminator D f eat , i.e. a two layer fully connected network. The biggest challenge for the pixel level discriminator D pixel is its tendency of over-fitting to the training set. From our observations in experiments, the validation accuracy starts going down when the training loss for the pixel discriminator gets very low,. Indeed, if the calibrator is optimized towards to a pixel level discriminator that overfits, it looses the generalization power. Therefore, we apply following tricks to the inputs of pixel level discriminator to prevent it from overfitting: (1) A image patch is randomly taken from the image. (2) The pixels of the patch is randomly shuffled in the spatial axis. By applying the above two tricks, the overfitting is mitigated. Data Calibrator. The data calibrator's goal is to fool both the pixel level discriminator D pixel and feature level discriminator D pixel by the following loss function:</p><formula xml:id="formula_7">L Calibrator = −E[y G1 log(D f eat (M s (G 3 ))) +y G1 log(D f eat (M s (G 4 ))) +y G1 log(D pixel (G 3 )) +y G1 log(D pixel (G 4 ))],<label>(7)</label></formula><p>from which the learned calibrator is expected to learn knowledge in source and target domain and satisfies <ref type="bibr" target="#b1">(2)</ref>. The total training loss of our data calibrator can be divided into two parts. When the calibrator tries to fool domain discriminators to treat G 3 as G 1 , the calibrator tends to approximate the identity mapping. On contrast, when the calibrator tries to fool domain discriminators to treat G 4 as G 1 , the calibrator is to calibrate target domain images to mitigate the domain shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>MNIST to USPS USPS to MNIST SVHN to MNIST Average Acc. ADDN <ref type="bibr" target="#b34">[35]</ref> 90.1 95. The ResNet generator <ref type="bibr" target="#b14">[15]</ref> is used as the architecture of the calibrator for digits and GTA5 to CityScapes experiments. It consists of downsampling layers, upsampling layers and skip connections, as shown in <ref type="figure">Figure 4</ref>. It is noted that the performance does not simply get better when the calibrator network is getting larger. However, reducing the width can improve training as it is believed that it prevents the data calibrator from overfitting when the training data is not sufficient. Additionally, applying L ∞ norm constrain to the output of the data calibrator plays an important role in GTA5 to CityScapes adaptation. We will give a more detailed discussion on this constrain in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation and Results</head><p>In this section, we evaluate our method under unsupervised domain adaptation setting on digits and driving scene semantic segmentation tasks.</p><p>Digits We evaluate our method on three commonly used digits datasets: MNIST <ref type="bibr" target="#b16">[17]</ref>, USPS, and SVHN <ref type="bibr" target="#b21">[22]</ref>. We use the same data processing and LeNet architecture as Hoffman et al. <ref type="bibr" target="#b11">[12]</ref> and perform three unsupervised domain adaptation tasks: USPS to MNIST, MNIST to USPS and SVHN to MNIST. We report our results of using unstylized source images and stylized source images produced by Cy-cleGAN <ref type="bibr" target="#b46">[47]</ref> respectively.</p><p>GTA5 to CityScapes GTA5 <ref type="bibr" target="#b25">[26]</ref> is a synthetic driving scene dataset and CityScapes <ref type="bibr" target="#b3">[4]</ref> is a real world driving scene dataset. The GTA5 dataset has 24966 densely labeled RGB images of size 1914 × 1052, which contains 19 common classes with CityScapes, as we included in <ref type="table">Table 2</ref>. The CityScapes dataset contains 5000 densely labeled RGB images of size 2040 × 1016 from 27 cities. In this work, we use DRN-26 <ref type="bibr" target="#b42">[43]</ref> as the source classifier. We use the released DRN-26 model from CyCADA <ref type="bibr" target="#b11">[12]</ref> as our source classifier, which is trained in stylized GTA5 images.</p><p>All components are implemented using Pytorch. For digits experiments, source classifiers and other components are trained with the Adam optimizer with learning rate 1e-4. We use batches of 128 samples from each domain and the images are zero-centered and rescaled to <ref type="bibr">[−1, 1]</ref>. For GTA5 to CityScapes experiments, we use Adam optimizer with learning rate 1e-4 with batch size 6. We use same LeNet architecture as CyCADA for all digits experiments and DRN-26 <ref type="bibr" target="#b42">[43]</ref> for GTA5 to CityScapes task. Our best results are obtained within 50 epochs for digits and within 10 epochs for GTA5 to CityScapes.</p><p>Details about other components such as architecture of the data calibrator and domain discriminators can be found at Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Digits Experiments</head><p>As we show in <ref type="figure" target="#fig_1">Figure 3</ref>, the learnt representation of source classifier is not as bad as we thought. To prove that, we show that without training a new classifier or using stylized source images produced by GANs, we can just use the source classifier trained in the source domain and train a data calibrator to modify the images to fit the source classifier's representation. As we show in <ref type="table" target="#tab_0">Table 1</ref>, using data calibrator alone can outperform previous methods in average accuracy. For difficult task such as SVHN to MNIST, we can further boost our performance by using stylized source images <ref type="bibr" target="#b46">[47]</ref> as source domain, resulting in 7% performance improvement compared to CyCADA, another method that leverages stylized source images for unsupervised domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Trade off Among Domains</head><p>As we discuss in Section 1, existing methods omit to show the trade off between source domain performance and target domain performance. In this subsection, we show that many existing methods have poor source and target domain performance trade off. We use the released code from Cy-CADA <ref type="bibr" target="#b11">[12]</ref>,ADDA <ref type="bibr" target="#b34">[35]</ref> and MCD <ref type="bibr" target="#b28">[29]</ref>, follow their setting and train their adapted models to get similar reported target domain performance. We then test their adapted model on the source domain and target domain, report the performance before domain adaptation, after domain adaptation. We observe from <ref type="figure">Figure 2</ref> that, while ADDA has  <ref type="table">Table 2</ref>. Adaptation between GTA5 and CityScapes. Source only shows results of DRN-26 <ref type="bibr" target="#b42">[43]</ref> trained in GTA5 and tested in CityScapes. Target only shows results of DRN-26 trained in CityScapes and tested in CityScapes. Our method outperforms CyCADA in mean IoU, freqency weighted IoU and pixel accuracy. In particular, our frequency weighted IoU is 2.7% better than CyCADA. close performance at USPS to MNIST as ours in the target domain, but its source domain performance is 5% lower than ours. CyCADA has a lot higher target domain performance compared to ADDA, however, it sacrifices source domain performance significantly. MCD is better than the other two in performance trade off, but it uses a baseline that has over-parameterized fully connected layers and does not converge well when we replace their backbone with the same LeNet architecture other approaches and ours use. While our method can be further improved by using GAN generated images as source domain, using the data calibrator alone without stylized images can already surpass these methods in both source domain performance and target domain performance as indicated by <ref type="figure">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">GTA5 to Cityscapes</head><p>GTA5 to Cityscapes is a unsupervised domain adaptation task that is closer to real world setting. Compared to classification task, segmentation task is more challenging because that finer domain adaptation methods are required to mitigate domain shift in pixel levels.</p><p>As shown in <ref type="table" target="#tab_0">Table 1</ref>, our method has better results in all three commonly used metrics such as mIoU, fwIoU, and pixel accuracy. In particular, our fwIoU is 2.7% better than CyCADA. In <ref type="figure" target="#fig_3">Figure 5</ref>, we visualize our semantic segmentation results. From (s-b) to two rows at (t-b), we observe the performance degradation brought by the domain shift. (s-c) and (t-c) shows the segmentation results produced by our method. Our method largely mitigates the performance degradation in target domain as well as maintaining source domain performance. Because we improve the accuracy of cars by a large margin, the visualization for cars are quite close to the ground truth annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>This section is organized as following: Section 5.1 focuses on the analysis of calibrated images in the frequency domain. In Section 5.2, we discuss the connection between adversarial attack and domain adaptation. In Section 5.3, deployment of the data calibrator will be discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Fourier Perspective</head><p>We use Fast Fourier Transform(FFT) to show images before and after adding calibration. It can be seen in <ref type="figure" target="#fig_5">Figure 6</ref> that the high frequency information is decreased after images are added with the output of our data calibrator. High frequency information is often related to textures that varies significantly across domains. Yin et al. <ref type="bibr" target="#b41">[42]</ref> demonstrates that naturally trained models are biased towards high frequency information, which makes models suffer from high frequency noise. Our method might help remove these high frequency information from images thus mitigating the domain shift problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Connection to Adversarial Attack</head><p>Compared to other methods that train classifiers to adapt to target domains, in our domain adaptation framework, once trained in the source domain, the source classifier is not updated and we fully rely on the representation learnt in the source domain to perform tasks in the target domain. Thus the additive calibration produced by our data calibrator needs to figure out how to transform target domain images to a form that better fits the source classifier's representation.</p><p>But what does it mean by modifying target domain images to better fit the source classifier's representation? We first hypothesize that there are two candidate explanations of what the data calibrator does: (1) the data calibrator acts as a style transfer GAN that converts the style of target domain images to source domain images's thus achieve domain adaptation. (2) the data calibrator learns to manipulate non-robust features that are useful to neural networks but are intriguing to human <ref type="bibr" target="#b13">[14]</ref>. Our data calibrator might learn to suppress these non-robust features thus mitigate the issue brought by the domain shift.</p><p>As can be observed from <ref type="figure" target="#fig_5">Figure 6</ref>, the images modified by our calibrator do not change their appearance in the way the style transfer GAN usually does. We also follow the convention of adversarial attack <ref type="bibr" target="#b8">[9]</ref> to limit L ∞ of the calibration and provide the plot in Appendix. Our best result in <ref type="table">Table 2</ref> is obtained by limiting the L ∞ of calibration to 0.01, so small that a human might not be able to tell. Essentially, our data calibrator is trained to produce a perturbation that fools the domain discriminators with human imperceivbale perturbation, which is very similar to the behavior of adversarial attacks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b8">9]</ref>. This suggests that our data calibrator is not performing style transfer but leveraging intriguing hints to mitigate the domain shift problem. Our method suggests that there is a potential connection between adversarial attack and domain adaptation and our results should be interesting to both research community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Calibrator for Deployment</head><p>As we discuss in Section 1, one of the limitations of existing domain adaptation methods is the lack of flexibility. As far as we know, most existing domain adaptation methods will require finetune the deployed model when there is a new target domain. However, the deployed model is usually compressed and stored in specialized hardwares thus adapting the deployed models to new domains requires a long, costly process and might not be fast enough for timesensitive applications.</p><p>On contrast, our method does not require updating the deployed model and has greater flexibility when adapting to a new domain is desired. Additionally, the overhead brought by the calibrator is moderate. We tested the number of parameters of the classifier and data calibrator. For digits experiment, the number of parameter of LeNet is 3.1 millions while the data calibrator has 0.18 millions of parameters, only 5.8% compared to the model. For GTA5 to CityScapes experiments, the DRN-26 model has 20.6 millions of parameters while our data calibrator only has 0.05 millions of parameters, only 0.24% compared to the DRN-26 model.</p><p>We thereby conclude that the proposed data calibrator is light-weight compared to the deployed model and does not bring too much overhead during deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In summary, the proposed method not only achieves state-of -the-art performance in unsupervised domain adaptation for digits classification task and driving scene semantic segmentation task, but also be suitable for deployed models to adapt to new domains without the need to update their weights. This approach provides a feasible solution for online unsupervised domain adaptation. While the community is trying to build a monolithic model that can work across as many domains as possible, the separable approach we propose is also worth investigating.  ℒ " Ball vs.Semantic Segmentation Performance ℒ " Ball of Calibration <ref type="figure">Figure 8</ref>. Performance vs. L∞ ball of calibration produced by the calibrator. We show that with calibration that is imperceivable to human, we can achieve state-of-the-art domain adaptation performance. Calibration with large L∞ ball has worse performance, probably due to overfitting or models' poor rosbutness to pixel modification in general  <ref type="table">Table 3</ref>. Overhead of data calibrator. We show that our calibrator is light-weight both in terms of number of parameters and flops. Even for network as tiny as LeNet, the calibrator is small compared to it</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Concept Illustration. (a) The source classifier in labeled source domain. (b) The source classifier in unlabeled target domain. (c) Existing methods that are developed to learn domaininvariant features. (d) In real world, the testing set consists of both source domain images and target domain images. (e) The proposed method keeps the representation of source classifier and calibrates target images to fit the source classifier's representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>SVHN to MNIST task. Source classifier LeNet is trained in SVHN. (a) The source classifier's prediction on SVHN. (b) The source classifier's prediction on MNIST. (c) The source classifier's prediction on SVHN, with data calibrator. (d) The source classifier's prediction on MNIST, with data calibrator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Number of parameters:~0.25%*N N</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>(t-a) Test Image(CityScapes) (t-b) Source Prediction (t-c) Our Prediction (t-d) Ground Truth (s-a) Test Image(GTA5) (s-b) Source Prediction (s-c) Our Prediction (s-d) Ground Truth Semantic Segmentation results for GTA5 to CityScapes. (s-a) Test images from GTA5. (s-b) Predictions from the model trained in GTA5. (s-c) Our prediction. (s-d) Ground truth annotations for test images. (t-a) Test images from CityScapes. (t-b) Predictions from the model trained in GTA5. (t-c) Predictions from our method. (t-d) Ground truth annotations for test images. .2 79.9 24.6 16.2 32.8 33.1 31.8 81.7 29.2 66.3 63.0 14.3 81.8 21.0 26.5 8.5 16.7 24.0 40.5 75.1 84.0 Target 97.3 79.8 88.6 32.5 48.2 56.3 63.6 73.3 89.0 58.9 93.0 78.2 55.2 92.2 45.0 67.3 39.6 49.9 73.6 67.4 89.6 94.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Images from SVHN to MNIST adaptation Images before and after being calibrated and their view in the frequency domain. The appearance of images are not changed much unlike what style transfer GANs do. In frequency domain, high frequency information is reduced.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Network architectures used for digits experiments . We show the source classifier Fs, proposed calibrator Gc, pixel level domain discriminator D pixel and feature level domain discriminator D f eat .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results on digits datasets for unsupervised domain adaptation. Our method achieves state-of-the-art performance without using stylized source images. Our method can be further improved by using stylized source images.</figDesc><table><row><cell>2</cell><cell>80.1</cell><cell>88.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Adapting to continuously shifting domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Bobu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised pixellevel domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3722" to="3731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Progressive feature alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="627" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L Imagenet Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>ieee conf comput vis pattern recognit</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.7495</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Eie: efficient inference engine on compressed deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ardavan</forename><surname>Pedram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="243" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03213</idno>
		<title level="m">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adversarial examples are not bugs, they are features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02175</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02791</idno>
		<title level="m">Learning transferable features with deep adaptation networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Few-shot adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeid</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quinn</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed</forename><surname>Iranmanesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianfranco</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6670" to="6680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Covariate shift and local learning by distribution matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><surname>Quiñonero-Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10811</idno>
		<title level="m">Do imagenet classifiers generalize to imagenet? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Admm-nn: An algorithm-hardware co-design framework of dnns using alternating direction methods of multipliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ao</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaokai</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehai</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="925" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Stephan R Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">From source to target and back: symmetric bi-directional adaptive gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8099" to="8108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4068" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Incremental adversarial domain adaptation for continually changing environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Wulfmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International conference on robotics and automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning semantic representations for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5419" to="5428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Topology attack and defense for graph neural networks: An optimization perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongge</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsui-Wei</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Structured adversarial attack: Towards general implementation and better interpretability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Erdogmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adversarial robustness vs. model compression, or both?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Shaokai Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Henrik</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Lambrechts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aojun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A fourier perspective on model robustness in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><forename type="middle">Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gilmer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08988</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A systematic dnn weight pruning framework using alternating direction method of multipliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaokai</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wujie</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makan</forename><surname>Fardad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On the design of black-box adversarial examples by leveraging gradient-free optimization and operator splitting method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nghia</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavya</forename><surname>Kailkhura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="121" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Incremental network quantization: Towards lossless cnns with low-precision weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03044</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
