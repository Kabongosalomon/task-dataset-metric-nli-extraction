<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX 1 Deeply Supervised Salient Object Detection with Short Connections</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX 1 Deeply Supervised Salient Object Detection with Short Connections</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Salient object detection</term>
					<term>short connection</term>
					<term>deeply supervised network</term>
					<term>semantic segmentation</term>
					<term>edge detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent progress on salient object detection is substantial, benefiting mostly from the explosive development of Convolutional Neural Networks (CNNs). Semantic segmentation and salient object detection algorithms developed lately have been mostly based on Fully Convolutional Neural Networks (FCNs). There is still a large room for improvement over the generic FCN models that do not explicitly deal with the scale-space problem. Holistically-Nested Edge Detector (HED) provides a skip-layer structure with deep supervision for edge and boundary detection, but the performance gain of HED on saliency detection is not obvious. In this paper, we propose a new salient object detection method by introducing short connections to the skip-layer structures within the HED architecture. Our framework takes full advantage of multi-level and multi-scale features extracted from FCNs, providing more advanced representations at each layer, a property that is critically needed to perform segment detection. Our method produces state-of-theart results on 5 widely tested salient object detection benchmarks, with advantages in terms of efficiency (0.08 seconds per image), effectiveness, and simplicity over the existing algorithms. Beyond that, we conduct an exhaustive analysis on the role of training data on performance. Our experimental results provide a more reasonable and powerful training set for future research and fair comparisons.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T HE goal in salient object detection is to identify the most visually distinctive objects or regions in an image and then segment them out from the background. Different from other segmentation-like tasks, such as semantic segmentation, salient object detection pays more attention to very few objects that are interesting and attractive. Such a useful property allows salient object detection to commonly serve as the first step to a variety of computer vision applications including image and video compression <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, image segmentation <ref type="bibr" target="#b3">[4]</ref>, content-aware image editing <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, object recognition <ref type="bibr" target="#b6">[7]</ref>, weakly supervsied segmantic segmentation <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref> visual tracking <ref type="bibr" target="#b11">[12]</ref>, nonphoto-realist rendering <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, photo synthesis <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, information discovery <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, image retrieval <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, action recognition <ref type="bibr" target="#b20">[21]</ref> etc.</p><p>Earlier salient object detection methods were mainly inspired by cognitive studies of visual attention <ref type="bibr" target="#b21">[22]</ref> where contrast plays the most important role in saliency detection. Taking this fact into consideration, various hand-crafted features have been designed, employing either global or local cues (See <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> for reviews). However, as these handcrafted features are based on the prior knowledge of existing datasets, they cannot be extended to be successfully useful in all cases. Although some works have attempted to develop different schemes to combine these features rather than utilizing individual ones, the resulting saliency maps are still far away from being satisfactory, specially when encountering complex and cluttered scenes. To overcome • Q. <ref type="bibr">Hou</ref> • A preliminary version of this work appeared at CVPR <ref type="bibr" target="#b0">[1]</ref>. The source code are publicly available via our project page: http://mmcheng.net/dss/.</p><p>the drawbacks caused by human priors, learning based methods (e.g. <ref type="bibr" target="#b24">[25]</ref>) appear to better integrate different types of features to improve the generalization ability. Nevertheless, because many fusion details are designed manually, the enriched feature representations still suffer from low contrast and fail to detect salient objects in cluttered scenes.</p><p>In a variety of computer vision tasks, such as image classification <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, semantic segmentation <ref type="bibr" target="#b28">[29]</ref>, edge detection <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref>, object detection <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, and pedestrian detection <ref type="bibr" target="#b32">[33]</ref>, convolutional neural networks (CNNs) <ref type="bibr" target="#b33">[34]</ref> have successfully broken the limits of traditional handcrafted features. The emergence of fully convolutional neural networks (FCNs) <ref type="bibr" target="#b28">[29]</ref> have further boosted the development of these research areas, providing a more principled learning method. Such an end-to-end learning tool also motivates recent research efforts of using FCNs for salient object detection <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Benefiting from the enormous amount of parameters in FCNs, a large margin of performance gain has been made compared to previous approaches. The holistically-nested edge detector (HED) <ref type="bibr" target="#b25">[26]</ref> model, which explicitly deals with the scale space problem, has led to large improvements over generic FCN models in the context of edge detection. Though the mechanism of fusing the multi-level features extracted from different scales provides a much more natural way to edge detection, it is incompetent to do segmentation related tasks. Edge detection is an easier task since it does not rely too much on high-level semantic feature representations. This explains why skip-layer structure with deep supervision in the HED model does not lead to obvious performance gain for saliency detection. Experimental results also support this statement as shown in <ref type="figure">Fig. 1</ref>.</p><p>In this paper, we focus on skip-layer structure with deep supervision. Instead of simply fusing the multi-level features extracted from different scales, we consider such a problem in a top-down view. As demonstrated in <ref type="figure">Fig. 1</ref>  Proposed HED-based Proposed HED-based <ref type="figure">Fig. 1</ref>. Visual comparison of saliency maps produced by the HED-based method <ref type="bibr" target="#b25">[26]</ref> and ours. Though saliency maps produced by deeper (4-6) side output (s-out) look similar, because of the introduced short connections, each shallower (1-3) side output can generate satisfactory saliency maps and hence a better output result.</p><p>we observe that 1) deeper side outputs encode high-level semantic knowledge and hence can better locate where the salient objects are. However, due to the down-sampling operations in FCNs, the predicted maps are normally with irregular shapes especially when the input image is complex and cluttered (see the bottle image), and 2) shallower side outputs capture rich spatial information. They are capable of successfully highlighting the boundaries of those salient objects in spite of the resulting messy prediction maps. Based on these phenomenons, an intuitive idea for yielding better saliency maps is to reasonably combine these multilevel features. This motivates us to develop a new method for salient object detection by introducing short connections to the skip-layer structure within the HED <ref type="bibr" target="#b25">[26]</ref> architecture. By having a series of short connections from deeper side outputs to the shallower ones, our new framework offers two advantages:</p><p>1) high-level features can be transformed to shallower side-output layers and thus can help them better locate the most salient region, and 2) shallower side-output layers can learn rich lowlevel features that can help refine the sparse and irregular prediction maps from deeper side-output layers.</p><p>By combining features from different levels, the resulting architecture provides rich multi-scale feature maps at each layer, a property that is essentially needed to do efficient salient object detection. Our approach is fully convolutional and no other prior information such as superpixels is needed. It takes only 0.08s to produce a prediction map with resolution of 300 × 400 pixels. Other than improving the state-of-the-art results, we conduct exhaustive analysis on the behavior of different training sets as there is no universal training set for a fair comparison in the salient object detection field. Our goal is to offer a more unified training set and meanwhile build a fair benchmarking environment for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Over the past two decades, an extremely rich set of saliency detection methods have been developed. The majority of salient object detection methods are based on hand-crafted local features <ref type="bibr" target="#b37">[38]</ref>- <ref type="bibr" target="#b39">[40]</ref>, global features <ref type="bibr" target="#b40">[41]</ref>- <ref type="bibr" target="#b42">[43]</ref>, or both <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b43">[44]</ref>. A complete survey of these methods is beyond the scope of this paper and we refer the readers to recent survey papers <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b44">[45]</ref> for details. Here, we mainly focus on discussing recent salient object detection methods based on deep learning architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CNN-Based Saliency Models</head><p>Compared with traditional methods that use hand-crafted features, CNN-based methods have refreshed all the previous state-of-the-art records in nearly every sub-field of computer vision, including salient object detection. In <ref type="bibr">[</ref>  <ref type="bibr" target="#b36">[37]</ref>, (b) FCN-8s <ref type="bibr" target="#b28">[29]</ref> (c) HED <ref type="bibr" target="#b25">[26]</ref>, (d) and (e) different patterns of our proposed architecture. As can be seen, a series of short connections are introduced in our architecture for combining the advantages of both deeper layers and shallower layers. More interestingly, the last one can be viewed as a generalized version of all the formers. saliency of each query region. Liu et al. <ref type="bibr" target="#b35">[36]</ref> designed a twostage deep network, in which a coarse prediction map was produced, followed by a recurrent CNN to refine the details of the prediction map hierarchically and progressively. In <ref type="bibr" target="#b34">[35]</ref>, a deep contrast network was proposed by leveraging the contrast information of the input images. It combined a pixel-level fully convolutional stream and a segmentwise spatial pooling stream. A fully connected conditional random field (CRF) is also used for further refining the prediction maps from the contrast network. In <ref type="bibr" target="#b50">[51]</ref>, Wang et al. proposed to leverage the advantages of recurrent fully convolutional networks. By doing so, their recurrent fully convolutional network allowed them to continuously refine previous prediction maps by correcting prediction errors. A pre-training strategy using semantic segmentation data is exploited for extracting generic representations of salient objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Skip-Layer Structures</head><p>Very recently, great progress has been made in segment detection because of CNNs and their flexible architectures. Of these versatile structures, skip-layer structures have been widely accepted by most researchers owning to their capability of fusing multi-level and multi-scale features. Early-stage skip-layer structures such as Hypercolumn <ref type="bibr" target="#b36">[37]</ref> and DCL <ref type="bibr" target="#b34">[35]</ref> have made breakthroughs in their respective fields. They, however, only simply fuse the skip layers with different scales for more advanced feature representation building as shown in <ref type="figure">Fig. 2(a)</ref>. Differently, FCN-like structures <ref type="bibr" target="#b28">[29]</ref> (see <ref type="figure">Fig. 2</ref>(b)) considered a better way to utilize multi-level features, gradually fusing the features from upper layers to lower ones. In <ref type="bibr" target="#b25">[26]</ref>, Xie and Tu proposed a scheme with deep supervision for each side output (skip layer). Other than fusing all skip layers together, a series of side losses are added after each side output for preserving more details of the edge information. <ref type="figure">Fig. 2</ref>(c) shows a simplified version of these architecture.</p><p>Despite the fact that multi-level and multi-scale features have been taken into account and significant progress has been made by these developments very recently, there is still a large room for improvement over the generic CNN models that do not explicitly deal with the scale-space problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DEEP SUPERVISION WITH SHORT CONNEC-TIONS</head><p>This section describes our approach and some implementation details. Before that, let us first take a look at the observations.  <ref type="figure">Fig. 3</ref>. The proposed network architecture. Our architecture is based on VGGNet <ref type="bibr" target="#b27">[28]</ref> for better comparison with previous CNN-based methods. As there are totally 6 different scales in VGGNet, 6 side outputs are introduced, each of which is represented by different colors. Besides the side loss for each side output, a fusion loss is employed for capturing features of different levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Observations</head><p>As pointed out in most previous works, a good salient object detection network should be deep enough such that multi-level features can be learned. Further, it should have multiple stages with different strides so as to learn more inherent features from different scales. A good candidate for such requirements might be the HED network <ref type="bibr" target="#b25">[26]</ref>, in which a series of side-output layers are added after the last convolutional layer of each stage in the VGGNet <ref type="bibr" target="#b27">[28]</ref>. However, experimental results show that this architecture is not suitable for salient object detection. <ref type="figure">Fig. 1</ref> provides such an illustration. The reasons for this phenomenon are twofold. On the one hand, saliency detection, requiring homogeneous regions, is quite different from edge detection that demands a special treatment. A good saliency detection algorithm should be capable of extracting the most visually distinctive objects and regions from an image instead of simple edge information. On the other hand, the features generated from lower stages are too convoluted and the saliency maps obtained from the deeper side-output layers are short of regularity.</p><p>To overcome the aforementioned problems, we propose a top-down method to reasonably combine both low-level and high-level features for accurate saliency detection. The following subsections are dedicated to a detailed description of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">HED-based saliency detection</head><p>To better understand our proposed approach, we start out with the standard HED architecture <ref type="bibr" target="#b25">[26]</ref> as well as its extended version, a special case of this work, for salient object detection and gradually move on to our proposed architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">HED architecture</head><p>In the HED architecture <ref type="bibr" target="#b25">[26]</ref>, 5 side outputs are introduced, each of which is directly connected to the last convolutional layer of each stage. Let T = {(X n , Z n ), n = 1, . . . , N } denote the training data set, where X n = {x (n) j , j = 1, . . . , |X n |} is the input image and Z n = {z (n) j , j = 1, . . . , |X n |}, z (n) j ∈ [0, 1] denotes the corresponding continuous ground truth saliency map for X n . In the sequel, we omit the subscript n for notational convenience since we assume the inputs are all independent of one another. We denote the collection of all standard network layer parameters as W. Without loss of generality, we further suppose that there are totally M side outputs. Each side output is associated with a classifier, in which the corresponding weights can be represented by w = (w <ref type="bibr" target="#b0">(1)</ref> , w <ref type="bibr" target="#b1">(2)</ref> , . . . , w (M ) ). Thus, the side objective function of HED can be given by</p><formula xml:id="formula_0">L side (W, w) = M m=1 α m l (m) side W, w (m) ,<label>(1)</label></formula><p>where α m is the weight of the mth side loss and l (m) side denotes the image-level class-balanced cross-entropy loss function <ref type="bibr" target="#b25">[26]</ref> for the mth side output. Besides, a weightedfusion layer is added to better capture the advantage of each side output. The fusion loss at the fusion layer can be expressed as</p><formula xml:id="formula_1">L fuse (W, w, f ) = σ Z, h( M m=1 f m A (m) side ) ,<label>(2)</label></formula><p>where f = (f 1 , . . . , f M ) is the fusion weights, A (m) side are activations of the mth side output, h(·) denotes the sigmoid function, and σ(·, ·) denotes the distance between the ground truth map and the fused predictions, which is set to be image-level class-balanced cross-entropy loss <ref type="bibr" target="#b25">[26]</ref>. Therefore, the final loss function is given by</p><formula xml:id="formula_2">L final W, w, f ) = L fuse W, w, f ) + L side W, w).<label>(3)</label></formula><p>HED connects each side output to the last convolutional layer in each stage of the VGGNet <ref type="bibr" target="#b27">[28]</ref>, respectively conv1 2, conv2 2, conv3 3, conv4 3, conv5 3. Each side output is composed of a one-channel convolutional layer with the kernel size 1 × 1 followed by an up-sampling layer for learning edge information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Enhanced HED architecture</head><p>In this part, we extend the HED architecture for salient object detection. During our experiments, we observe that deeper layers can better locate the most salient regions, so based on the architecture of HED we connect another side output to the last pooling layer (pool5) in VGGNet <ref type="bibr" target="#b27">[28]</ref>. Besides, since salient object detection is a more difficult task than edge detection, we add two another convolutional layers with different filter channels and spatial sizes in each side output, which can be found in <ref type="figure">Fig. 4</ref>. We use the same bilinear interpolation operation as in HED for up-sampling. We also use a standard cross-entropy loss and compute the loss function over all pixels in a training image X = {x j , j = 1, . . . , |X|} and saliency map pool5 512, 7 × 7 512, 7 × 7 1, 1 × 1 <ref type="figure">Fig. 4</ref>. Details of each side output. (n, k × k) means that the number of channels and the kernel size are n and k, respectively. "Layer" means which layer the corresponding side output is connected to. "1" "2" and "3" represent three convolutional layers that are used in each side output. Note that the first two convolutional layers in each side output are followed by a ReLU layer for nonlinear transformation.</p><formula xml:id="formula_3">Z = {z j , j = 1, . . . , |Z|}.</formula><p>Our loss function can be defined as follows:</p><formula xml:id="formula_4">l (m) side (W,ŵ (m) ) = − zj ∈Z z j log Pr z j = 1|X; W,ŵ (m) + (1 − z j ) log Pr z j = 0|X; W,ŵ (m) ,<label>(4)</label></formula><p>where Pr z j = 1|X; W,ŵ (m) represents the probability of the activation value at location j in the mth side output, which can be computed by h(a</p><formula xml:id="formula_5">(m) j ), whereÂ (m) side = {a (m) j , j = 1, .</formula><p>. . , |X|} are activations of the mth side output. Similar to <ref type="bibr" target="#b25">[26]</ref>, we add a weighted-fusion layer to connect each side activation. The loss function at the fusion layer in our case can be represented bŷ</p><formula xml:id="formula_6">L fuse (W,ŵ, f ) =σ Z, M m=1 f mÂ (m) side ,<label>(5)</label></formula><p>whereÂ (m) side is the new activations of the mth side output 1 , M = M + 1, andσ(·, ·) represents the distance between the ground truth map and the new fused predictions, which has the same form as in Eqn. <ref type="bibr" target="#b3">(4)</ref>.</p><p>A comparison of salient object detection results between the original HED and enhanced HED is shown in <ref type="figure" target="#fig_5">Fig. 7</ref>. It can be easily found that a large margin of about 3% improvement has been achieved. In spite of such improvement, as shown in <ref type="figure">Fig. 1</ref>, the saliency maps from shallower side outputs still look messy and the deeper side outputs produce irregular results as well. In addition, the deeper side outputs can indeed locate the salient objects, but some detailed information is still lost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Short connections</head><p>The insight of our approach is that deeper side outputs are capable of finding the location of salient regions but at the expense of the loss of details, while shallower ones focus on low-level features but are short of global information. These phenomenons inspire us to utilize the following way to appropriately combine different side outputs such that the most visually distinctive objects can be extracted. <ref type="bibr" target="#b0">1</ref>. We add a new side output in our enhanced HED architecture. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Formulation</head><p>Mathematically, our new side activationsR (m) side at the mth side output can be given bỹ</p><formula xml:id="formula_7">R (m) side =    M i=m+1 r m iR (i) side +Â (m) side , for m = 1, . . . , 5 A (m) side , for m = 6 (6) where r m</formula><p>i is the weight of short connection from side output i to side output m (i &gt; m). We can drop out some short connections by directly setting r m i to 0. The new side loss function and fusion loss function can be respectively represented bỹ</p><formula xml:id="formula_8">L side (W,w, r) =M m=1 α ml (m) side W,w (m) , r<label>(7)</label></formula><p>andL</p><formula xml:id="formula_9">fuse (W,w, f , r) =σ Z, M m=1 f mR (m) side ,<label>(8)</label></formula><p>where r = {r m i }, i &gt; m. Note that this timel (m) side represents the standard cross-entropy loss which we have defined in Eqn. <ref type="bibr" target="#b3">(4)</ref>. Thus, our new final loss function can be written as</p><formula xml:id="formula_10">L final W,w, f , r) =L fuse W,w, f , r) +L side W,w, r).<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Construction</head><p>The backbone of our new architecture is the enhanced HED which has been described in Section 3.2.2. <ref type="figure" target="#fig_2">Fig. 5</ref> illustrates how to construct short connections from side output 4 to side output 2. The score maps in side outputs 3 and 4 are first upsampled by simple bilinear interpolation and then concatenated to the original score map in side output 2. The hyper-parameters of bilinear interpolation can be derived according to the context. As salient object detection is a class-agnostic task, we further weight the foregoing score maps which have been enclosed by a dashed bounding box in <ref type="figure" target="#fig_2">Fig. 5</ref> and introduce another 1 × 1 convolutional layer as the new score map of side output 2. A similar approach can be used for side outputs to which more than one short connection is connected. For instance, let us assume that 3 short connections are connected to side output 2. There would be 4 score maps being concatenated together within the dashed bounding box. Our architecture can be functionally considered as two closely connected stages, which we call saliency locating stage and details refinement stage, respectively. The main focus of saliency locating stage is on looking for the most salient regions in a given image. For details refinement stage, we introduce a top-down method, a series of short connections from deeper side-output layers to shallower ones. The reason for such a consideration is that with the help of deeper side information, lower side outputs can both accurately predict the salient objects and refine the results from deeper side outputs, resulting in dense and accurate saliency maps. We further test the effectiveness of our proposed architecture by running a number of ablation experiments and showing the corresponding quantitative and visual results in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>Our network is based on the publicly available Caffe library <ref type="bibr" target="#b51">[52]</ref> and the open implementation of FCN <ref type="bibr" target="#b28">[29]</ref>. As mentioned above, we choose VGGNet <ref type="bibr" target="#b27">[28]</ref> as our pre-trained model for better comparison with other works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Inference</head><p>Although a series of short connections are introduced, the quality of the prediction maps produced by the deeper and the shallower side outputs is still unsatisfactory. Regarding this fact, during the testing phase, we adopt a more complicated combination of these side outputs. LetZ 1 , · · · ,Z 6 denote the score map of each side output, respectively. They can be computed byZ m = h(R (m) side ). Recall that h(·) in our case is the sigmoid function. Therefore, the fusion output map can be computed bỹ</p><formula xml:id="formula_11">Z fuse = h 4 m=2 f mR (m) side .<label>(10)</label></formula><p>To avoid the negative effect caused by the bad quality of the prediction map from the deepest and shallowest side outputs, we also useZ 2 ,Ẑ 3 , andẐ 4 to help further fill in the lost details. As a result, the final output map during inference can be represented bỹ</p><formula xml:id="formula_12">Z final = Mean(Z fuse ,Z 2 ,Z 3 ,Z 4 ).<label>(11)</label></formula><p>Surprisingly, we found that such a combination do help improve the results by a little margin. This is due to the fact that although the fusion output map incorporates the aggregation of each side output, some detailed information in the fusion output map is still missed. Regarding the quality of each side output map (see <ref type="figure">Fig. 1</ref>), we decide to use Eqn. <ref type="bibr" target="#b10">(11)</ref> as the final output map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Smoothing Method</head><p>Though our model can precisely find the salient objects in an image, the boundary information of the resulting saliency maps is still lost for those complex scenes. To further improve spatial coherence and quality of our saliency maps, we adopt the fully connected conditional random field (CRF) method <ref type="bibr" target="#b52">[53]</ref> as a selective layer during the inference phase. The energy function of CRF is given by</p><formula xml:id="formula_13">E(x) = i θ i (x i ) + i,j θ ij (x i , x j ),<label>(12)</label></formula><p>where x is the label prediction for pixels. To make our model more competitive, instead of directly using the predicted maps as the input of the unary term, we leverage the following unary term</p><formula xml:id="formula_14">θ i (x i ) = − logŜ i τ h(Ŝ i ) ,<label>(13)</label></formula><p>whereŜ i denotes normalized saliency value of pixel x i , h(·) is the sigmoid function, and τ is a scale parameter. The pairwise potential is defined as</p><formula xml:id="formula_15">θ ij (x i , x j ) = µ(x i , x j ) w 1 exp − p i − p j 2 2σ 2 α − I i − I j 2 2σ 2 β + w 2 exp − p i − p j 2 2σ 2 γ ,<label>(14)</label></formula><p>where µ(x i , x j ) = 1 if x i = x j and zero, otherwise. I i and p i are pixel value and position of x i , respectively. Parameters w 1 , w 2 , σ α , σ β , and σ γ control the importance of each Gaussian kernel.</p><p>In this paper, we employ a publicly available implementation of <ref type="bibr" target="#b52">[53]</ref>, called PyDenseCRF 2 . Since there are only two classes in our case, we use the inferred posterior probability of each pixel being salient as the final saliency map directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Parameters</head><p>The hyper-parameters used in this work include learning rate (1e-8), weight decay (0.0005), momentum (0.9), loss weight for each side output <ref type="bibr" target="#b0">(1)</ref>. We use full-resolution images to train our network, and the mini-batch size is set to 10. The kernel weights in newly added convolutional layers are all initialized with random numbers. Our fusion layer weights are all initialized with 0.1667 in the training phase. The parameters in the fully connected CRF are determined using cross validation on the validation set. In our experiments, τ is set to 1.05, and w 1 , w 2 , σ α , σ β , and σ γ are set to 3.0, 3.0, 60.0, 8.0, and 5.0, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS AND ANALYSES</head><p>In this section, we introduce utilized datasets and evaluation criteria and report the performance of our proposed approach. Besides, a number of ablation experiments are performed for analyzing the importance of each component of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate our approach on 5 representative datasets, including MSRA-B <ref type="bibr" target="#b42">[43]</ref>, ECSSD <ref type="bibr" target="#b53">[54]</ref>, HKU-IS <ref type="bibr" target="#b46">[47]</ref>, PASCALS <ref type="bibr" target="#b54">[55]</ref>, and SOD <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>, all of which are available online. These datasets all contain a large number of images as well as well-segmented annotations and have been widely used recently.</p><p>MSRA-B contains 5,000 images from hundreds of different categories. Because of its diversity and large quantity, MSRA-B has been one of the most widely used datasets in salient object detection literature. Most images in this dataset have only one salient object, and hence it has gradually become a standard dataset for evaluating the capability of processing simple scenes. ECSSD contains 1,000 2. https://github.com/lucasb-eyer/pydensecrf semantically meaningful but structurally complex natural images. HKU-IS is another large-scale dataset that contains more than 4000 challenging images. Most of images in this dataset have low contrast with more than one salient object. PASCALS contains 850 challenging images (each composed of several objects), all of which are chosen from the validation set of the PASCAL VOC 2010 segmentation dataset. We also evaluate our system on the SOD dataset, which is a subset of the BSDS dataset. It contains 300 images, most of which possess multiple salient objects. All of these datasets consist of ground truth human annotations.</p><p>In order to preserve the integrity of the evaluation and obtain a fair comparison with existing approaches, we utilize the same training and validation sets as in <ref type="bibr" target="#b24">[25]</ref> and test over all of the datasets using the same model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>We use three universally-agreed, standard metrics (see also <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b57">[58]</ref>) to evaluate our model including precision-recall curves, F-measure, and the mean absolute error (MAE). For a given continuous saliency map S, we convert it to a binary mask B using a threshold. Then its precision and recall are computed as precision = |B ∩ Z|/|B| and recall = |B ∩ Z|/|Z|, respectively, where | · | accumulates the non-zero entries in a mask. Averaging the precision and recall values over the saliency maps of a given dataset yields the PR curve.</p><p>To comprehensively evaluate the quality of a saliency map, the F-measure metric is used, which is defined as</p><formula xml:id="formula_16">F β = (1 + β 2 )P recision × Recall β 2 P recision + Recall .<label>(15)</label></formula><p>As suggested by previous works, we choose β 2 to be 0.3 for stressing the importance of the precision value. LetŜ andẐ denote the continuous saliency map and the ground truth that are normalized to [0, 1]. The mean absolute error (MAE) score can be computed as</p><formula xml:id="formula_17">M AE = 1 H × W H i=1 W j=1 |Ŝ(i, j) =Ẑ(i, j)|.<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Analysis</head><p>We experiment with different design options and different short connection patterns to illustrate the effectiveness of each component of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Various Short Connection Patterns</head><p>Our architecture as shown in <ref type="figure">Fig. 3</ref> is so flexible that can be regarded as the generalized model of most existing architectures, such as those depicted in <ref type="figure">Fig. 2</ref>. To better show the strength of our proposed approach, we use different network architectures as listed in <ref type="figure">Fig. 2</ref> for salient object detection. Besides the Hypercolumns architecture <ref type="bibr" target="#b36">[37]</ref> and the HED-based architecture <ref type="bibr" target="#b25">[26]</ref>, we implement three representative patterns using our proposed approach. The first one is formulated as follows, which is a similar architecture to <ref type="figure">Fig. 2(d)</ref>.   <ref type="bibr" target="#b54">[55]</ref>. (c, k × k) × n means that there are n convolutional layers with c channels and size k × k. Note that the last convolutional layer in each side output is unchanged as listed in <ref type="figure">Fig. 4</ref>. In each setting, we only modify one parameter while keeping all others unchanged so as to emphasize the importance of each chosen parameter.</p><p>The second pattern is represented as follows which is much more complex than the first one. </p><p>The quantitative results are listed in <ref type="figure" target="#fig_5">Fig. 7</ref>. As can be seen from <ref type="figure" target="#fig_5">Fig. 7</ref>, by adding another side output and two additional convolutional layers in each side output, we have a performance gain of 2.5 points in terms of F-measure. In addition, with the increase of short connections, our approach gradually achieves better performance. Although there is no performance gain obtained when Pattern 1 is used compared with the enhanced HED structure, a gain of 0.8 points can be achieved when we turn to Pattern 2. Another 0.6 points gain can also be obtained when Pattern 3 is considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Details of Side-Output Layers</head><p>We run several ablation experiments to explore the best side output settings. The detailed information of each side-output layer in each experiment has been shown in <ref type="figure">Fig. 6</ref>. We use Pattern 3 in <ref type="figure" target="#fig_5">Fig. 7</ref> as our baseline model. To highlight the importance of different parameters, we adopt the variable-controlling method that only changes one parameter at a time. Besides, all the results are tested on PASCALS dataset for fair comparison. Compared with the fourth experiment, the first one exploits more channels but the same F-measure score is obtained. This means that more channels for each side output cannot bring in additional performance gain. In the second experiment, we tried to reduce 1 convolutional layer in each side output but it turns out that such an operation decreases the performance by 1.5 points. In spite of a small decrease, it is enough to account for the importance of introducing two convolutional layers in each side output. Furthermore, we attempt to reduce the large kernel size in deeper side outputs. Similarly, this leads to a slight decrease in F-measure. All the above experiments demonstrate that the side output settings we use are reasonable and appropriate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Upsampling Operation</head><p>In our approach, we use the in-network bilinear interpolation to perform upsampling in each side output. As implemented in <ref type="bibr">[</ref>  for our side outputs with different strides. Since the prediction maps generated by deep side-output layers are not dense enough, we also try to use the "hole algorithm" to make the prediction maps in deep side outputs denser. We adopt the same technique as in <ref type="bibr" target="#b34">[35]</ref>. However, according to our experiments, using such a method yields a worse performance. We notice that as the fusion prediction map gets denser, some non-salient pixels are wrongly predicted as salient ones even though the CRF is used thereafter. The F-measure score on the validation set is decreased by nearly 1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Data Augmentation</head><p>Data augmentation has been proven to be very useful in many learning-based vision tasks. As done in most previous works, we flip all the training images horizontally, resulting in an augmented image set with twice larger than the original one. We found that such an operation further improves the performance by more than 0.5%. In addition, we also try to crop the input images to a fixed size 321×321. However, experimental results show that such an operation decrease our performance by more than 0.5 points. This may be because input images with full size contain richer information that allows our network to better capture the salient objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">Different Backbones</head><p>We also extend our work by replacing the VGGNet with ResNet-101 <ref type="bibr" target="#b58">[59]</ref> as the backbone. Taking into account the network structure of ResNet-101, we only use the bottom 5 side outputs in <ref type="figure">Fig. 4</ref>, which are connected to conv1, res2c, res3b3, res4b22, and res5c, respectively. We keep other settings unchanged. We show the results on the bottom of <ref type="figure">Fig. 10</ref>. With the same training set, there is a further onepoint improvement on each dataset in terms of F-measure score on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.6">The Proposed CRF Model</head><p>Most previous works <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b52">[53]</ref> only use the negative log likelihood as the unary term in their CRF model. Differently from them, we introduce a modulating factor that aims to give positive predictions more confidence as shown in Eqn. <ref type="bibr" target="#b12">(13)</ref>. This is reasonable as most of the predictions are correct through observing the MAE scores. In our experiments, we found that adding such a modulating factor helps little on improving the F-measure scores but is able to further reduce the MAE scores (i.e. , reduce wrong predictions) by around 0.3 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with the State-of-the-art</head><p>We compare the proposed approach with 7 recent CNNbased methods, including MDF <ref type="bibr" target="#b46">[47]</ref>, DS <ref type="bibr" target="#b59">[60]</ref>, DCL <ref type="bibr" target="#b34">[35]</ref>, ELD <ref type="bibr" target="#b49">[50]</ref>, MC <ref type="bibr" target="#b48">[49]</ref>, RFCN <ref type="bibr" target="#b50">[51]</ref>, and DHS <ref type="bibr" target="#b35">[36]</ref>. Four classical methods are also considered including RC <ref type="bibr" target="#b40">[41]</ref>, CHM <ref type="bibr" target="#b60">[61]</ref>, DSR <ref type="bibr" target="#b61">[62]</ref>, and DRFI <ref type="bibr" target="#b24">[25]</ref>, which have been proven to be the best in the benchmark study of Borji et al. <ref type="bibr" target="#b22">[23]</ref>. It is worth mentioning that though more training images is able to bring us better results as shown in <ref type="figure" target="#fig_8">Fig. 14, our</ref> results here are mainly based on 2500 training images from MSRA-B dataset for fair comparison with existing works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Visual Comparison</head><p>To exhibit the superiority of our proposed approach compared against the above-mentioned methods, we select multiple representative images from different datasets which incorporate a variety of difficult circumstances, including complex scenes, salient objects with center bias, salient objects with different sizes, low contrast between foreground and background, etc., and show the visual comparisons in <ref type="figure">Fig. 8</ref>. We manually split the selected images into multiple groups which are separated by solid lines. We also give each group multiple tags describing their properties. Taking all circumstances into account, it can be easily seen that our proposed method not only highlights the right salient regions but also produces coherent boundaries. It is also worth mentioning that thanks to the short connections, our approach gives salient regions more confidence, yielding higher contrast between salient objects and the background. More importantly, it generates connected regions, which greatly strengthens the ability of our model. These advantages make our results very close to the ground truth and hence better than other methods in almost all circumstances which are shown in <ref type="figure">Fig. 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">PR Curve</head><p>We compare our approach with the existing methods in terms of PR curve here. In <ref type="figure">Fig. 9</ref>, we depict the PR curves produced by our approach and previous state-of-the-art methods on 3 popular datasets. It is obvious that FCNbased methods substantially outperform other methods. More importantly, among all FCN-based methods, the PR curve of our approach is especially outstanding in the upper left corners of the coordinates. We can also find that the precision of our approach is much higher when the recall score is close to 1, reflecting that our false positives are much lower than other methods. This also indicates that our strategy of combining low-level and high-level features in terms of short connections is essential such that the resultant saliency maps look much closer to the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">F-measure and MAE</head><p>We also compare our approach with the existing methods in terms of F-meature and MAE scores. The quantitative results are shown in <ref type="figure">Fig. 10</ref>. As can be seen, our approach achieves the best score (maximum F-measure and MAE) on all datasets as listed in <ref type="figure">Fig. 10</ref>. On the ECSSD and SOD datasets, our approach improves the current best maximum F-measure by 1 point, which is a large margin as the values are already very close to ideal value 1. In regard to MAE scores, our approach achieves a more than 1-point decrease on MSRA-B and PASCALS datasets. On the other datasets, there are still at least 0.09 points improvements. This implies that the number of wrong predictions in our case is significantly less than the other methods.</p><p>Besides, we also observe that the proposed approach behaves even better on more difficult datasets, such as HKUIS <ref type="bibr" target="#b46">[47]</ref>, PASCALS <ref type="bibr" target="#b54">[55]</ref>, and SOD <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>, which contain a large number of images with multiple salient objects. This indicates that our method is capable of detecting and segmenting the most salient object, while other methods often fail at one of these stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">The Existence of Saliency</head><p>To date, most existing salient object detection methods focus on datasets in which at least one salient object exists. However, in many real-world scenarios, salient objects do not always exists. Therefore, methods based on the above assumption may easily lead to incorrect prediction results when applied to scenes without any salient objects in them. To solve this problem, we propose to introduce another branch into our network to predict the saliency existence of the input image. The new branch is composed of a global average pooling layer, followed by a multi-layer perceptron (MLP) as the regressor to recognize the existence of saliency as done in many classification networks <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b58">[59]</ref>. The global average pooling layer is used to transform feature maps with different shapes into the same size so that the resulting feature vectors can be fed into the MLP. Like <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b31">[32]</ref>, the MLP here consists of three fully-connected layers, all of which are with 1,024 neurons except the last one which has two. The softmax loss is used to optimize the new branch.</p><p>In our experiments, we use the same training set as in <ref type="bibr" target="#b62">[63]</ref>, which contains 5,000 background images (i.e. images without salient objects in them) and 5,000 images from MSRA10K <ref type="bibr" target="#b40">[41]</ref>. For these background images, the gradients from the salient object detection module are not allowed to back-propagate so that the resulting prediction maps would not be interfered. We found that this operation is essential. The hyper-parameters used here are the same to our salient object detection experiments. We train our network for 24,000 iterations and decrease the learning rate by a factor of 10 at 20,000 iterations. We test our model on three datasets, including JSOD <ref type="bibr" target="#b62">[63]</ref>, MSRA-B <ref type="bibr" target="#b42">[43]</ref> and ECSSD <ref type="bibr" target="#b53">[54]</ref>. <ref type="figure">Fig. 11</ref> lists the results compared to another two works SSVM <ref type="bibr" target="#b62">[63]</ref> and Wang et al. <ref type="bibr" target="#b63">[64]</ref>. Since there is a clear separation between JSOD dataset (mostly containing pure textures) and other two datasets (MSRA-B and ECSSD mostly contain images with clear salient objects), the classification results on all datasets have been already saturated (very close to the ideal value "1"). Thus,  <ref type="figure">Fig. 8</ref>. Selected results from various datasets. We split the selected images into multiple groups, which are separated by solid lines. To better show the capability of processing different scenes for each approach, we highlight the features of images in each group.</p><p>we expect more challenging dataset which better reflect real world difficulties would be developed in near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Timing</head><p>Our network is fully convolutional, which allows it to run very fast compared against most previous salient object detection methods. When trained on the MSRA-B dataset which contains 2,500 training images, our network takes less than 8 hours for 12,000 iterations. Interestingly, though 10,000 iterations are enough for convergence, we found another 2,000 iterations still bring us a small performance gain in MAE. During the inference stage, it takes us about 0.08s to process an input image of size 300 × 400. This is extremely faster than most of the previous works, such as DCL <ref type="bibr" target="#b34">[35]</ref> which need more than 1s for each image of the same size. With our CRF layer considered, another 0.4 seconds are needed. As a result, our overall time cost is less than 0.5s for an image of size 300 × 400.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>In this section, we conduct useful analysis on our proposed approach, which we believe would be helpful for researchers to develop more powerful methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Failure Case Analysis</head><p>Some failure predictions of our approach have been shown in <ref type="figure">Fig. 12</ref>. As can be seen, these failure cases can be categorized into three circumstances in general. The first one is actually the common defect of CNN-based salient object detection methods, in which the salient objects cannot be completely segmented out, leaving a small part of the salient object missed. Typical examples are the images shown in DHS <ref type="bibr" target="#b35">[36]</ref> RFCN <ref type="bibr" target="#b50">[51]</ref> MDF <ref type="bibr" target="#b46">[47]</ref> ELD <ref type="bibr" target="#b49">[50]</ref> MC <ref type="bibr" target="#b48">[49]</ref> DRFI <ref type="bibr" target="#b24">[25]</ref> DSR <ref type="bibr" target="#b61">[62]</ref> CHM <ref type="bibr" target="#b60">[61]</ref> RC <ref type="bibr" target="#b40">[41]</ref>   <ref type="bibr" target="#b34">[35]</ref> DHS <ref type="bibr" target="#b35">[36]</ref> RFCN <ref type="bibr" target="#b50">[51]</ref> MDF <ref type="bibr" target="#b46">[47]</ref> ELD <ref type="bibr" target="#b49">[50]</ref> MC <ref type="bibr" target="#b48">[49]</ref> DRFI <ref type="bibr" target="#b24">[25]</ref> DSR <ref type="bibr" target="#b61">[62]</ref> CHM <ref type="bibr" target="#b60">[61]</ref> RC <ref type="bibr" target="#b40">[41]</ref>  DHS <ref type="bibr" target="#b35">[36]</ref> RFCN <ref type="bibr" target="#b50">[51]</ref> MDF <ref type="bibr" target="#b46">[47]</ref> ELD <ref type="bibr" target="#b49">[50]</ref> MC <ref type="bibr" target="#b48">[49]</ref> DRFI <ref type="bibr" target="#b24">[25]</ref> DSR <ref type="bibr" target="#b61">[62]</ref> CHM <ref type="bibr" target="#b60">[61]</ref> RC <ref type="bibr" target="#b40">[41]</ref> (1) MSRA10K (2) ECSSD (3) HKUIS <ref type="figure">Fig. 9</ref>. Precision (vertical axis) recall (horizontal axis) curves on three popular salient object datasets.</p><p>Training MSRA-B <ref type="bibr" target="#b42">[43]</ref> ECSSD <ref type="bibr" target="#b53">[54]</ref> HKU-IS <ref type="bibr" target="#b46">[47]</ref> PASCALS <ref type="bibr" target="#b54">[55]</ref> SOD <ref type="bibr" target="#b56">[57]</ref> Methods the first row of <ref type="figure">Fig. 12</ref>. In the second circumstance, the main body of the salient object cannot be extracted or nonsalient regions are predicted to be salient. As shown in the middle row of <ref type="figure">Fig. 12</ref>, this case is mostly caused by complex backgrounds and very low contrast. The last type of failure cases is caused by transparent objects as shown in the bottom row of <ref type="figure">Fig. 12</ref>. Though our approach can detect some parts of the transparent objects, to segment the complete objects out is still very difficult. We argue that three possible remedies can be used to solve the aforementioned problems. First of all, a promising solution is to provide more prior knowledge on segment level so that regions with similar textures or colors can be detected simultaneously. Because of the internal structure of CNNs, the correlations of two positions in the score map are decided by the learnable weights of the former layers, making this problem difficult to be solved by the networks themselves. Segment-level information allows CNNs to correct those wrong predictions in the Circumstance 1 mentioned above. In addition, segment-level information can also serve as a post-processing tool to further refine the predicted saliency maps by a simple voting strategy. Secondly, more powerful training data should be presented, including both simple and complex scenes. As listed in <ref type="figure" target="#fig_8">Fig. 14,</ref> training data with complex scenes can substantially help improve the performance on both easy and difficult datasets. Another solution should be designing more advanced models and then extracting more powerful feature representations to deal with challenging inputs with complex structures <ref type="bibr" target="#b64">[65]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Benchmarking Training Set</head><p>The selection of training set is one of the important aspects for a learning based algorithm. A good training set will definitely improve the learning ability, leading to a more gen- erative model that can perform well on almost all scenes, even with complex background. However, the training sets of recent learning based approaches are different and none of these works have explored which training set is the best. <ref type="figure">Fig. 10</ref> lists the details of different training sets that existing approaches have used. Furthermore, training on different datasets with different sizes makes the comparisons unfair. Albeit the number of training images is not proportional to the performance gain, the size and quality of different training sets break the fair comparisons among different approaches. One can observe in <ref type="figure">Fig. 10</ref> that some of them only use a training set with 2,500 images while some others leverage around 10,000 images for training.</p><p>In this section, we attempt to thoroughly analyze the effect of utilizing different datasets for training based on our proposed approach. Our goal is to provide a new, unified, convincing, and large-scale training set based on existing datasets for future research. To do so, we perform a number of experiments and show exhaustive comparisons among 6 widely-used and publicly available datasets, which can be found in <ref type="figure" target="#fig_7">Fig. 13</ref>. Notice that all the training lists will be made publicly available. During testing phase, we use both the max F-measure score and MAE score as measuring metrics. Notice that since most datasets contain more than 5,000 images, each model is trained for 16,000 iterations here. An exception is the model trained on ECSSD with 6,000 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Dataset Quality Measuring</head><p>To exhibit the quality of datasets better, each time we train on one of them, except for the SOD dataset which has only 300 images and the PASCALS dataset which has a lowly consistent behavior, and test on all the test sets. As ECSSD contains less than 2,000 images, all the images are used for training and hence no image is left for testing. For the remaining large-scale datasets, if default splits are provided then they will be used directly. Otherwise, we split the dataset in a ratio of 6:1:3 for training, validation, and testing, respectively.</p><p>Detailed experimental results have been shown in <ref type="figure" target="#fig_7">Fig. 13</ref>. As there is a large overlap between MSRA-B and MSRA10K datasets, we only show the results on MSRA-B instead of both. According to the results shown in <ref type="figure" target="#fig_7">Fig. 13</ref>, the following conclusion can be drawn. First, the best result on each dataset is always obtained by training on the corresponding training set, and the phenomenon is especially obvious for DUT-OMRON. This might be caused by the characteristics of the images in each dataset, making different datasets favor different features. Consequently, we argue that it is inappropriate to directly compare performance numbers that are achieved by different models trained on different datasets (see also <ref type="figure" target="#fig_7">Fig. 13</ref>). Second, having more training images does not necessarily entail better performance. As can be seen in <ref type="figure" target="#fig_7">Fig. 13</ref>, training on ECSSD dataset allows us to achieve the best performance on the SOD dataset despite of having only 1,000 training images. In regard to the above-mentioned issues, a compromise solution is to construct a unified, composite, and versatile dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Beyond Training on Individual Datasets</head><p>We select 4 datasets from <ref type="figure" target="#fig_7">Fig. 13</ref> to build composite datasets for comparisons. Though the MSRA10K is more than twice bigger than MSRA-B dataset, models trained on it have a competitive performance compared to those trained on the MSRA-B dataset. Here we just keep MSRA-B for training due to its high-quality images and annotations. Therefore, there are totally 11 different combinations which have been shown in the second column of <ref type="figure" target="#fig_8">Fig. 14.</ref> During the testing phase, we also use the six test sets mentioned above for fair comparisons.</p><p>From the results in <ref type="figure" target="#fig_8">Fig. 14,</ref> the following conclusions can be drawn. First of all, a larger training set does not necessarily mean higher test performance. This phenomenon can be observed through comparing Scheme 3 with other schemes. Despite only 3,500 training images, this combination performs better than those with more than 6,000 training images. It is true that the quality of annotations might be an essential reason that causes such a problem. However, such a consideration is beyond the scope of this paper. All conclusions here are based on the assumption that each dataset we use is with well-segmented annotations.</p><p>Second, an inappropriate combination of datasets may result in worse performance compared with individual datasets. By comparing schemes 4 and 0, one can find that despite better performance on HKU-IS, PASCALS, and SOD datasets there are still slight decreases when testing on MSRA-B and DUT-OMRON datasets.</p><p>Through this series of experiments, we aimed to emphasis that a training set with a large quantity of images may not be capable of bringing in better performance gain. A good training set should take into account as many cases as possible. However, because of the diversity of existing datasets, it is hard to obtain a convincing dataset that can behave the consistency among all existing datasets. In regard to the current state in salient object detection, we recommend using our Scheme 11 in <ref type="figure" target="#fig_8">Fig. 14 as</ref> training set for fair comparison and fitting decreasing performance bias caused by different training sets. Another severe problem in salient object detection is that most datasets are no longer challenging. An explicit effect is that the differences between different models are difficult to be distinguished because of the close performance on existing datasets. We  hope that more challenging datasets with complex scenes and high consistency would be presented in the near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we presented a deeply supervised network for salient object detection. Instead of directly connecting loss layers to the last layer of each stage, we introduce a series of short connections between shallower and deeper side-output layers. With these short connections, the activation of each side-output layer gains the capability of both highlighting the entire salient object and accurately locating its boundary. A fully connected CRF is also employed for correcting wrong predictions and further improving spatial coherence. Our experiments demonstrate that these mechanisms result in more accurate saliency maps over a variety of images. Our approach significantly advances the state-of-the-art and is capable of capturing salient regions in both simple and difficult cases, which further verifies the merit of the proposed architecture. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Illustration of short connections inFig. 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>R</head><label></label><figDesc>side , for m = 1, . . . , 5 A (m) side . for m = 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>R</head><label></label><figDesc>side . for m = 5, 6 (18) The last pattern, the one used in this paper, is given bỹ R side , for m = 3, 4 A (m) side . for m = 5, 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>The performance of different architectures on PASCALS dataset<ref type="bibr" target="#b54">[55]</ref>. '*' represents the pattern used in this paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 13 .</head><label>13</label><figDesc>(3103) 0.890 0.060 0.895 0.079 0.888 0.059 0.811 0.113 0.814 0.141 0Performance when different training sets are used. The best results are highlighted in bold. Notice that all the results here are without CRF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 14 .</head><label>14</label><figDesc>Detailed information of different training sets and the corresponding results on 5 datasets. The best results are highlighted in bold. All the results are obtained without any post-processing. Here we use the initials of each dataset for convenience.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, M.M. Cheng, and X. Hu are with CCCE, Nankai University. Borji is with the Center for Research in Computer Vision, University of Central Florida (aborji@crcv.ucf.edu) • Z. Tuo is with the University of California at San Diego.</figDesc><table /><note>M.M. Cheng is the corresponding author (cmm@nankai.edu.cn).• A.• P.H.S. Torr is with the University of Oxford.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, arXiv:1611.04849v4 [cs.CV] 16 Mar 2018</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>128, 3 × 3 128, 3 × 3 1, 1 × 1 2 conv2 2 128, 3 × 3 128, 3 × 3 1, 1 × 1 3 conv3 3 256, 5 × 5 256, 5 × 5 1, 1 × 1 4 conv4 3 256, 5 × 5 256, 5 × 5 1, 1 × 1 5 conv5 3 512, 5 × 5 512, 5 × 5 1, 1 × 1 6</figDesc><table><row><cell cols="2">No. Layer</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>1</cell><cell>conv1 2</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>× 3) × 2 (128, 3 × 3) × 2 (256, 5 × 5) × 2 (512, 5 × 5) × 2 (1024, 5 × 5) × 2 (1024, 7 × 7) × 2 0.830 2 (128, 3 × 3) × 1 (128, 3 × 3) × 1 (256, 5 × 5) × 1 (256, 5 × 5) × 1 (512, 5 × 5) × 1 (512, 7 × 7) × 1 0.815 3 (128, 3 × 3) × 2 (128, 3 × 3) × 2 (256, 3 × 3) × 2 (256, 3 × 3) × 2 (512, 5 × 5) × 2 (512, 5 × 5) × 2 0.820 4 (128, 3 × 3) × 2 (128, 3 × 3) × 2 (256, 5 × 5) × 2 (256, 5 × 5) × 2 (512, 5 × 5) × 2 (512, 7 × 7) × 2 0.830 Fig. 6. Comparisons of different side output settings and their performance on PASCALS dataset</figDesc><table><row><cell>No.</cell><cell>Side output 1</cell><cell>Side output 2</cell><cell>Side output 3</cell><cell>Side output 4</cell><cell>Side output 5</cell><cell>Side output 6</cell><cell>F β</cell></row><row><cell>1</cell><cell>(128, 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Fig. 10. Quantitative comparisons with 11 methods on 5 popular datasets. The ResNet-101<ref type="bibr" target="#b58">[59]</ref> version of our approach (i.e. 'Ours †') clearly outperforms its VGGNet version. For fair comparison, we exclude 'Ours †' and highlight the best result of each column in bold. Here we use the initials of each dataset for convenience.</figDesc><table><row><cell></cell><cell cols="2">Dataset</cell><cell cols="2">#Images</cell><cell>F β</cell><cell>MAE</cell><cell>F β</cell><cell>MAE</cell><cell>F β</cell><cell>MAE</cell><cell>F β</cell><cell>MAE</cell><cell>F β</cell><cell>MAE</cell></row><row><cell>RC [41]</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>0.817</cell><cell>0.138</cell><cell>0.741</cell><cell>0.187</cell><cell>0.726</cell><cell>0.165</cell><cell>0.640</cell><cell>0.225</cell><cell>0.657</cell><cell>0.242</cell></row><row><cell>CHM [61]</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>0.809</cell><cell>0.138</cell><cell>0.722</cell><cell>0.195</cell><cell>0.728</cell><cell>0.158</cell><cell>0.631</cell><cell>0.222</cell><cell>0.655</cell><cell>0.249</cell></row><row><cell>DSR [62]</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>0.812</cell><cell>0.119</cell><cell>0.737</cell><cell>0.173</cell><cell>0.735</cell><cell>0.140</cell><cell>0.646</cell><cell>0.204</cell><cell>0.655</cell><cell>0.234</cell></row><row><cell>DRFI [25]</cell><cell cols="2">MB</cell><cell cols="2">2,500</cell><cell>0.855</cell><cell>0.119</cell><cell>0.787</cell><cell>0.166</cell><cell>0.783</cell><cell>0.143</cell><cell>0.679</cell><cell>0.221</cell><cell>0.712</cell><cell>0.215</cell></row><row><cell>MC [49]</cell><cell cols="2">MK</cell><cell cols="2">8,000</cell><cell>0.872</cell><cell>0.062</cell><cell>0.822</cell><cell>0.107</cell><cell>0.781</cell><cell>0.098</cell><cell>0.721</cell><cell>0.147</cell><cell>0.708</cell><cell>0.184</cell></row><row><cell>ELD [50]</cell><cell cols="2">MK</cell><cell cols="2">9,000</cell><cell>0.914</cell><cell>0.042</cell><cell>0.865</cell><cell>0.981</cell><cell>0.844</cell><cell>0.071</cell><cell>0.767</cell><cell>0.121</cell><cell>0.760</cell><cell>0.154</cell></row><row><cell>MDF [47]</cell><cell cols="2">MB</cell><cell cols="2">2,500</cell><cell>0.885</cell><cell>0.104</cell><cell>0.833</cell><cell>0.108</cell><cell>0.860</cell><cell>0.129</cell><cell>0.764</cell><cell>0.145</cell><cell>0.785</cell><cell>0.155</cell></row><row><cell>DS [60]</cell><cell cols="2">MB</cell><cell cols="2">2,500</cell><cell>-</cell><cell>-</cell><cell>0.810</cell><cell>0.160</cell><cell>-</cell><cell>-</cell><cell>0.818</cell><cell>0.170</cell><cell>0.781</cell><cell>0.150</cell></row><row><cell>RFCN [51]</cell><cell cols="2">MK</cell><cell cols="2">10,000</cell><cell>0.926</cell><cell>0.062</cell><cell>0.898</cell><cell>0.097</cell><cell>0.895</cell><cell>0.079</cell><cell>0.827</cell><cell>0.118</cell><cell>0.805</cell><cell>0.161</cell></row><row><cell>DHS [36]</cell><cell cols="2">MK + D</cell><cell cols="2">9,500</cell><cell>-</cell><cell>-</cell><cell>0.905</cell><cell>0.061</cell><cell>0.892</cell><cell>0.052</cell><cell>0.820</cell><cell>0.091</cell><cell>0.823</cell><cell>0.127</cell></row><row><cell>DCL + [35]</cell><cell cols="2">MB</cell><cell cols="2">2,500</cell><cell>0.916</cell><cell>0.047</cell><cell>0.898</cell><cell>0.071</cell><cell>0.907</cell><cell>0.048</cell><cell>0.822</cell><cell>0.108</cell><cell>0.832</cell><cell>0.126</cell></row><row><cell>Ours</cell><cell cols="2">MB</cell><cell cols="2">2,500</cell><cell>0.927</cell><cell>0.028</cell><cell>0.915</cell><cell>0.052</cell><cell>0.913</cell><cell>0.039</cell><cell>0.830</cell><cell>0.080</cell><cell>0.842</cell><cell>0.118</cell></row><row><cell>Ours  †</cell><cell cols="2">MB</cell><cell cols="2">2,500</cell><cell>0.936</cell><cell>0.030</cell><cell>0.928</cell><cell>0.048</cell><cell>0.920</cell><cell>0.035</cell><cell>0.838</cell><cell>0.092</cell><cell>0.850</cell><cell>0.119</cell></row><row><cell cols="2">Methods</cell><cell cols="6">JSOD [63] MSRA-B [43] ECSSD [54]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Wang et al. [64]</cell><cell cols="2">90.64%</cell><cell></cell><cell>89.26%</cell><cell cols="2">70.50%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SSVM [63]</cell><cell cols="2">99.22%</cell><cell></cell><cell>98.66%</cell><cell cols="2">94.40%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell cols="2">98.84%</cell><cell></cell><cell>99.05%</cell><cell cols="2">96.8%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Fig. 11. The prediction accuracy of our saliency existence branch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">compared to SSVM [63] and Wang et al. [64]. The best result of each</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">column is highlighted in bold.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Source</cell><cell>GT</cell><cell cols="2">Ours</cell><cell cols="2">Source</cell><cell>GT</cell><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Fig. 12. Failure cases selected from multiple datasets. As can be seen,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">most cases are caused by complex background, low contrast between</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">foreground and background, and transparent objects.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Qibin Hou is currently a Ph.D. Candidate with College of Computer Science and Control Engineering, Nankai University, under the supervision of Prof. Ming-Ming Cheng. His research interests include deep learning, image processing, and computer vision. Ming-Ming Cheng received his PhD degree from Tsinghua University in 2012. Then he did 2 years research fellow, with Prof. Philip Torr in Oxford. He is now a professor at Nankai University, leading the Media Computing Lab. His research interests includes computer graphics, computer vision, and image processing. He received research awards including ACM China Rising Star Award, IBM Global SUR Award, CCF-Intel Young Faculty Researcher Program, etc. Hu is currently a Master student with College of Computer Science and Control Engineering, Nankai University, under the supervision of Prof. Ming-Ming Cheng. His research interests include deep learning, image processing, and computer vision. Ali Borji received the PhD degree in cognitive neurosciences from the Institute for Studies in Fundamental Sciences (IPM), 2009. He is currently an assistant professor at Center for Research in Computer Vision, University of Central Florida. His research interests include visual attention, visual search, machine learning, neurosciences, and biologically plausible vision models. Tu received the BE degree from the Beijing Information Technology Institute, the ME degree from Tsinghua University, and the PhD degree in computer science from Ohio State University. He is an associate professor of cognitive science with the University of California, San Diego (UCSD). His main research interests include computer vision, machine learning, and neural computation. Torr received the PhD degree from Oxford University. After working for another 3 years at Oxford, he worked for 6 years as a research scientist for Microsoft Research, first in Redmond, then in Cambridge, founding the vision side of the Machine Learning and Perception Group. He is now a professor at Oxford University. He has won awards from several top vision conferences, including ICCV, CVPR, ECCV, NIPS etc. He is a Royal Society Wolfson Research Merit Award holder.</figDesc><table><row><cell>Xiaowei Zhuowen Philip H.S.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was supported by NSFC (NO. 61620106008, 61572264), Huawei Innovation Research Program, CAST YESS Program, and IBM Global SUR award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="198" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video salient object detection via cross-frame cellular automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="325" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Saliency driven total variation segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Donoser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Urschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Repfinder: finding approximately repeated scene elements for image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">83</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A shapepreserving approach to image resizing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1897" to="1906" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Is bottomup attention useful for object recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rutishauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">in IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stc: A simple to complex framework for weaklysupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2314" to="2320" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Mining pixels: Weakly supervised semantic segmentation using image labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Massiceti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>EMMCVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to segment with image-level annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="234" to="244" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive object tracking by learning background context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Frintrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Sihite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Worksh</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Artistic minimal rendering with lines and blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-K</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graphical Models</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="208" to="229" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast saliency-aware multi-modality image fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Pauwels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>De Zeeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="page" from="70" to="80" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sketch2photo: Internet image montage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Internet visual media processing: a survey with graphics and vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="393" to="405" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Web-image driven best views of 3d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Vis. Comput</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised object class discovery via saliency-guided multiple class learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3218" to="3225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3-D object retrieval and recognition with hypergraph analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4290" to="4303" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Intelligent visual media processing: When graphics meets vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Sci</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Tech</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Saliency guided local and global descriptors for effective action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdulmunem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Visual Media</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="97" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Computational modeling of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="194" to="203" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">State-of-the-art in visual attention modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="207" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="http://people.cs.umass.edu/∼hzjiang/" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="268" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Richer convolutional features for edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Perceptual generative adversarial networks for small object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Is faster r-cnn doing well for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">P. IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep contrast learning for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dhsnet: Deep hierarchical saliency network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="678" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bayesian saliency via low and mid level cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1689" to="1698" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Saliencyrank: Two-stage manifold ranking for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-F</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Visual Media</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="309" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="569" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Efficient salient region detection with soft image abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Crook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1529" to="1536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Exploiting local and global patch rarities for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="478" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Salient object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.5878</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Supercnn: A superpixelwise convolutional neural network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="http://www.shengfenghe.com/" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="330" to="344" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="http://i.cs.hku.hk/∼yzyu/vision.html" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep networks for saliency detection via local estimation and global search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3183" to="3192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://github.com/Robert0812/deepsaldet" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep saliency with encoded low level distance map and high level features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gayoung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu-Wing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Junmo</surname></persName>
		</author>
		<ptr target="https://github.com/gylee1103/SaliencyELD" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Saliency detection with recurrent fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Design and perceptual validation of performance measures for salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Movahedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Worksh</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="733" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deepsaliency: Multi-task deep neural network model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://github.com/zlmzju/DeepSaliency" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3919" to="3930" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Contextual hypergraph modeling for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3328" to="3335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Saliency detection via dense and sparse reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2976" to="2983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Joint salient object detection and existence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Comput. Sci</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Salient object detection for searched web images via global saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Structuremeasure: A New Way to Evaluate Foreground Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
