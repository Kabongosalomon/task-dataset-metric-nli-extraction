<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">JL-DCF: Joint Learning and Densely-Cooperative Fusion Framework for RGB-D Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keren</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of CS</orgName>
								<orgName type="institution">Nankai University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge-Peng</forename><surname>Ji</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijun</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">JL-DCF: Joint Learning and Densely-Cooperative Fusion Framework for RGB-D Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a novel joint learning and denselycooperative fusion (JL-DCF) architecture for RGB-D salient object detection. Existing models usually treat RGB and depth as independent information and design separate networks for feature extraction from each. Such schemes can easily be constrained by a limited amount of training data or over-reliance on an elaborately-designed training process. In contrast, our JL-DCF learns from both RGB and depth inputs through a Siamese network. To this end, we propose two effective components: joint learning (JL), and densely-cooperative fusion (DCF). The JL module provides robust saliency feature learning, while the latter is introduced for complementary feature discovery. Comprehensive experiments on four popular metrics show that the designed framework yields a robust RGB-D saliency detector with good generalization. As a result, JL-DCF significantly advances the top-1 D3Net model by an average of ∼1.9% (S-measure) across six challenging datasets, showing that the proposed framework offers a potential solution for real-world applications and could provide more insight into the cross-modality complementarity task. The code will be available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Salient object detection (SOD) aims at detecting the objects in a scene that humans would naturally focus on <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b77">78]</ref>.</p><p>It has many useful applications, including object segmentation and recognition <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b78">79]</ref>, image/video compression <ref type="bibr" target="#b23">[24]</ref>, video detection/summarization <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b40">41]</ref>, content-based image editing <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b62">63]</ref>, informative common object discovery <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b71">72]</ref>, and image retrieval <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b36">37]</ref>. Many SOD models have been developed under the assumption that the inputs are individual RGB/color images <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b73">[74]</ref><ref type="bibr" target="#b74">[75]</ref><ref type="bibr" target="#b75">[76]</ref> or sequences <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b67">68]</ref>. As depth cameras such as Kinect * Corresponding author: Deng-Ping Fan (dengpfan@gmail.com)  <ref type="figure">Figure 1</ref>: Applying deep saliency models DHS <ref type="bibr" target="#b37">[38]</ref> and DSS <ref type="bibr" target="#b28">[29]</ref>, which are fed with an RGB image (1 st row) or a depth map (2 nd row). Both of the models are trained on a single RGB modality. By contrast, our JL-DCF considers both modalities and thus generates better results (last column).</p><p>and RealSense become more and more popular, SOD from RGB-D inputs ("D" refers to depth) is emerging as an attractive topic. Although a number of prior works have tried to explore the role of depth in saliency analysis, several issues remain: (i) Deep-based RGB-D SOD methods are still underexplored: Despite more than one hundred papers on RGB SOD models being published since 2015 <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b68">69]</ref>, there are only a few deep learning-based works focusing on RGB-D SOD. The first model utilizing convolutional neural networks (CNNs) for RGB-D SOD <ref type="bibr" target="#b48">[49]</ref>, which adopts a shallow CNN as the saliency map integration model, was described in 2017. Since then, only a dozen deep models have been proposed, as summarized in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b72">73]</ref>, leaving large room for further improvement in performance.</p><p>(ii) Less effective feature extraction and fusion: Most learning-based models fuse features of different modalities either by early-fusion <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b54">55]</ref> or late-fusion <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b59">60]</ref>. Although these two simple strategies have achieved encouraging progress in this field in the past (as pointed out in <ref type="bibr" target="#b3">[4]</ref>), they face challenges in either extracting representative multi-modal features or effectively fusing them. While other works have adopted a middle-fusion strategy <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b79">80]</ref>, which conducts independent feature extraction and fusion using individual CNNs, their sophisticated net-work architectures and large number of parameters require an elaborately-designed training process and large amount of training data. Unfortunately, high-quality depth maps are still sparse <ref type="bibr" target="#b76">[77]</ref>, which may lead to sub-optimal solutions of deep learning-based models. Motivation. To tackle RGB-D SOD, we propose a novel joint learning and densely-cooperative fusion (JL-DCF) architecture that outperforms all existing deep learning-based techniques. Our method adopts the middle-fusion strategy mentioned above. However, different from previous works which conduct independent feature extraction from RGB and depth views, JL-DCF effectively extracts deep hierarchical features from RGB and depth inputs simultaneously, through a Siamese network (shared backbone). The underlying motivation is that, although depth and RGB images come from different modalities, they nevertheless share similar features/cues, such as strong figure-ground contrast <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>, closure of object contours <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b52">53]</ref>, and connectivity to image borders <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b58">59]</ref>. This makes crossmodal transferring feasible, even for deep models. As evidenced in <ref type="figure">Fig. 1</ref>, a model trained on a single RGB modality, like DHS <ref type="bibr" target="#b37">[38]</ref>, can sometimes perform well in the depth view. Nevertheless, a similar model, like DSS <ref type="bibr" target="#b28">[29]</ref>, could also fail in the depth view without proper adaption or transferring.</p><p>To the best of our knowledge, the proposed JL-DCF scheme is the first to leverage such transferability in deep models, by treating a depth image as a special case of a color image and employing a shared CNN for both RGB and depth feature extraction. Additionally, we develop a densely-cooperative fusion strategy to reasonably combine the learned features of different modalities. This paper provides two main contributions:</p><p>• We introduce a general framework for RGB-D SOD, called JL-DCF, which consists of two sub-modules: joint learning and densely-cooperative fusion. The key features of these two components are their robustness and effectiveness, which will be beneficial for future modeling in related multi-modality tasks in computer vision. In particular, we advance the state-of-the-art (SOTA) by a significant average of ∼2% (F-measure score) across six challenging datasets. • We present a thorough evaluation of 14 SOTA methods <ref type="bibr">[4-6, 13, 18, 20, 25, 26, 34, 46, 49, 55, 60, 77]</ref>, which is the largest-scale comparison in this field to date. Besides, we conduct a comprehensive ablation study, including using different input sources, learning schemes, and feature fusion strategies, to demonstrate the effectiveness of JL-DCF. Some interesting findings also encourage further research in this field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Traditional. The pioneering work for RGB-D SOD was produced by Niu et al. <ref type="bibr" target="#b42">[43]</ref>, who introduced disparity con-trast and domain knowledge into stereoscopic photography to measure stereo saliency. After Niu's work, various handcrafted features/hypotheses originally applied for RGB SOD were extended to RGB-D, such as center-surround difference <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34]</ref>, contrast <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b43">44]</ref>, background enclosure <ref type="bibr" target="#b19">[20]</ref>, center/boundary prior <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b58">59]</ref>, compactness <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, or a combination of various saliency measures <ref type="bibr" target="#b54">[55]</ref>. All the above models rely heavily on heuristic hand-crafted features, resulting in limited generalizability in complex scenarios.</p><p>Deep-based. Recent advances in this field have been obtained by using deep learning and CNNs. Qu et al. <ref type="bibr" target="#b48">[49]</ref> first utilized a CNN to fuse different low-level saliency cues for judging the saliency confidence values of superpixels. Shigematsu et al. <ref type="bibr" target="#b52">[53]</ref> extracted ten superpixel-based handcrafted depth features capturing the background enclosure cue, depth contrast, and histogram distance. These features are fed to a CNN, whose output is shallowly fused with the RGB feature output to compute superpixel saliency. A recent trend in this field is to exploit fully convolutional neural networks (FCNs) <ref type="bibr" target="#b51">[52]</ref>. Chen et al. <ref type="bibr" target="#b3">[4]</ref> proposed a bottom-up/top-down architecture <ref type="bibr" target="#b47">[48]</ref>, which progressively performs cross-modal complementarity-aware fusion in its top-down pathway. Han et al. <ref type="bibr" target="#b25">[26]</ref> modified/extended the structure of the RGB-based deep neural network in order for it to be applicable for the depth view and then fused the deep representations of both views via a fully-connected layer. A three-stream attention-aware network was proposed in <ref type="bibr" target="#b4">[5]</ref>, which extracts hierarchical features from RGB and depth inputs through two separate streams. Features are then progressively combined and selected via attention-aware blocks in the third stream. A new multi-scale multi-path fusion network with cross-modal interactions was proposed in <ref type="bibr" target="#b5">[6]</ref>. <ref type="bibr" target="#b39">[40]</ref> and <ref type="bibr" target="#b30">[31]</ref> formulated a four-channel input by concatenating RGB and depth. The input is later fed to a single-stream recurrent CNN and an FCN with short connections, respectively. <ref type="bibr" target="#b79">[80]</ref> employed a subsidiary network to obtain depth features and used them to enhance the intermediate representation in an encoderdecoder architecture. Zhao et al. <ref type="bibr" target="#b76">[77]</ref> proposed a model that generates a contrast-enhanced depth map, which is later used as a prior map for feature enhancement in subsequent fluid pyramid integration. Fan et al. <ref type="bibr" target="#b17">[18]</ref> constructed a new RGB-D dataset called the Salient Person (SIP) dataset, and introduced a depth-depurator network to judge whether a depth map should be concatenated with the RGB image to formulate an input signal.</p><p>Generally, as summarized by previous literature <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b76">77]</ref>, the above approaches can be divided into three categories: (a) Early-fusion <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b54">55]</ref>, (b) late-fusion <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b59">60]</ref> and (c) middle-fusion <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b79">80]</ref>. Middle-fusion complements (a) and (b), since both feature-extraction and subsequentfusion are handled by relatively deep CNNs. As a conse-  quence, high-level concepts can be learnt from both modalities and complex integration rules can be mined. Besides, performing individual deep supervision for RGB and depth is straightforward. The proposed JL-DCF scheme falls into the middle-fusion strategy.</p><formula xml:id="formula_0">+ + + + + CM5 CM4 CM3 CM2 CM1 FA1</formula><p>However, unlike the aforementioned methods <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b79">80]</ref>, where the two feature extraction streams are independent, we propose to utilize a Siamese architecture <ref type="bibr" target="#b10">[11]</ref>, where both the network architecture and weights are shared. This results in two major benefits: 1) Cross-modal knowledgesharing becomes straightforward via joint learning; 2) The model parameters are largely reduced as only one shared network is needed, leading to facilitated learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The overall architecture of the proposed JL-DCF is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. It follows the classic bottom-up/top-down strategy <ref type="bibr" target="#b47">[48]</ref>. For illustrative purpose, <ref type="figure" target="#fig_1">Fig. 2</ref> depicts an example backbone with six hierarchies that are common in the widely-used VGG <ref type="bibr" target="#b53">[54]</ref> and ResNet <ref type="bibr" target="#b27">[28]</ref>. The architecture consists of a JL component and a DCF component. The JL component conducts joint learning for the two modalities using a Siamese network. It aims to discover the commonality between these two views from a "modelbased perspective, since their information can be merged into model parameters via back-propagation. As seen in <ref type="figure" target="#fig_1">Fig. 2</ref>, the hierarchical features jointly learned by the backbone are then fed to the subsequent DCF component. DCF is dedicated to feature fusion and its layers are constructed in a densely-cooperative way. In this sense, the complementarity between RGB and depth modalities can be explored from a "feature-based" perspective. To perform cross-view feature fusion, in the DCF component, we elaborately design a cross-modal fusion module (CM module in <ref type="figure" target="#fig_1">Fig. 2</ref>). Details about JL-DCF will be given in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Joint Learning (JL)</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref> (gray part), the inputs of the JL component are an RGB image together with its corresponding depth map. We first normalize the depth map into intervals [0, 255] and then convert it to a three-channel map through color mapping. In our implementation, we use the naive gray color mapping, which is equivalent to replicating the single channel map into three channels. Note that other color mapping <ref type="bibr" target="#b0">[1]</ref> or transformations, like the mean used in <ref type="bibr" target="#b25">[26]</ref>, could also be considered for generating the threechannel representation. Next, the three-channel RGB image and transformed depth map are concatenated to formulate a batch, so that the subsequent CNN backbone can perform parallel processing. Note that, unlike previous earlyfusion schemes aforementioned, which often concatenate the RGB and depth inputs in the 3 rd channel dimension, our scheme concatenates in the 4 th dimension, often called the batch dimension. For example, in our case a transformed 320 × 320 × 3 depth and a 320 × 320 × 3 RGB map will formulate a batch of size 320 × 320 × 3 × 2, rather than 320 × 320 × 6.</p><p>The hierarchical features from the shared CNN backbone are then leveraged in a side-output way like <ref type="bibr" target="#b28">[29]</ref>. Since the side-output features have varied resolutions and channel numbers (usually the deeper, the more channels), we first employ a set of CP modules (CP1∼CP6 in <ref type="figure" target="#fig_1">Fig. 2</ref>) to compress the side-output features to an identical, smaller num-ber, denoted as k. We do this for the following two reasons: (1) Using a large number of feature channels for subsequent decoding is memory and computationally expensive and <ref type="formula" target="#formula_3">(2)</ref> Unifying the number of feature channels facilitates various element-wise operations. Note that, here, the outputs from our CP modules are still batches, which are denoted as the thicker black arrows in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>Coarse localization can provide the basis for the following top-down refinement <ref type="bibr" target="#b47">[48]</ref>. In addition, jointly learning the coarse localization guides the shared CNN to learn to extract independent hierarchical features from the RGB and depth views simultaneously. In order to enable the CNN backbone to coarsely locate the targets from both the RGB and depth views, we apply deep supervision to the JL component in the last hierarchy. To conduct this, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, we add a (1 × 1, 1) convolutional layer on the CP6 module to achieve coarse prediction. The depth and RGB-associated outputs are supervised by the downsampled ground truth map. The generated loss in this stage is called the global guidance loss L g .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Densely-cooperative Fusion (DCF)</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref> (light green part), the output batch features from the CP modules contain depth and RGB information. They are fed to the DCF component, which can be deemed a decoder that performs multi-scale crossmodal fusion. Firstly, we design a CM (cross-modal fusion) module to split and then merge the batch features <ref type="figure" target="#fig_1">(Fig. 2</ref>, bottom-right). This module first splits the batch data and then conducts "addition and multiplication" feature fusion, which we call cooperative fusion. Mathematically, let a batch feature be denoted by {X rgb , X d }, where X rgb , X d represent the RGB and depth parts, each with k channels, respectively. The CM module conducts the fusion as:</p><formula xml:id="formula_1">CM ({X rgb , X d }) = X rgb ⊕ X d ⊕ (X rgb ⊗ X d ),<label>(1)</label></formula><p>where "⊕" and "⊗" denote element-wise addition and multiplication. The blended features output from the CM modules are still made up of k channels. Compared to elementwise addition "⊕", which exploits feature complementarity, element-wise multiplication "⊗" puts more emphasis on commonality. These two properties are generally important in cross-view fusion.</p><p>One may argue that such a CM module could be replaced by channel concatenation, which generates 2k-channel concatenated features. However, we find such a choice tends to result in the learning process being trapped in a local optimum, where it becomes biased towards only RGB information. The reason seems to be that the channel concatenation does indeed involve feature selection rather than explicit feature fusion. This leads to degraded learning outcomes, where only RGB features dominate the final prediction. Note that, as will be shown in Section 4.4, solely using RGB input can also achieve fairly good performance  <ref type="figure">H, k)</ref> denote width, height, channel number, respectively <ref type="figure">Figure 3</ref>: Inception structure used for the FA modules in <ref type="figure" target="#fig_1">Fig.   2</ref>. All convolutional layers and max-pooling layers have stride 1, therefore maintaining spatial feature sizes. Unlike the original Inception module <ref type="bibr" target="#b57">[58]</ref>, we adapt it to have the same input/output channel number k.</p><formula xml:id="formula_2">Conv (1x1, k/2) Conv (3x3, k/4) Conv (1x1, k/4) Conv (5x5, k/4) Max-pool (3x3) Conv (1x1, k/4) c c (W,</formula><p>in the proposed framework. Comparisons between our CM modules and concatenation will be given in Section 4.4.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the fused features from CM1∼CM6 are fed to a decoder augmented with a dense connection <ref type="bibr" target="#b29">[30]</ref>. Using the dense connection promotes the blending of depth and RGB features at various scales. Therefore, unlike the traditional UNet-like decoder <ref type="bibr" target="#b49">[50]</ref>, an aggregation module FA takes inputs from all levels deeper than itself. Specifically, FA denotes a feature aggregation module performing non-linear aggregation. To this end, we use the Inception module <ref type="bibr" target="#b57">[58]</ref> shown in <ref type="figure">Fig. 3</ref>, which performs multi-level convolutions with filter size 1 × 1, 3 × 3, 5 × 5, and max-pooling. Note that the FA module in our framework is flexible. Other modules may also be considered in the future to improve the performance.</p><p>Finally, the FA module with the finest features is denoted as FA1, whose output is then fed to a (1×1, 1) convolutional layer to generate the final activation and then ultimately the saliency map. This final prediction is supervised by the resized ground truth (GT) map during training. We denote the loss generated in this stage as L f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Function</head><p>The overall loss function of our scheme is composed of the global guidance loss L g and final loss L f . Assume that G denotes supervision from the ground truth, S c rgb and S c d denote the coarse prediction maps contained in the batch after module CP6, and S f is the final prediction after module FA1. The overall loss function is defined as:</p><formula xml:id="formula_3">L total = L f (S f , G) + λ x∈{rgb,d} L g (S c x , G),<label>(2)</label></formula><p>where λ balances the emphasis of global guidance, and we adopt the widely used cross-entropy loss for L g and L f as:</p><formula xml:id="formula_4">L(S, G) = − i [G i log(S i ) + (1 − G i ) log(1 − S i )],<label>(3)</label></formula><p>where i denotes pixel index, and S ∈ {S c rgb , S c d , S f }.  <ref type="bibr" target="#b15">[16]</ref>, max F-measure (F max β ) <ref type="bibr" target="#b2">[3]</ref>, max E-measure (E max φ ) <ref type="bibr" target="#b16">[17]</ref> and MAE (M ) <ref type="bibr" target="#b44">[45]</ref> of SOTA methods and the proposed JL-DCF on six RGB-D datasets. The best performance is highlighted in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head><p>ACSD <ref type="bibr" target="#b33">[34]</ref> LBE <ref type="bibr" target="#b19">[20]</ref> DCMC <ref type="bibr" target="#b12">[13]</ref> MDSF <ref type="bibr" target="#b54">[55]</ref> SE <ref type="bibr" target="#b24">[25]</ref> DF <ref type="bibr" target="#b48">[49]</ref> AFNet <ref type="bibr" target="#b59">[60]</ref> CTMF <ref type="bibr" target="#b25">[26]</ref> MMCI <ref type="bibr" target="#b5">[6]</ref> PCF <ref type="bibr" target="#b3">[4]</ref> TANet <ref type="bibr" target="#b4">[5]</ref> CPFP <ref type="bibr" target="#b76">[77]</ref> DMRA <ref type="bibr" target="#b45">[46]</ref> D3Net <ref type="bibr" target="#b17">[18]</ref> JL-DCF Ours </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Metrics</head><p>Experiments are conducted on six public RGB-D benchmark datasets: NJU2K <ref type="bibr" target="#b33">[34]</ref> (2000 samples), NLPR <ref type="bibr" target="#b43">[44]</ref> (1000 samples), STERE <ref type="bibr" target="#b42">[43]</ref> (1000 samples), RGBD135 <ref type="bibr" target="#b9">[10]</ref> (135 samples), LFSD <ref type="bibr" target="#b34">[35]</ref> (100 samples), and SIP <ref type="bibr" target="#b17">[18]</ref> (929 samples). Following <ref type="bibr" target="#b76">[77]</ref>, we choose the same 700 samples from NLPR and 1500 samples from NJU2K to train our algorithms. The remaining samples are used for testing. For fair comparisons, we apply the model trained on this training set to other datasets. For evaluation, we adopt four widely used metrics, namely S-measure (S α ) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b76">77]</ref>, maximum F-measure (F max β ) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29]</ref>, maximum E-measure (E max φ ) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, and MAE (M ) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b44">45]</ref>. The definitions for these metrics are omitted here and readers are referred to the related papers. Note that, since the E-measure metric was originally proposed in <ref type="bibr" target="#b16">[17]</ref> for evaluating binary maps, to extend it for comparing a non-binary saliency map against a binary ground truth map, we follow a similar strategy to F max β . Specifically, we first binarize a saliency map into a series of foreground maps using all possible threshold values in [0, 255], and then report the maximum E-measure among them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>The proposed JL-DCF scheme is generally independent from the network backbone. In this work, we implement two versions of JL-DCF based on VGG-16 <ref type="bibr" target="#b53">[54]</ref> and ResNet-101 <ref type="bibr" target="#b27">[28]</ref>, respectively. We fix the input size of the network as 320 × 320 × 3. Simple gray color mapping is adopted to convert a depth map into a three-channel map.</p><p>VGG-16 configuration: For the VGG-16 with the fullyconnected layers removed and having 13 convolutional layers, the side path1∼path6 are successively connected to conv1 2, conv2 2, conv3 3, conv4 3, conv5 3, and pool5. Inspired by <ref type="bibr" target="#b28">[29]</ref>, we add two extra convolutional layers into side path1∼path6. To augment the resolution of the coarsest feature maps from side path6, while at the same time preserving the receptive field, we let pool5 have a stride of 1 and instead use dilated convolution <ref type="bibr" target="#b6">[7]</ref> with a rate of 2 for the two extra side convolutional layers. In general, the coarsest features produced by our final modified VGG-16 backbone have a spatial size of 20 × 20, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>ResNet-101 configuration: Similar to the VGG-16 case above, the spatial size of the coarsest features produced by our modified ResNet-101 backbone is also 20 × 20. As the first convolutional layer of ResNet already has a stride of 2, the features from the shallowest level have a spatial size of 160 × 160. To obtain the full size (320 × 320) features without trivial up-sampling, we borrow the conv1 1 and conv1 2 layers from VGG-16 for feature extraction. Side path1∼path6 are connected to conv1 2, and conv1, res2c, res3b3, res4b22, res5c of the ResNet-101, respectively. We also change the stride of the res5a block from 2 to 1, but subsequently use dilated convolution with rate 2.</p><p>Decoder configuration: All CP modules in <ref type="figure" target="#fig_1">Fig. 2</ref>   are Inception modules. Up-sampling is achieved by simple bilinear interpolation. As depicted in <ref type="figure" target="#fig_1">Fig. 2</ref>, to align the feature sizes in the decoder, the output from an FA module is up-sampled by various factors. In an extreme case, the output from FA5 is up-sampled by a factor of 2, 4, 8, and 16. The final output from FA1 has a spatial size of 320 × 320, which is identical to the initial input.</p><p>Training setup: We implement JL-DCF on Caffe <ref type="bibr" target="#b32">[33]</ref>. During training, the backbone <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b53">54]</ref> is initialized by the pre-trained parameters of DSS <ref type="bibr" target="#b28">[29]</ref>, and other layers are randomly initialized. We fine-tune the entire network through end-to-end joint learning. Training data is augmented by mirror reflection to generate double the amount of data. The momentum parameter is set as 0.99, the learning rate is set to lr = 10 −9 , and the weight decay is 0.0005. The weight λ in Eq. (2) is set as 256 (=16 2 ) to balance the loss between the low-and high-resolution predictions. Stochastic Gradient Descent learning is adopted and accelerated by an NVIDIA 1080Ti GPU. The training time is about 20 hours/18 hours for 40 epochs under the ResNet-101/VGG-16 configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons to SOTAs</head><p>We compare JL-DCF (ResNet configuration) with 14 SOTA methods. Among the competitors, DF <ref type="bibr" target="#b48">[49]</ref>, AFNet <ref type="bibr" target="#b59">[60]</ref>, CTMF <ref type="bibr" target="#b25">[26]</ref>, MMCI <ref type="bibr" target="#b5">[6]</ref>, PCF <ref type="bibr" target="#b3">[4]</ref>, TANet <ref type="bibr" target="#b4">[5]</ref>, CPFP <ref type="bibr" target="#b76">[77]</ref>, D3Net <ref type="bibr" target="#b17">[18]</ref>, DMRA <ref type="bibr" target="#b45">[46]</ref> are recent deep learningbased methods, while ACSD <ref type="bibr" target="#b33">[34]</ref>, LBE <ref type="bibr" target="#b19">[20]</ref>, DCMC <ref type="bibr" target="#b12">[13]</ref>, MDSF <ref type="bibr" target="#b54">[55]</ref>, SE <ref type="bibr" target="#b24">[25]</ref> are traditional techniques using various hand-crafted features/hypotheses. Quantitative results are shown in <ref type="table" target="#tab_2">Table 1</ref>. Notable performance gains of JL-DCF over existing and recently proposed techniques, like CPFP <ref type="bibr" target="#b76">[77]</ref>, D3Net <ref type="bibr" target="#b17">[18]</ref> and DMRA <ref type="bibr" target="#b45">[46]</ref>, can be seen in all four metrics. This validates the consistent effectiveness of JL-DCF and its generalizability. Some visual examples are shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. JL-DCF appears to be more effective at utilizing depth information for cross-modal compensation, making it better for detecting target objects in the RGB-D mode. Additionally, the deeply-supervised coarse predictions are listed in <ref type="figure" target="#fig_3">Fig. 4</ref>. One can see that they provide basic object localization support for the subsequent cross-modal refinement, and our densely-cooperative fusion architecture learns an adaptive and "image-dependent" way of fusing such support with the hierarchical multi-view features. This proves that the fusion process does not degrade in either of the two views (RGB or depth), leading to boosted performance after fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>We conduct thorough ablation studies by removing or replacing components from the full implementation of JL-DCF. We set the ResNet version of JL-DCF as  reference, and then compare various ablation experiments to it. We denote this reference version as "JL-DCF (ResNet+CM+RGB-D)", where "CM" refers to the usage of CM modules and "RGB-D" refers to both RGB and depth inputs.</p><p>Firstly, to compare different backbones, a version "JL-DCF (VGG+CM+RGB-D)" is trained by replacing the ResNet backbone with VGG, while keeping other settings unchanged. To validate the effectiveness of the adopted cooperative fusion modules, we train another version "JL-DCF (ResNet+C+RGB-D)", by replacing the CM modules with a concatenation operation. To demonstrate the effectiveness of combining RGB and depth, we train two versions "JL-DCF (ResNet+RGB)" and "JL-DCF (ResNet+D)" respectively, where all the batch-related operations (such as CM modules) in <ref type="figure" target="#fig_1">Fig. 2</ref> are replaced with identity mappings, while all the other settings, including the dense decoder and deep supervision, are kept unchanged. Note that this validation is important to show that our network has learned complementary information by fusing RGB and depth. Lastly, to illustrate the benefit of joint learning, we train a scheme "SL-DCF (VGG+CM+RGB-D)" using two separate backbones for RGB and depth. "SL" stands for "Separate Learning", in contrast to the proposed "Joint Learning". In this test, we adopt VGG-16, which is smaller, since using two separate backbones leads to almost twice the overall model size.</p><p>Quantitative comparisons for various metrics are shown in <ref type="table">Table 2</ref>. Two SOTA methods CPFP <ref type="bibr" target="#b76">[77]</ref> and D3Net <ref type="bibr" target="#b17">[18]</ref> are listed for reference. <ref type="figure" target="#fig_5">Fig. 5</ref> shows visual ablation comparisons. Five different observations can be made:</p><p>ResNet-101 vs. VGG-16: From the comparison between columns "A" and "B" in <ref type="table">Table 2</ref>, the superiority of the ResNet backbone over VGG-16 is evident, which is consistent with previous works. Note that the VGG version of our scheme still outperforms the leading methods CPFP (VGG-16 backbone) and D3Net (ResNet backbone).</p><p>Effectiveness of CM modules: Comparing columns "A" and "C" demonstrates that changing the CM modules into concatenation operations leads to a certain amount of degeneration. The underlying reason is that the whole network tends to bias its learning towards only RGB information, while ignoring depth, since it is able to achieve fairly good results (column "D") by doing so on the most datasets. Although concatenation is a popular way to fuse features, the learning may become easily trapped without appropriate guidance. In contrast, our CM modules perform the "explicit fusion operation" across RGB and depth modalities. Combining RGB and depth: The effectiveness of combining RGB and depth for boosting the performance is clearly validated by the consistent improvement over most datasets (compare column "A" with columns "D" and "E"). The only exception is on STERE <ref type="bibr" target="#b42">[43]</ref>, with the reason being that the quality of depth maps in this dataset is much worse compared to other datasets. Visual examples are shown in <ref type="figure" target="#fig_5">Fig. 5</ref>, in the 3 rd and 4 th rows. We find that many depth maps from STERE are too coarse and have very inaccurate object boundaries, misaligning with the true objects. Absorbing such unreliable depth information may, in turn, degrade the performance. Quantitative evidence can be seen in <ref type="table">Table 2</ref>, column "E" (STERE dataset), where solely using depth cues achieves much worse performance (about 16%/20% lower on S α /F max β comparing to RGB) than on other datasets.</p><p>RGB only vs. depth only: The comparison between columns "D" and "E" in <ref type="table">Table 2</ref> proves that using RGB data for saliency estimation is superior to using depth in most cases, indicating that the RGB view is generally more informative. However, using depth information achieves better results than RGB on SIP <ref type="bibr" target="#b17">[18]</ref> and RGBD135 <ref type="bibr" target="#b9">[10]</ref>, as visualized in <ref type="figure" target="#fig_5">Fig. 5</ref>. This implies that the depth maps from the two datasets are of relatively good quality.</p><p>Efficiency of JL component: Existing models usually use separate learning approaches to extract features from RGB and depth data, respectively. In contrast, our JL-DCF adopts a joint learning strategy to obtain the features from an RGB and depth map simultaneously. We compare the two learning strategies and find that using separate learning (two separate backbones) is likely to increase the training difficulties. <ref type="figure" target="#fig_6">Fig. 6</ref> shows typical learning curves for such a case. In the separate learning setting, where the initial learning rate is lr = 10 −9 , the network is easily trapped in a local optimum with high loss, while the joint learning setting (shared network) can converge nicely. Further, for separate learning, if the learning rate is set to lr = 10 −10 , <ref type="table">Table 2</ref>: Quantitative evaluation for ablation studies described in Section 4.4. For different configurations, "A": JL-DCF (ResNet+CM+RGB-D), "B": JL-DCF (VGG+CM+RGB-D), "C": JL-DCF (ResNet+C+RGB-D), "D": JL-DCF (ResNet+RGB), "E": JL-DCF (ResNet+D), "F": SL-DCF (VGG+CM+RGB-D). <ref type="bibr">Metric</ref>  the learning process is rescued from local oscillation but converges slowly compared to our joint learning strategy. As shown in columns "B" and "F" in <ref type="table">Table 2</ref>, the resulting converged model after 40 epochs achieves worse performance than JL-DCF, namely 1.1%/1.76% overall drop on S α /F max β . We attribute the better performance of JL-DCF to its joint learning from both RGB and depth data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present a novel framework for RGB-D based SOD, named JL-DCF, which is based on joint learning and densely-cooperative fusion. Experimental results show the feasibility of learning a shared network for salient object localization in RGB and depth views, simultaneously, to achieve accurate prediction. Moreover, the denselycooperative fusion strategy employed is effective for exploiting cross-modal complementarity. JL-DCF shows superior performance against SOTAs on six benchmark datasets and is supported by comprehensive ablation studies. Our framework is quite flexible and general, and its inner modules could be replaced by their counterparts for further improvement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Block diagram of the proposed JL-DCF framework for RGB-D SOD. The JL (joint learning) component is shown in gray, while the DCF (densely-cooperative fusion) component is shown in light green. CP1∼CP6: Feature compression modules. FA1∼FA6: Feature aggregation modules. CM1∼CM6: Cross-modal fusion modules. "H" denotes the spatial size of output feature maps on a particular stage. See Section 3 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>NJU2K [ 34 ] 022 LFSD [ 35 ] 078 SIP [ 18 ]</head><label>340223507818</label><figDesc>M ↓ 0.169 0.208 0.111 0.122 0.090 0.093 0.068 0.055 0.065 0.049 0.046 0.038 0.030 0.030 0.Sα ↑ 0.727 0.729 0.746 0.694 0.692 0.783 0.730 0.788 0.779 0.786 0.794 0.820 0.839 0.824 0.854 F max β ↑ 0.763 0.722 0.813 0.779 0.786 0.813 0.740 0.787 0.767 0.775 0.792 0.821 0.852 0.815 0.862 E max φ ↑ 0.829 0.797 0.849 0.819 0.832 0.857 0.807 0.857 0.831 0.827 0.840 0.864 0.893 0.856 0.893 M ↓ 0.195 0.214 0.162 0.197 0.174 0.146 0.141 0.127 0.139 0.119 0.118 0.095 0.083 0.106 0.Sα ↑ 0.732 0.727 0.683 0.717 0.628 0.653 0.720 0.716 0.833 0.842 0.835 0.850 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Visual comparisons of JL-DCF with SOTA RGB-D saliency models. The jointly learned coarse prediction maps (S c rgb and S c d ) from RGB and depth are also shown together with the final maps (S f ) of JL-DCF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Visual examples from NLPR, STERE, RGB135, SIP datasets for ablation studies. Generally, the full implementation of JL-DCF (ResNet+CM+RGB-D, highlighted in the red box) achieves the closest results to the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Learning curve comparison between joint learning (JL-DCF) and separate learning (SL-DCF).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Quantitative measures: S-measure (Sα)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>are 3 × 3 convolutions with k = 64 filters, and all FA modules</figDesc><table><row><cell>RGB</cell><cell>Depth</cell><cell>MDSF</cell><cell>SE</cell><cell>DF</cell><cell>AFNet</cell><cell>CTMF</cell><cell>MMCI</cell><cell>PCF</cell><cell>TANet</cell><cell>CPFP</cell><cell>D3Net</cell><cell>DMRA</cell><cell>JL-DCF</cell><cell>GT</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Sα ↑ .878 .895 .903 .897 .900 .895 .865 .886 F max β ↑ .877 .889 .903 .899 .898 .892 .863 .883 E max φ ↑ .926 .932 .944 .939 .937 .937 .916 .929 M ↓ .053 .051 .043 .044 .045 .046 .063 .053 Sα ↑ .888 .906 .925 .920 .924 .922 .873 .901 F max β ↑ .868 .885 .916 .907 .914 .909 .843 .881 E max φ ↑ .932 .946 .962 .959 .961 .957 .930 .946 M ↓ .036 .034 .022 .026 .023 .025 .041 .033 Sα ↑ .879 .891 .905 .894 .906 .909 .744 .886 F max β ↑ .874 .881 .901 .889 .899 .901 .708 .876 E max φ ↑ .925 .930 .946 .938 .945 .946 .834 .931 M ↓ .051 .054 .042 .046 .041 .038 .110 .053 RGBD135 [10] Sα ↑ .872 .904 .929 .913 .916 .903 .918 .893 F max β ↑ .846 .885 .919 .905 .906 .894 .906 .876 E max φ ↑ .923 .946 .968 .955 .957 .947 .967 .950 M ↓ .038 .030 .022 .026 .025 .027 .027 .033 Sα ↑ .820 .832 .854 .833 .852 .845 .752 .826 F max β ↑ .821 .819 .862 .840 .854 .846 .764 .828 E max φ ↑ .864 .864 .893 .877 .893 .889 .816 .864 M ↓ .095 .099 .078 .091 .078 .083 .126 .101 Sα ↑ .850 .864 .879 .866 .870 .855 .872 .865 F max β ↑ .851 .862 .885 .873 .873 .857 .877 .863 E max φ ↑ .903 .910 .923 .916 .916 .908 .920 .913 M ↓ .064 .063 .051 .056 .055 .061 .056 .061</figDesc><table><row><cell></cell><cell>CPFP D3Net A</cell><cell>B</cell><cell>C</cell><cell>D</cell><cell>E</cell><cell>F</cell></row><row><cell>NJU2K</cell><cell>[34]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NLPR</cell><cell>[44]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>STERE</cell><cell>[43]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LFSD</cell><cell>[35]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SIP</cell><cell>[18]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Creating a color map to be used to convert a gray image to color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamil</forename><surname>Al Azzeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hussein</forename><surname>Alhatamleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ziad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Alqadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khalil Abuzalata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="31" to="34" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Salient object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVM</title>
		<imprint>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Progressively complementarityaware fusion network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youfu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3051" to="3060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Three-stream attention-aware network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youfu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2825" to="2835" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-modal fusion network with multi-scale multi-path and cross-modal interactions for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youfu</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="376" to="385" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
		</author>
		<title level="m">Sketch2photo: Internet image montage. ACM TOG</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Depth enhanced saliency detection method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjian</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Conference on Internet Multimedia Computing and Service</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Going from rgb to rgbd saliency: A depth-guided transformation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Kwong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCYB</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Saliency detection for stereoscopic images based on depth confidence analysis and multiple cues fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunping</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE SPL</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="819" to="823" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Importance filtering for image retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Salient objects in clutter: Bringing salient object detection to the foreground</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Hua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="196" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structure-measure: A New Way to Evaluate Foreground Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhanced-alignment measure for binary foreground map evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="698" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking RGB-D salient object detection: Models, datasets, and large-scale benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNNLS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Shifting more attention to video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8554" to="8564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Local background enclosure for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2343" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attentive feedback network for boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1623" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Database saliency for fast image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaojing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="359" to="369" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Context-aware saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Stas Goferman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayellet</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2376" to="2383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="198" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Salient object detection for rgb-d image via saliency evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Bei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cnns-based rgb-d saliency detection via cross-view transfer and multiview fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCYB</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3171" to="3183" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised extraction of visual attention objects in color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">King</forename><surname>Ngi Ngan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Jiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="145" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="815" to="828" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rgbd salient object detection using spatially coherent deep learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Posheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Han</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsu-Feng</forename><surname>Hsiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Digital Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image co-segmentation via saliency co-fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Koteswar Rao Jerripothula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1896" to="1909" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Depth saliency based on anisotropic centersurround difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1115" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Saliency detection on light field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2806" to="2813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stereoscopic saliency model using contrast and depth-guided-background prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangfang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhua</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laiyun</forename><surname>Qing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="page" from="2227" to="2238" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A model of visual attention for natural image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengping</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCC-C</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="728" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dhsnet: Deep hierarchical saliency network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="678" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised salient object segmentation based on kernel density estimation and two-phase graph cut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liquan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinzhu</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">King</forename><surname>Ngi Ngan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1275" to="1289" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Salient object detection for rgb-d image by single stream recurrent convolution neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quntao</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">363</biblScope>
			<biblScope unit="page" from="46" to="57" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A generic framework of user attention model and its application in video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Fei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Jiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="907" to="919" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A framework for visual saliency detection with applications to image thumbnailing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Cifarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2232" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Leveraging stereopsis for saliency analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhen</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="454" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rgbd salient object detection: A benchmark and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="92" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="733" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Depth-induced multi-scale recurrent attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongri</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7254" to="7263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep light-field-driven saliency detection from a single view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongri</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkun</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="904" to="911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Pedro O Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="75" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rgbd salient object detection via deep fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangqiong</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingxiong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2274" to="2285" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Is bottom-up attention useful for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ueli</forename><surname>Rutishauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, pages II-II</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning rgb-d salient object detection using background enclosure, depth contrast, and top-down features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riku</forename><surname>Shigematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2749" to="2757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Depth-aware salient object detection and segmentation via multiscale discriminative saliency fusion and bootstrap learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangke</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangling</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwei</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4204" to="4216" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pyramid dilated deeper convlstm for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanyuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kin-Man</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="715" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Attention based auto image cropping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Stentiford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICVS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Rgb-d salient object detection via minimum barrier distance transform and saliency fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE SPL</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="663" to="667" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Adaptive fusion for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="55277" to="55284" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Salient object detection in the deep learning era: An in-depth survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuxia</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09146</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Zero-shot video object segmentation via attentive graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9236" to="9245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep cropping via attention box prediction and aesthetics assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2186" to="2194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">An iterative and cooperative top-down and bottom-up inference network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5968" to="5977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Salient object detection driven by fixation prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingping</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1711" to="1720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Correspondence driven saliency transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5025" to="5034" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Revisiting video saliency prediction in the deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning unsupervised video object segmentation through visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanyuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3064" to="3074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Salient object detection with pyramid attention and salient edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1448" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Salient object segmentation via effective integration of saliency and objectness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liquan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1742" to="1756" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Detection of co-salient objects by looking deep and wide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="232" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Co-saliency detection via a self-paced multiple-instance learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="865" to="878" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">UC-Net: uncertainty inspired rgb-d saliency detection via conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatemeh Sadat</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Learning uncertain convolutional features for accurate saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="212" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Progressive attention guided recurrent network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqing</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="714" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Contrast prior and fluid pyramid integration for rgbd salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Xing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3927" to="3936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">EGNet: Edge guidance network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Xing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jufeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8779" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Multi-mutual consistency induced transfer subspace learning for human motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">PDNet: prior-model guided depth-enhanced network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunbiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="199" to="204" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
