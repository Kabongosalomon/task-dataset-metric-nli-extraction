<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cascade Feature Aggregation for Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Su</surname></persName>
							<email>suzhihui002@pingan.com.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Nanjing Development Team</orgName>
								<orgName type="institution" key="instit2">Ping An Technology (Shenzhen) Co., Ltd</orgName>
								<address>
									<settlement>Nanjing, Jiangsu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Nanjing Development Team</orgName>
								<orgName type="institution" key="instit2">Ping An Technology (Shenzhen) Co., Ltd</orgName>
								<address>
									<settlement>Nanjing, Jiangsu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohui</forename><surname>Zhang</surname></persName>
							<email>zhangguohui128@pingan.com.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Nanjing Development Team</orgName>
								<orgName type="institution" key="instit2">Ping An Technology (Shenzhen) Co., Ltd</orgName>
								<address>
									<settlement>Nanjing, Jiangsu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Nanjing Development Team</orgName>
								<orgName type="institution" key="instit2">Ping An Technology (Shenzhen) Co., Ltd</orgName>
								<address>
									<settlement>Nanjing, Jiangsu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianda</forename><surname>Sheng</surname></persName>
							<email>shengjianda720@pingan.com.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Nanjing Development Team</orgName>
								<orgName type="institution" key="instit2">Ping An Technology (Shenzhen) Co., Ltd</orgName>
								<address>
									<settlement>Nanjing, Jiangsu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cascade Feature Aggregation for Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Human Pose Estimation</term>
					<term>Multi-stage Cascade Network</term>
					<term>Feature Aggregation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human pose estimation plays an important role in many computer vision tasks and has been studied for many decades. However, due to complex appearance variations from poses, illuminations, occlusions and low resolutions, it still remains a challenging problem. Taking the advantage of high-level semantic information from deep convolutional neural networks is an effective way to improve the accuracy of human pose estimation. In this paper, we propose a novel Cascade Feature Aggregation (CFA) method, which cascades several hourglass networks for robust human pose estimation. Features from different stages are aggregated to obtain abundant contextual information, leading to robustness to poses, partial occlusions and low resolution. Moreover, results from different stages are fused to further improve the localization accuracy. The extensive experiments on MPII datasets and LIP datasets demonstrate that our proposed CFA outperforms the state-of-the-art and achieves the best performance on the state-of-the-art benchmark MPII.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation refers to the task of recognizing postures by localizing body keypoints from images. It is vital prerequisite step for many computer vision tasks such as human action recognition <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b32">34]</ref>, tracking <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b34">36]</ref>, human-computer interaction <ref type="bibr" target="#b35">[37]</ref> and video surveillance <ref type="bibr" target="#b39">[41]</ref>. In the past few decades, many efforts are devoted to build robust human pose estimation models under the controlled and uncontrolled setting <ref type="bibr" target="#b38">[40]</ref>. The typical methods include pictorial structures models, hierarchical models and non-tree models <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b41">43]</ref>. The pictorial structures model constructs a classical tree-structured graphical framework by exploring spatial correlations between parts of the body and kinematic priors that couple connected limbs. The hierarchical models represent the relationships between parts at different scales in a hierarchical tree structure, leading to capture high-order relationships among parts and characterize an exponential number of plausible poses <ref type="bibr" target="#b43">[46]</ref>. Non-tree models use loops to augment the tree structure with additional edges, which can well capture symmetry, occlusion and long-range relationships. Although the methods mentioned above achieves promising results for human pose estimation under the controlled settings, they usually degenerate severely under the wild scenario due to complex appearance variations from various poses, different illuminations, partial occlusions, etc.</p><p>Recently great progresses have been achieved by employing deep neural networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b38">40]</ref>. As an encoder-decoder architecture, hourglass network <ref type="bibr" target="#b6">[7]</ref> can well capture information of various scales for robust human pose estimation. Moreover, several hourglass networks are stacked to implement a mechanism for repeated bottom-up, top-down inference allowing for reevaluation of initial estimates and features across the whole image. As an encoder-decoder model, hourglass use a highway to connect the encoder and decoder parts. More detail local information is brought to decoder to improve the performance. Different from stacked hourglass, CPN <ref type="bibr" target="#b37">[39]</ref> design another strategy for cascading multiple stages, i.e., a GlobalNet followed by a RefineNet. The GlobalNet aims at obtaining an initial human pose which is slightly different from FPN <ref type="bibr" target="#b42">[44]</ref> by applying 1 x 1 convolutional kernel before each element-wise sum procedure in the up-sampling process. The following RefineNet is designed to explicitly address the "hard" keypoints. Besides the top-down methods mentioned above, the bottom-up method like OpenPose <ref type="bibr" target="#b38">[40]</ref> can also achieve promising results for real-time human pose estimation.</p><p>In this paper, we further push the frontier of the area by resorting to cascade deep networks. We propose a novel Cascade Feature Aggregation (CFA) method to improve the human pose estimation results by leveraging featrues at different stages of the cascade networks, as shown in <ref type="figure">Fig. 1</ref> Instead of a single stage network, our CFA consists of everal hourglass networks, each figuring out part of the nonlinearity for mapping the input image to the corresponding human body keypoints. Specically, the first stage predicts human poses by taking an input as input and the following stage levearges the featuers of shallow and deep layers from the previous stage to further improve the estimation results. Features from different stages are aggregated to obtain both local detailed information and global context information for robust human pose estimation. Benifited from the advantages of aggregating features from different stages, our CFA is more robust to poses, illuminations and partial occlusions than <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9]</ref>. Besides, our CFA fuse the results from each stage to further improve the human pose estimation results.</p><p>The main contributions of this paper can be summarized as: 1) We proposed a novel Cascade Feature Aggregation (CFA) method for robust human pose estimation by leveraging features from different stages under a cascade structure.</p><p>2) By fusing the results from different stages, CFA can further improve the results for human pose estimation.</p><p>3) Our CFA outperforms the state-of-the-art methods and achieves the best performance on the MPII benchmark.  <ref type="figure">Figure 1</ref>. The architecture of the proposed CFA. The CFA consists of several stages and each stage is designed to predict the keypoints of the human body. We perform feature aggregation by taking the features from the preview stage as the input for the current stage. Besides, results from multiple stages are fused to further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Human pose estimation is an active research topic for decades. Traditional methods rely on hand-craft features, which formulate the problem of human keypoints estimation as a tree-structured or graphical model problem. Recently, many approaches take advantage of deep convolutional neural network to enhance the performance of pose estimation. In terms of network architecture, current human pose estimation methods could be divided into two categories: single-stage approaches and multi-stage approaches.</p><p>Single-Stage Approach. The most single-stage approaches concentrate on designing the basic network structure. Hourglass is one of the most typical single-stage approaches. The hourglass considers the information at different scale. High layers have a coherent understanding of the full body, and high way connections bring the essential local evidence to high layers for identifying features like faces and hands.</p><p>Many researches are devoted to improving the basic network architecture of hourglass. CPN takes the advantage of both hourglass and FPN, leading to the best results on COCO 2017. Xiao et al. simplify the hourglass by removing the high way connection and find four is the best kernel size for deconvolution <ref type="bibr" target="#b9">[11]</ref>. Multi-Stage Approach. The multi-stage approach focuses on building multiple stages to further improving the performance. CPM <ref type="bibr" target="#b22">[24]</ref> firstly introduces multi-stage structure with several convolution and pooling layers, which reaches 88.5% PCKh on MPII test set. CPM illustrates that a sequential architecture composed of convolutional networks is capable of implicitly learning a spatial model for pose by communicating increasingly refined uncertainty-preserving beliefs between stages. Stack hourglass stacks several hourglass networks to achieve more powerful architecture for human pose estimation. For approximately the same number of parameters, the most improvement in final performance for each successive network increases from stacking. Finally, the stacked hourglass improves the PCKh to 90.9% on MPII. Furthermore, several attempts have been made to improve the backbone network for each stage of stacked hourglass. Xiao proposed HRUs to replace the basic high way connections <ref type="bibr" target="#b27">[29]</ref>, which can enrich Generally multi-stage architecture performs better than single-stage methods. And an elaborate design for each stage is also important. Hourglass is a good basic architecture for human pose estimation. The main difference of our CFA from the Belagiannis et al. <ref type="bibr" target="#b20">[22]</ref> is that we employ multi-stage structure rather than RNN and induce connections between predictions for better results. Moreover, instead of using large convolution kernels proposed in Belagiannis et al. <ref type="bibr" target="#b20">[22]</ref> and Wei et al. <ref type="bibr" target="#b22">[24]</ref>, we obtain large receptive fields by aggregating features. The key difference between our CFA and Newell <ref type="bibr" target="#b6">[7]</ref>, Chu <ref type="bibr" target="#b27">[29]</ref>, Zhang <ref type="bibr" target="#b0">[1]</ref> is that the existing methods only take the outputs from the previous stage as the input for the current stage while our CFA takes both the inputs and outputs from the previous stage for the next stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Network Architecture</head><p>In this paper, we present a novel Cascade Feature Aggregation (CFA) for robust human pose estimation.</p><p>Firstly, we will give an overview of our CFA. Secondly, we will describe the backbone model designed for each stage. Thirdly, we will illustrate the details of cascade feature aggregation. Then we will introduce a roust result fusion approach by leveraging heatmaps of several stages. And finally, we will give a detailed discussion about the difference from the existing works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>As shown in <ref type="figure">Fig. 1</ref>, the proposed CFA attempts to design a general cascade framework in a coarse-to-fine architecture. Specifically, the CFA framework consists of several successive cascaded hourglass with features aggregations. Each stage attempts to characterize the nonlinear mappings from body image to body keypoints in same feature input transformed from the original image.</p><p>The first stage endeavors to roughly approximate the body keypoint locations, and therefore an hourglass is designed for predict the keypoints. After getting an estimation of keypoints from the first stage, the successive hourglasses make an effort to refine the shape by combining the preview prediction and current prediction. Furthermore, the final heat map is averaged by the last few stages for further improving the keypoints predictions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Backbone model</head><p>To meets the requirements of capturing various information of different scales, we resort to Hourglass as our backbone model. In the original Hourglass, 4 is chosen as the kernel size of deconvolution <ref type="bibr" target="#b9">[11]</ref>, which can ensure every point is interpolated by 4 points when conducting up sampling. To improve Hourglass, we employ a deeper network architecture ResNet as backbone model for both encoder and decoder parts. In our experiments, we find that it is useful to make the networks deeper for both the encoder and decoder and is more effective for the encoder. The reason behind it is that encoder with deep network can well capture semantic information at different scales. Several typical ResNets like ResNet-50, ResNet-101 and ResNet-152 are compared and investigated in Sec. 4.3.</p><p>For backbone model, the seeking function f can be expressed as ( )</p><formula xml:id="formula_0">( ) ( ) ( ) = f f f f x x , where</formula><p>x is the input image for prediction,</p><formula xml:id="formula_1">1 2 3</formula><p>,, f ff is three parts of the network, and i a is recorded as the result of i f , then we can achieve that ( )</p><formula xml:id="formula_2">11 a fx = , ( ) 22 1 a f a = , ( ) 33 2 a f a = , 3 ya = .</formula><p>Finally we calculate prediction of the keypoints z by () z g y = , g donates the strategy to calculate the keypoint from feature map. In this paper, we treat the coordinate of max value on the heatmap as the prediction function g . <ref type="figure">Figure 2</ref>. ResNet based hourglass network. The network is an encoder-decoder model, which is designed based on hourglass. ResNet is used as the basic structure for the encoder parts. Highway connection is employed from the encoder to the decoder.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cascaded Feature aggregation</head><p>Here, j donates the j-th stage. For the first stage, the network output    </p><formula xml:id="formula_5">( ) 1,1 1,1 a f x =<label>(3)</label></formula><p>For the successive stages, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Results improvement by fusing heatmaps</head><p>We fuse the heatmaps of the last few stages to further improve the results. As shown in <ref type="figure" target="#fig_11">Fig.4</ref>, the prediction is determined by averaging the output heatmaps of last several stages:  Differences with Zhang et al. <ref type="bibr" target="#b0">[1]</ref>. Both Zhang et al. <ref type="bibr" target="#b0">[1]</ref> and our CFA use hourglass for each stage and prediction fusion with prediction aggregated from preview stage. But they differ in the following aspects: 1)</p><p>Zhang et al. <ref type="bibr" target="#b0">[1]</ref> use a PGNN for further enhancing the accuracy. They believe that the contextual information propagated from confident parts through graph helps reducing error. Differently, we cascade several stages to get high-level semantic features which can capture both global semantic information and local detailed information, leading to better results. 2) Zhang et al. use the ResNet-50 as backbone model, the high layers feature map of preview stage as the input of current stage. We also use ResNet as the backbone model, and ResNet-101 is suggested for first stage. ResNet has four residual blocks, only the last three blocks are used in the following stages, and the first stage is shared by input aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we firstly illustrate the experimental settings for the evaluations including the datasets and metric for comparison; secondly explanation the training details; thirdly investigation of our CFA;</p><p>then presenting the improvement of results fusion and inference of training with additional data; finally compare the proposed CFA with the state-of-the-art methods.</p><p>We train and test our model on a server with 8 NVIDIA Tesla V100 16G GPUs. All the experiments are doing on the PyTorch 1.0 with NVIDIA apex. All the batch normal used in the net is cross-GPU synchronized batch normal. The batch size is 64 for every experiment. The input size of image is 384x384, multi augmentation is used, like rotation, flipping, scaling, brightness, contrast, saturation, hue. The start of learning rate is 5e-4 and the decay weight is 0.3 for epoch 6, 10, 13. The gradient scheme is Adam. We compare our method with a few state-of-the-art methods, i.e., DLCM <ref type="bibr" target="#b1">[2]</ref>, Adversarial PoseNet <ref type="bibr" target="#b29">[31]</ref>,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and evaluation metric</head><p>MCA <ref type="bibr" target="#b27">[29]</ref>. The standard PCKh metric <ref type="bibr" target="#b9">[11]</ref> is employed for evaluation. All the performance listed below is showed in PCKh @ 0. We further exploit the network architectures for the following stages. In our experiments, we fix the first stage with ResNet-101 and evaluate the successive stages with ResNet-50 and ResNet-101 respectively.</p><p>Results of different stages are shown in <ref type="table" target="#tab_3">Table 2</ref>. ResNet-50 achieves the comparable results with ResNet-101. It can be concluded that RestNet-50 is deep enough for the following stages. After determining the backbone models for each stage, we explore how many stages of CFA is enough for human pose estimation. We employ ResNet-101 for the first stage and ResNet-50 for the other stages.</p><p>The model is trained on MPII dataset. As shown in <ref type="figure" target="#fig_12">Fig. 5</ref>, the performance is increased gradually with more and more stages. After cascading four stages, the performance is keeping stable.</p><p>Besides, more training data may need more stages to well capture the complex non-linearity from images to human poses. So, we do a series of experiments to seek how many stages can achieve the performance peak with more training data. The model is trained on both the MPII and HSSK datasets. As seen in <ref type="figure" target="#fig_12">Fig. 5</ref>, cascading one more stage can achieve better result and additional training data is helpful for improving final results. The influence of additional training data is detailly discussed in the following section. In the following experiments, we all takes ResNet-101 for the first stage and ResNet-50 for the last stages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comprehensive evaluations for each stage and results fusion</head><p>In this section, we give the comprehensive evaluations for each stage and the results fusion. As shown in <ref type="table" target="#tab_4">Table 3</ref>, CFA with more and more stages performs better and better. Moreover, for CFA with quad-stages, the performance gradually improves from 94.35 at stage 1 to 95.82 at stage 4, which demonstrate the effectiveness of cascade feature aggregation. Benefited from the results fusing, the human pose estimation result can be further improved to 95.85. We can easily find that the lower stage's performance will be increased with the depth of the stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Training with additional data</head><p>To exploit how training data contributes to the performance improvement for human pose estimation,</p><p>we conduct experiments on MPII dataset and HSSK database. As shown in <ref type="table" target="#tab_5">Table 4</ref>, the model trained on only MPII dataset achieves 89.95 PCKh @ 0.5, and the performance keeps stable with four stages. After training with additional dataset HSSK, the performance is significantly improved to 92.24 and cascading more stages, i.e., five stages can achieve better results. The additional data is crucial for improving performance.  Firstly, we evaluate our CFA with the existing methods on MPII. Ke et al. stack the stages by using high-level feature map from preview stage and achieve 92.1 PCKh @ 0.5 <ref type="bibr" target="#b8">[9]</ref>. Zhang et al. <ref type="bibr" target="#b0">[1]</ref> further use the relation between keypoints to improve performance and achieve better results than Ke et al. <ref type="bibr" target="#b8">[9]</ref>. Our CFA performs better than Zhang et al. <ref type="bibr" target="#b0">[1]</ref> and achieves the best results of 93.3 PCKh @ 0.5 on the MPII, which attributes the cascade feature aggregation and results fusion. <ref type="figure" target="#fig_13">Fig. 6</ref> shows the visualization results of our CFA for the first stage and the last stage. As seen, the results of the first stage may fail when two people intersect or body parts are partially occluded. Thanks to the global semantic features from the last stage, the results can be significantly improved and CFA nearly achieves the perfect results for the hard cases mentioned above.  Furthermore, we conduct experiments on LIP dataset. The comparison results are shown in <ref type="table" target="#tab_7">Table 6</ref>. As seen, the similar conclusion can be obtained that our CFA achieves the best results, which demonstrate the effectiveness of capturing both local detailed information and global semantic information by cascade feature aggregation. <ref type="figure">Fig. 7</ref> shows some failure cases of our method. As seen, the performance degrades on some images which has complex illumination, low resolution and motion blur, partially due to the lack of such samples in training set. <ref type="figure">Figure 7</ref>. The visualization results of some failure cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we propose a novel method CFA for robust human pose estimation, which cascades several hourglasses and aggregates features of low-level, middle-level and high-level to well capture local detailed information and global semantic information. Moreover, the proposed CFA exploits ResNet-101</p><p>and ResNet-50 for the first stage and the following stages respectively, which achieves a good trade-off of both accuracy and efficiency. The CFA achieves better results than the state-of-the-art methods like Ke et al. <ref type="bibr" target="#b8">[9]</ref> and Zhang et al. <ref type="bibr" target="#b0">[1]</ref>. on two datasets MPII and LIP. Besides, the experiments results show that the data diversity is extremely important for improving performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Moreover,</head><label></label><figDesc>Tang et al. attempt to add connections and intermediate supervision to avoid potentially large state spaces for higher-level parts, which can overcome some overlapping parts and clutter backgrounds problems [2]. Different from the hourglass and its variant, OpenPose predict the keypoints under a bottom-up pipeline, which first detect the body parts and then joint them up. The algorithm can perform in real-time for human pose estimation on a Nvidia GTX 1080 Ti GPU platform.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>the expressive power of conventional residual unit. Xiao et al. also use multi-context attention to guide context modeling, and CRF replace the global softmax for spatial correlation modeling during training. It achieves a further improvement of 91.5% on MPII. Yang et al. propose PRMs to replace the basic convolutions and deconvolutions [42], which enhances the invariance in scales of the DCNNs and improve the PCKh to 92.0%. Ke et al. [2] propose multi-scale supervision network to enhance the supervision, which works hand-in-hand with the structure-aware loss design, to infer high-order structural matching of detected body keypoints, and improve the PCKh to 92.1%. Li et al. employ ResNet as an encode part and propose feature aggregation across stages, which results in the best performance on COCO keypoint challenge 2018.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>.</head><label></label><figDesc>We use gaussian kernel to present the p points in an image with p channels, i.e., the heatmaps for p points, denoted as i y . The human pose estimation task can be reformulated as seeking a mapping function ( )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>, , 3</head><label>3</label><figDesc>ij a is learn to approximate the ground truth heatmap * i y . For the latter stages, a non-linearity function ,4 j  is designed by taking the heatmap from previous stage as input and the output from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>are further added with , ,3 ij a to achieve the final heatmap estimation. It can amplify points with high confidents and block the miss predicted points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 .a</head><label>3</label><figDesc>Aggregation between different stage of CFA. There are three different aggregation between different stages. The input aggregation brings the local detail information for miss predicted point for a second time prediction. The feature aggregation takes the high layer sematic to input layer. And the prediction aggregation keeps the prediction more stable. is the input of current stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>3 f 1 a</head><label>31</label><figDesc>respectively in the backbone model mentioned in Sec. 3.2. For the first stage, 1,is calculated from the original image:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>consists of three parts, i.e., low-level features, middle-level features and high-level features from the previous stage, as shown inFig. 3. The low-level features contain detailed local information which is beneficial for localizing the exact positions for human parts. While high-level features contain global semantic information, which can improve the performance for partial occlusions and complex backgrounds. Specifically, the ,1 j a can be achieved by the following equation: donate low level, middle level, high level aggregation functions respectively from the preview stage for the inputs of the current stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>denotes the heatmaps of the last N stages. The fusion strategy makes the results more stable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 4 .</head><label>4</label><figDesc>Heatmap fusion. The final heat map is averaged by the last several prediction of heat map.3.5. DiscussionsDifferences with Li et al.<ref type="bibr" target="#b5">[6]</ref>. Both model use hourglass and the coarse-to-fine supervision for different stages. But they differ in the following aspects: Li et al. use cross stage feature aggregation. two separate information flows are introduced from down-sampling and up-sampling units of the previous stage to the down-sampling procedure of the current stage for each scale. But we use low level, middle level and high-level aggregation as the input of next stage. We think different features from different stages are aggregated to obtain both local detailed information and global context information for robust human pose estimation. The high-level global context can highlight the important local parts, which can help refine the predictions. On the other hand, the low-level original detail information can help re-predict the miss predicted keypoints based on the global information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 5 .</head><label>5</label><figDesc>Comparison of different multi-stage models (PCKh @ 0.5). For each line from left to right, for example ResNet-101 cascaded with ResNet-50, the first point means single stage of ResNet-101, the second point means a double-stages model constructed from ResNet-101 and cascaded with ResNet-50, and so on.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 6 .</head><label>6</label><figDesc>The visualization results of CFA for the first and last stages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Multi-stage model is designed to enhance the prediction of the invisible keypoints. Suppose we have a</figDesc><table><row><cell>training set gaussian function. We calculate the predicted heatmap of each stage by, ( )  , N ii x z , we mapping the ground truth keypoints  1 i =</cell><cell>i z to the heatmap * i y by</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>5, which means PCK measure that uses the matching threshold as 50% of the head segment length. Moreover, the double flipping test method is utilized for better performance, which calculates the final heat map by averaging the heat maps of the original image and the flipped image. The final predictions are determined by selecting the point from the maximum response of the multi-stage heat maps.When conducting experiments on MPII, two kind of training sets are used, i.e., only the MPII training dataset, and both the MPII training dataset and HSSK (Human Skeletal System Keypoints [10]). The first stage of CFA is initialized from the model trained on ImageNet and the parameters of other stages are randomly initialized. In our experiments, it is hard for CFA of more than three stages to achieve our CFA consists of multi-stages, we investigate different ResNet architectures as backbone models for each stage. Firstly, we evaluate different backbone models, i.e., ResNet-50, ResNet-101 and ResNet-152 for the first stage.Table 1shows the prediction results of stage one for human pose estimation.MPII training data is used for training. As seen, ResNet-101 performs better than ResNet-50 and ResNet-152 performs slightly better than ResNet-101. The performance is gradually improved with deeper and deeper networks. In consideration of both accuracy and computation cost, ResNet-101 is chosen for the first stage.</figDesc><table><row><cell cols="2">Since Table 1. Performance of backbone models</cell></row><row><cell cols="2">The first Stage PCKh @ 0.5</cell></row><row><cell>ResNet-50</cell><cell>88.48</cell></row><row><cell>ResNet-101</cell><cell>89.26</cell></row><row><cell>ResNet-152</cell><cell>89.59</cell></row></table><note>4.2. Training detailsconvergence with random initializations, which means you should firstly train a triple-stages CFA and then employ the triple-stages CFA model for initializations for leaning a quad-stages CFA model. For experiments on LIP, all the models are initialized for the models trained on MPII.4.3. Investigation of our CFA</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">Compared between different successive ResNet (PCKh @ 0.5)</cell></row><row><cell>Net</cell><cell>Double-Stage</cell><cell>Triple-Stage</cell><cell>Quad-Stage</cell></row><row><cell>Cascaded with ResNet-50</cell><cell>89.72</cell><cell>89.95</cell><cell>89.95</cell></row><row><cell>Cascaded with ResNet-101</cell><cell>89.73</cell><cell>89.94</cell><cell>89.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Intermediate and fused predictions for multi-stage model</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">(MPII Validation, PCKh @ 0.5)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Net</cell><cell>Stage</cell><cell>Head</cell><cell cols="2">Shoulder Elbow</cell><cell>Wrist</cell><cell>Hip</cell><cell cols="3">Knee Ankle Total</cell></row><row><cell>Single-Stage</cell><cell>1</cell><cell>95.49</cell><cell>95.33</cell><cell>90.29</cell><cell>85.81</cell><cell cols="2">88.74 86.49</cell><cell>82.01</cell><cell>89.26</cell></row><row><cell>Double-Stages</cell><cell>1</cell><cell>95.32</cell><cell>95.00</cell><cell>90.35</cell><cell>86.04</cell><cell cols="2">89.48 87.17</cell><cell>81.92</cell><cell>89.41</cell></row><row><cell>Double-Stages</cell><cell>2</cell><cell>95.43</cell><cell>95.05</cell><cell>90.59</cell><cell>86.52</cell><cell cols="2">89.62 87.69</cell><cell>82.51</cell><cell>89.72</cell></row><row><cell>Triple-Stages</cell><cell>1</cell><cell>94.97</cell><cell>95.55</cell><cell>90.87</cell><cell>86.29</cell><cell cols="2">88.14 87.07</cell><cell>82.66</cell><cell>89.52</cell></row><row><cell>Triple-Stages</cell><cell>2</cell><cell>95.31</cell><cell>95.65</cell><cell>91.07</cell><cell>86.51</cell><cell cols="2">88.35 87.35</cell><cell>83.28</cell><cell>89.77</cell></row><row><cell>Triple-Stages</cell><cell>3</cell><cell>96.00</cell><cell>95.65</cell><cell>91.07</cell><cell>86.46</cell><cell cols="2">89.25 87.09</cell><cell>83.63</cell><cell>89.95</cell></row><row><cell>Triple-Stages</cell><cell>fused</cell><cell>96.13</cell><cell>95.72</cell><cell>91.33</cell><cell>86.40</cell><cell cols="2">89.25 87.53</cell><cell>83.61</cell><cell>90.06</cell></row><row><cell>Quad-Stages</cell><cell>1</cell><cell>94.35</cell><cell>95.44</cell><cell>90.81</cell><cell>86.64</cell><cell cols="2">88.78 87.06</cell><cell>82.35</cell><cell>89.53</cell></row><row><cell>Quad-Stages</cell><cell>2</cell><cell>94.69</cell><cell>95.46</cell><cell>91.10</cell><cell>86.81</cell><cell>88.7</cell><cell>87.63</cell><cell>83.53</cell><cell>89.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">CFA trained on different datasets (PCKh @ 0.5)</cell><cell></cell></row><row><cell>Data</cell><cell cols="2">Single-Stage Double-Stage</cell><cell>Triple-Stage</cell><cell>Quad-Stage</cell><cell>Five-Stage</cell></row><row><cell>MPII Dataset</cell><cell>89.26</cell><cell>89.72</cell><cell>89.95</cell><cell>89.95</cell><cell>-</cell></row><row><cell>MPII &amp; HSSK Datasets</cell><cell>90.36</cell><cell>91.43</cell><cell>91.94</cell><cell>92.15</cell><cell>92.24</cell></row><row><cell cols="3">4.6. Comparison with the Existing Methods</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table><row><cell cols="7">Comparison with the existing methods on MPII test set (PCKh @ 0.5)</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Head Shoulder Elbow</cell><cell cols="2">Wrist Hip</cell><cell cols="3">Knee Ankle PCKh</cell></row><row><cell>Pishchulin et al. [13]</cell><cell>74.3</cell><cell>49.0</cell><cell>40.8</cell><cell>34.1</cell><cell>36.5</cell><cell>34.4</cell><cell>35.2</cell><cell>44.1</cell></row><row><cell>Tompson et al. [14]</cell><cell>95.8</cell><cell>90.3</cell><cell>80.5</cell><cell>74.3</cell><cell>77.6</cell><cell>69.7</cell><cell>62.8</cell><cell>79.6</cell></row><row><cell>Carreira et al. [15]</cell><cell>95.7</cell><cell>91.7</cell><cell>81.7</cell><cell>72.4</cell><cell>82.8</cell><cell>73.2</cell><cell>66.4</cell><cell>81.3</cell></row><row><cell>Tompson et al. [16]</cell><cell>96.1</cell><cell>91.9</cell><cell>83.9</cell><cell>77.8</cell><cell>80.9</cell><cell>72.3</cell><cell>64.8</cell><cell>82.0</cell></row><row><cell>Hu &amp; Ramanan. [17]</cell><cell>95.0</cell><cell>91.6</cell><cell>83.0</cell><cell>76.6</cell><cell>81.9</cell><cell>74.5</cell><cell>69.5</cell><cell>82.4</cell></row><row><cell>Pishchulin et al. [18]</cell><cell>94.1</cell><cell>90.2</cell><cell>83.4</cell><cell>77.3</cell><cell>82.6</cell><cell>75.7</cell><cell>68.6</cell><cell>82.4</cell></row><row><cell>Lifshitz et al. [19]</cell><cell>97.8</cell><cell>93.3</cell><cell>85.7</cell><cell>80.4</cell><cell>85.3</cell><cell>76.6</cell><cell>70.2</cell><cell>85.0</cell></row><row><cell>Gkioxary et al. [20]</cell><cell>96.2</cell><cell>93.1</cell><cell>86.7</cell><cell>82.1</cell><cell>85.2</cell><cell>81.4</cell><cell>74.1</cell><cell>86.1</cell></row><row><cell>Rafi et al. [21]</cell><cell>97.2</cell><cell>93.9</cell><cell>86.4</cell><cell>81.3</cell><cell>86.8</cell><cell>80.6</cell><cell>73.4</cell><cell>86.3</cell></row><row><cell>Belagiannis &amp; Zisserman [22]</cell><cell>97.7</cell><cell>95.0</cell><cell>88.2</cell><cell>83.0</cell><cell>87.9</cell><cell>82.6</cell><cell>78.4</cell><cell>88.1</cell></row><row><cell>Insafutdinov et al. [23]</cell><cell>96.8</cell><cell>95.2</cell><cell>89.3</cell><cell>84.4</cell><cell>88.4</cell><cell>83.4</cell><cell>78.0</cell><cell>88.5</cell></row><row><cell>Wei et al. [24]</cell><cell>97.8</cell><cell>95.0</cell><cell>88.7</cell><cell>84.0</cell><cell>88.4</cell><cell>82.8</cell><cell>79.4</cell><cell>88.5</cell></row><row><cell>Bulat &amp; Tzimiropoulos [25]</cell><cell>97.9</cell><cell>95.1</cell><cell>89.9</cell><cell>85.3</cell><cell>89.4</cell><cell>85.7</cell><cell>81.7</cell><cell>89.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Comparison with the existing methods on LIP test set (PCKh @ 0.5)</figDesc><table><row><cell>Method</cell><cell>Head</cell><cell>Shoulder</cell><cell cols="2">Elbow Wrist</cell><cell>Hip</cell><cell>Knee</cell><cell>Ankle</cell><cell>UBody</cell><cell>Total</cell></row><row><cell>Hybrid Pose Machine, Liao et al.</cell><cell>71.7</cell><cell>87.1</cell><cell>82.3</cell><cell>78.2</cell><cell>69.2</cell><cell>77.0</cell><cell>73.5</cell><cell>79.8</cell><cell>77.2</cell></row><row><cell>BUPTMM-POSE, Liu et al.</cell><cell>90.4</cell><cell>87.3</cell><cell>81.9</cell><cell>78.8</cell><cell>68.5</cell><cell>75.3</cell><cell>75.8</cell><cell>84.8</cell><cell>80.2</cell></row><row><cell>anonymous</cell><cell>93.9</cell><cell>88.6</cell><cell>82.2</cell><cell>78.3</cell><cell>69.7</cell><cell>73.3</cell><cell>73.2</cell><cell>86.0</cell><cell>80.6</cell></row><row><cell>Tightly Connected ResNet-101</cell><cell>93.2</cell><cell>87.9</cell><cell>82.3</cell><cell>79.8</cell><cell>69.1</cell><cell>75.5</cell><cell>75.4</cell><cell>85.9</cell><cell>81.0</cell></row><row><cell>anonymous</cell><cell>92.8</cell><cell>88.5</cell><cell>83.4</cell><cell>82.0</cell><cell>67.1</cell><cell>76.4</cell><cell>76.3</cell><cell>86.8</cell><cell>81.4</cell></row><row><cell>anonymous</cell><cell>93.1</cell><cell>89.6</cell><cell>84.5</cell><cell>82.4</cell><cell>69.8</cell><cell>77.3</cell><cell>77.7</cell><cell>87.5</cell><cell>82.5</cell></row><row><cell>MPP, Tao et al.</cell><cell>93.6</cell><cell>89.7</cell><cell>84.5</cell><cell>82.8</cell><cell>70.1</cell><cell>77.0</cell><cell>78.5</cell><cell>87.8</cell><cell>82.8</cell></row><row><cell>anonymous</cell><cell>93.6</cell><cell>89.8</cell><cell>85.0</cell><cell>83.7</cell><cell>70.4</cell><cell>78.8</cell><cell>78.9</cell><cell>88.1</cell><cell>83.3</cell></row><row><cell>anonymous</cell><cell>94.2</cell><cell>91.9</cell><cell>87.5</cell><cell>84.2</cell><cell>75.0</cell><cell>83.4</cell><cell>83.0</cell><cell>89.6</cell><cell>85.9</cell></row><row><cell>Zll</cell><cell>94.3</cell><cell>92.1</cell><cell>87.8</cell><cell>85.3</cell><cell>75.3</cell><cell>82.8</cell><cell>82.3</cell><cell>90.0</cell><cell>86.0</cell></row><row><cell>anonymous</cell><cell>94.2</cell><cell>92.4</cell><cell>88.5</cell><cell>86.1</cell><cell>76.0</cell><cell>83.6</cell><cell>82.5</cell><cell>90.4</cell><cell>86.5</cell></row><row><cell>cs_ft-2, Li et al.</cell><cell>94.4</cell><cell>92.4</cell><cell>88.3</cell><cell>86.0</cell><cell>76.1</cell><cell>84.7</cell><cell>85.0</cell><cell>90.4</cell><cell>87.0</cell></row><row><cell>LTL</cell><cell>94.9</cell><cell>93.1</cell><cell>89.9</cell><cell>87.6</cell><cell>75.9</cell><cell>84.9</cell><cell>84.4</cell><cell>91.4</cell><cell>87.5</cell></row><row><cell>EP108</cell><cell>95.0</cell><cell>93.0</cell><cell>89.1</cell><cell>86.7</cell><cell>75.9</cell><cell>85.5</cell><cell>86.2</cell><cell>91.1</cell><cell>87.6</cell></row><row><cell>IR-new</cell><cell>94.1</cell><cell>93.1</cell><cell>88.4</cell><cell>86.5</cell><cell>78.0</cell><cell>85.9</cell><cell>86.0</cell><cell>90.6</cell><cell>87.6</cell></row><row><cell>ByteDance-SEU-Baseline, Su et al.</cell><cell>95.8</cell><cell>94.4</cell><cell>91.7</cell><cell>89.6</cell><cell>80.2</cell><cell>89.5</cell><cell>89.2</cell><cell>93.0</cell><cell>90.2</cell></row><row><cell>JDAI-human, Li et al.</cell><cell>95.9</cell><cell>94.8</cell><cell>92.3</cell><cell>90.4</cell><cell>81.4</cell><cell>90.3</cell><cell>90.2</cell><cell>93.4</cell><cell>90.9</cell></row><row><cell>Ours Five-Stage Model</cell><cell>95.8</cell><cell>94.7</cell><cell>92.5</cell><cell>90.4</cell><cell>81.7</cell><cell>90.6</cell><cell>90.5</cell><cell>93.4</cell><cell>91.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Human Pose Estimation with Spatial Contextual Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01760</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeply learned compositional models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning to Refine Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fieraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao-Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rethinking on Multi-Stage Networks for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00148</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-scale structure-aware network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lipeng</forename><surname>Ke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down reasoning with hierarchical rectified gaussians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human pose estimation using deep consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ita</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Chained predictions using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An Efficient Convolutional Network for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umer</forename><surname>Rafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Quantized densely connected u-nets for efficient landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Knowledge-guided deep fractal neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghan</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiquan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1246" to="1259" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Human pose regression by combining indirect part detection and contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Diogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Tabia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Picard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02322</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self adversarial training for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Jung</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jui-Ting</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asia-Pacific Signal and Information Processing Association Annual Summit and Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adversarial PoseNet: A structure-aware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An approach to pose-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An expressive deep model for human action parsing from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhujin</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adaptive occlusion state estimation for human pose tracking under self-occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">-</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Gyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="649" to="661" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint action recognition and pose estimation from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Real-Time Human Pose Recognition in Parts from Single Depth Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the Acm</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="116" to="124" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08008</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3d human pose estimation: A review of the literature and analysis of covariates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Sarafianos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Using linking features in learning non-parametric part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Human pose estimation using body parts dependent joint regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Dantone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Exploring the spatial hierarchy of mixture models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasa</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
