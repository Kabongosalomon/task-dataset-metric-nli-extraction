<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hyperspherical Variational Auto-Encoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">R</forename><surname>Davidson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Falorsi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hyperspherical Variational Auto-Encoders</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Variational Auto-Encoder (VAE) is one of the most used unsupervised machine learning models. But although the default choice of a Gaussian distribution for both the prior and posterior represents a mathematically convenient distribution often leading to competitive results, we show that this parameterization fails to model data with a latent hyperspherical structure. To address this issue we propose using a von Mises-Fisher (vMF) distribution instead, leading to a hyperspherical latent space. Through a series of experiments we show how such a hyperspherical VAE, or S-VAE, is more suitable for capturing data with a hyperspherical latent structure, while outperforming a normal, N -VAE, in low dimensions on other data types.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>First introduced by <ref type="bibr" target="#b19">Kingma and Welling (2013)</ref>; <ref type="bibr" target="#b29">Rezende et al. (2014)</ref>, the Variational Auto-Encoder (VAE) is an unsupervised generative model that presents a principled fashion for performing variational inference using an autoencoding architecture. Applying the non-centered parameterization of the variational posterior , further simplifies sampling and allows to reduce bias in calculating gradients for training. Although the default choice of a Gaussian prior is mathematically convenient, we can show through a simple example that in some cases it breaks the assumption of an uninformative prior leading to unstable results. Imagine a dataset on the circle Z ⊂ S 1 , that is subsequently embedded in R N using a transformation f to obtain f : Z → X ⊂ R N . Given two hidden units, an autoencoder quickly discovers * Equal contribution. the latent circle, while a normal VAE becomes highly unstable. This is to be expected as a Gaussian prior is concentrated around the origin, while the KL-divergence tries to reconcile the differences between S 1 and R 2 . A more detailed discussion of this 'manifold mismatch' problem will follow in subsection 2.3.</p><p>The fact that some data types like directional data are better explained through spherical representations is long known and well-documented <ref type="bibr" target="#b23">(Mardia, 1975;</ref><ref type="bibr" target="#b9">Fisher et al., 1987)</ref>, with examples spanning from protein structure, to observed wind directions. Moreover, for many modern problems such as text analysis or image classification, data is often first normalized in a preprocessing step to focus on the directional distribution. Yet, few machine learning methods explicitly account for the intrinsically spherical nature of some data in the modeling process. In this paper, we propose to use the von Mises-Fisher (vMF) distribution as an alternative to the Gaussian distribution. This replacement leads to a hyperspherical latent space as opposed to a hyperplanar one, where the Uniform distribution on the hypersphere is conveniently recovered as a special case of the vMF. Hence this approach allows for a truly uninformative prior, and has a clear advantage in the case of data with a hyperspherical interpretation. This was previously attempted by <ref type="bibr" target="#b15">Hasnat et al. (2017)</ref>, but crucially they do not learn the concentration parameter around the mean, κ.</p><p>In order to enable training of the concentration parameter, we extend the reparameterization trick for rejection sampling as recently outlined in <ref type="bibr" target="#b24">Naesseth et al. (2017)</ref> to allow for n additional transformations. We then combine this with the rejection sampling procedure proposed by <ref type="bibr" target="#b33">Ulrich (1984)</ref> to efficiently reparameterize the VAE 1 .</p><p>We demonstrate the utility of replacing the normal distribution with the von Mises-Fisher distribution for generating latent representations by conducting a range of experiments in three distinct settings. First, we show that our S-VAEs outperform VAEs with the Gaussian variational posterior (N -VAEs) in recovering a hyperspherical latent structure. Second, we conduct a thorough comparison with N -VAEs on the MNIST dataset through an unsupervised learning task and a semi-supervised learning scenario. Finally, we show that S-VAEs can significantly improve link prediction performance on citation network datasets in combination with a Variational Graph Auto-Encoder (VGAE) <ref type="bibr" target="#b20">(Kipf and Welling, 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">VARIATIONAL AUTO-ENCODERS 2.1 FORMULATION</head><p>In the VAE setting, we have a latent variable model for data, where z ∈ R M denotes latent variables, x is a vector of D observed variables, and p φ (x, z) is a parameterized model of the joint distribution. Our objective is to optimize the log-likelihood of the data, log p φ (x, z)dz. When p φ (x, z) is parameterized by a neural network, marginalizing over the latent variables is generally intractable. One way of solving this issue is to maximize the Evidence Lower Bound (ELBO)</p><formula xml:id="formula_0">log p φ (x, z)dz ≥ E q(z) [log p φ (x|z)]+ − KL(q(z)||p(z)), (1)</formula><p>where q(z) is the approximate posterior distribution, belonging to a family Q. The bound is tight if q(z) = p(z|x), meaning q(z) is optimized to approximate the true posterior. While in theory q(z) should be optimized for every data point x, to make inference more scalable to larger datasets the VAE setting introduces an inference network q ψ (z|x; θ) parameterized by a neural network that outputs a probability distribution for each data point x. The final objective is therefore to maximize</p><formula xml:id="formula_1">L(φ, ψ) = E q ψ (z|x;θ) [log p φ (x|z)]+ − KL(q ψ (z|x; θ)||p(z)),<label>(2)</label></formula><p>In the original VAE both the prior and the posterior are defined as normal distributions. We can further efficiently approximate the ELBO by Monte Carlo estimates, using the reparameterization trick <ref type="bibr" target="#b19">(Kingma and Welling, 2013;</ref><ref type="bibr" target="#b29">Rezende et al., 2014)</ref>. This is done by expressing a sample of z ∼ q ψ (z|x; θ), as z = h(θ, ε, x), where h is a reparameterization transformation and ε ∼ s(ε) is some noise random variable independent from θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">THE LIMITATIONS OF A GAUSSIAN DISTRIBUTION PRIOR</head><p>Low dimensions: origin gravity In low dimensions, the Gaussian density presents a concentrated probability mass around the origin, encouraging points to cluster in the center. This is particularly problematic when the data is divided into multiple clusters. Although an ideal latent space should separate clusters for each class, the normal prior will encourage all the cluster centers towards the origin. An ideal prior would only stimulate the variance of the posterior without forcing its mean to be close to the center. A prior satisfying these properties is a uniform over the entire space. Such a uniform prior, however, is not well defined on the hyperplane.</p><p>High dimensions: soap bubble effect It is a wellknown phenomenon that the standard Gaussian distribution in high dimensions tends to resemble a uniform distribution on the surface of a hypersphere, with the vast majority of its mass concentrated on the hyperspherical shell. Hence it would appear interesting to compare the behavior of a Gaussian approximate posterior with an approximate posterior already naturally defined on the hypersphere. This is also motivated from a theoretical point of view, since the Gaussian definition is based on the L 2 norm that suffers from the curse of dimensionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">BEYOND THE HYPERPLANE</head><p>Once we let go of the hyperplanar assumption, the possibility of a uniform prior on the hypersphere opens up. Mirroring our discussion in the previous subsection, such a prior would exhibit no pull towards the origin allowing clusters of data to evenly spread over the surface with no directional bias. Additionally, in higher dimensions, the cosine similarity is a more meaningful distance measure than the Euclidean norm.</p><p>Manifold mapping In general, exploring VAE models that allow a mapping to distributions in a latent space not homeomorphic to R D is of fundamental interest. Consider data lying in a small M -dimensional manifold M, embedded in a much higher dimensional space X = R N . For most real data, this manifold will likely not be homeomorphic to R M . An encoder can be considered as a smooth map enc :  M. Yet, since D &gt; M , when taking random points in the latent space they will most likely not be in emb(M) resulting in a poorly reconstructed sample.</p><formula xml:id="formula_2">X → Z = R D from the original space to Z.</formula><p>The VAE tries to solve this problem by forcing M to be mapped into an approximate posterior distribution that has support in the entire Z. Clearly, this approach is bound to fail since the two spaces have a fundamentally different structure. This can likely produce two behaviors: first, the VAE could just smooth the original embedding emb(M) leaving most of the latent space empty, leading to bad samples. Second, if we increase the KL term the encoder will be pushed to occupy all the latent space, but this will create instability and discontinuity, affecting the convergence of the model. To validate our intuition we performed a small proof of concept experiment using M = S 1 , which is visualized in <ref type="figure" target="#fig_0">Figure 1</ref>. Note that as expected the auto-encoder in <ref type="figure" target="#fig_0">Figure 1</ref>(b) mostly recovers the original latent space of <ref type="figure" target="#fig_0">Figure 1</ref>(a) as there are no distributional restrictions. In <ref type="figure" target="#fig_0">Figure 1</ref>(c) we clearly observe for the N -VAE that points collapse around the origin due to the KL, which is much less pronounced in <ref type="figure" target="#fig_0">Figure 1</ref>(d) when its contribution is scaled down. Lastly, the S-VAE almost perfectly recovers the original circular latent space. The observed behavior confirms our intuition.</p><p>To solve this problem the best option would be to directly specify a Z homeomorphic to M and distributions on M. However, for real data discovering the structure of M will often be a difficult inference task. Nevertheless, we believe this shows investigating VAE architectures that map to posterior distributions defined on manifolds different than the Euclidean space is a topic worth exploring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">REPLACING GAUSSIAN WITH VON MISES-FISHER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">VON MISES-FISHER DISTRIBUTION</head><p>The von Mises-Fisher (vMF) distribution is often seen as the Normal Gaussian distribution on a hypersphere. Analogous to a Gaussian, it is parameterized by µ ∈ R m indicating the mean direction, and κ ∈ R ≥0 the concentration around µ. For the special case of κ = 0, the vMF represents a Uniform distribution. The probability density function of the vMF distribution for a random unit vector z ∈ R m (or z ∈ S m−1 ) is then defined as</p><formula xml:id="formula_3">q(z|µ, κ) = C m (κ) exp (κµ T z),<label>(3)</label></formula><formula xml:id="formula_4">C m (κ) = κ m/2−1 (2π) m/2 I m/2−1 (κ) ,<label>(4)</label></formula><p>where ||µ|| 2 = 1, C m (κ) is the normalizing constant, and I v denotes the modified Bessel function of the first kind at order v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">KL DIVERGENCE</head><p>As previously emphasized, one of the main advantages of using the vMF distribution as an approximate posterior is that we are able to place a uniform prior on the latent space. The KL divergence term KL(vMF(µ, κ)||U (S m−1 )) to be optimized is:</p><formula xml:id="formula_5">κ I m/2 (k) I m/2−1 (k) + log C m (κ) − log 2(π m/2 ) Γ(m/2) −1 ,<label>(5)</label></formula><p>see Appendix B for complete derivation. Notice that since the KL term does not depend on µ, this parameter is only optimized in the reconstruction term. The above expression cannot be handled by automatic differentiation packages because of the modified Bessel function in C m (κ). Thus, to optimize this term we derive the gradient with respect to the concentration parameter:</p><formula xml:id="formula_6">∇ κ KL(vMF(µ, κ)||U (S m−1 )) = 1 2 k I m/2+1 (k) I m/2−1 (k) + − I m/2 (k) I m/2−2 (k) + I m/2 (k) I m/2−1 (k) 2 + 1 ,<label>(6)</label></formula><p>Algorithm 1 vMF sampling</p><formula xml:id="formula_7">Input: dimension m, mean µ, concentration κ sample v ∼ U (S m−2 ) sample ω ∼ g(ω|κ, m) ∝ exp(ωκ)(1 − ω 2 ) 1 2 (m−3) {acceptance-rejection sampling} z ← (ω; ( √ 1 − ω 2 )v ) U ← Householder(e 1 , µ) {Householder transform} Return: U z</formula><p>where the modified Bessel functions can be computed without numerical instabilities using the exponentially scaled modified Bessel function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SAMPLING PROCEDURE</head><p>To sample from the vMF we follow the procedure of <ref type="bibr" target="#b33">Ulrich (1984)</ref>, outlined in Algorithm 1. We first sample from a vMF q(z|e 1 , κ) with modal vector e 1 = (1, 0, · · · , 0). Since the vMF density is uniform in all the m − 2 dimensional sub-hyperspheres {x ∈ S m−1 | e 1 x = ω}, the sampling technique reduces to sampling the value ω from the univariate density</p><formula xml:id="formula_8">g(ω|κ, m) ∝ exp(κω)(1 − ω 2 ) (m−3)/2 , ω ∈ [−1, 1],</formula><p>using an acceptance-rejection scheme. After getting a sample from q(z|e 1 , κ) an orthogonal transformation U (µ) is applied such that the transformed sample is distributed according to q(z|µ, κ). This can be achieved using a Householder reflection such that U (µ)e 1 = µ. A more in-depth explanation of the sampling technique can be found in Appendix A.</p><p>It is worth noting that the sampling technique does not suffer from the curse of dimensionality, as the acceptancerejection procedure is only applied to a univariate distribution. Moreover in the case of S 2 , the density g(ω|κ, 3) reduces to g(ω|κ, 3) ∝ exp(kω)1 <ref type="bibr">[−1,+1]</ref> (ω) which can be directly sampled without rejection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">N-TRANSFORMATION REPARAMETERIZATION TRICK</head><p>While the reparameterization trick is easily implementable in the normal case, unfortunately it can only be applied to a handful of distributions. However a recent technique introduced by Naesseth et al. (2017) allows to extend the reparameterization trick to the wide class of distributions that can be simulated using rejection sampling. Dropping the dependence from x for simplicity, assume the approximate posterior is of the form g(ω|θ) and that it can be sampled by making proposals from r(ω|θ). If the proposal distribution can be reparameterized we can still perform the reparameterization trick. Let ε ∼ s(ε), and ω = h(ε, θ), a reparameterization of the proposal distribution, r(ω|θ). Performing the reparameterization trick for g(ω|θ) is made possible by the fundamental lemma proven in <ref type="bibr" target="#b24">(Naesseth et al., 2017)</ref>:</p><p>Lemma 1. Let f be any measurable function and ε ∼ π(ε|θ) = s(ε) g(h(ε, θ)|θ) r(h(ε, θ)|θ) the distribution of the accepted sample. Then:</p><formula xml:id="formula_9">E π(ε|θ) [f (h(ε, θ))] = f (h(ε, θ))π(ε|θ)dε = f (ω)g(ω|θ)dω = E g(ω|θ) [f (ω)],<label>(7)</label></formula><p>Then the gradient can be taken using the log derivative trick:</p><formula xml:id="formula_10">∇ θ E g(ω|θ) [f (ω)] = ∇ θ E π(ε|θ) [f (h(ε, θ))] = E π(ε|θ) [∇ θ f (h(ε, θ))]+ + E π(ε|θ) f (h(ε, θ))∇ θ log g(h(ε, θ)|θ) r(h(ε, θ)|θ) ,<label>(8)</label></formula><p>However, in the case of the vMF a different procedure is required. After performing the transformation h(ε, θ) and accepting/rejecting the sample, we sample another random variable v ∼ π 2 (v), and then apply a transfor-</p><formula xml:id="formula_11">mation z = T (h(ε, θ), v; θ), such that z ∼ q ψ (z|θ)</formula><p>is distributed as the approximate posterior (in our case a vMF). Effectively this entails applying another reparameterization trick after the acceptance/rejection step. To still be able to perform the reparameterization we show that Lemma 1 fundamentally still holds in this case as well.</p><p>Lemma 2. Let f be any measurable function and ε ∼ π 1 (ε|θ) = s(ε) g(h(ε, θ)|θ) r(h(ε, θ)|θ) the distribution of the accepted sample. Also let v ∼ π 2 (v), and T a transformation that depends on the parameters such that if z = T (ω, v; θ) with ω ∼ g(ω|θ), then ∼ q(z|θ):</p><formula xml:id="formula_12">E (ε,v)∼π1(ε|θ)π2(v) [f (T (h(ε, θ), v; θ))] = f (z)q(z|θ)dz = E q(z|θ) [f (z)],<label>(9)</label></formula><p>Proof. See Appendix C.</p><p>With this result we are able to derive a gradient expression similarly as done in equation 8. We refer to Appendix D for a complete derivation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">BEHAVIOR IN HIGH DIMENSIONS</head><p>The surface area of a hypersphere is defined as  where m is the dimensionality and r the radius. Notice that S(m − 1) → 0, as m → ∞. However, even for m &gt; 20 we observe a vanishing surface problem (see <ref type="figure" target="#fig_6">Figure 6</ref> in Appendix E). This could thus lead to unstable behavior of hyperspherical models in high dimensions.</p><formula xml:id="formula_13">S(m − 1) = r m 2(π m/2 ) Γ(m/2) ,<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Extending the VAE The majority of VAE extensions focus on increasing the flexibility of the approximate posterior. This is usually achieved through normalizing flows <ref type="bibr" target="#b28">(Rezende and Mohamed, 2015)</ref>, a class of invertible transformations applied sequentially to an initial reparameterizable density q 0 (z 0 ), allowing for more complex posteriors. Normalizing flows can be considered orthogonal to our approach. While allowing for a more flexible posterior, they do not modify the standard normal prior assumption. In <ref type="bibr" target="#b10">(Gemici et al., 2016)</ref> a first attempt is made to extend normalizing flows to Riemannian manifolds. However, as the method relies on the existence of a diffeomorphism between R N and S N , it is unsuited for hyperspheres.</p><p>One approach to obtain a more flexible prior is to use a simple mixture of Gaussians (MoG) prior <ref type="bibr" target="#b8">(Dilokthanakul et al., 2016)</ref>. The recently introduced VampPrior model <ref type="bibr" target="#b32">(Tomczak and Welling, 2018)</ref> outlines several advantages over the MoG and instead tries to learn a more flexible prior by expressing it as a mixture of approximate posteriors. A non-parametric prior is proposed in <ref type="bibr" target="#b25">Nalisnick and Smyth (2017)</ref>, utilizing a truncated stick-breaking process. Opposite to these approaches, we aim at using a non-informative prior to simplify the inference.</p><p>The closest approach to ours is a VAE with a vMF distribution in the latent space used for a sentence generation task by <ref type="bibr" target="#b14">(Guu et al., 2018)</ref>. While formally this approach is cast as a variational approach, the proposed model does not reparameterize and learn the concentration parameter κ, treating it as a constant value that remains the same for every approximate posterior instead. Critically, as indicated in Equation 5, the KL divergence term only depends on κ therefore leaving κ constant means never explicitly optimizing the KL divergence term in the loss.</p><p>The method then only optimizes the reconstruction error by adding vMF noise to the encoder output in the latent space to still allow generation. Moreover, using a fixed global κ for all the approximate posteriors severely limits the flexibility and the expressiveness of the model. <ref type="formula" target="#formula_1">(2018)</ref>, a general model to perform Bayesian inference in Riemannian Manifolds is proposed. Following other Stein-related approaches, the method does not explicitly define a posterior density but approximates it with a number of particles. Despite its generality and flexibility, it requires the choice of a kernel on the manifold and multiple particles to have a good approximation of the posterior distribution. The former is not necessarily straightforward, while the latter quickly becomes computationally unfeasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-Euclidean Latent Space In Liu and Zhu</head><p>Another approach by <ref type="bibr" target="#b26">Nickel and Kiela (2017)</ref>, capitalizes on the hierarchical structure present in some data types. By learning the embeddings for a graph in a non-euclidean negative curvature hyperbolical space, they show this topology has clear advantages over embedding these objects in a Euclidean space. Although they did not use a VAE-based approach, that is, they did not build a probabilistic generative model of the data interpreting the embeddings as latent variables, this approach shows the merit of explicitly adjusting the choice of latent topology to the data used. A Hyperspherical Perspective As noted before, a distinction must be made between models dealing with the challenges of intrinsically hyperspherical data like omnidirectional video, and those attempting to exploit some latent hyperspherical manifold. A recent example of the first can be found in <ref type="bibr" target="#b7">Cohen et al. (2018)</ref>, where spherical CNNs are introduced. While flattening a spherical image produces unavoidable distortions, the newly defined convolutions take into account its geometrical properties.</p><p>The most general implementation of the second model type was proposed by <ref type="bibr" target="#b12">Gopal and Yang (2014)</ref>, who introduced a suite of models to improve cluster performance of high-dimensional data based on mixture of vMF distributions. They showed that reducing an object representation to its directional components increases clusterability over standard methods like K-Means or Latent Dirichlet Allocation <ref type="bibr" target="#b4">(Blei et al., 2003)</ref>.</p><p>Specific applications of the vMF can be further found ranging from computer vision, where it is used to infer structure from motion <ref type="bibr" target="#b13">(Guan and Smith, 2017)</ref> in spherical video, or structure from texture <ref type="bibr" target="#b34">(Wilson et al., 2014)</ref>, to natural language processing, where it is utilized in text analysis <ref type="bibr" target="#b2">(Banerjee et al., 2003</ref><ref type="bibr" target="#b3">(Banerjee et al., , 2005</ref> and topic modeling <ref type="bibr" target="#b1">(Banerjee and Basu, 2007;</ref><ref type="bibr" target="#b27">Reisinger et al., 2010)</ref>.</p><p>Additionally, modeling data by restricting it to a hypersphere provides some natural regularizing properties as noted in <ref type="bibr" target="#b22">(Liu et al., 2017)</ref>. Finally <ref type="bibr" target="#b0">Aytekin et al. (2018)</ref> show on a variety of deep auto-encoder models that adding L2 normalization to the latent space during training, i.e. forcing the latent space on a hypersphere, improves clusterability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we first perform a series of experiments to investigate the theoretical properties of the proposed S-VAE compared to the N -VAE. In a second experiment, we show how S-VAEs can be used in semi-supervised tasks to create a better separable latent representation to enhance classification. In the last experiment, we show that the S-VAE indeed presents a promising alternative to N -VAEs for data with a non-Euclidean latent representation of low dimensionality, on a link prediction task for three citation networks. All architecture and hyperparameter details are given in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">RECOVERING HYPERSPHERICAL LATENT REPRESENTATIONS</head><p>In this first experiment we build on the motivation developed in Subsection 2.3, by confirming with a synthetic data example the difference in behavior of the N -VAE and S-VAE in recovering latent hyperspheres. We first generate samples from a mixture of three vMFs on the circle, S 1 , as shown in <ref type="figure" target="#fig_0">Figure 1(a)</ref>, which subsequently are mapped into the higher dimensional R 100 by applying a noisy, non-linear transformation. After this, we in turn train an auto-encoder, a N -VAE, and a S-VAE. We further investigate the behavior of the N -VAE, by training a model using a scaled down KL divergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The resulting latent spaces, displayed in <ref type="figure" target="#fig_0">Figure  1</ref>, clearly confirm the intuition built in Subsection 2.3. As expected, in <ref type="figure" target="#fig_0">Figure 1(b)</ref> the auto-encoder is perfectly capable to embed in low dimensions the original underlying data structure. However, most parts of the latent space are not occupied by points, critically affecting the ability to generate meaningful samples.</p><p>In the N -VAE setting we observe two types of behaviours, summarized by <ref type="figure" target="#fig_0">Figures 1(c) and 1(d)</ref>. In the first we observe that if the prior is too strong it will force the posterior to match the prior shape, concentrating the samples in the center. However, this prevents the N -VAE to correctly represent the true shape of the data and creates instability problems for the decoder around the origin. On the contrary, if we scale down the KL term, we observe that the samples from the approximate posterior maintain a shape that reflects the S 1 structure smoothed with Gaussian noise. However, as the approximate posterior differs strongly from the prior, obtaining meaningful samples from the latent space again becomes problematic.</p><p>The S-VAE on the other hand, almost perfectly recovers the original dataset structure, while the samples from the approximate posterior closely match the prior distribution. This simple experiment confirms the intuition that having a prior that matches the true latent structure of the data, is crucial in constructing a correct latent representation that preserves the ability to generate meaningful samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">EVALUATION OF EXPRESSIVENESS</head><p>To compare the behavior of the N -VAE and S-VAE on a data set that does not have a clear hyperspherical latent structure, we evaluate both models on a reconstruction task using dynamically binarized MNIST <ref type="bibr" target="#b30">(Salakhutdinov and Murray, 2008)</ref>. We analyze the ELBO, KL, negative reconstruction error, and marginal log-likelihood (LL) for both models on the test set. The LL is estimated using importance sampling with 500 sample points <ref type="bibr" target="#b6">(Burda et al., 2016)</ref>.</p><p>Results Results are shown in <ref type="table" target="#tab_1">Table 1</ref>. We first note that in terms of negative reconstruction error the S-VAE outperforms the N -VAE in all dimensions. Since the S-VAE uses a uniform prior, the KL divergence increases more strongly with dimensionality, which results in a higher ELBO. However in terms of log-likelihood (LL) the S-VAE clearly has an edge in low dimensions (d = 2, 5, 10) and performs comparable to the N -VAE in d = 20. This empirically confirms the hypothesis of Subsection 2.2, showing the positive effect of having a uniform prior in low dimensions. In the absence of any origin pull, the data is able to cluster naturally, utilizing the entire latent space which can be observed in <ref type="figure" target="#fig_2">Figure 2</ref>. Note that in <ref type="figure" target="#fig_2">Figure 2(a)</ref> all mass is concentrated around the center, since the prior mean is zero. Conversely, in <ref type="figure" target="#fig_2">Figure 2</ref>(b) all available space is evenly covered due to the uniform prior, resulting in more separable clusters in S 2 compared to R 2 . However, as dimensionality increases, the Gaussian distribution starts to approximate a hypersphere, while its posterior becomes more expressive than the vMF due to the higher number of variance parameters. Simultaneously, as described in Subsection 3.5, the surface area of the vMF starts to collapse limiting the available space.</p><p>In <ref type="figure">Figure 7</ref> and 8 of Appendix G, we present randomly generated samples from the N -VAE and the S-VAE, respectively. Moreover, in <ref type="figure">Figure 9</ref> of Appendix G, we show 2-dimensional manifolds for the two models. Interestingly, the manifold given by the S-VAE indeed results in a latent space where digits occupy the entire space and there is a sense of continuity from left to right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">SEMI-SUPERVISED LEARNING</head><p>Having observed the S-VAE's ability to increase clusterability of data points in the latent space, we wish to further investigate this property using a semi-supervised classification task. For this purpose we re-implemented the M1 and M1+M2 models as described in , and evaluate the classification accuracy of the S-VAE and the N -VAE on dynamically binarized MNIST. In the M1 model, a classifier utilizes the latent features obtained using a VAE as in experiment 5.2. The M1+M2 model is constructed by stacking the M2 model on top of M1, where M2 is the result of augmenting the VAE by introducing a partially observed variable y, and combining the ELBO and classification objective. This concatenated model is trained end-to-end 3 .</p><p>This last model also allows for a combination of the two topologies due to the presence of two distinct latent variables, z 1 and z 2 . Since in the M2 latent space the class assignment is expressed by the variable y, while z 2 only needs to capture the style, it naturally follows that the  N -VAE is more suited for this objective due to its higher number of variance parameters. Hence, besides comparing the S-VAE against the N -VAE, we additionally run experiments for the M1+M2 model by modeling z 1 , z 2 respectively with a vMF and normal distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>As can be see in <ref type="table" target="#tab_2">Table 2</ref>, for M1 the S-VAE outperforms the N -VAE in all dimensions up to d = 40. This result is amplified for a low number of observed labels. Note that for both models absolute performance drops as the dimensionality increases, since K-NN used as the classifier suffers from the curse of dimensionality. Besides reconfirming superiority of the S-VAE in d &lt; 20, its better performance than the N -VAE for d = 20 was unexpected. This indicates that although the loglikelihood might be comparable(see <ref type="table" target="#tab_1">Table 1</ref>) for higher dimensions, the S-VAE latent space better captures the cluster structure.</p><p>In the concatenated model M1+M2, we first observe in <ref type="table" target="#tab_3">Table 3</ref> that either the pure S-VAE or the S+N -VAE model yields the best results, where the S-VAE almost always outperforms the N -VAE. Our hypothesis regarding the merit of a S+N -VAE model is further confirmed, as displayed by the stable, strong performance across all different dimensions. Furthermore, the clear edge in clusterability of the S-VAE in low dimensional z 1 as already observed in <ref type="table" target="#tab_2">Table 2</ref>, is again evident. As the dimensionality of z 1 , z 2 increases, the accuracy of the N -VAE improves, reducing the performance gap with the S-VAE. As previously noticed the S-VAE performance drops when dim z2 = 50, with the best result being obtained for dim z1 = dim z2 = 10. In fact, it is worth noting that for this setting the S-VAE obtains comparable results to the original settings of , while needing a considerably smaller latent space. Finally, the end-to-end trained S+N -VAE model is able to reach a significantly higher classification accuracy than the original results reported by , 96.7±.1.</p><p>The M1+M2 model allows for conditional generation. Similarly to , we set the latent variable z 2 to the value inferred from the test image by the inference network, and then varied the class label y. In <ref type="figure" target="#fig_0">Figure 10</ref> of Appendix H we notice that the model is able to disentangle the style from the class. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">LINK PREDICTION ON GRAPHS</head><p>In this experiment, we aim at demonstrating the ability of the S-VAE to learn meaningful embeddings of nodes in a graph, showing the advantages of embedding objects in a non-Euclidean space. We test hyperspherical reparameterization on the recently introduced Variational Graph Auto-Encoder (VGAE) <ref type="bibr" target="#b20">(Kipf and Welling, 2016</ref>), a VAE model for graph-structured data. We perform training on a link prediction task on three popular citation network datasets <ref type="bibr" target="#b31">(Sen et al., 2008)</ref>: Cora, Citeseer and Pubmed.</p><p>Dataset statistics and further experimental details are summarized in Appendix F.3. The models are trained in an unsupervised fashion on a masked version of these datasets where some of the links have been removed. All node features are provided and efficacy is measured in terms of average precision (AP) and area under the ROC curve (AUC) on a test set of previously removed links. We use the same training, validation, and test splits as in <ref type="bibr" target="#b20">Kipf and Welling (2016)</ref>, i.e. we assign 5% of links for validation and 10% of links for testing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>In <ref type="table" target="#tab_4">Table 4</ref>, we show that our model outperforms the N -VGAE baseline on two out of the three datasets by a significant margin. The log-probability of a link is computed as the dot product of two embeddings. In a hypersphere, this can be interpreted as the cosine similarity between vectors. Indeed we find that the choice of a dot product scoring function for link prediction is problematic in combination with the normal distribution on the latent space. If embeddings are close to the zero-center, noise during training can have a large destabilizing effect on the angle information between two embeddings. In practice, the model finds a solution where embeddings are "pushed" away from the zero-center, as demonstrated in <ref type="figure" target="#fig_4">Figure 3(a)</ref>. This counteracts the pull towards the center arising from the standard prior and can overall lead to poor modeling performance. By constraining the embeddings to the surface of a hypersphere, this effect is mitigated, and the model can find a good separation of the latent clusters, as shown in <ref type="figure" target="#fig_4">Figure 3(b)</ref>.</p><p>On Pubmed, we observe that the S-VAE converges to a lower score than the N -VAE. The Pubmed dataset is significantly larger than Cora and Citeseer, and hence more complex. The N -VAE has a larger number of variance parameters for the posterior distribution, which might have played an important role in better modeling the relationships between nodes. We further hypothesize that not all graphs are necessarily better embedded in a hyperspher-ical space and that this depends on some fundamental topological properties of the graph. For instance, the already mentioned work from <ref type="bibr" target="#b26">Nickel and Kiela (2017)</ref> shows that hyperbolical space is better suited for graphs with a hierarchical, tree-like structure. These considerations prefigure an interesting research direction that will be explored in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>With the S-VAE we set an important first step in the exploration of hyperspherical latent representations for variational auto-encoders. Through various experiments, we have shown that S-VAEs have a clear advantage over N -VAEs for data residing on a known hyperspherical manifold, and are competitive or surpass N -VAEs for data with a non-obvious hyperspherical latent representation in lower dimensions. Specifically, we demonstrated S-VAEs improve separability in semi-supervised classification and that they are able to improve results on state-of-the-art link prediction models on citation graphs, by merely changing the prior and posterior distributions as a simple drop-in replacement.</p><p>We believe that the presented research paves the way for various promising areas of future work, such as exploring more flexible approximate posterior distributions through normalizing flows on the hypersphere, or hierarchical mixture models combining hyperspherical and hyperplanar space. Further research should be done in increasing the performance of S-VAEs in higher dimensions; one possible solution of which could be to dynamically learn the radius of the latent hypersphere in a full Bayesian setting.</p><p>A SAMPLING PROCEDURE <ref type="figure">Figure 4</ref>: Overview of von Mises-Fisher sampling procedure. Note that as ω is a scalar, the procedure does not suffer from the curse of dimensionality.</p><p>The general algorithm for sampling from a vMF has been outlined in Algorithm 1. The exact form of the distribution of the univariate distribution g(ω|k) is:</p><formula xml:id="formula_14">g(ω|k) = 2(π m/2 ) Γ(m/2) C m (k) exp(ωk)(1 − ω 2 ) 1 2 (m−3) B( 1 2 , 1 2 (m − 1)) ,<label>(11)</label></formula><p>Samples from this distribution are drawn using an acceptance/rejection algorithm when m = 3. The complete procedure is described in Algorithm 2. The Householder reflection (see Algorithm 3 for details) simply finds an orthonormal transformation that maps the modal vector e 1 = (1, 0, · · · , 0) to µ. Since an orthonormal transformation preserves the distances all the points in the hypersphere will stay in the surface after mapping. Notice that even the transform U z = (I − 2uu )z , can be executed in O(m) by rearranging the terms.    </p><formula xml:id="formula_15">b ← −2k + 4k 2 + (m − 1) 2 m − 1 a ← (m − 1) + 2k + 4k 2 + (m − 1) 2 4 d ← 4ab (1 + b) − (m − 1) ln(m − 1) repeat Sample ε ∼ Beta( 1 2 (m − 1), 1 2 (m − 1)) ω ← h(ε, k) = 1 − (1 + b)ε 1 − (1 − b)ε t ← 2ab 1 − (1 − b)ε Sample u ∼ U(0, 1) until (m − 1) ln(t) − t + d ≥ ln(u) Return: ω</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B KL DIVERGENCE DERIVATION</head><p>The KL divergence between a von-Mises-Fisher distribution q(z|µ, k) and an uniform distribution in the hypersphere (one divided by the surface area of S m−1 ) p(x) = 2(π m/2 ) Γ(m/2) −1 is:</p><formula xml:id="formula_16">KL[q(z|µ, k) || p(z)] = S m−1 q(z|µ, k) log q(z|µ, k) p(z) dz (12) = S m−1 q(z|µ, k) log C m (k) + kµ T z − log p(z) dz (13) = kµ E q [z] + log C m (k) − log 2(π m/2 ) Γ(m/2) −1<label>(14)</label></formula><p>= k I m/2 (k) I m/2−1 (k) + (m/2 − 1) log k − (m/2) log(2π) − log I m/2−1 (k)</p><formula xml:id="formula_17">+ m 2 log π + log 2 − log Γ( m 2 ), g cor = log p φ (x|z) · − I m/2 I m/2−1 + ∇ κ ωκ + 1 2 (m − 3) log(1 − ω 2 ) + log | −2b ((b − 1)ε + 1) 2 | ,<label>(15)</label></formula><formula xml:id="formula_18">where b = −2k + 4k 2 + (m − 1) 2 m − 1 (40) ω = h(ε, θ) = 1 − (1 + b)ε 1 − (1 − b)ε (41) z = T (h(ε, θ), v; θ),<label>(39)</label></formula><p>And the term ∇ κ ωκ + 1 2 (m − 3) log(1 − ω 2 ) + log | −2b ((b − 1)ε + 1) 2 | can be computed by automatic differentiation packages.  Architecture and hyperparameters For both the encoder and the decoder we use MLPs with 2 hidden layers of respectively, <ref type="bibr">[256,</ref><ref type="bibr">128]</ref> and <ref type="bibr">[128,</ref><ref type="bibr">256]</ref> hidden units. We trained until convergence using early-stopping with a look ahead of 50 epochs. We used the Adam optimizer <ref type="bibr" target="#b17">(Kingma and Ba, 2015)</ref> with a learning rate of 1e-3, and mini-batches of size 64. Additionally, we used a linear warm-up for 100 epochs <ref type="bibr" target="#b5">(Bowman et al., 2016)</ref>. The weights of the neural network were initialized according to <ref type="bibr" target="#b11">(Glorot and Bengio, 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E COLLAPSE OF THE SURFACE AREA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 EXPERIMENT 5.3</head><p>Architecture and Hyperparameters For M1 we reused the trained models of the previous experiment, and used K-nearest neighbors (K-NN) as a classifier with k = 5. In the N -VAE case we used the Euclidean distance as a distance metric. For the S-VAE the geodesic distance arccos(x y) was employed. The performance was evaluated for N = [100, 600, 1000] observed labels. The stacked M1+M2 model uses the same architecture as outlined by , where the MLPs utilized in the generative and inference models are constructed using a single hidden layer, each with 500 hidden units. The latent space dimensionality of z 1 , z 2 were both varied in <ref type="bibr">[5,</ref><ref type="bibr">10,</ref><ref type="bibr">50]</ref>. We used the rectified linear unit (ReLU) as an activation function. Training was continued until convergence using early-stopping with a look ahead of 50 epochs on the validation set. We used the Adam optimizer with a learning rate of 1e-3, and mini-batches of size 100. All neural network weight were initialized according to <ref type="bibr" target="#b11">(Glorot and Bengio, 2010)</ref>. N was set to 100, and the α parameter used to scale the classification loss was chosen between [0.1, 1.0]. Crucially, we train this model end-to-end instead of by parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 EXPERIMENT 5.4</head><p>Architecture and Hyperparameters We are training a Variational Graph Auto-encoder (VGAE) model, a state-ofthe-art link prediction model for graphs, as proposed in <ref type="bibr" target="#b20">Kipf and Welling (2016)</ref>. For a fair comparison, we use the same architecture as in the original paper and we just change the way the latent space is generated using the vMF distribution instead of a normal distribution. All models are trained for 200 epochs on Cora and Citeseer, and 400 epochs on Pubmed with the Adam optimizer. Optimal learning rate lr ∈ {0.01, 0.005, 0.001}, dropout rate p do ∈ {0, 0.1, 0.2, 0.3, 0.4} and number of latent dimensions d z ∈ {8, 16, 32, 64} are determined via grid search based on validation AUC performance. For S-VGAE, we omit the d z = 64 setting as some of our experiments ran out of memory. The model is trained with a single hidden layer with 32 units and with document features as input, as in <ref type="bibr" target="#b20">Kipf and Welling (2016)</ref>. The weights of the neural network were initialized according to <ref type="bibr" target="#b11">(Glorot and Bengio, 2010)</ref>. For testing, we report performance of the model selected from the training epoch with highest AUC score on the validation set. Different from <ref type="bibr" target="#b20">(Kipf and Welling, 2016)</ref>, we train both the N -VGAE and the S-VGAE models using negative sampling in order to speed up training, i.e. for each positive link we sample, uniformly at random, one negative link during every training epoch. All experiments are repeated 5 times, and we report mean and standard error values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3.1 FURTHER EXPERIMENTAL DETAILS</head><p>Dataset statistics are summarized in <ref type="table" target="#tab_8">Table 6</ref>. Final hyperparameter choices found via grid search on the validation splits are summarized in <ref type="table" target="#tab_9">Table 7</ref>.   H VISUALIZATION OF CONDITIONAL GENERATION <ref type="figure" target="#fig_0">Figure 10</ref>: Visualization of handwriting styles learned by the model, using conditional generation on MNIST of M1+M2 with dim(z 1 ) = 50, dim(z 2 ) = 50, S+N . Following , the left most column shows images from the test set. The other columns show analogical fantasies of x by the generative model, where in each row the latent variable z 2 is set to the value inferred from the test image by the inference network and the class label y is varied per column.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Plots of the original latent space (a) and learned latent space representations in different settings, where β is a re-scaling factor for weighting the KL divergence. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) R 2 latent space of the N -VAE.(b) Hammer projection of S 2 latent space of the S-VAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Latent space visualization of the 10 MNIST digits in 2 dimensions of both N -VAE (left) and S-VAE (right). (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) R 2 latent space of the N -VGAE.(b) Hammer projection of S 2 latent space of the S-VGAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Latent space of unsupervised N -VGAE and S-VGAE models trained on Cora citation network. Colors denote documents classes which are not provided during training. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Geometric representation of a single sample in S 2 , where ω ∼ g(ω|k) and v ∼ U (S 1 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Plot of the unit hyperspherical surface area against dimensionality. The surface area has a maximum for m = 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>F</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Random samples from N -VAE of MNIST for different dimensionalities of latent space. Random samples from S-VAE of MNIST for different dimensionalities of latent space. (a) N -VAE (b) S-VAE Figure 9: Visualization of the 2 dimensional manifold of MNIST for both the N -VAE and S-VAE. Notice that the N -VAE has a clear center and all digits are spread around it. Conversely, in the S-VAE instead all digits occupy the entire space and there is a sense of continuity from left to right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The restriction of the encoder to M, enc| M : M → Z will also be a smooth mapping. However since M is not homeomorphic to Z if D ≤ M , then enc| M cannot be a homeomorphism.</figDesc><table><row><cell>That is, there exists no invertible and globally continuous</cell></row><row><cell>mapping between the coordinates of M and the ones of</cell></row><row><cell>Z. Conversely if D &gt; M then M can be smoothly</cell></row><row><cell>embedded in Z for D sufficiently large 2 , such that</cell></row><row><cell>enc| M : M → enc| M (M) =: emb(M) ⊂ Z is a</cell></row><row><cell>homeomorphism and emb(M) denotes the embedding of</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Summary of results (mean and standard-deviation over 10 runs) of unsupervised model on MNIST. RE and KL correspond respectively to the reconstruction and the KL part of the ELBO. Best results are highlighted only if they passed a student t-test with p &lt; 0.01. 73±.83 -137.08±.83 -129.84±.91 7.24±.11 -132.50±.73 -133.72±.85 -126.43±.91 7.28±.14 d = 5 -110.21±.21 -112.98±.21 -100.16±.22 12.82±.11 -108.43±.09 -111.19±.08 -97.84±.13 13.35±.06 14±.56 23.77±.49 -90.87±.34 -101.26±.33 -67.75±.70 33.50±.45</figDesc><table><row><cell>Method</cell><cell>LL</cell><cell cols="2">N -VAE L[q]</cell><cell>RE</cell><cell>KL</cell><cell>LL</cell><cell cols="2">S-VAE L[q]</cell><cell>RE</cell><cell>KL</cell></row><row><cell cols="2">d = 2 -135.d = 10 -93.84±.30</cell><cell>-98.36±.30</cell><cell cols="4">-78.93±.30 19.44±.14 -93.16±.31</cell><cell>-97.70±.32</cell><cell cols="2">-77.03±.39 20.67±.08</cell></row><row><cell>d = 20</cell><cell>-88.90±.26</cell><cell>-94.79±.19</cell><cell cols="4">-71.29±.45 23.50±.31 -89.02±.31</cell><cell>-96.15±.32</cell><cell cols="2">-67.65±.43 28.50±.22</cell></row><row><cell>d = 40</cell><cell>-88.93±.30</cell><cell>-94.91±.18</cell><cell cols="2">-71.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Summary of results (mean accuracy and standard-deviation over 20 runs) of semi-supervised K-NN on MNIST. Best results are highlighted only if they passed a student t-test with p &lt; 0.01.</figDesc><table><row><cell>Method</cell><cell>100 N -VAE S-VAE N -VAE S-VAE N -VAE S-VAE 600 1000</cell></row><row><cell>d = 2</cell><cell>72.6±2.1 77.9±1.6 80.8±0.5 84.9±0.6 81.7±0.5 85.6±0.5</cell></row><row><cell>d = 5</cell><cell>81.8±2.0 87.5±1.0 90.9±0.4 92.8±0.3 92.0±0.2 93.4±0.2</cell></row><row><cell>d = 10</cell><cell>75.7±1.8 80.6±1.3 88.4±0.5 91.2±0.4 90.2±0.4 92.8±0.3</cell></row><row><cell>d = 20</cell><cell>71.3±1.9 72.8±1.6 88.3±0.5 89.1±0.6 90.1±0.4 91.1±0.3</cell></row><row><cell>d = 40</cell><cell>72.3±1.6 67.7±2.3 88.0±0.5 87.4±0.7 90.3±0.5 90.4±0.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Summary of results of semi-supervised model M1+M2 on MNIST.</figDesc><table><row><cell cols="2">Method</cell><cell></cell><cell>100</cell></row><row><cell cols="3">dim z1 dim z2 N +N</cell><cell>S+S</cell><cell>S+N</cell></row><row><cell></cell><cell>5</cell><cell cols="2">90.0±.4 94.0±.1 93.8±.1</cell></row><row><cell>5</cell><cell>10</cell><cell cols="2">90.7±.3 94.1±.1 94.8±.2</cell></row><row><cell></cell><cell>50</cell><cell cols="2">90.7±.1 92.7±.2 93.0±.1</cell></row><row><cell></cell><cell>5</cell><cell cols="2">90.7±.3 91.7±.5 94.0±.4</cell></row><row><cell>10</cell><cell>10</cell><cell cols="2">92.2±.1 96.0±.2 95.9±.3</cell></row><row><cell></cell><cell>50</cell><cell cols="2">92.9±.4 95.1±.2 95.7±.1</cell></row><row><cell></cell><cell>5</cell><cell cols="2">92.0±.2 91.7±.4 95.8±.1</cell></row><row><cell>50</cell><cell>10</cell><cell cols="2">93.0±.1 95.8±.1 97.1±.1</cell></row><row><cell></cell><cell>50</cell><cell cols="2">93.2±.2 94.2±.1 97.4±.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results for link prediction in citation networks.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">N -VGAE S-VGAE</cell></row><row><cell>Cora</cell><cell>AUC AP</cell><cell>92.7±.2 93.2±.4</cell><cell>94.1±.1 94.1±.3</cell></row><row><cell>Citeseer</cell><cell>AUC AP</cell><cell>90.3±.5 91.5±.5</cell><cell>94.7±.2 95.2±.2</cell></row><row><cell>Pubmed</cell><cell>AUC AP</cell><cell>97.1±.0 97.1±.0</cell><cell>96.0±.1 96.0±.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Algorithm 2 g(ω|k) acceptance-rejection samplingInput: dimension m, concentration κ Initialize values:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Algorithm 3 Householder transformInput: mean µ, modal vector e 1 u ← e 1 − µ u ← u</figDesc><table><row><cell>||u ||</cell></row><row><cell>U ← I − 2uu</cell></row><row><cell>Return: U</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Expected number of samples needed before acceptance, computed using Monte Carlo estimate with 1000 samples varying dimensionality and concentration parameters. Notice that the sampling complexity increases in κ, but decreases as the dimensionality, d, increases.</figDesc><table><row><cell>d = 5</cell><cell>1.020 1.171</cell><cell>1.268</cell><cell>1.398</cell><cell>1.397</cell><cell>1.426</cell><cell>1.458</cell><cell>1.416</cell><cell>1.440</cell></row><row><cell>d = 10</cell><cell>1.008 1.094</cell><cell>1.154</cell><cell>1.352</cell><cell>1.411</cell><cell>1.407</cell><cell>1.369</cell><cell>1.402</cell><cell>1.419</cell></row><row><cell>d = 20</cell><cell>1.001 1.031</cell><cell>1.085</cell><cell>1.305</cell><cell>1.342</cell><cell>1.367</cell><cell>1.409</cell><cell>1.410</cell><cell>1.407</cell></row><row><cell>d = 40</cell><cell>1.000 1.011</cell><cell>1.027</cell><cell>1.187</cell><cell>1.288</cell><cell>1.397</cell><cell>1.433</cell><cell>1.402</cell><cell>1.423</cell></row><row><cell cols="2">d = 100 1.000 1.000</cell><cell>1.006</cell><cell>1.092</cell><cell>1.163</cell><cell>1.317</cell><cell>1.360</cell><cell>1.398</cell><cell>1.416</cell></row></table><note>κ = 1 κ = 5 κ = 10 κ = 50 κ = 100 κ = 500 κ = 1000 κ = 5000 κ = 10000</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Dataset statistics for citation network datasets.</figDesc><table><row><cell>Dataset</cell><cell>Nodes</cell><cell cols="2">Edges Features</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>1,433</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>4,732</cell><cell>3,703</cell></row><row><cell cols="3">Pubmed 19,717 44,338</cell><cell>500</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Best hyperparameter settings found for citation network datasets.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>lr</cell><cell>p do d z</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/nicola-decao/s-vae arXiv:1804.00891v2 [stat.ML] 26 Sep 2018</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">By the Whitney embedding theorem any smooth real Mdimensional manifold can be smoothly embedded in R 2M</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">It is worth noting that in the original implementation by the stacked model did not converge well using end-to-end training, and used the extracted features of the M1 model as inputs for the M2 model instead.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Rianne van den Berg, Jonas Köhler, Pim de Haan, Taco Cohen, Marco Federici, and Max Welling for insightful discussions. T.K. is supported by SAP SE Berlin. J.M.T. was funded by the European Commission within the Marie Skłodowska-Curie Individual Fellowship (Grant No. 702666, Deep learning and Bayesian inference for medical imaging).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 GRADIENT OF KL DIVERGENCE</head><p>Using</p><p>and ∇ k log C m (k) = ∇ k (m/2 − 1) log k − (m/2) log(2π) − log I m/2−1 (k)</p><p>= − I m/2 (k)</p><p>then</p><p>Notice that we can use I exp m/2 = exp(−k)I m/2 for numerical stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C PROOF OF LEMMA 2</head><p>Lemma 3 (2). Let f be any measurable function and ε ∼ π 1 (ε|θ) = s(ε) g(h(ε, θ)|θ) r(h(ε, θ)|θ) the distribution of the accepted sample. Also let v ∼ π 2 (v), and T a transformation that depends on the parameters such that if z = T (ω, v; θ) with ω ∼ g(ω|θ), then z ∼ q(z|θ):</p><p>Proof.</p><p>Using the same argument employed by <ref type="bibr" target="#b24">Naesseth et al. (2017)</ref> we can apply the change of variables ω = h(ε, θ) rewrite the expression as:</p><p>Where in * we applied the change of variables z = T (ω, v; θ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D REPARAMETERIZATION GRADIENT DERIVATION D.1 GENERAL EXPRESSION DERIVATION</head><p>We can then proceed as in 8 using Lemma 2 and the the log derivative trick to compute the gradient of the expectation term ∇ θ E q(z|θ) [f (z)]:</p><p>where g rep is the reparameterization term and g cor the correction term. Since h is invertible in ε, <ref type="bibr" target="#b24">Naesseth et al. (2017)</ref> show that ∇ θ log q(h(ε, θ), θ) r((h(ε, θ), θ) in g cor simplifies to:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 GRADIENT CALCULATION</head><p>In our specific case we want to take the gradient w.r.t. θ of the expression:</p><p>The gradient can be computed using the Lemma 2 and the subsequent gradient derivation with f (z) = p φ (x|z). As specified in Section 3.4 we optimize unbiased Monte Carlo estimates of the gradient. Therefore fixed one datapoint x and sampled (ε, v) ∼ π 1 (ε|θ)π 2 (v) the gradient is:</p><p>With</p><p>where g rep is simply the gradient of the reconstruction loss w.r.t θ and can be easily handled by automatic differentiation packages.</p><p>For what concerns g cor we notice that the terms g() and h() do not depend on µ. Thus the g cor term w.r.t. µ is 0 an all the following calculations can will be only w.r.t. κ. We therefore have:</p><p>and ∇ κ log g(ω|k) = ∇ κ log C m (k) + ωk + 1 2 (m − 3) log(1 − ω 2 ) (37)</p><p>So, putting everything together we have:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aytekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cricri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aksu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00187</idno>
		<title level="m">Clustering and unsupervised anomaly detection with l2 normalized deep auto-encoder representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Topic models over text streams: A study of batch and online unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICDM</title>
		<imprint>
			<biblScope unit="page" from="431" to="436" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Generative model-based clustering of directional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>SIGKDD</publisher>
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Clustering on the unit hypersphere using von misesfisher distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLP</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1345" to="1382" />
			<date type="published" when="2005-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMRL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<title level="m">Importance weighted autoencoders. ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Khler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">Spherical CNNs. ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep unsupervised clustering with gaussian mixture variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dilokthanakul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A M</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Salimbeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<idno>abs/1611.02648</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Statistical analysis of spherical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">I</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Embleton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Gemici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<title level="m">Normalizing flows on riemannian manifolds. NIPS Bayesian Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Von mises-fisher clustering models. ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="154" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structure-frommotion in spherical video using the von mises-fisher distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="711" to="723" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Generating sentences by editing prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasnat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bohné</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Milgram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gentric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04264</idno>
		<title level="m">von mises-fisher mixture model-based deep learning: Application to face verification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient gradientbased inference through transformations between bayes nets and neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page" from="1782" to="1790" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="3581" to="3589" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>abs/1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Variational Graph Auto-Encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Bayesian Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Riemannian Stein Variational Gradient Descent for Bayesian Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<title level="m">Deep hyperspherical learning. NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3953" to="3963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Statistics of directional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V</forename><surname>Mardia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological</title>
		<imprint>
			<biblScope unit="page" from="349" to="393" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reparameterization Gradients through Acceptance-Rejection Sampling Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Naesseth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="page" from="489" to="498" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Stick-breaking variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Poincaré embeddings for learning hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="6341" to="6350" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Silverthorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<title level="m">Spherical topic models. ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<title level="m">Variational inference with normalizing flows. ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the quantitative analysis of deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page" from="872" to="879" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Vae with a vampprior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1214" to="1223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Computer generation of distributions on the m-sphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ulrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="158" to="163" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spherical and hyperbolic embeddings of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pekalska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2255" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
