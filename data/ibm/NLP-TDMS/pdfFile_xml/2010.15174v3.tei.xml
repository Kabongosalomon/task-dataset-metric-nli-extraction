<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Perceptual Quality by Phone-Fortified Perceptual Loss using Wasserstein Distance for Speech Enhancement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsun-An</forename><surname>Hsieh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Information Technology Innovation</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Information Technology Innovation</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szu-Wei</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Information Technology Innovation</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xugang</forename><surname>Lu</surname></persName>
							<email>xugang.lu@nict.go.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">National Institute of Information and Communications Technology</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tsao</surname></persName>
							<email>yu.tsao@citi.sinica.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Information Technology Innovation</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Perceptual Quality by Phone-Fortified Perceptual Loss using Wasserstein Distance for Speech Enhancement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: Speech enhancement</term>
					<term>perceptual loss</term>
					<term>con- trastive predictive coding</term>
					<term>representation learning</term>
					<term>self- supervised learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Speech enhancement (SE) aims to improve speech quality and intelligibility, which are both related to a smooth transition in speech segments that may carry linguistic information, e.g. phones and syllables. In this study, we propose a novel phonefortified perceptual loss (PFPL) that takes phonetic information into account for training SE models. To effectively incorporate the phonetic information, the PFPL is computed based on latent representations of the wav2vec model, a powerful selfsupervised encoder that renders rich phonetic information. To more accurately measure the distribution distances of the latent representations, the PFPL adopts the Wasserstein distance as the distance measure. Our experimental results first reveal that the PFPL is more correlated with the perceptual evaluation metrics, as compared to signal-level losses. Moreover, the results showed that the PFPL can enable a deep complex U-Net SE model to achieve highly competitive performance in terms of standardized quality and intelligibility evaluations on the Voice Bank-DEMAND dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In real-world speech-related applications, speech signals may be contaminated by environmental noise, and thus constrain the achievable performance on target tasks. To address this issue, speech enhancement (SE) has been studied for decades. Numerous signal processing-based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> have been proposed. These methods are based on the assumed statistical properties of speech and noise signals. Unfortunately, SE performance may drop drastically when these assumptions are unfulfilled. With recent advances in neural network (NN) models, SE performance has increased notably. Well-known NN models, such as deep denoising autoencoder (DDAE) <ref type="bibr" target="#b4">[5]</ref>, deep neural networks (DNNs) <ref type="bibr" target="#b5">[6]</ref>, recurrent neural networks (RNNs) <ref type="bibr" target="#b6">[7]</ref>, long short-term memory (LSTM) <ref type="bibr" target="#b7">[8]</ref>, convolutional neural networks (CNNs) <ref type="bibr" target="#b8">[9]</ref>, fully convolutional networks (FCNs) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, convolutional recurrent neural networks (CRNNs) <ref type="bibr" target="#b11">[12]</ref>, and generative adversarial networks (GANs) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> have made notable improvements over traditional signal processing-based SE methods.</p><p>For these NN-based SE approaches, designing a suitable objective function is a crucial factor. Traditionally, point-wise distances are often used to form the objective functions. Pointwise distances, such as L 1 and/or L 2 norms between paired noisy-clean speech signals, attempt to recover information on a signal level. Recent studies have revealed that objective functions based on point-wise distances may not fully reflect the perceptual difference between noisy and clean speech signals. As the purpose of SE is to recover speech quality and intelligibility, objective functions that consider perceptual metrics have been investigated for NN-based SE. In some studies, perceptual metrics were modified to their differentiable alternatives for convenient gradient calculations to optimize the NN parameters. Some notable works are the perceptual evaluationbased loss function <ref type="bibr" target="#b19">[20]</ref>, joint source-to-distortion ratio (SDR) perceptual evaluation for speech quality optimization <ref type="bibr" target="#b20">[21]</ref>, and modified short-time objective intelligibility (STOI) loss functions for network optimization <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. Along this line, several studies focus on training NN models with target metrics for SE tasks <ref type="bibr" target="#b23">[24]</ref>, as well as with GAN approaches like HiFi-GAN <ref type="bibr" target="#b17">[18]</ref> and MetricGAN <ref type="bibr" target="#b18">[19]</ref>. Another class of approaches focused on building loss functions in the spaces mapped by certain pre-trained classifiers. For example, in style transfer studies of computer vision, <ref type="bibr" target="#b24">[25]</ref> proposed training feed-forward networks based on perceptual loss. In <ref type="bibr" target="#b25">[26]</ref>, the authors proposed utilizing an acoustic scene (AS) recognition network's latent spaces for the loss function, termed deep feature loss (DFL). For further improvement over DFL, <ref type="bibr" target="#b26">[27]</ref> proposed the perceptual ensemble regularization loss (PERL) as a variant of DFL that gathers several pre-trained models related to speech or acoustic tasks, achieving state-of-the-art performance in terms of quality. Despite the success, it remains unclear how acoustic event (AE) or AS classifiers benefit SE.</p><p>In this paper, we propose a novel phone-fortified perceptual loss (PFPL) for training SE models. The PFPL modifies the original DFL in two aspects. First, the PFPL intends to consider the phonetic information embedded in the speech signals. Therefore, rather than using the AS recognition models, the PFPL is computed based on the latent representations of the wav2vec model <ref type="bibr" target="#b27">[28]</ref>, a powerful self-supervised encoder that renders rich phonetic information. Second, as the distance used in the original DFL ignores the geometry of the distributions of the latent representations, PFPL adopts the Wasserstein distance <ref type="bibr" target="#b28">[29]</ref> as the distance measure. In this way, the SE training can be seen as an optimal transport problem that transforms the distributions of noisy speech to that of clean speech. Experimental results first confirmed that the PFPL can enable a deep complex U-Net SE model to achieve highly competitive performance on the Voice Bank-DEMAND dataset. A series of ablation studies investigated the effectiveness of individual parts in the PFPL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>In this section, we first present DFL and PERL, which was mentioned in the previous section, with a more detailed discussion in Section 2.1. We then review the perceptual metrics approximated with trained networks. Such a network can work as a discriminator in a GAN or a stand-alone metric. Last but not arXiv:2010.15174v3 [cs.SD] 27 Apr 2021 <ref type="figure">Figure 1</ref>: A demonstration of the proposed network. The enhancement model estimates a cRM by the noisy spectrum, and consequently produces an enhanced spectrum. The PFPL then compares the semantic difference of clean speech and the enhanced one.</p><formula xml:id="formula_0">STFT ... !,# iSTFT !,# # !,# &amp; Skip connections L PFPL Phone-Fortified Perceptual Loss Large-DCUnet-20 Φ $%&amp;'&amp;() ( ) Φ $%&amp;'&amp;() ( &amp;)</formula><p>least, we review methods that maximize the mutual information between contexts in Section 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">DFL and PERL</head><p>The idea to incorporate AS recognition in SE was first proposed in DFL <ref type="bibr" target="#b25">[26]</ref>. According to <ref type="bibr" target="#b17">[18]</ref>, the latent features from a pretrained recognition network (used for machine perception) are used to approximate human perception (SE, in this case). Analogous to DFL, <ref type="bibr" target="#b26">[27]</ref> extend the idea to ensemble of six types of pre-trained networks, including AE classifiers and speech encoders. In spite of that PERL-AE uses AE classifier alone and yields the best result, it remains unexplainable due to the complication of characteristics in AEs, which are too difficult to be analyzed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">MetricGAN and HiFi-GAN</head><p>MetricGAN <ref type="bibr" target="#b18">[19]</ref> applies a discriminator (also called Quality-Net <ref type="bibr" target="#b29">[30]</ref>) to approximate the behavior of the evaluation functions of interest. The predicted score can also be treated as a special case of perceptual loss, with an embedding dimension equal to 1. Due to the limited dimensions, Quality-Net is easily fooled by the speech generated by the updated generator. Therefore, MetricGAN needs to alternatively train between the generator and the discriminator which slows down its efficiency. HiFi-GAN <ref type="bibr" target="#b17">[18]</ref> incorporates the idea of GAN and deep feature loss. However, its deep feature loss is based on the discriminator, which may not be highly related to human perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Contrastive Predictive Coding (CPC) and wav2vec</head><p>Recent studies of representation learning have shown capabilities in extracting representative features without supervision. For instance, CPC <ref type="bibr" target="#b30">[31]</ref> is an self-supervised method that proposes to extract task-agnostic features from high-dimensional data. It was the contrastive loss that helps capture features which maximize the amount of underlying shared information of the observation and its latent representation. As a result, selfsupervised methods that can extract features with phonetic information from speech signals drew our attention. We then focus on speech-related applications that utilizes representation learning approaches.</p><p>The self-supervised automatic speech recognition (ASR) wav2vec <ref type="bibr" target="#b27">[28]</ref>, utilizing the CPC technique, has shown great performance in recognition accuracy and thus fits our interests. In practice, speech signals are first encoded with an encoder network that extracts features rich in phonetic information. An ASR decoder is then trained based on these features as inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Wasserstein Distance</head><p>The Wasserstein distance <ref type="bibr" target="#b28">[29]</ref> is a measurement of two probability distributions on a metric space (M, d) with d : M × M → R a metric on M. The Wasserstein distance of two densities µ and ν is defined as:</p><formula xml:id="formula_1">Wp(µ, ν) := inf γ∈Γ(µ,ν) M×M d(x, y) p dγ(x, y) 1 p<label>(1)</label></formula><p>where Γ(µ, ν) denotes a set of all possible measures (or couplings) on M×M with marginals µ and ν. Here, γ(x, y) is the coupling, that is, a joint distribution of marginals µ and ν, representing any possible transport plan from µ to ν. Comparing to L p distances that only regard the amount of mass transported, Eq. (1) shows that Wasserstein distance additionally takes the transport method into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed framework</head><p>In this section, we start with introducing a complex U-Net adopted, which was widely utilized in several studies <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref> that has been confirmed to achieve promising results. In the followings, we will describe the PFPL, which is a perceptual loss incorporated with Wasserstein distance in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Architecture</head><p>Inspired by the deep complex U-Net (DCUnet) in <ref type="bibr" target="#b33">[34]</ref>, we designed a modified framework that estimates complex ratio masks (cRM) for a noisy complex spectrum with a different normalization mechanism. More specifically, as shown in <ref type="figure">Fig. 1</ref>, a noisy speech signal is first converted to a complex spectrum through short-time Fourier transform (STFT), and the enhancement model generates a cRM. Subsequently, the noisy spectrum is multiplied by the cRM in a point-wise manner to derive the final enhanced spectrum, which is transformed to a waveform by inverse STFT (iSTFT). Here, according to <ref type="bibr" target="#b33">[34]</ref>, a scheme that produces cRM with a complex neural network (cRMCn) is used in this work, and we take the Large-DCUnet-20 as a reference architecture for our enhancement model. As a number of previous works <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> have indicated that instance normalization outperforms batch normalization on generation tasks by preserving the independence of samples in a mini-batch, we substitute the batch normalization layers with instance normalization layers. To describe the enhancement process precisely, given the noisy input speech signal x, the noisy spectrum X t,f is produced by STFT, such that X t,f = STFT(x). Then, the enhancement model generates a cRM M t,f to produce the enhanced spectrum thatŶ t,f = M t,f · X t,f , and transforms it to the enhanced waveformŷ by iSTFT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Phone-Fortified Perceptual Loss</head><p>For training SE models, point-wise loss functions are commonly used in either time domain or time-frequency domain.</p><p>Despite that these approaches have achieved promising results, point-wise losses remain numerically inconsistent with perceptual evaluations such as PESQ or STOI. Unlike point-wise losses that measure distances in the signal level, the perceptual loss is devised to measure the distance in the latent space <ref type="bibr" target="#b24">[25]</ref>.</p><p>The design of perceptual losses requires an appropriate feature extractor. In the SE scenario, the estimates of a system are speech signals. It is thus our desire to carry out loss computation that is able to preserve attributes in speech signals (i.e., phones, speaker characteristics, etc.) in the training stage. Some proposed a supervised pre-trained encoder for loss computation, like <ref type="bibr" target="#b25">[26]</ref>. However, these methods can suffer from the drawback of one-hot encoding in which the correlations between categories were ignored since the labels are in an orthogonal (high-dimensional) space <ref type="bibr" target="#b36">[37]</ref>. As a consequence, the correlations between phones could be underestimated and thus restrict the capability of preserving attributes. Hence, we employ wav2vec, a self-supervised encoder, to compute the PFPL in a non-orthogonal (low-dimensional) space that is more capable of preserving attributes. Owing to the fact that speech signals carry linguistic information (e.g., phones, syllables, etc.) more often than noises do, we prefer to use models that generate features which are representative for phonetic information. Meanwhile, note that CNNs are known for the shift-invariance, which is analogous to the perceptual evaluation of speech quality (PESQ) <ref type="bibr" target="#b37">[38]</ref> which is insensitive to shifts in a short-time. As stated above, we believe the CNN-based wav2vec encoder (denoted as Φwav2vec) is suitable for the design of our loss function.</p><p>In contrast to previous works on perceptual loss that utilize activations in multiple layers, we merely extract the final outputs for efficiency. Formally, as the densities remain unknown, we define the PFPL by the Kantorovich-Rubinstein <ref type="bibr" target="#b38">[39]</ref> dual form of Wasserstein distance:</p><formula xml:id="formula_2">LPFPL(y,ŷ) := y −ŷ 1 + sup f ∈F Eµ [f (c)] − Eν [f (ĉ)] (2)</formula><p>where c = Φwav2vec(y) andĉ = Φwav2vec(ŷ) are the features of the clean speech y and the enhanced speechŷ, respectively. Here, µ and ν are the densities of c andĉ in the latent space, and f is a function belonging to a set F : {f :</p><formula xml:id="formula_3">R n → R n | f (x1) − f (x2) ≤ 1 x1 − x2 , ∀x1, x2 ∈ R n } of all 1-Lipschitz functions.</formula><p>For the given paired enhanced speech and clean speech, the PFPL minimizes the distance between the distributions of the estimates and the corresponding targets in a space of phonetic representations. Please note Eq. (2) that the PFPL includes an mean absolute error (MAE) loss to measure the signal-level difference. The effect of the MAE loss will be discussed in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we begin with the selected dataset and the evaluation metrics that were used as a standard benchmark. Next, we provide visualizations that demonstrate that the features generated by the PFPL are correlated with PESQ and STOI. Finally, it is shown that the proposed modification achieves competitive performance in terms of qualities and intelligibility. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Voice Bank-DEMAND Dataset</head><p>To compare our proposed SE system with other recent approaches, the Voice Bank-DEMAND dataset <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> was used for evaluation. In this dataset, utterances recorded by 28 speakers out of a total 30 speakers were used for training, and the utterances from the remaining 2 speakers were used for testing.</p><p>In the training set, noisy mixtures were synthesized using 10 types of noise at 4 different SNR levels, ranging from 0 dB to 15 dB, and 5 types of unseen noises, ranging from 2.5 dB to 17.5 dB were added to the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>Following prior works evaluated on the Voice Bank-DEMAND dataset, we used five metrics, which were CSIG, CBAK, COVL, introduced in <ref type="bibr" target="#b41">[42]</ref>, PESQ, and STOI to measure the performance of the proposed method. CSIG, CBAK, and COVL demonstrate the signal distortion, background intrusiveness, and the overall quality with the same scale of mean opinion score, respectively. PESQ and STOI quantify the perceptual quality and the intelligibility of a speech signal. All of the above-mentioned metrics are better with higher scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Regarding SE as an Optimal Transport Problem</head><p>As mentioned in Section 3.2, latent representations of wav2vec render rich phonetic information. <ref type="figure" target="#fig_0">Fig. 2(a)</ref> demonstrates a t-SNE analysis of five phones, which are properly separated, confirming that the latent representations generated by wav2vec carry rich phonetic information. <ref type="figure" target="#fig_0">Fig. 2(b)</ref> shows that most of the noisy and clean speech are highly distinguishable in the latent space. Based on the observations from <ref type="figure" target="#fig_0">Fig. 2</ref>, we can consider the training procedure of SE as an optimal transport problem that aims to search for a transformation mapping the distributions of noisy speech signals to that of the clean ones. Based on this concept, we decide to replace the L p distance and use the Wasserstein distance as the distance measure to compute the perceptual loss for the PFPL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Correlation of Perceptual Metrics to Losses</head><p>To analyze the relation between perceptual metrics and other losses, we compared several different losses to the corresponding metric scores on the testing set. Here, we illustrate the correlations of PESQ and STOI to five losses including, MAE, mean squared error (MSE), weighted source-to-distortion ratio (wSDR), DFL, and the proposed PFPL. Each point represents an utterance. From <ref type="figure">Fig. 3</ref>, we note that MAE and MSE have Ours (-0.75) <ref type="figure">Figure 3</ref>: Illustration the correlations of PESQ and STOI to different losses. To quantify how much a loss is correlated to a metric, we note the Pearson correlation coefficient in the parentheses. The higher absolute value of PCC indicates higher correlation. similar correlations to PESQ and STOI, and the four groups of points of wSDR represent the four SNR levels in the testing set. For the first three losses, there is no obvious correlation to the two metrics. However, the more obvious tendencies are that DFL and PFPL correlate to PESQ and STOI. Here, the Pearson correlation coefficient (PCC) is utilized to quantify the correlation between metrics and losses. The PCCs of losses are shown inside the parentheses in <ref type="figure">Fig. 3</ref>. The PCC of PFPL is much higher than all the other metrics' being compared. From <ref type="table" target="#tab_0">Table 1 and Table 2</ref>, although DFL is more correlated with PESQ than the other three signal-level metrics, it has similar results to wSDR, MAE, and MSE. Because the PFPL measures how different the features are in terms of phonetic information salient to the human auditory system, it is reasonable that the PFPL is highly correlated with PESQ and STOI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study on the PFPL</head><p>In <ref type="table" target="#tab_0">Table 1</ref>, we compare prior approaches using GAN-based methods and specialized losses for auditory perception. Our approach achieved the highest PESQ score among all the compared methods. To understand PFPL, we compare several losses with the same model structure and conduct an ablation study on the PFPL. In <ref type="table" target="#tab_0">Table 1</ref>, PFPL-W denotes PFPL using the L p distance, and PFPL-W-MAE denotes PFPL-W without using the MAE loss. From <ref type="table" target="#tab_1">Table 2</ref>, point-wise losses (wSDR, MSE, and MAE) yield lower PESQ but higher CBAK comparing to the perceptual loss alone (i.e., PFPL-W-MAE). The low CBAK performance is caused by the point-wise difference ignored during training. This problem can be solved by adding MAE (i.e., the first term in Eq. (2)) to the objective function, and accordingly PFPL-W yields an improved CBAK score from 3.05 to 3.52. Finally, by comparing PFPL and PFPL-W, the effect of the Wasserstein distance is confirmed, and our best results in terms of quality and intelligibility is attained by PFPL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a novel PFPL loss for training SE models. The PFPL is derived based on the latent representations of the wav2vec model, which carry rich phonetic information. Meanwhile, the PFPL uses the Wasserstein distance as the distance measure. Accordingly, the SE training can be seen as an optimal transport problem that aims to move the latent representation distributions of noisy speech to that of clean speech. The experimental results first revealed that the PFPL has very high correlations with perceptual metrics as compared to other related loss functions. Moreover, the SE model trained with the PFPL outperforms several well-known and related works in terms of standardized evaluation metrics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>t-SNE analysis on wav2vec encoded feature map. (a) Feature map of five phone classes. (b) Feature map of clean and noisy utterances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Our proposed method versus some well performing methods with respect to different metrics. DFL † shows the results from the official source code and released parameters.</figDesc><table><row><cell>Model</cell><cell cols="3">PESQ CSIG CBAK COVL STOI</cell></row><row><cell>Noisy</cell><cell cols="3">1.97 3.35 2.44 2.63 0.92</cell></row><row><cell>Wiener [3]</cell><cell cols="2">2.22 3.23 2.68 2.67</cell><cell>-</cell></row><row><cell>SEGAN [13]</cell><cell cols="3">2.16 3.48 2.94 2.80 0.93</cell></row><row><cell>DFL [26]</cell><cell>-</cell><cell>3.86 3.33 3.22</cell><cell>-</cell></row><row><cell>DFL  †</cell><cell cols="3">2.58 3.80 2.72 3.19 0.93</cell></row><row><cell>MetricGAN [19]</cell><cell cols="3">2.86 3.99 3.18 3.42 0.94</cell></row><row><cell>HiFi-GAN [18]</cell><cell cols="2">2.94 4.07 3.07 3.49</cell><cell>-</cell></row><row><cell>SDR-PESQ [21]</cell><cell cols="2">3.01 4.09 3.54 3.55</cell><cell>-</cell></row><row><cell>T-GSA [43]</cell><cell cols="2">3.06 4.18 3.59 3.62</cell><cell>-</cell></row><row><cell cols="4">PERL-wav2vec [27] 2.92 4.16 3.37 3.54 0.94</cell></row><row><cell>PFP</cell><cell cols="3">3.15 4.18 3.60 3.67 0.95</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>† https://github.com/francoisgermain/SpeechDenoisingWithDeepFeatureLosses.git.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the ablations of PFPL and the pointwise losses with respect to evaluation metrics.</figDesc><table><row><cell>Loss</cell><cell cols="5">PESQ CSIG CBAK COVL STOI</cell></row><row><cell>wSDR [34]</cell><cell>2.58</cell><cell>3.00</cell><cell>3.18</cell><cell>2.76</cell><cell>0.93</cell></row><row><cell>MSE</cell><cell>2.60</cell><cell>3.31</cell><cell>3.19</cell><cell>2.94</cell><cell>0.93</cell></row><row><cell>MAE</cell><cell>2.62</cell><cell>3.47</cell><cell>3.20</cell><cell>3.02</cell><cell>0.93</cell></row><row><cell>PFPL-W-MAE</cell><cell>3.09</cell><cell>4.22</cell><cell>3.05</cell><cell>3.67</cell><cell>0.94</cell></row><row><cell>PFPL-W</cell><cell>3.11</cell><cell>4.15</cell><cell>3.52</cell><cell>3.64</cell><cell>0.95</cell></row><row><cell>PFPL</cell><cell>3.15</cell><cell>4.18</cell><cell>3.60</cell><cell>3.67</cell><cell>0.95</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Suppression of acoustic noise in speech using spectral subtraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TASSP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="120" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enhancement and bandwidth compression of noisy speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oppenheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1586" to="1604" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A speech enhancement method based on kalman filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Loizou</surname></persName>
		</author>
		<title level="m">Speech enhancement: theory and practice</title>
		<imprint>
			<publisher>CRC press</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Speech enhancement based on deep denoising autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="page" from="436" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A regression approach to speech enhancement based on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM TASLP</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Single-channel speech separation with memory-enhanced recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speech enhancement with lstm recurrent neural networks and its application to noise-robust asr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. LVA/ICA</title>
		<meeting>LVA/ICA</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutionalrecurrent neural networks for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zarar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tashev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end waveform utterance enhancement for direct evaluation metrics optimization by fully convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kawai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM TASLP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1570" to="1584" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tcnn: Temporal convolutional neural network for real-time speech enhancement in the time domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A convolutional recurrent neural network for real-time speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Segan: Speech enhancement generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrà</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Time-frequency masking-based speech enhancement using generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Patil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On adversarial training and loss functions for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sergan: Speech enhancement using relativistic generative adversarial networks with gradient penalty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Baby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verhulst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved wasserstein conditional generative adversarial network speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Wireless Communications and Networking</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">181</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hifi-gan: High-fidelity denoising and dereverberation based on speech deep features in adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Metricgan: Generative adversarial networks based black-box metric scores optimization for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A deep learning loss function based on the perceptual evaluation of the speech quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martín-Doñas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peinado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1680" to="1684" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">End-to-end multi-task denoising for joint sdr and pesq optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Kharmy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09146</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Monaural speech enhancement using deep neural networks by maximizing a short-time objective intelligibility measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Perceptually guided speech enhancement using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning with learned loss function: Speech enhancement with quality-net to improve perceptual evaluation of speech quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="26" to="30" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Speech denoising with deep feature losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Perceptual loss based speech denoising with an ensemble of audio pattern recognition and self-supervised models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kataria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">wav2vec: Unsupervised pre-training for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The distance between two random vectors with given dispersion matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Olkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pukelsheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="257" to="263" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Quality-net: An end-to-end non-intrusive speech quality assessment model based on blstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Interspeech</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Coarse-to-fine optimization for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Al-Dahle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Dccrn: Deep complex convolution recurrent network for phase-aware speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00264</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Phase-aware speech enhancement with deep complex u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Beyond one-hot encoding: Lower dimensional target embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="21" to="31" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Optimal transport -Old and new</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">338</biblScope>
			<biblScope unit="page">973</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The voice bank corpus: Design, collection and data analysis of a large regional accent speech database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. O-COCOSDA/CASLRE</title>
		<meeting>O-COCOSDA/CASLRE</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Demand: a collection of multi-channel recordings of acoustic noise in diverse environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thiemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Meetings Acoust</title>
		<meeting>Meetings Acoust</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Evaluation of objective quality measures for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Loizou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TSAP</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="229" to="238" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">T-gsa: Transformer with gaussian-weighted self-attention for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
