<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Context-Aware Loss Function for Action Spotting in Soccer Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Cioppa</surname></persName>
							<email>anthony.cioppa@uliege.be</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Deliège</surname></persName>
							<email>adrien.deliege@uliege.be</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
							<email>silvio.giancola@kaust.edu.sa</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaust</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">Van</forename><surname>Droogenbroeck</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rikke</forename><surname>Gade</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Liège</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Liège</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">KAUST</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Liège</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Aalborg University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Aalborg University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Context-Aware Loss Function for Action Spotting in Soccer Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In video understanding, action spotting consists in temporally localizing human-induced events annotated with single timestamps. In this paper, we propose a novel loss function that specifically considers the temporal context naturally present around each action, rather than focusing on the single annotated frame to spot. We benchmark our loss on a large dataset of soccer videos, SoccerNet, and achieve an improvement of 12.8% over the baseline. We show the generalization capability of our loss for generic activity proposals and detection on ActivityNet, by spotting the beginning and the end of each activity. Furthermore, we provide an extended ablation study and display challenging cases for action spotting in soccer videos. Finally, we qualitatively illustrate how our loss induces a precise temporal understanding of actions and show how such semantic knowledge can be used for automatic highlights generation.</p><p>(*) Denotes equal contributions. Code available at https:// github.com/cioppaanthony/context-aware-loss.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Aside from automotive, consumer, and robotics applications, sports is considered one of the most valuable applications in computer vision <ref type="bibr" target="#b53">[54]</ref>, capping $91 billion of annual market revenue <ref type="bibr" target="#b30">[31]</ref>, with $28.7 billion from the European Soccer market alone <ref type="bibr" target="#b13">[15]</ref>. Recent advances helped provide automated tools to understand and analyze broadcast games. For instance, current computer vision methods can localize the field and its lines <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b22">24]</ref>, detect players <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b62">63]</ref>, their motion <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b39">40]</ref>, their pose <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b66">67]</ref>, their team <ref type="bibr" target="#b25">[27]</ref>, track the ball position <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b55">56]</ref> and the camera motion <ref type="bibr" target="#b38">[39]</ref>. Understanding spatial frame-wise information is useful to enhance the visual experience of sports viewers <ref type="bibr" target="#b46">[47]</ref> and to gather players statistics <ref type="bibr" target="#b56">[57]</ref>, but it misses higher-level game understanding. For broadcast producers, <ref type="figure">Figure 1</ref>. Context-aware loss function. We design a novel loss that leverages the temporal context around an action spot (at a temporal shift of 0). We heavily penalize the frames far-distant from the action and decrease the penalty for those gradually closer. We do not penalize the frames just before the action to avoid providing misleading information as its occurrence is uncertain, but we heavily penalize those just after, as the action has occurred. it is of paramount importance to have a deeper understanding of the game actions. For instance, live broadcast production follows specific patterns when particular actions occur; sports live reporters comment on the game actions; and highlights producers generate short summaries by ranking the most representative actions within the game. In order to automate these production tasks, computer vision methods should understand the salient actions of a game and respond accordingly. While spatial information is widely studied and quite mature, localizing actions in time remains a challenging task for current video understanding algorithms.</p><p>In this paper, we target the action spotting challenge, with a primary application on soccer videos. The task of action spotting has been defined as the temporal localization of human-induced events annotated with a single timestamp <ref type="bibr" target="#b19">[21]</ref>. Inherent difficulties arise from such annotations: their sparsity, the absence of start and end times of the actions, and their temporal discontinuities, i.e. the unsettling fact that adjacent frames may be annotated differently albeit being possibly highly similar. To overcome these issues, we propose a novel loss that leverages the temporal context information naturally present around the actions, as depicted in <ref type="figure">Figure 1</ref>. To highlight its generality and versatility, we showcase how our loss can be used for the task of activity localization in ActivityNet <ref type="bibr" target="#b21">[23]</ref>, by spotting the beginning and end of each activity. Using the network BMN introduced in <ref type="bibr" target="#b33">[34]</ref> and simply substituting their loss with our enhanced context-aware spotting loss function, we show an improvement of 0.15% in activity proposal leading to a direct 0.38% improvement in activity detection on Activi-tyNet <ref type="bibr" target="#b21">[23]</ref>. On the large-scale action spotting soccer-centric dataset, SoccerNet <ref type="bibr" target="#b19">[21]</ref>, our network substantially increases the Average-mAP spotting metric from 49.7% to 62.5%.</p><p>Contributions. We summarize our contributions as follows. (i) We present a new loss function for temporal action segmentation further used for the task of action spotting, which is parameterized by the time-shifts of the frames from the ground-truth actions. (ii) We improve the performance of the state-of-the-art method on ActivityNet <ref type="bibr" target="#b21">[23]</ref> by including our new contextual loss to detect activity boundaries, and improve the action spotting baseline of Soccer-Net <ref type="bibr" target="#b19">[21]</ref> by 12.8%. (iii) We provide detailed insights into our action spotting performance, as well as a qualitative application for automatic highlights generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Broadcast soccer video understanding. Computer vision tools are widely used in sports broadcast videos to provide soccer analytics <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b56">57]</ref>. Current challenges lie in understanding high-level game information to identify salient game actions <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b59">60]</ref>, perform automatic game summarization <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b60">61]</ref> and report commentaries of live actions <ref type="bibr" target="#b64">[65]</ref>. Early work uses camera shots to segment broadcasts <ref type="bibr" target="#b14">[16]</ref>, or analyze production patterns to identify salient moments of the game <ref type="bibr" target="#b45">[46]</ref>. Further developments have used low-level semantic information in Bayesian frameworks <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b54">55]</ref> to automatically detect salient game actions.</p><p>Machine learning-based methods have been proposed to aggregate temporally hand-crafted features <ref type="bibr" target="#b3">[5]</ref> or deep frame features <ref type="bibr" target="#b27">[28]</ref> into recurrent networks <ref type="bibr" target="#b43">[44]</ref>. Soccer-Net <ref type="bibr" target="#b19">[21]</ref> provides an in-depth analysis of deep frame feature extraction and aggregation for action spotting in soccer game broadcasts. Multi-stream networks merge additional optical flow <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b58">59]</ref> or excitement <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b50">51]</ref> information to improve game highlights identification. Furthermore, attention models are fed with per-frame semantic information such as pixel information <ref type="bibr" target="#b11">[13]</ref> or player localization <ref type="bibr" target="#b31">[32]</ref> to extract targeted frame features. In our work, we leverage the temporal context information around actions to handle the intrinsic temporal patterns representing these actions.</p><p>Deep video understanding models are trained with largescale datasets. While early works leveraged small custom video sets, a few large-scale datasets are available and worth mentioning, in particular Sports-1M <ref type="bibr" target="#b29">[30]</ref> for generic sports video classification, MLB-Youtube <ref type="bibr" target="#b42">[43]</ref> for baseball activity recognition, and GolfDB <ref type="bibr" target="#b40">[41]</ref> for golf swing sequencing. These datasets all tackle specific tasks in sports. In our work, we use SoccerNet <ref type="bibr" target="#b19">[21]</ref> to assess the performance of our context-aware loss for action spotting in soccer videos.</p><p>Video understanding. Recent video challenges <ref type="bibr" target="#b21">[23]</ref> include activity localization, that find temporal boundaries of activities. Following object localization, two-stage approaches have been proposed including proposal generation <ref type="bibr" target="#b7">[9]</ref> and classification <ref type="bibr" target="#b6">[8]</ref>. SSN <ref type="bibr" target="#b68">[69]</ref> models each action instance with a structured temporal pyramid and TURN TAP <ref type="bibr" target="#b18">[20]</ref> predicts action proposals and regresses the temporal boundaries, while GTAN <ref type="bibr" target="#b37">[38]</ref> dynamically optimizes the temporal scale of each action proposal with Gaussian kernels. BSN <ref type="bibr" target="#b35">[36]</ref>, MGG <ref type="bibr" target="#b36">[37]</ref> and BMN <ref type="bibr" target="#b33">[34]</ref> regress the time of activity boundaries, showing state-ofthe-art performances on both ActivityNet 1.3 <ref type="bibr" target="#b21">[23]</ref> and Thumos'14 <ref type="bibr" target="#b28">[29]</ref>. Alternatively, ActionSearch <ref type="bibr" target="#b2">[4]</ref> tackles the spotting task iteratively, learning to predict which frame to visit next in order to spot a given activity. However, this method requires sequences of temporal annotations by human annotators to train the models that are not readily available for datasets outside ActivityNet. Also, Alwassel et al. <ref type="bibr" target="#b1">[3]</ref> define an action spot as positive as soon as it lands within the boundary of an activity, which is less constraining than the action spotting defined in SoccerNet <ref type="bibr" target="#b19">[21]</ref>.</p><p>Recently, Sigurdsson et al. <ref type="bibr">[52]</ref> question boundaries sharpness and show that human agreement on temporal boundaries reach an average tIoU of 72.5% for Charades <ref type="bibr" target="#b52">[53]</ref> and 58.7% on MultiTHUMOS <ref type="bibr" target="#b63">[64]</ref>. Alwassel et al. <ref type="bibr" target="#b1">[3]</ref> confirm such disparity on ActivityNet <ref type="bibr" target="#b21">[23]</ref>, but also show that it does not constitute a major roadblock to progress in the field. Different from activity localization, SoccerNet <ref type="bibr" target="#b19">[21]</ref> proposes an alternative action spotting task for soccer action understanding, leveraging a well-defined set of soccer rules that define a single temporal anchor per action. In our work, we improve the SoccerNet <ref type="bibr" target="#b19">[21]</ref> action spotting baseline by introducing a novel context-aware loss that temporally slices the vicinity of the action spots. Also, we integrate our loss for generic activity localization and detection on a boundary-based method <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b35">36]</ref>. <ref type="figure">Figure 2</ref>. Action context slicing. We define six temporal segments around each ground-truth action spot, each of which induces a specific behavior in our context-aware loss function when training the network. Far before and far after the action, its influence is negligible, thus we train the network not to predict an action. Just before the action, we do not influence the network since a particular context may or may not result in an action (i.e. an attacking phase may lead to a goal). Just after the action, its contextual information is rich and unambiguous as the action has just occurred (i.e. a goal leads to celebrating). Hence, we train the network to predict an action. Finally, we define transition zones for our loss function to be smooth, in which we softly train the network not to predict an action. For each class c, the temporal segments are delimited by specific slicing parameters K c i and are materialized through our time-shift encoding, which contains richer temporal context information about the action than the initial binary spotting annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>We address the action spotting task by developing a context-aware loss for a temporal segmentation module, and a YOLO-like loss for an action spotting module that outputs the spotting predictions of the network. We first present the re-encoding of the annotations needed for the segmentation and spotting tasks, then we explain how the losses of these modules are computed based on the re-encodings. Problem definition. We denote by C the number of classes of the action spotting problem. Each action is identified by a single action frame annotated as such. Each frame of a given video is annotated with either a one-hot encoded vector with C components for the action frames or a vector of C zeros for the background frames. We denote by N F the number of frames in a video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Encoding</head><p>To train our network, the initial annotations are reencoded in two different ways: with a time-shift encoding used for the temporal segmentation loss, and with a YOLOlike encoding used for the action spotting loss. Time-shift encoding (TSE) for temporal segmentation. We slice the temporal context around each action into segments related to their distance from the action, as shown in <ref type="figure">Figure 2</ref>. The segments regroup frames that are either far before, just before, just after, far after an action, or in transition zones between these segments.</p><p>We use the segments in our temporal segmentation module so that its segmentation scores reflect the following ideas. (1) Far before an action spot of some class, we cannot foresee its occurrence. Hence, the score for that class should indicate that no action is occurring. (2) Just before an action, its occurrence is uncertain. Therefore, we do not influence the score towards any particular direction. (3) Just after an action has happened, plenty of visual cues allow for the detection of the occurrence of the action. The score for its class should reflect the presence of the action. (4) Far after an action, the score for its class should indicate that it is not occurring anymore. The segments around the actions of class c are delimited by four temporal context slicing parameters K c 1 &lt; K c 2 &lt; 0 &lt; K c 3 &lt; K c 4 as shown in <ref type="figure">Figure 2</ref>. The context slicing is used to perform a time-shift encoding (TSE) of each frame x of a video with a vector of length C, containing class-wise information on the relative location of x with respect to its closest past or future actions. The TSE of x for class c, noted s c (x), is the time-shift (i.e. difference in frame indices) of x from either its closest past or future ground-truth action of class c, depending on which has the dominant influence on x. We set s c (x) as the timeshift from the past action if either (i) x is just after the past action; or (ii) x is in the transition zone after the past action, but is far before the future action; or (iii) x is in the transition zones after the past and before the future actions while being closer to the past action. In all other cases, s c (x) is the time-shift from the future action. If x is both located far after the past action and far before the future action, selecting either of the two time-shifts has the same effect in our loss. Furthermore, for the frames located either before the first or after the last annotated action of class c, only one time-shift can be computed and is thus set as s c (x). Finally, if no action of class c is present in the video, then we set s c (x) = K c 1 for all the frames. This induces the same behavior in our loss as if they were all located far before their closest future action. <ref type="figure">Figure 3</ref>. Pipeline for action spotting. We propose a network made of a frame feature extractor and a temporal CNN outputting C class feature vectors per frame, a segmentation module outputting per-class segmentation scores, and a spotting module extracting 2 + C values per spotting prediction (i.e. the confidence score s for the spotting, its location t and a per-class prediction).</p><p>YOLO-like encoding for action spotting. Inspired by YOLO <ref type="bibr" target="#b44">[45]</ref>, each ground-truth action of the video engenders an action vector composed of 2 + C values. The first value is a binary indicator of the presence (= 1) of the action. The second value is the location of the frame annotated as the action, computed as the index of that frame divided by N F . The remaining C values represent the one-hot encoding of the action. We encode a whole video containing N GT actions in a matrix Y of dimension N GT × (2 + C), with each line representing an action vector of the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Loss and Network Design</head><p>Temporal segmentation loss. The TSE parameterizes the temporal segmentation loss described below. For clarity, we denote by p the segmentation score for a frame x to belong to class c output by the segmentation module, and s as the TSE of x for class c. We detail the loss generated by p in this setting, noted L(p, s). First, in accordance with <ref type="figure">Figure 2</ref>, we compute L(p, s) as follows:</p><formula xml:id="formula_0">L(p, s) =                                − ln(1 − p) s ≤ K c − ln(1 − p) s ≥ K c 4 .<label>(6)</label></formula><p>Then, following the practice in <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b47">48]</ref> to help the network focus on improving its worst segmentation scores, we zero out the loss for scores that are satisfying enough. In the case of Equation (4) when s = 0, we say that a score is satisfactory when it exceeds some maximum margin τ max . In the cases of Equations (1) and <ref type="formula" target="#formula_0">(6)</ref>, we say that a score is satisfactory when it is lower than some minimum margin τ min . The range of values for p that leads to zeroing out the loss varies with s and the slicing parameters in most cases. This is achieved by revising L(p, s) as in Equations <ref type="formula">(7)</ref> and <ref type="bibr" target="#b6">(8)</ref>. <ref type="figure">Figure 1</ref> shows a representation ofL(p, s).</p><formula xml:id="formula_1">L(p, s) = max(0, L(p, s) + ln(τ max )) 0 ≤ s &lt; K c 3 (7) max(0, L(p, s) + ln(1 − τ min )) otherwise. (8)</formula><p>Finally, the segmentation loss L seg for a given video of frames x 1 , . . . , x N F is given in Equation <ref type="formula" target="#formula_2">(9)</ref>.</p><formula xml:id="formula_2">L seg = 1 C N F N F i=1 C c=1L (p c (x i ), s c (x i ))<label>(9)</label></formula><p>Action spotting loss. Let N pred be a fixed number of action spotting predictions generated by our network for each video. Those predictions are encoded inŶ of dimension</p><formula xml:id="formula_3">N pred × (2 + C), similarly to Y.</formula><p>We leverage an iterative one-to-one matching algorithm to pair each of the N GT ground-truth actions with a prediction. First, we match each ground-truth location of Y ·,2 with its closest predicted location inŶ ·,2 , and viceversa (i.e. we match the predicted locations with their closest ground-truth locations). Next, we form pairs of (ground-truth, predicted) locations that reciprocally match, we remove them from the process, and we iterate until all ground truths are coupled with a prediction. Consequently, we buildŶ M as a reorganized version of the actions encoded inŶ, such that Y i,2 andŶ M i,2 reciprocally match for all i ≤ N GT .</p><p>We define the action spotting loss L as in Equation <ref type="formula">(10)</ref>. It corresponds to a weighted sum of the squared errors between the matched predictions and a regularization on the confidence score of the unmatched predictions.</p><formula xml:id="formula_4">L as = NGT i=1 2+C j=1 α j Y i,j −Ŷ M i,j 2 + β Npred i=NGT+1 Ŷ M i,1 2 (10)</formula><p>Complete loss. The final loss L is presented in Equation (11) as a weighted sum of L seg and L as .</p><formula xml:id="formula_5">L = L as + λ seg L seg<label>(11)</label></formula><p>Network for action spotting. The architecture of the network is illustrated in <ref type="figure">Figure 3</ref> and further detailed in the supplementary material. We leverage frame feature representations for the videos (e.g. ResNet) provided with the dataset, embodied as the output of the frame feature extractor of <ref type="figure">Figure 3</ref>. The temporal CNN of <ref type="figure">Figure 3</ref> is composed of a spatial two-layer MLP, followed by four multi-scale 3D convolutions (i.e. across time, features and classes). The temporal CNN outputs a set of C ×f features for each frame organized in C feature vectors (one per class) of size f , as in <ref type="bibr" target="#b47">[48]</ref>. These features are input into a segmentation module, in which we use Batch Normalization <ref type="bibr" target="#b24">[26]</ref> and sigmoid activations. The closeness of the C vectors obtained in this way to a pre-defined vector gives the C segmentation scores output by the segmentation module, as <ref type="bibr" target="#b12">[14]</ref>. The C × f features obtained previously are concatenated with the C scores and fed to the action spotting module, as shown in <ref type="figure">Figure 3</ref>. It is composed of three successive temporal maxpooling and 3D convolutions, and outputs N pred vectors of dimension (2 + C). The first two elements of these vectors are sigmoid-activated, the C last are softmax-activated. The activated vectors are stacked to produce the predictionŶ of dimension N pred × (2 + C) for the action spotting task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our new context-aware loss function in two scenarios: the action spotting task of SoccerNet <ref type="bibr" target="#b19">[21]</ref>, and activity localization and detection tasks on ActivityNet <ref type="bibr" target="#b21">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiments on SoccerNet</head><p>Data. Three classes of action are annotated in SoccerNet by Giancola et al. <ref type="bibr" target="#b19">[21]</ref>: goals, cards, and substitutions, so C = 3 in this case. They identify each action by one annotated frame: the moment the ball crosses the line for goal, the moment the referee shows a player a card for card, and the moment a new player enters the field for substitution. We train our network on the frame features already provided with the dataset. Giancola et al. first subsampled the raw videos at 2 fps, then they extracted the features with a backbone network and reduced them by PCA to 512 features for each frame of the subsampled videos. Three sets of features are provided, each extracted with a particular backbone network: I3D <ref type="bibr" target="#b9">[11]</ref>, C3D <ref type="bibr" target="#b57">[58]</ref>, and ResNet <ref type="bibr" target="#b20">[22]</ref>. Action spotting metric. We measure performances with the action spotting metric introduced in SoccerNet <ref type="bibr" target="#b19">[21]</ref>. An action spot is defined as positive if its temporal offset from its closest ground truth is less than a given tolerance δ. The average precision (AP) is estimated based on Precision-Recall curves, then averaged between classes (mAP). An Average-mAP is proposed as the AUC of the mAP over different tolerances δ ranging from 5 to 60 seconds. Experimental setup. We train our network on batches of chunks. We define a chunk as a set of N F contiguous frame feature vectors. We set N F = 240 to maintain a high training speed while retaining sufficient contextual information. This size corresponds to a clip of 2 minutes of raw video. A batch contains chunks extracted from a single raw video. We extract a chunk around each ground-truth action, such that the action is randomly located within the chunk. Then, to balance the batch, we randomly extract N GT /C chunks composed of background frames only. An epoch ends when the network has been trained on one batch per The number of action spotting predictions generated by the network is set to N pred = 5, as we observed that no chunks of 2 minutes of raw video contain more than 5 actions. We train the network during 1000 epochs, with an initial learning rate lr = 10 −3 linearly decreasing to 10 −6 . We use Adam as the optimizer with default parameters <ref type="bibr" target="#b32">[33]</ref>.</p><p>For the segmentation loss, we set the margins τ max = 0.9 and τ min = 0.1 in Equations <ref type="formula">(7)</ref> and <ref type="formula">(8)</ref>, following the practice in <ref type="bibr" target="#b47">[48]</ref>. For the action spotting loss in Equation <ref type="formula">(10)</ref>, we set α j = 1 for j = 2, while α 2 is optimized (see below) to find an appropriate weighting for the location components of the predictions. Similarly, β is optimized to find the balance between the loss of the action vectors and the regularization of the remaining predictions. For the final loss in Equation <ref type="formula" target="#formula_5">(11)</ref>, we optimize λ seg to find the balance between the two losses. Hyperparameter optimization. For each set of features (I3D, C3D, ResNet), we perform a joint Bayesian optimization [1] on the number of frame features f extracted per class, on the temporal receptive field r of the network (i.e. temporal kernel dimension of the 3D convolutions), and on the parameters α 2 , β, λ seg . Next, we perform a grid search optimization on the slicing parameters K c i . For ResNet, we obtain f = 16, r = 80, α 2 = 5, β = 0.5, λ seg = 1.5. For goals (resp. cards, substitutions) we have K 1 = −40 (resp. −40, −80), K 2 = −20 (resp. −20, −40), K 3 = 120 (resp. 20, 20), and K 4 = 180 (resp. <ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b39">40)</ref>. Given the framerate of 2 fps, those values can be translated to seconds by scaling them down by a factor of 2. The value r = 80 corresponds to a temporal receptive field of 20 seconds on both sides of the central frame in the temporal dimension of the 3D convolutions. Main results. The performances obtained with the optimized parameters are reported in <ref type="table" target="#tab_0">Table 1</ref>. As shown, we establish a new state-of-the-art performance on the action spotting task of SoccerNet, outperforming the previous benchmark by a comfortable margin, for all the frame fea-tures. ResNet gives the best performance, as also observed in <ref type="bibr" target="#b19">[21]</ref>. A sensitivity analysis of the parameters K c i reveals robust performances around the optimal values, indicating that no heavy fine-tuning is required for the context slicing. Also, performances largely decrease as the slicing is strongly reduced, which emphasizes its usefulness. Ablation study. Since the ResNet features provide the best performance, we use them with their optimized parameters for the following ablation studies. (i) We remove the segmentation module, which is equivalent to setting λ seg = 0 in Equation <ref type="bibr" target="#b9">(11)</ref>. This also removes the context slicing and the margins τ max and τ min . (ii) We remove the action context slicing such that the ground truth for the segmentation module is the raw binary annotations, i.e. all the frames must be classified as background except the action frames. This is equivalent to setting</p><formula xml:id="formula_6">K 1 = −1 = K 2 = −K 3 = −K 4 . (iii)</formula><p>We remove the margins that help the network focus on improving its worst segmentation scores, by setting τ max = 1, τ min = 0 in Equations <ref type="formula">(7)</ref> and <ref type="bibr" target="#b6">(8)</ref>. (iv) We remove the iterative one-to-one matching between the ground truth Y and the predictionsŶ before the action spotting loss, which is equivalent to usingŶ instead ofŶ M in Equation <ref type="formula">(10)</ref>. The results of the ablation studies are shown in <ref type="table">Table 2</ref>.</p><p>From an Average-mAP perspective, the auxiliary task of temporal segmentation improves the performance on the action spotting task (from 58.9% to 62.5%), which is a common observation in multi-task learning <ref type="bibr" target="#b65">[66]</ref>. When the segmentation is performed, our temporal context slicing gives a significant boost compared to using the raw binary annotations (from 57.8% to 62.5%). This observation is in accordance with the sensitivity analysis. It also appears that it is preferable to not use the segmentation at all rather than using the segmentation with the raw binary annotations (58.9% vs 57.8%), which further underlines the usefulness of the context slicing. A boost in performance is also observed when we use the margins to help the network focus on improving its worst segmentation scores (from 59.0% to 62.5%). Eventually, <ref type="table">Table 2</ref> shows that it is extremely beneficial to match the predictions of the network with the ground truth before the action spotting loss (from 46.8% to 62.5%). This makes sense since there is no point in evaluating the network on its ability to order its predictions, which is a hard and unnecessary constraint. The large impact of the matching is also justified by its direct implication in the action spotting task assessed through the Average-mAP. Results through game time. In soccer, it makes sense to analyze the performance of our model through game time, since the actions are not uniformly distributed throughout the game. For example, a substitution is more likely to occur during the second half of a game. We consider nonoverlapping bins corresponding to 5 minutes of game time and compute the Average-mAP for each bin. <ref type="figure" target="#fig_0">Figure 4</ref> shows the evolution of this metric through game time.  <ref type="table">Table 2</ref>. Ablation study. We perform ablations by (i) removing the segmentation (λ seg = 0), hence the slicing and the margins;</p><p>(ii) removing the context slicing (K1 = −1 = K2 = −K3 = −K4); (iii) removing the margins that help the network focus on improving its worst segmentation scores (τmin = 0, τmax = 1); (iv) removing the matching (usingŶ instead ofŶ M in L as ). Each part evidently contributes to the overall performance. It appears that actions occurring during the first five minutes of a half-time are substantially more difficult to spot than the others. This may be partially explained by the occurrence of some of these actions at the very beginning of a half-time, for which the temporal receptive field of the network requires the chunk to be temporally padded. Hence, some information may be missing to allow the network to spot those actions. Besides, when substitutions occur during the break, they are annotated as such on the first frame of the second halves of the matches, which makes them practically impossible to spot. In the test set, this happens for 28% of the matches. None of these substitutions are spotted by our model, which thus degrades the performances during the first minutes of play in the second halves of the matches. However, they merely represent 5% of all the substitutions, and removing them from the evaluation only boosts our Average-mAP by 0.7% (from 62.5% to 63.2%).</p><p>Results as function of action vicinity. We investigate whether actions are harder to spot when they are close to each other. We bin the ground-truth actions based on the distance that separates them from the previous (or next, depending on which is the closest) ground-truth action, regardless of their classes. Then, we compute the Average-mAP for each bin. The results are represented in <ref type="figure" target="#fig_1">Figure 5</ref>.</p><p>We observe that the actions are more difficult to spot  when they are close to each other. This could be due to the reduced number of visual cues, such as replays, when an action occurs rapidly after another and thus must be broadcast. Some confusion may also arise because the replays of the first action can still be shown after the second action, e.g. a sanctioned foul followed by a converted penalty. This analysis also shows that the action spotting problem is challenging even when the actions are further apart, as the performances in <ref type="figure" target="#fig_1">Figure 5</ref> eventually plateau. Per-class results. We perform a per-class analysis in a similar spirit as the Average-mAP metric. For a given class, we fix a tolerance δ around each annotated action to determine positive predictions and we aggregate these results in a confusion matrix. An action is considered spotted when its confidence score exceeds some threshold optimized for the F 1 score on the validation set. From the confusion matrix, we compute the precision, recall and F 1 score for that class and for that tolerance δ. Varying δ from 5 to 60 seconds provides the evolution of the three metrics as a function of the tolerance. <ref type="figure">Figure 6</ref> shows these curves for goals for our model and for the baseline <ref type="bibr" target="#b19">[21]</ref>. The results for cards and substitutions are provided in supplementary material. <ref type="figure">Figure 6</ref> shows that most goals can be efficiently spotted by our model within 10 seconds around the ground truth (δ = 20 seconds). We achieve a precision of 80% for that tolerance. The previous baseline plateaus within 20 seconds (δ = 40 seconds) and still has a lower performance. In particular for goals, many visual cues facilitate their spotting, e.g. multiple replays, particular camera views, or celebrations from the players and from the public.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on ActivityNet</head><p>In this section, we evaluate our context-aware loss in a more generic task than action spotting in soccer videos. We tackle the Activity Proposal and Activity Detection tasks of the challenging ActivityNet dataset, for which we use the ResNet features provided with the dataset at 5 fps. Setup. We use the current state-of-the-art network, namely BMN <ref type="bibr" target="#b33">[34]</ref>, with the code provided in <ref type="bibr">[2]</ref>. BMN is equipped with a temporal evaluation module (TEM), which plays a similar role as our temporal segmentation module. We re-  <ref type="figure">Figure 6</ref>. Per-class results (goals). A prediction of class goal is a true positive (TP) with tolerance δ when it is located at most δ/2 seconds from a ground-truth goal. The baseline results are obtained from the best model of <ref type="bibr" target="#b19">[21]</ref>. Our model spots most goals within 10 seconds around the ground truth (δ = 20 seconds).</p><p>place the loss associated with the TEM by our novel temporal segmentation loss L seg . The slicing parameters are set identically for all the classes and are optimized with respect to the AUC performance on the validation set by grid search with the constraint K 1 = 2K 2 = −2K 3 = −K 4 . The optimization yields the best results where K 1 = −14.</p><p>Results. The average performances on 20 runs of our experiment and of the BMN base code [2] are reported in <ref type="table">Table 3</ref>. Our novel temporal segmentation loss improves the performance obtained with BMN [2] by 0.15% and 0.12% for the activity proposal task (AR@100 and AUC) and by 0.38% for the activity detection task (Average-mAP). These increases compare with some recent increments, while being obtained just by replacing their TEM loss by our contextaware segmentation loss. The network thus has the same architecture and number of parameters. We conjecture that our loss L seg , through its particular context slicing, helps train the network by modelling the uncertainty surrounding the annotations. Indeed, it has been shown in <ref type="bibr" target="#b1">[3,</ref><ref type="bibr">52]</ref> that a large variability exists among human annotators on which frames to annotate as the beginning and the end of the activities of the dataset. Let us note that in BMN, the TEM loss is somehow adapted around the action frames in order to mitigate the penalization attributed to their neighboring frames. Our work goes one step further, by directly designing a temporal context-aware segmentation loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Automatic Highlights Generation for Soccer</head><p>Some action spotting and temporal segmentation results are shown in <ref type="figure">Figure 7</ref>. It appears that some sequences of play have a high segmentation score for some classes but do not lead, quite rightly, to an action spotting. It turns Method AR@100</p><p>AUC Average-mAP Lin et al. <ref type="bibr" target="#b34">[35]</ref> 73.01 64.40 29.17 Gao et al. <ref type="bibr" target="#b17">[19]</ref> 73.17 65.72 -BSN <ref type="bibr" target="#b35">[36]</ref> 74. <ref type="bibr" target="#b14">16</ref>   <ref type="table">Table 3</ref>. Results on ActivityNet validation set for the proposal task (AR@100, AUC) and for the detection task (Average-mAP).</p><p>For our experiments, we report the average values on 20 runs. <ref type="figure">Figure 7</ref>. Action spotting and segmentation for the 2 nd half of the "Remuntada" FCB -PSG. Ground truth actions, temporal segmentation curves, and spotting results are illustrated. We can identify unannotated interesting actions using our segmentation.</p><p>out that these sequences are often related to unannotated actions of supplementary classes that resemble those considered so far, such as unconverted goal opportunities and unsanctioned fouls. Video clips of the two actions identified in <ref type="figure">Figure 7</ref> are provided in the supplementary material.</p><p>To quantify the spotting results of goal opportunities, we can only compute the precision metric since these actions are not annotated. We manually inspect each video sequence of the test set where the segmentation score for goals exceeds some threshold η but where no ground-truth goal is present. We decide whether the sequence is a goal opportunity or not by asking two frequent observers of soccer games if they would include it in the highlights of the match. The sequence is a true positive when they both agree to include it and a false positive, otherwise. The precision is then computed for that η. By gradually decreasing η from 0.9 to 0.3, we obtain the precision curve shown in <ref type="figure" target="#fig_3">Figure 8</ref>. It appears that 80% of the sequences with a segmentation score larger than η = 0.5 are considered goal opportunities.</p><p>As a direct by-product, we derive an automatic highlights generator without explicit supervision. We extract a video clip starting 15 seconds before each spotting of a goal or a card and ending 20 seconds after. We proceed likewise for the sequences with a segmentation score ≥ 0.5 for goals. We dismiss substitutions as they rarely appear in highlights. We assemble the clips chronologically to produce the highlights video, provided in supplementary material. Evaluating its quality is subjective, but we found its content to be adequate, even if the montage could be improved. Indeed, only sequences where a goal, a goal opportunity, or a foul occurs are selected. This reinforces the usefulness of the segmentation, as it provides a direct overview of the proceedings of the match, including proposals for unannotated actions that are interesting for highlights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We tackle the challenging action spotting task of Soc-cerNet with a novel context-aware loss for segmentation and a YOLO-like loss for the spotting. The former treats the frames according to their time-shift from their closest ground-truth actions. The latter leverages an iterative matching algorithm that alleviates the need for the network to order its predictions. To show generalization capabilities, we also test our context-aware loss on ActivityNet. We improve the state-of-the-art on ActivityNet by 0.15% in AR@100, 0.12% in AUC, and 0.38% in Average-mAP, by only including our context-aware loss without changing the network architecture. We achieve a new state-of-the art on SoccerNet, surpassing by far the previous baseline (from 49.7% to 62.5% in Average-mAP) and spotting most actions within 10 seconds around their ground truth. Finally, we leverage the resulting segmentation results to identify unannotated actions such as goal opportunities and derive a highlights generator without specific supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Supplementary Material</head><p>A supplementary video summarizing our work is available on https://youtu.be/FAeWxs0d4_o.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Notations</head><p>Let us recall the following notations from the paper:</p><p>• C is the number of classes in the spotting task.</p><p>• N F is the number of frames in the chunk considered.</p><p>• N GT is the number of ground-truth actions in the chunk considered.</p><p>• N pred is the number of predictions output by the network for the spotting task.</p><p>• f is the number of features computed for each class, for each frame, before the segmentation module (see <ref type="figure">Figure 9</ref>).</p><p>• r is the temporal receptive field of the network (used in the temporal convolutions).</p><p>•Ŷ regroups the spotting predictions of the network, and has dimension N pred × (2 + C). The first column represents the confidence scores for the spots, the second contains the predicted locations, and the other are perclass classification scores.</p><p>• Y encodes the ground-truth action vectors of the chunk considered, and has dimension N GT × (2 + C).</p><p>• K c i (i = 1, 2, 3, 4) denotes the context slicing parameters of class c.</p><p>We also use the following notations for the layers of a convolutional neural network:</p><p>• FC(n) is a fully connected layer (e.g. in a multi-layer perceptron) between any vector to a vector of size n.</p><p>• ReLU is the rectified linear unit.</p><p>• Conv(n, p × q) is a convolutional layer with n kernels of dimensions p × q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Detailed Network Architecture for SoccerNet</head><p>The architecture of the network used in the paper for the action spotting task of SoccerNet <ref type="bibr" target="#b19">[21]</ref>, as depicted in <ref type="figure">Figure 9</ref>, is detailed hereafter.</p><p>1. Frame feature extractor and temporal CNN. Soc-cerNet <ref type="bibr" target="#b19">[21]</ref> provides three frame feature extractors with different backbone architectures (I3D, C3D, and ResNet). Each of them respectively extracts 1024, 4096, and 2048 features that are further reduced to 512 features with a Principal Component Analysis (PCA). We use the PCA-reduced features provided with the dataset as input of our temporal CNN.</p><p>The aim of the temporal CNN is to provide Cf features for each frame, while mixing temporal information across the frames. It transforms an input of shape N F × 512 into an output of shape N F × Cf .</p><p>First, each frame is input to a 2-layer MLP to reduce the dimensionality of the feature vectors of each frame. We design its architecture as: FC(128) -ReLU -FC(32) -ReLU. We thus obtain a set of N F × 32 features, which we note F MLP .</p><p>Then, F MLP is input to a spatio-temporal pyramid, i.e. it is input in parallel to each of the following layers of the pyramid: Finally, we feed these features to a Conv(Cf, 3 × 152) layer, which produces a set of N F × Cf features, noted F TCNN .</p><p>2. Segmentation module. This module produces a segmentation score per class for each frame. It transforms F TCNN into an output of dimension N F × C, through the following steps:</p><formula xml:id="formula_7">• Reshape F TCNN to have dimension N F × C × f .</formula><p>• Use a frame-wise Batch Normalization.</p><p>• Activate with a sigmoid so that each frame has, for each class, a feature vector v ∈ (0, 1) f .</p><p>• For each frame, for each class, compute the distance d between v and the center of the unit hypercube (0, 1) f , i.e. a vector composed of 0.5 for its f components.</p><formula xml:id="formula_8">Hence, d ∈ [0, √ f /2].</formula><p>• The segmentation score is obtained as 1 − 2d/ √ f , which belongs to [0, 1]. This way, scores close to 1 for a class (i.e. v close to the center of the cube) can be interpreted as indicating that the frame is likely to belong to that class.</p><p>The segmentation scores ζ seg output by the segmentation module thus has dimension N F ×C and is assessed through the segmentation loss L seg .</p><p>3. Spotting module. The spotting module takes as input F TCNN and ζ seg , and outputs the spotting predictionsŶ of the network. It is composed of the following layers: <ref type="figure">Figure 9</ref>. Pipeline for action spotting. We propose a network made of a frame feature extractor and a temporal CNN outputting C class feature vectors per frame, a segmentation module outputting per-class segmentation scores, and a spotting module extracting 2 + C values per spotting prediction (i.e. the confidence score s for the spotting, its location t and a per-class prediction).</p><p>• ReLU on F TCNN , then concatenate with ζ seg . This results in N F × (Cf + C) features.</p><p>• Temporal max-pooling 3 × 1 with a 2 × 1 stride.</p><p>• Conv(32, 3 × (Cf + C)) -ReLU</p><p>• Temporal max-pooling 3 × 1 with a 2 × 1 stride.</p><p>• Conv(16, 3 × 32) -ReLU</p><p>• Temporal max-pooling 3 × 1 with a 2 × 1 stride.</p><p>• Flatten the resulting features, which yields F spot .</p><p>• Feed F spot to a FC(2N pred ) layer, then reshape to N pred × 2 and use sigmoid activation. This produces the confidence scores and the predicted locations for the action spots.</p><p>• Feed F spot to a FC(CN pred ) layer, then reshape to N pred ×C and use softmax activation on each row. This produces the per-class predictions for the action spots.</p><p>• Concatenate the confidence scores, predicted locations, and per-class predictions to produce the spotting predictionsŶ of shape N pred × (2 + C).</p><p>Eventually,Ŷ is assessed through the action spotting loss L as .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Iterative One-to-One Matching</head><p>The iterative one-to-one matching between the predicted locationsŶ ·,2 and the ground-truth locations Y ·,2 described in the paper is illustrated in <ref type="figure">Figure 10</ref>. It is further detailed mathematically in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicted locations</head><p>Ground-truth locations</p><formula xml:id="formula_9">( ) ̂ . , 2 (</formula><p>) .</p><p>, 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicted locations</head><p>Ground-truth locations</p><formula xml:id="formula_10">( ) ̂ . , 2 (</formula><p>) .</p><p>, 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicted locations</head><p>Ground-truth locations  <ref type="figure">Figure 10</ref>. Iterative one-to-one matching. Example of the iterative one-to-one matching. At iteration 1, each ground-truth location is matched with its closest predicted location (green arrows), and vice-versa (brown arrows). Locations that match each other are permanently matched (gray arrows), and the process is repeated with the remaining locations at iteration 2. In this case, two iterations suffice to match all the ground-truth locations with a predicted location, as evidenced by the absence of available groundtruth location for iteration 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Details on the Time-Shift Encoding (TSE)</head><p>The time-shift encoding (TSE) described in the paper is further detailed below. We note s c (x) the TSE of frame x related to class c.</p><p>We denote s c p (resp. s c f ) the difference between the frame index of x and the frame index of its closest past (resp.</p><p>• if s c p ≥ K c 4 : keep s c f , because x is located far after the past action, which does not influence x anymore.</p><p>For completeness, let us recall the following details mentioned in the main paper. If x is both located far after the past action and far before the future action, selecting either of the two time-shifts has the same effect in our loss. Furthermore, for the frames located either before the first or after the last annotated action of class c, only one timeshift can be computed and is thus set as s c (x). Finally, if no action of class c is present in the video, then we set s c (x) = K c 1 for all the frames. This induces the same behavior in our loss as if they were all located far before their closest future action.</p><p>The TSE is used to shape our novel context-aware loss function for the temporal segmentation module. The cases described above ensure the temporal continuity of the loss, regardless of the proximity between two actions of the same class, excepted at frames annotated as ground-truth actions. This temporal continuity can be visualized in <ref type="figure">Figure 11</ref>, which shows a representation ofL(p, s) (analogous to <ref type="figure">Figure 1)</ref> when two actions are close to each other. It is further illustrated in the video clip 3dloss.mp4 provided with this document, where we gradually vary the location of the second action. For each location of the second action, the TSE of all the frames is re-computed, and so is the loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Extra Analyses</head><p>Per-class results. As for the class goal in <ref type="figure">Figure 6</ref> of the main paper, <ref type="figure">Figures 12 and 13</ref> display the number of TP, FP, FN and the precision, recall and F 1 metrics for the classes card and substitution as a function of the tolerance δ allowed for the localization of the spots. <ref type="figure">Figure 12</ref> shows that most cards can be efficiently spotted by our model within 15 seconds around the ground truth (δ = 30 seconds). We achieve a precision of 66% for that tolerance. The previous baseline plateaus within 20 seconds (δ = 40 seconds) and still has a lower performance. <ref type="figure">Figure 13</ref> shows that most substitutions can be efficiently spotted by our model within 15 seconds around the ground  <ref type="figure">Figure 12</ref>. Per-class results (cards). A prediction of class card is a true positive (TP) with tolerance δ when it is located at most δ/2 seconds from a ground-truth card. The baseline results are obtained from the best model of <ref type="bibr" target="#b19">[21]</ref>. Our model spots most cards within 15 seconds around the ground truth (δ = 30 seconds). truth (δ = 30 seconds). We achieve a precision of 73% for that tolerance. The previous baseline reaches a similar performance for that tolerance, and reaches 82% within 60 seconds (δ = 120 seconds) around the ground truth.</p><p>Except for the precision metric for the substitutions with tolerances larger than 20 seconds, our model outperforms the previous baseline of SoccerNet <ref type="bibr" target="#b19">[21]</ref>. As mentioned in the paper, for goals, many visual cues facilitate their spotting, e.g. multiple replays, particular camera views, or celebrations from the players and from the public. Cards and substitutions are more difficult to spot since the moment the referee shows a player a card and the moment a new player enters the field to replace another are rarely replayed (e.g. for cards, the foul is replayed, not the sanction). Also, the number of visual cues that allow their identification is reduced, as these actions generally do not lead to celebrations from the players or the public. Besides, cards and substitutions may not be broadcast in full screen, as they are sometimes merely shown from the main camera and are thus barely visible. Finally, substitutions occurring during the half-time are practically impossible to spot, as said in the main paper. Segmentation loss analysis. We provide a supplementary analysis on the λ seg parameter, which balances the segmentation loss and the action spotting loss in Equation 11 of the main paper. We fix different values of λ seg and train a network for each value. We show the segmentation scores on one game for the goal class in <ref type="figure" target="#fig_0">Figure 14</ref>. We also display the Average-mAP for the whole test set for the different values of λ seg .</p><p>It appears that extreme values of λ seg substantially influence both the action spotting performance and the segmentation curves, hence the automatic highlights genera-  <ref type="figure">Figure 13</ref>. Per-class results (substitutions). A prediction of class substitution is a true positive (TP) with tolerance δ when it is located at most δ/2 seconds from a ground-truth substitution. The baseline results are obtained from the best model of <ref type="bibr" target="#b19">[21]</ref>. Our model spots most substitutions within 15 seconds around the ground truth (δ = 30 seconds). tion. Small values (i.e. λ seg ≤ 0.1) produce a useless segmentation for spotting the interesting unannotated goal opportunities. This is because the loss does not provide a sufficiently strong feedback for the segmentation task as it does not penalize enough the segmentation scores. These values of λ seg also lead to a decrease in the Average-mAP for the action spotting task, as already observed in the ablation study presented in the main paper. Moreover, very large values (λ seg ≥ 100) penalize too much the unannotated goal opportunities, for which the network is then forced to output very small segmentation scores. Such actions are thus more difficult to retrieve for the production of highlights. These values of λ seg also lead to a large decrease in the Average-mAP for the action spotting task, as the feedback of the segmentation loss overshadows the feedback of the spotting loss. Finally, it seems that for λ seg ∈ [1, 10], the spotting performance is high while providing informative segmentation scores on goal opportunities. These values lead to the spotting of several goal opportunities, shown in <ref type="figure" target="#fig_0">Figure 14</ref>, which might be included in the highlights automatically generated for this match by the method described in the main paper. Comments on improvements on ActivityNet. In <ref type="table">Table 3</ref>, we report the averages over samples of 20 results for each metric, that we further analyze statistically below for the Av.-mAP. First, following D'Agostino's normality test, we can reasonably assume that the samples are normally distributed, since we obtain p-values &gt; 0.1 (0.28 for BMN and 0.24 for ours respectively). The standard deviations of the samples are 0.08% and 0.07%. Since the difference between the averages is 0.38%, the normal distributions overlap beyond two standard deviations from their centers, <ref type="figure" target="#fig_0">Figure 14</ref>. Influence of λ seg on the segmentation and spotting results of the second half of the famous "Remuntada" match, Barcelona -PSG, for the class goal, for different values of λ seg . The best Average-mAP for the spotting task is located around λ seg = 1.5, while the best value for spotting unannotated goal opportunities might be around λ seg = 10. For this value, several meaningful goal opportunities have a high segmentation score: (a) a shot on a goal post, (b) a free kick, (c) lots of dribbles in the rectangle, and (d) a headshot right above the goal. which shows that our improvements are beyond noise domain. Furthermore, Bartlett's test for equal variances gives a p-value of 0.62 (&gt; 0.1), which allows us to use Student's t-test to check whether the two samples can be assumed to have the same mean or not. We obtain a p-value of 2.3 × 10 −18 , which strongly indicates that our results are significantly different from those of BMN and hence confirm the significant improvement. For the AR@100 and AUC, similar analyses give final p-values of 7.4 × 10 −3 and  <ref type="figure" target="#fig_1">Figure 15</ref>. Extra action spotting and segmentation results.</p><p>These results are obtained on the second half of the match Barcelona -Espanyol in December 2016. Ground truth actions, temporal segmentation curves, and spotting results (green stars) are illustrated. Unannotated actions can be identified and included in the highlights using our segmentation. For example, a goal opportunity occurs around the 29 th minute. A false positive spot for a card is predicted by our network around the 9 th minute.</p><p>As it corresponds to a severe unsanctioned foul, it is fine for our automatic highlights generator to include it in the summary of the match.</p><p>9.8 × 10 −2 , which corroborates the statistical significance of our improvements. <ref type="figure" target="#fig_1">Figure 15</ref> shows additional action spotting and segmentation results. We can identify actions that are unannotated but display high segmentation scores such as goal opportunities and unsanctioned fouls. A goal opportunity around the 29 th minute can be identified through the segmentation results. Besides, a false positive spot (green star) for a card is predicted around the 9 th minute, further supported by a high segmentation score. A manual inspection reveals that a severe unsanctioned foul occurs at this moment. The automatic highlights generator presented in the main paper would include it in the summary of the match. Even though this foul does not lead to a card for the offender, the content of this sequence corresponds to an interesting action that would be tolerable in a highlights video. <ref type="figure">Figure 16</ref> shows a frame for which our network provides a high segmentation score and a false positive spot around the 26 th minute (i.e. 71 st minute of the match) for substitutions in <ref type="figure">Figure 7</ref> of the main paper. We can see that the LED panel used by the referee to announce substitutions is visible on the frame. This may indicate that the network learns, quite rightly, to associate this panel with substitutions. As a matter of fact, at this moment, even the commentator announces that a substitution is probably imminent. <ref type="figure">Figure 16</ref>. False positive spot of a substitution for the second half of the famous "Remuntada" match, Barcelona -PSG, in March 2017. The LED panel used to announce substitutions is visible on the left, which presumably explains why the network predicted the sequence around this frame as a substitution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6.">Extra Actions and Highlights Generation</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>Performance as function of game time. Average-mAP spotting performance over the game time with all ground-truth actions of the dataset binned in 5 minute intervals. It appears that actions around the half-time break are more challenging to spot. Number of actions for each bin. Our performance (62.5%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>Performance as function of action vicinity. Average-mAP spotting performance per bin of ground-truth actions grouped by distance (in seconds) from their closest ground-truth action. It appears that nearby actions are more challenging to spot. Number of actions for each bin. Our performance (62.5%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>Precision for goal opportunities, as a function of the threshold on the segmentation score to exceed for manually inspecting a sequence. For scores larger than η = 0.5, a precision of 0.8 is achieved, i.e. 80% of the sequences inspected were goal opportunities. Number of sequences inspected per threshold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>•</head><label></label><figDesc>Conv(8, r/7 × 32) -ReLU • Conv(16, r/3 × 32) -ReLU • Conv(32, r/2 × 32) -ReLU • Conv(64, r × 32) -ReLU producing 8 + 16 + 32 + 64 = 120 features for each frame, which are concatenated with F MLP to obtain a set of N F × 152 features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results on SoccerNet. Average-mAP (in %) on the test set of SoccerNet for the action spotting task. We establish a new state-of-the-art performance. training video. At each epoch, new batches are re-computed for each video for data augmentation purposes. Each raw video is time-shift encoded before training. Each new training chunk is encoded with the YOLO-like encoding.</figDesc><table><row><cell>Method</cell><cell cols="3">Frame features I3D C3D ResNet</cell></row><row><cell>SoccerNet baseline 5s [21]</cell><cell>-</cell><cell>-</cell><cell>34.5</cell></row><row><cell>SoccerNet baseline 60s [21]</cell><cell>-</cell><cell>-</cell><cell>40.6</cell></row><row><cell>SoccerNet baseline 20s [21]</cell><cell>-</cell><cell>-</cell><cell>49.7</cell></row><row><cell>Vats et al. [62]</cell><cell>-</cell><cell>-</cell><cell>57.5</cell></row><row><cell>Ours</cell><cell cols="2">53.6 57.7</cell><cell>62.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Algorithm 1: Iterative matching between ground-truth and predicted locations.</p><p>Data: Y,Ŷ ground-truth and predicted locations Result: Matching couples (y,ŷ) ∈ Y ×Ŷ Algorithm:</p><p>Save matching couple (yŷ,ŷ); Remove yŷ from Y andŷ fromŶ ; end end end • if s c p &lt; K c 3 : keep s c p , because x is located just after the past action, which still strongly influences x.</p><p>x is in the transition zone after the past action, whose influence weakens, thus the decision depends on how far away is the future action:</p><p>if s c f ≤ K c 1 : keep s c p , because x is located far before the future action, which does not yet influence x.</p><p>if s c f &gt; K c 1 : The future action may be close enough to influence x: * if</p><p>x is closer to the just after region of the past action than it is to the just before region of the future action, with respect to the size of the transition zones. * else: keep s c f , because the future action influences x more than the past action.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Code</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bmn</surname></persName>
		</author>
		<ptr target="https://github.com/JJBOY/BMN-Boundary-Matching-Network.Lastaccessed" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diagnosing error in temporal action detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Action Search: Spotting Targets in Videos and Its Application to Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Action classification in soccer videos with long short-term memory recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moez</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atilla</forename><surname>Baskurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks (ICANN)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Leveraging contextual cues for generating basketball highlights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Bettadapura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM international conference on Multimedia (ACM-MM)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-Person 3D Pose Estimation and Tracking in Sports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis</forename><surname>Bridgeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Volino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Yves</forename><surname>Guillemaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-End, Single-Stream Temporal Action Detection in Untrimmed Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SST: Single-Stream Temporal Action Proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Temporal hockey action recognition via pose and optical flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Neher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanav</forename><surname>Vats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Clausi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Zelek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ARTHuS: Adaptive Real-Time Human Segmentation in Sports Through Online Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Cioppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Deliege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Istasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><forename type="middle">De</forename><surname>Vleeschouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Van Droogenbroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Bottom-Up Approach Based on Semantics for the Interpretation of the Main Camera Stream in Soccer Games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Cioppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Deliege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Van Droogenbroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">HitNet: a neural network with capsules embedded in a Hit-or-Miss layer, extended with hybrid data augmentation and ghost capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Deliège</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Cioppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Van Droogenbroeck</surname></persName>
		</author>
		<idno>abs/1806.06519</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Market size of the European football market from</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deloitte</surname></persName>
		</author>
		<ptr target="https://www.statista.com/statistics/261223/european-soccer-market-total-revenue/.1" />
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
	<note>in billion euros</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic soccer video analysis and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Murat Tekalp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehrotra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="796" to="807" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust camera calibration for sport videos using court models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Farin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Krabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Effelsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Storage and Retrieval Methods and Applications for Multimedia 2004</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5307</biblScope>
			<biblScope unit="page" from="80" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What will happen next? Forecasting player moves in sports videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CTAP: Complementary Temporal Action Proposal Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TURN TAP: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SoccerNet: A Scalable Dataset for Action Spotting in Soccer Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohieddine</forename><surname>Amine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarek</forename><surname>Dghaily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2018-06-02" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sports field localization via deep structured models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namdar</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic analysis of soccer video using dynamic Bayesian network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Lin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang-Chia</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Yuan</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="749" to="760" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Istasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><forename type="middle">De</forename><surname>Vleeschouwer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Associative Embedding for Team Discrimination</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic Soccer Video Event Detection Based on a Deep Neural Network Combined CNN and RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Tools with Artificial Intelligence (ICTAI)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu-Gang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS Challenge: Action Recognition with a Large Number of Classes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Large-scale Video Classification with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Global sports market -total revenue from</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Kearney</surname></persName>
		</author>
		<idno>2019-10-30</idno>
		<ptr target="https://www.statista.com/statistics/370560/worldwide-sports-market-revenue/.1" />
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>in billion U.S. dollars</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Soccer Event Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Lazzerini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaetano</forename><surname>Calabrese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><surname>Serafini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing and Pattern Recognition (IPPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">BMN: Boundary-Matching Network for Temporal Action Proposal Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal Convolution Based Action Proposal: Submission to ActivityNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<idno>abs/1707.06750</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">BSN: Boundary Sensitive Network for Temporal Action Proposal Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-granularity Generator for Temporal Action Proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gaussian Temporal Awareness Networks for Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchen</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pan-tilt-zoom SLAM for Sports Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jikai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A survey on player tracking in soccer videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Manafifard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Ebadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid Abrishami</forename><surname>Moghaddam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="46" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">GolfDB: A Video Database for Golf Swing Sequencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Mcnally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanav</forename><surname>Vats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dulhanty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mcphee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Computer Vision in Sports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fine-Grained Activity Recognition in Baseball Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Detecting events and key actors in multi-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Football video segmentation based on video production strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Reede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Joemon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval (ECIR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Soccer on Your Tabletop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Rematas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dynamic Routing Between Capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30 (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017-12" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A Deep Architecture for Multimodal Summarization of Soccer Games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Sanabria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Precioso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Menguy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia (ACM-MM) Workshops</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Generation of Ball Possession Statistics in Soccer Using Minimum-Cost Flow Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amlan</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipti Prasad</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Automatic Cricket Highlight Generation Using Event-Driven and Excitement-Based Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushkar</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemant</forename><surname>Sadana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apaar</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Elmadjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramanian</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">What actions are needed for understanding human actions in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Computer vision artificial intelligence (AI) market revenues worldwide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Statista</surname></persName>
		</author>
		<ptr target="https://www.statista.com/statistics/641922/worldwide-artificial-intelligence-computer-vision-market-revenues/.1" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>by application (in million U.S. dollars</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Event detection and summarization in soccer videos using Bayesian network and copula</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Tavassolipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Karimian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohreh</forename><surname>Kasaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="291" to="304" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Technology</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Soccer: Who Has the Ball? Generating Visual Analytics and Player Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajkumar</forename><surname>Theagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Pala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bir</forename><surname>Bhanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Computer vision for sports: Current applications and research topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rikke</forename><surname>Gade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning Spatiotemporal Features with 3D Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Goal!! Event detection in sports video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Tsagkatakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Jaber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Tsakalides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="15" to="20" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Football Action Recognition Using Hierarchical LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takamasa</forename><surname>Tsunoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhiro</forename><surname>Komori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Matsugu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Flexible Automatic Football Filming and Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Turchini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Galteri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Ferracani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Becchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia (ACM-MM) Workshops</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Event detection in coarsely annotated sports videos via 1d temporal convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanav</forename><surname>Vats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnaz</forename><surname>Fani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Walters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Clausi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Zelek</surname></persName>
		</author>
		<ptr target="https://bit.ly/3b4TiTf.5" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Robust player detection and tracking in broadcast soccer video based on enhanced particle filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="94" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="375" to="389" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Fine-Grained Video Captioning for Sports Narrative</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanyu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling Task Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Refining Joint Locations for Human Pose Tracking in Sports Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Einfalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Lienhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7094" to="7103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
