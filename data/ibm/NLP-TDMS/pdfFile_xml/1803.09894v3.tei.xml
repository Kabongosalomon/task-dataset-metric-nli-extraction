<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Scale Structure-Aware Network for Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lipeng</forename><surname>Ke</surname></persName>
							<email>kelipeng15@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ching</forename><surname>Chang</surname></persName>
							<email>mchang2@albany.edu</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">University at Albany</orgName>
								<orgName type="institution" key="instit2">State University of New York</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
							<email>hgqi@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
							<email>slyu@albany.edu</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">University at Albany</orgName>
								<orgName type="institution" key="instit2">State University of New York</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Scale Structure-Aware Network for Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Human pose estimation · Conv-deconv network · Multi- scale supervision</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We develop a robust multi-scale structure-aware neural network for human pose estimation. This method improves the recent deep conv-deconv hourglass models with four key improvements: (1) multiscale supervision to strengthen contextual feature learning in matching body keypoints by combining feature heatmaps across scales, (2) multiscale regression network at the end to globally optimize the structural matching of the multi-scale features, (3) structure-aware loss used in the intermediate supervision and at the regression to improve the matching of keypoints and respective neighbors to infer a higher-order matching configurations, and (4) a keypoint masking training scheme that can effectively fine-tune our network to robustly localize occluded keypoints via adjacent matches. Our method can effectively improve state-of-theart pose estimation methods that suffer from difficulties in scale varieties, occlusions, and complex multi-person scenarios. This multi-scale supervision tightly integrates with the regression network to effectively (i) localize keypoints using the ensemble of multi-scale features, and (ii) infer global pose configuration by maximizing structural consistencies across multiple keypoints and scales. The keypoint masking training enhances these advantages to focus learning on hard occlusion samples. Our method achieves the leading position in the MPII challenge leaderboard among the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human pose estimation refers to the task of recognizing postures by localizing body keypoints (head, shoulders, elbows, wrists, knees, ankles, etc.) from images. We focus on the problem of single-person pose estimation from a single RGB image with the input of a rough bounding box of a person, while the pose and the activity of the person can be arbitrary. The task is challenging due to the large variability of human body appearances, lighting conditions, complex background and occlusions, body physique and posture structures of the activities performed by the subject. The inference is further sophisticated when the case extends to multi-person scenarios.</p><p>Human pose estimation has been studied extensively <ref type="bibr" target="#b15">[16]</ref>. Traditional methods rely on hand-craft features <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b4">5]</ref>. With the prosperity of Deep Neural Networks (DNN), Convolutional Neural Networks (CNN) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26]</ref>, in particular the hourglass models <ref type="bibr" target="#b17">[18]</ref> and their variants <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27]</ref> have demonstrated remarkable performance in human pose estimation. The repeated bottom-up and top-down processing within the hourglass modules can reliably extract posture features across scales and viewing variabilities, and thus effectively localize body keypoints for pose estimation.</p><p>Although great progress has been made, state-of-the-art DNN-based pose estimation methods still suffer from several problems ( <ref type="figure" target="#fig_0">Fig. 1):</ref> (1) Scale instability: Slight perturbation of the input bounding box from the person detector (such as the SSD <ref type="bibr" target="#b14">[15]</ref>) can cause abrupt changes in the pose estimation, due to the influence of such dominating scales. Such scale instability causes unreliable pose estimations, and even the latest hourglass methods ( <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27]</ref>) tend to overfit body keypoints in a particular scale (out of all scales in the deconv pyramid), which results in a domination of a single scale. Current practice to handle this scale instability (e.g. widely used in the MPII pose estimation challenge <ref type="bibr" target="#b0">[1]</ref>), is to repeatedly performing pose estimations in multiple trials of various scales, and output the result with the highest score. This clearly shows the lack of a consistent scale representation in limitations of the existing methods. This will be addressed in this work in § 3.1 and § 3.2.</p><p>(2) Insufficient structural priors: The second issue is how to effectively incorporate the structure of human body as priors in the deep network for pose estimation. Such priors can provide key information to solve challenges of pose estimation in real-world scenarios with complex multi-person activities and cluttered backgrounds, where body keypoint occlusions and matching ambiguities are the bottlenecks. In these challenge cases, accurate keypoint localization is not the only factor for successful pose estimation, as there will be questions on how best to associate the keypoints (invisible, or multiple visible ones among possibilities) to infer the global pose configuration. Known body structural priors can provide valuable cues to infer the locations of the hidden body parts from the visible ones. We propose to model the skeleton with an intermediate structural loss ( § 3.3) and through the use of a global regression network at the end ( § 3.2). We further develop a keypoint masking scheme to improve the training of our network on challenging cases of severely occluded keypoints ( § 3.4).</p><p>In this paper, we propose a holistic framework to effectively address the drawbacks in the existing state-of-art hourglass networks. Our method is based on two neural networks: the multi-scale supervision network (MSS-net) and the multi-scale regression network (MSR-net).</p><p>In MSS-net, a layer-wise loss term is added at each deconv layer to allow explicit supervision of scale-specific features in each layer of the network. This multi-scale supervision enables effective learning of multi-scale features that can better capture local contextual features of the body keypoints. In addition, coarse-to-fine deconvolution along the resolution pyramid also follows a paradigm similar to the attention mechanism to focus on and refine keypoint matches. The MSR-net takes output from multiple stacks of MSS-nets to perform a global keypoint regression by fusing multiple scales of keypoint heatmaps to determine the pose output.</p><p>In addition to the MSS-net and MSR-net which can jointly learn to match keypoints across multiple scales of features, we explicitly match connected keypoint pairs based on the connectivity and structure of human body parts. For example, the connectivity from the elbow to the lower-arm and to the wrist can be leveraged in the inference of an occluded wrist, when the elbow and lower-arm are visible. Hence, we add a structure-aware loss aims to improve the capacities of the current deep networks in modeling structural priors for pose estimation. This structure loss improves the estimations of occluded keypoints in complex or crowded scenarios. Lastly, our keypoint masking training scheme serves as an effective data augmentation approach to enhance the learning of the MSS-net and MSR-net together, to better recognize occluded poses from difficult training samples.</p><p>The main contributions of this paper can be summarized as follows:</p><p>-We introduce the multi-scale supervision network (MSS-net) together with the multi-scale regression network (MSR-net) to combine the rich multi-scale features to improve the robustness in keypoint localization by matching features across all scales. -Both the MSS-net and MSR-net are designed using a structure-aware loss to explicitly learn the human skeletal structures from multi-scale features that can serve a strong prior in recovering occlusions in complex scenes. -We propose a keypoint masking training scheme that can fine-tune our network pipeline by generating effective training samples to focus the training on difficult cases with keypoint occlusions and cluttered scenes. <ref type="figure" target="#fig_1">Fig. 2</ref> summarizes our multi-scale structure-aware network pipeline.</p><p>Experimental evaluations show that our method achieves state-of-the-art results on the MPII pose challenge benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Image-based human pose estimation has many applications, for a comprehensive survey, see <ref type="bibr" target="#b15">[16]</ref>. Early approaches such as the histogram of oriented gradients (HOG) and deformable parts model (DPM) rely on hand-craft features and graphical models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b4">5]</ref>. These methods suffer from the limited representation capabilities and are not extensible to complex scenarios. Pose estimation using deep neural networks (DNN) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26]</ref> has shown superior performance in recent years, due to the availability of larger training datasets and powerful GPUs. DeepPose developed by Toshev et al. <ref type="bibr" target="#b3">[4]</ref> was an early attempt to directly estimate the postural keypoint positions from the observed image. Tompson et al. <ref type="bibr" target="#b22">[23]</ref> adopted the heatmap representation of body keypoints to improve their localization during training. A Markov random field (MRF) inspired spatial model was used to estimate keypoint relationship. Chu et al. <ref type="bibr" target="#b10">[11]</ref> proposed a transform kernel method to learn the inter-relationships between highly correlated keypoints using a bi-directional tree.</p><p>Recently, Wei et al. <ref type="bibr" target="#b25">[26]</ref> used very deep sequential conv-deconv architecture with large receptive fields to directly perform pose matching on the heatmaps. They also enforced intermediate supervision between conv-deconv pairs to prevent gradient vanish, thus a very deep network became feasible, and the deeper network can learn the keypoints relationship with lager receptive field. The hourglass module proposed by Newell et al. <ref type="bibr" target="#b17">[18]</ref> is an extension of Wei et al. with the addition of residual connections between the conv-deconv sub-modules. The hourglass module can effectively capture and combine features across scales. Chu et al. <ref type="bibr" target="#b11">[12]</ref> adopted stacked hourglass networks to generate attention maps from features at multiple resolutions with various semantics. Yang et al. <ref type="bibr" target="#b26">[27]</ref> designed a pyramid residual module (PRM) to enhance the deep CNN invariance across scales, by learning the convolutional filters on various feature scales.</p><p>State-of-the-art DNNs for pose estimation are still limited in the capability of modeling human body structural for effective keypoint matching. Existing methods rely on a brute-force approach by increasing network depth to implicitly enrich the keypoint relationship modeling capability. A major weakness in this regard is the ambiguities arising from the occlusions, clutter backgrounds, or multiple body parts in the scene. In the MPII pose benchmark <ref type="bibr" target="#b0">[1]</ref>, many methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27]</ref> rely on repeating their pose estimation pipeline multiple times in various scales, in order to improve performance by a small margin using averaging of results. This indicates the lack of an effective solution to handle scale and structural priors in the modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our multi-scale structure-aware network consists of two types of subnetworks: the multi-scale supervision network (MSS-net), which can be repeated for multiple stack, and the multi-scale regression network (MSR-net) at the end, see <ref type="figure" target="#fig_1">Fig. 2</ref>. Specifically, MSS-net is based on the conv-deconv hourglass module <ref type="bibr" target="#b17">[18]</ref> trained with multi-scale loss supervision. The MSR-net performs a final pose structural regression by matching multi-scale keypoint heatmaps and their high-order associations. Both the MSS-net and the MSR-net share a common structure-aware loss function, which is designed to ensure effective multi-scale structural feature learning. The training of the whole pipeline is fine-tuned using the keypoint masking training scheme to focus on learning hard samples.</p><p>We describe two key observations that motivates the design of our method. First, the conv-deconv hourglass stacks capture rich features for keypoint detection across large variability in appearances and scales. However, such capability is very sensitive to a particular scale in the multi-scale pyramid, and lacks of a robust and consistent response across scales. This leads us to add explicit layerwise supervisions to each of the deconv layer in the training of our MSS-net.</p><p>Secondly, the output of the MSS-net hourglass model is a set of heatmaps, and each heatmap corresponds to the location likelihood of each body keypoint (elbows, wrists, ankles, knees, etc). To train the MSS-net, the heatmaps are supervised against the ground-truth body keypoint heatmaps that are typically generated using 2D Gaussian blurring. At the testing of the MSS-net for pose estimation, the obtained heatmaps are mostly non-Gaussian, which variate according to the gesture of the subject. A key deficiency in the original hourglass model <ref type="bibr" target="#b17">[18]</ref> is that each keypoint heatmap is estimated independently, such that the relationship between the keypoints are not considered. In other words, structural consistency among detected keypoints are not optimized.</p><p>To ensure structural consistency in the pose estimation pipeline, we introduce the structure-aware loss in between the MSS-net hourglass modules that serve as the purpose of intermediate supervision, to better capture the adjacency and associations among the body keypoints. The structure-aware loss is also used in the MSR-net at the end of the pipeline, to globally oversee all keypoint heatmaps across all scales. This way a globally consistent pose configuration can be inferred as the final output. The MSR-net regression not only matches individual body keypoints (first-order consistency), but also matches pairwise consistencies among adjacent keypoints (second-order consistency). To illustrate, the co-occurrence of a matching pair between a hand/leg w.r.t. the head/torso with high confidence should provide stronger hypothesis, in comparison to the separated, uncorrelated individual matches for the final pose inference. The MSR-net is trained to perform such optimization across all body keypoints, all scales of features, and all pairwise correlations in a joint regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-Scale Supervision Network</head><p>The multi-scale supervision network (MSS-net) is designed to learn deep features across multiple scales. We perform multiple layer-wise supervision at each of the deconv layers of the MSS-net, where each layer corresponds to a certain scale.</p><p>The gray box at the bottom of <ref type="figure" target="#fig_1">Fig. 2</ref> depicts the MSS-net architecture.</p><p>Multi-scale supervision is performed by calculating the residual at each deconv layer using the corresponding down-sampled ground-truth heatmaps in the matching scale (e.g., 1/2, 1/4, 1/8 down-sampling). Specifically, to make equal the feature map dimensions in order to compute the residual at the corresponding scale, we use an 1-by-1 convolutional kernel for dimension reduction, to convert the high-dimensional deconv feature maps into the desired number of features, where the number of reduced dimension matches the number of body keypoints (which is also the number of heatmaps). On the other hand, the ground-truth keypoint feature map is down-sampled to match the corresponding extracted keypoint heatmap at each scale to compute the residual.</p><p>The multi-scale supervision network localizes body keypoints in a way similar to the 'attention model' <ref type="bibr" target="#b27">[28]</ref> used in the conventional resolution pyramid for image search. The activation areas in the low-res heatmap can provide guidance of the location refinement in the subsequent high-res layers, see <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>We describe the loss function L M S to train the multi-scale supervision network. The loss L M S is defined by summing the L 2 loss from the heatmaps of all keypoints across all scales, similar to the multi-scale loss function in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b17">18]</ref>. To detect the N = 16 keypoints (head, neck, pelvis, thorax, shoulders, elbows, wrists, knees, ankles, and hips), N heatmaps are generated after each convdeconv stack. The loss at the i-th scale compares the predicted heatmaps of all keypoints against the ground-truth heatmaps at the matching scale:</p><formula xml:id="formula_0">L i M S = 1 N N n=1 x,y ||P n (x, y) − G n (x, y)|| 2 ,<label>(1)</label></formula><p>where P n (x, y) and G n (x, y) represent the predicted and the ground-truth confidence maps at the pixel location (x, y) for the n-th keypoint, respectively. In standard dataset the ground-truth poses are provided as the keypoint locations. We follow the common practice for ground-truth heatmap generation as in Tompson et al. <ref type="bibr" target="#b23">[24]</ref>, where the n-th keypoint ground-truth heatmap G n (x, y) is generated using a 2D Gaussian centered at the keypoint location (x, y), with standard deviation of 1 pixel. <ref type="figure" target="#fig_1">Fig. 2 (bottom left, first row)</ref> shows a few examples of the ground-truth heatmaps for a certain keypoints. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Scale Regression Network</head><p>We use a fully convolutional multi-scale regression network (MSR-net) after the MSS-net conv-deconv stacks to globally refine the multi-scale keypoint heatmaps to improve the structural consistency of the estimated poses. The intuition is that the relative positions of arms and legs w.r.t. the head/torso provide useful action priors, which can be learned from the regression network by considering feature maps across all scales for pose refinement. The MSR-net takes the multi-scale heatmaps as input, and match them to the ground-truth keypoints at respective scales. This way the regression network can effectively combine heatmaps across all scales to refine the estimated poses.</p><p>The multi-scale regression network jointly optimizes the global body structure configuration via determining connectivity among body keypoints based on the mutli-scale features. This can be viewed as an extension to the work of the Convolutional Part Heatmap Regression <ref type="bibr" target="#b3">[4]</ref>, which only considers keypoint heatmap regression at the scale of the input image. The input image with the keypoint heatmaps can be seen as an attention method and provide larger resolution. In this case, the multi-scale regression network learns a scale-invariant and attention based structure model, thus provide better performance. Moreover, our multi-scale regression network optimizes the structure-aware loss, which matches individual keypoints as well as the higher-order association (pairs and triplets of keypoints) in estimating pose. The output from the multi-scale regression network is a comprehensive pose estimation that considers pose configurations across multiple feature scales, multiple keypoint associations, and high-order keypoint associations. <ref type="figure" target="#fig_3">Fig. 4</ref> shows the efficacy of the multi-scale, high-order keypoint regression performed in the MSR-net. The MSR-net works hand-in-hand with the MSSnet to explicitly model the high-order relationship among body parts, such that posture structural consistency can be maintained and refined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Structure-Aware Loss</head><p>It has been observed that deeper hourglass stacks lead to better pose estimation results <ref type="bibr" target="#b17">[18]</ref>. As the depth of hourglass stacks increases, gradient vanishing becomes a critical issue in training the network, where intermediate supervision <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27</ref>] is a common practice to alleviate gradient vanishing.</p><p>To this end, we design a structure-aware loss function following a graph to model the human skeletal structure. Specifically we introduced a human skeletal graph S (See <ref type="figure" target="#fig_2">Fig. 3(c)</ref> for a visualization of the human skeletal graph.) to define the structure-aware loss. Each node S n ∈ S represent a body keypoint of the human skeleton and its connected keypoints, n ∈ {1, ..., N }. The structure-aware loss at the i-th scale is formally defined as:</p><formula xml:id="formula_1">L i SA = 1 N N n=1 ||P i n − G i n || 2 + α N i=1 ||P i Sn − G i Sn || 2 .<label>(2)</label></formula><p>The first term is the multi-scale supervision loss L i M S in Eq.1 that represents individual keypoint matching loss. The second term represents the structural matching loss, where P Sn and G Sn are the combination of the heatmaps from individual keypoint n and its neighbors in graph S. Hyperparameter α is a weighing parameter balancing the two terms. <ref type="figure" target="#fig_1">Fig. 2 (bottom left)</ref> shows a breakdown visualization of how our skeletonguided structure-aware loss is calculated in traversing the keypoints and their relationships according to S. The top row in the sub-figure shows the intermediate loss defined on individual keypoints (e.g., the right ankle, knee, hip, pelvis, thorax, head, wrist, elbow) as used in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b17">18]</ref>. The bottom row shows our structure-aware loss defined for a set of connected keypoints.</p><p>We consider connected keypoints, e.g., head-thorax, shoulder-elbow, wristelbow, hip-knee, hip-hip, knee-ankle, in the bottom sub-figure of <ref type="figure" target="#fig_1">Fig. 2</ref>. Because that the elbows and knees has additional physical connections (to the shoulders and wrists, and the hips and ankles, respectively), the structure-aware loss in these two joints are three-way to include a triplet of connected keypoints, e.g., hip-knee-ankle, shoulder-elbow-wrist as in <ref type="figure" target="#fig_1">Fig. 2</ref>. In all cases, the list of structurally connected keypoints is empirically determined according to the human skeletal graph S, such that the loss can better capture the physical connectivity of the keypoints in the human body to obtain structural priors.</p><p>The structure-aware loss is used at two places in our network: <ref type="bibr" target="#b0">(1)</ref> in-between the MSS-net stacks as a means of intermediate supervision to enforce structural consistency while localizing keypoints; and (2) in the MSR-net to find a globally consistent pose configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Keypoint Masking Training</head><p>In the case of multi-person scenarios, more than one possible body keypoints can co-exist in the view. In occluded case, no keypoint can be visible observed.</p><p>To tackle these challenging scenarios, we develop a novel keypoint masking data augmentation scheme to increase training data to fine-tune our networks.</p><p>Specifically, occlusion of key points is an aspect that strongly affects the performance of pose estimation methods. As shown in <ref type="figure" target="#fig_4">Fig. 5 (a)</ref>, the left wrist of the person is occluded by the mug, however the occluded wrist indeed can be estimated by visible connected keypoint(left elbow) o r the libs connecting wrist and elbow. Another difficult case is where there is another person nearby, e.g. in <ref type="figure" target="#fig_4">Fig. 5 (c)</ref> that several people standing closely. In this case, the pose estimator may easily take the nearby person's keypoint as its own keypoint. One drawback of training the network using the original training set is that there usually exists insufficient amount of examples that contains the occlusion cases to train a deep network for accurate keypoint detection/localization. Conventional data augmentation method, such as the popular horizontally flipping, random crops and color jittering in classification, are not helpful in this case.</p><p>We propose a keypoint masking method to address this problem by copying and pasting body keypoint patches on the image for data augmentation. The main idea is to generate keypoint occluded training samples as well as the artificially inserted keypoints, such that the network can effectively improve its learning on these extreme cases. This data augmentation is easily doable from the known ground-truth keypoint annotations.</p><p>Specifically, we introduce two types of keypoint/occlusion sample generation methods: (1) As shown in <ref type="figure" target="#fig_4">Fig. 5 (b)</ref>, we copy a background patch and put it onto a keypoint to cover it, in order to simulate a keypoint occlusion. This kind of sample is useful for the learning of occlusion recovery. (2) As shown in <ref type="figure" target="#fig_4">Fig. 5  (d)</ref>, we copy a body keypoint patches and put it onto a nearby background, in order to simulate the multiple existing keypoints, the case that mostly occurs in multi-person scenarios. Since this data augmentation results in multiple identical keypoint patches, the solution to a successful pose estimation must rely on some sort of structural inference or knowledge. It is thus especially beneficial to finetune to our global keypoint regression network.</p><p>Overall this keypoint masking strategy can effectively improve the focus of learning on challenge cases, where important body keypoints are purposely masked out or artificially placed at wrong locations. The effect of keypoint masking training in improving both (1) the detection and localization of occluded keypoints and (2) global structure recognition will be evaluated in §4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Analysis</head><p>We train and test our model on a workstation with 4 NVIDIA GTX 1080Ti GPUs and two public datasets -the MPII Human Pose Dataset and Challenge <ref type="bibr" target="#b0">[1]</ref> and FLIC dataset <ref type="bibr" target="#b20">[21]</ref>. The MPII dataset consists of images taken from a wide range of real-world activities with full-body pose annotations. It is considered as the "de facto" benchmark for state-of-the-art pose estimation evaluation. The MPII dataset includes around 25K images containing over 40K subjects with annotated body joints, where 28K subjects are used for training, and the remaining 12k are used for testing. The FLIC dataset consists of 5, 003 selected images obtained from Hollywood movies. The images are annotated on the upper body, where the subjects are mostly facing the camera, thus there exists less keypoint occlusions.</p><p>Since the testing annotations for MPII are not available to the public, in our experiments, we perform training on a subset of the original training set, and perform hyper-parameter selection on a separated validation set, which contains around 3K subjects (that are in the original training set). We also report evaluation results that are reported from the MPII benchmark 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation</head><p>Training is conducted on the respective datasets (MPII, FLIC) using the SGD optimizer for 300 epochs. In this work, we use 8 stacks of hourglass modules for both training and testing. The training processes can be decided into three stages: (1) MSS-Net training, (2) MSR-Net training, and (3) the joint training of the MSS-Net and MSR-Net with keypoint masking. We use the same data augmentation technique as in the original hourglass work <ref type="bibr" target="#b17">[18]</ref> that includes rotation (+/− 30 degrees), and scaling (.75 to 1.25) throughout the training process. Due to GPU memory limitation, the input images were cropped and rescaled to 256x256 pixels. For the first stage we train the MSS-Net for 150 epochs, with the initial learning rate to be 5e-4. The learn rate is reduced by a factor of 5 when the performance dose not improve after 8 epochs. We then train the MSR-Net for 75 epochs with the MSS-Net parameters fixed. Finally the whole network pipeline is trained for 75 epoch with keypoint masking fine-tuning.</p><p>Testing is performed on both the MPII and FLIC datasets. Since this work focuses on single-person pose estimation, and there often exists multiple subjects in the scene. We use a conditional testing method -We first test pose estimation in the original scale assuming the subject appears at the image center. We then check if the detected body keypoint confidence is lower then a specific threshold. If so, no successful human pose is found. We then perturb the putative person location, and repeat the pose finding, to see if a refined pose can be found. The keypoint confidence thresholds τ c can be keypoint-dependent, and are empirically determined using the validation set. For the case multiple pose estimation test trials are performed, only the results with scores higher than a threshold τ s are selected for the fusion of the pose output. The value of τ s is also empirically determined from the validation set. We note that this testing refinement may reduce the testing performance of pose estimation, because the variation of the input (person bounding box) is also considered in the process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Results</head><p>Evaluation is conducted using the standard Percentage of Correct Keypoints (PCK) metric <ref type="bibr" target="#b21">[22]</ref> which reports the percentage of keypoint detection falling within a normalized distance of the ground truth. For the FLIC evaluation, PCK is set to the percentage of disparities between the detected pose keypoints w.r.t. the ground-truth after a normalization against a fraction of the torso size. For the MPII evaluation, such disparities are normalized by a fraction of the head size, which is denoted as PCK h .</p><p>FLIC: <ref type="table">Table 1</ref> summarizes the FLIC results, where our PCK reaches 99.2% for the elbow, and 97.3% for the wrist. Note that the elbows and wrists are the most difficult parts to localize in the FLIC dataset. Comparison with Newell et al. <ref type="bibr" target="#b17">[18]</ref> demonstrates the improvement of our structure-aware design in the MSS-net and MSR-net in our method. <ref type="table">Table 2</ref> summarizes the MPII evaluation results. Observe that our method achieves the highest total score (92.1) and state-of-the-art results across all keypoints on the MPII benchmark as well as the AUC score. In <ref type="figure" target="#fig_5">Fig. 6</ref> we show several pose estimation results on the MPII dataset. In <ref type="figure">Fig. 7</ref> we show some highly challenging examples with crowded scenes and severe occlusions. In this case, we run our pose estimation on the bounding box of each person, which is provided in the MPII dataset. Our method can extract complex poses for each targeted person, without confusing with other person's poses and in the presence of occlusions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPII:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Component Analysis</head><p>We performed a series of ablation experiments to investigate the effect of individual components in our method. The ablation study is conducted on the validation set <ref type="bibr" target="#b23">[24]</ref> of the MPII dataset. Note that our method can be reduced to the original hourglass model of Newell et al. <ref type="bibr" target="#b17">[18]</ref> after all newly proposed features are taken out. Thus, we analysis each proposed network design, i.e., the MSS-net, MSR-net, structure-aware loss, and keypoint masking, by comparing against Newell et al. with a baseline score of 87.1% at PCK h = 0.5.</p><p>Multi-scale supervision (MSS-net without structure-aware loss): We first evaluate the effect of the multi-scale supervision along. By adding the multiscale supervision at the deconv layers of hourglass model <ref type="bibr" target="#b17">[18]</ref>, the PCK h score improve from 87.1% to 87.6% and also with a significant computation reduction. <ref type="figure">Fig. 7</ref>. Pose estimation results with our method on two very challenging images from the MPII dataset with crowded scene and severe occlusions. Our method can reliably recover complex poses for each targeted person. This is because the original hourglass method <ref type="bibr" target="#b17">[18]</ref> is tested with input images of multiple scales (6 scales in our experiment), while the evaluation of our multiscale supervision network only need to be tested once in the original scale input. Our method does not require repeated runs and fusion of different scales as post-processing.</p><p>Multi-scale regression (MSS-net and MSR-net without structure-aware loss): To justify the contribution of multi-scale regression, we evaluate the effect of the second stage of our training pipeline (i.e. the MSR-net after the MSS-net is trained, without keypoint masking fine-tuning). The PCK h score here is 88.1% score, which is 0.4% improvement brought by the multi-scale regression.</p><p>Structure-aware loss (MSS-net and MSR-net with structure-aware loss): The next in our ablation pipeline is to use structure-aware loss in the training of MSS-net and MSR-net, in comparison to the original loss defined in Eq.1. The PCK h score we obtained here is 88.3%, which is a 0.3% improvement brought by the use of structure-aware loss for training.</p><p>Keypoint masking: After 75 epochs keypoint masking fine-tuning in the MSS-net and MSR-net pipeline with structure-aware loss, we achieve a 88.4% PCK h score. The keypoint masking contributes 0.1% PCK h improvement in this ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We describe an improved multi-scale structure-aware network for human pose estimation. The proposed multi-scale approach (multi-scale supervision and multiscale regression) works hand-in-hand with the structure-aware loss design, to infer high-order structural matching of detected body keypoints, that can improve pose estimation in challenging cases of complex activities, heavy occlusions, multiple subjects and cluttered backgrounds. The proposed keypoint masking training can focus the learning of the network on difficult samples. Our method achieve the leading position in the MPII challenge leaderboard among the stateof-the-art methods. Ablation study shows the contribution and advantage of each proposed components.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>State-of-the-art pose estimation networks face difficulties in diverse activities and complex scenes, which can be organized into three challenges: (top row) large scale varieties of body keypoints in the scenes, (middle row) occluded body parts or keypoints, (bottom row) ambiguities in matching multiple adjacent keypoints in crowded scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The proposed network consists of three components: (i) multi-scale supervision network (MSS-net, § 3.1), (ii) multi-scare regression network (MSR-net, § 3.2), and (iii) intermediate supervision using the structure-aware loss ( § 3.3). The whole network pipeline is fine-tuned using the keypoint masking training scheme ( § 3.4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>In the multi-scale supervision network, the refinement of keypoint localization in up-sampling resolution works in analogy to the 'attention' mechanism used in the conventional resolution pyramid search. (a) shows the multi-scale heatmaps of the keypoint of the thorax. (b) shows the refinement of the keypoint heatmaps during the deconv upsampling, where the location of the thorax is refined with increased accuracy. (c) shows our human skeletal graph with the visualization of keypoint connectivity links.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Muti-scale keypoint regression to disambiguate multiple peaks in the keypoint heatmaps. (a-b) shows an example of (a) keypoint prediction and (b) heatmap from the MSS-net hourglass stacks, which will be fed into the MSR-net for regression. (c-d) shows (c) the output keypoint locations and (d) heatmap after regression. Observe that the heatmap peaks in (d) are more focused compared to (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Keypoint masking to simulate the hard training samples. (a) is a common case in human pose estimation, the keypoint (left-wrist) is occluded by an object, but it can be estimated from the limbs. (c) is another difficult case, where the nearby persons' keypoint can be mismatched to the target person. Thus there are two kind of keypoint masking, (b) is the background keypoint masking which crop a background patch and paste on a keypoint to simulate the keypoint invisible, (d) is the keypoint duplicate masking which crop a keypoint patch and paste on another keypoint to simulate multiperson or multi-peak keypoint heatmap.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Example of pose estimation results on the MPII dataset using our method. (row 1) Examples with significant scale variations for keypoints. (row 2,3) Examples with multiple persons. (row 4,5) Examples with severe keypoint occlusions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Results on the FLIC dataset (PCK = 0.2) Evaluation results on the MPII pose dataset (PCK h = 0.5). Results were retrieved on 03/15/2018.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Elbow Wrist</cell></row><row><cell cols="5">Tompson et al. CVPR'15 [23] 93.1 92.4</cell></row><row><cell cols="4">Wei et al. CVPR'16 [26]</cell><cell>97.8 95.0</cell></row><row><cell cols="4">Newellet al. ECCV'16 [18]</cell><cell>99.0 97.0</cell></row><row><cell cols="2">Our model</cell><cell></cell><cell></cell><cell>99.2 97.3</cell></row><row><cell></cell><cell cols="4">Head Shoulder Elbow Wrist Hip Knee Ankle Total AUC</cell></row><row><cell>Our method</cell><cell cols="2">98.5 96.8</cell><cell cols="2">92.7 88.4 90.6 89.3 86.3 92.1 63.8</cell></row><row><cell>Chen et al. ICCV'17 [7]</cell><cell>98.1</cell><cell>96.5</cell><cell cols="2">92.5 88.5 90.2 89.6 86.0 91.9 61.6</cell></row><row><cell>Chou et al. arXiv'17 [9]</cell><cell cols="2">98.2 96.8</cell><cell cols="2">92.2 88.0 91.3 89.1 84.9 91.8 63.9</cell></row><row><cell>Chu CVPR'17 [12]</cell><cell cols="2">98.5 96.3</cell><cell cols="2">91.9 88.1 90.6 88.0 85.0 91.5 63.8</cell></row><row><cell cols="2">Luvizon et al. arXiv'17 [17] 98.1</cell><cell>96.6</cell><cell cols="2">92.0 87.5 90.6 88.0 82.7 91.2 63.9</cell></row><row><cell>Ning et al. TMM'17 [19]</cell><cell>98.1</cell><cell>96.3</cell><cell cols="2">92.2 87.8 90.6 87.6 82.7 91.2 63.6</cell></row><row><cell>Newell ECCV'16 [18]</cell><cell>98.2</cell><cell>96.3</cell><cell cols="2">91.2 87.1 90.1 87.4 83.6 90.9 62.9</cell></row><row><cell>Bulat ECCV'16 [4]</cell><cell>97.9</cell><cell>95.1</cell><cell cols="2">89.9 85.3 89.4 85.7 81.7 89.7 59.6</cell></row><row><cell>Wei CVPR'16 [26]</cell><cell>97.8</cell><cell>95.0</cell><cell cols="2">88.7 84.0 88.4 82.8 79.4 88.5 61.4</cell></row><row><cell cols="2">Insafutdinov ECCV'16 [13] 96.8</cell><cell>95.2</cell><cell cols="2">89.3 84.4 88.4 83.4 78.0 88.5 60.8</cell></row><row><cell>Belagiannis FG'17 [2]</cell><cell>97.7</cell><cell>95.0</cell><cell cols="2">88.2 83.0 87.9 82.6 78.4 88.1 58.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Recurrent human pose estimation. FG pp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="468" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Poselets: Body part detectors trained using 3D human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1365" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="717" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fast online upper body pose estimation from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<idno>BMVC. pp. 104.1-104.12. Swansea</idno>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Domain adaptation for upper body pose tracking in signed TV broadcasts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Magee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hogg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial posenet: A structureaware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1221" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mixing body-part sequences for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2361" to="2368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Self adversarial training for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1707.02439</idno>
		<ptr target="http://arxiv.org/abs/1707.02439" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4715" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4715" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5669" to="5678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">DeeperCut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="34" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A survey of human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="10" to="19" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Human pose regression by combining indirect part detection and contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<idno>abs/1710.02322</idno>
		<ptr target="http://arxiv.org/abs/1710.02322" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Dual path networks for multi-person human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<idno>abs/1710.10192</idno>
		<ptr target="http://arxiv.org/abs/1710.10192" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1913" to="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Multimodal decomposable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3674" to="3681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 27</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<title level="m">Convolutional pose machines. CVPR pp</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1290" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Diversified visual attention networks for fine-grained object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1245" to="1256" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
