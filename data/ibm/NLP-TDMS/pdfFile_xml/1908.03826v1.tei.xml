<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeblurGAN-v2: Deblurring (Orders-of-Magnitude) Faster and Better</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
							<email>kupyn@ucu.edu.ua</email>
							<affiliation key="aff0">
								<orgName type="institution">Ukrainian Catholic University</orgName>
								<address>
									<settlement>Lviv</settlement>
									<country key="UA">Ukraine</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">SoftServe</orgName>
								<address>
									<settlement>Lviv</settlement>
									<country key="UA">Ukraine</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetiana</forename><surname>Martyniuk</surname></persName>
							<email>t.martynyuk@ucu.edu.ua</email>
							<affiliation key="aff0">
								<orgName type="institution">Ukrainian Catholic University</orgName>
								<address>
									<settlement>Lviv</settlement>
									<country key="UA">Ukraine</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DeblurGAN-v2: Deblurring (Orders-of-Magnitude) Faster and Better</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new end-to-end generative adversarial network (GAN) for single image motion deblurring, named DeblurGAN-v2, which considerably boosts state-of-the-art deblurring efficiency, quality, and flexibility. DeblurGAN-v2 is based on a relativistic conditional GAN with a doublescale discriminator. For the first time, we introduce the Feature Pyramid Network into deblurring, as a core building block in the generator of DeblurGAN-v2. It can flexibly work with a wide range of backbones, to navigate the balance between performance and efficiency. The plugin of sophisticated backbones (e.g., Inception-ResNet-v2) can lead to solid state-of-the-art deblurring. Meanwhile, with light-weight backbones (e.g., MobileNet and its variants), DeblurGAN-v2 reaches 10-100 times faster than the nearest competitors, while maintaining close to state-ofthe-art results, implying the option of real-time video deblurring. We demonstrate that DeblurGAN-v2 obtains very competitive performance on several popular benchmarks, in terms of deblurring quality (both objective and subjective), as well as efficiency. Besides, we show the architecture to be effective for general image restoration tasks too. Our codes, models and data are available at: https: //github.com/KupynOrest/DeblurGANv2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper focuses on the challenging setting of singleimage blind motion deblurring. Motions blurs are commonly found from photos taken by hand-held cameras, or low-frame-rate videos containing moving objects. Blurs degrade the human perceptual quality, and challenge subsequent computer vision analytics. The real-world blurs typically have unknown and spatially varying blur kernels, and are further complicated by noise and other artifacts.</p><p>The recent prosperity of deep learning has led to significant progress in the image restoration field <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b27">28]</ref>. Specifically, generative adversarial networks (GANs) <ref type="bibr" target="#b8">[9]</ref>  DeblurGAN-V2 (MobileNet) DeblurGAN-V2 (MobileNetDSC) <ref type="figure">Figure 1</ref>: The SSIM-FLOPs trade-off plot on the GoPRO dataset. Compared to three state-of-the-art competitors (in blue): DeblurGAN <ref type="bibr" target="#b20">[21]</ref>, DeepDeblur <ref type="bibr" target="#b32">[33]</ref> and Scale-Recurrent Network (SRN) <ref type="bibr" target="#b44">[45]</ref>, DeblurGAN-v2 models (with different backbones, in red) are shown to achieve superior or comparable quality, and are much more efficient.</p><p>often yield sharper and more plausible textures than classical feed-forward encoders and witness success in image super-resolution <ref type="bibr" target="#b22">[23]</ref> and in-painting <ref type="bibr" target="#b52">[53]</ref>. Recently, <ref type="bibr" target="#b20">[21]</ref> introduced GAN to deblurring by treating it as a special image-to-image translation task <ref type="bibr" target="#b12">[13]</ref>. The proposed model, called DeblurGAN, was demonstrated to restore perceptually pleasing and sharp images, from both synthetic and real-world blurry images. DeblurGAN was also 5 times faster than its closest competitor as of then <ref type="bibr" target="#b32">[33]</ref>. Built on the success of DeblurGAN, this paper aims to make another substantial push on GAN-based motion deblurring. We introduce a new framework to improve over DeblurGAN, called DeblurGAN-v2 in terms of both deblurring performance and inference efficiency, as well as to enable high flexibility over the quality-efficiency spectrum. Our innovations are summarized as below 1 :</p><p>• Framework Level: We construct a new conditional GAN framework for deblurring. For the generator, we introduce the Feature Pyramid Network (FPN), which was originally developed for object detection <ref type="bibr" target="#b26">[27]</ref>, to the image restoration task for the first time. For the discriminator, we adopt a relativistic discriminator <ref type="bibr" target="#b15">[16]</ref> with a least-square loss wrapped <ref type="bibr" target="#b29">[30]</ref> inside, and with two columns that evaluate both global (image) and local (patch) scales respectively.</p><p>• Backbone Level: While the above framework is agnostic to the generator backbones, the choice would affect deblurring quality and efficiency. To pursue the state-of-the-art deblurring quality, we plug in a sophisticated Inception-ResNet-v2 backbone. To shift towards being more efficient, we adopt MobileNet, and further create its variant with depth-wise separable convolutions (MobileNet-DSC). The latter two become extremely compact in size and fast at inference.</p><p>• Experiment Level: We present very extensive experiments on three popular benchmarks to show the state-of-the-art (or close) performance (PSNR, SSIM, and perceptual quality) achieved by DeblurGAN-v2. In terms of the efficiency, DeblurGAN-v2 with MobileNet-DSC is 11 times faster than DeblurGAN <ref type="bibr" target="#b20">[21]</ref>, over 100 times faster than <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b44">45]</ref>, and has a model size of just 4 MB, implying the possibility of real-time video deblurring. We also present a subjective study of the deblurring quality on real blurry images. Lastly, we show the potential of our models in general image restoration, as extra flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work 2.1. Image Deblurring</head><p>Single image motion deblurring is traditionally treated as a deconvolution problem, and can be tackled in either a blind or a non-blind manner. The former assumes a given or pre-estimated blur kernel <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b51">52]</ref>. The latter is more realistic yet highly ill-posed. Earlier models rely on natural image priors to regularize deblurring <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b4">5]</ref>. However, most handcrafted priors cannot well capture the complicated blur variations in real images.</p><p>Emerging deep learning techniques have boosted the breakthrough in image restoration tasks. Sun et al. <ref type="bibr" target="#b42">[43]</ref> exploited a convolutional neural network (CNN) for blur kernel estimation. Gong et al. <ref type="bibr" target="#b7">[8]</ref> used a fully convolutional network to estimate the motion flow. Besides those kernelbased methods, end-to-end kernel-free CNN methods were as: "We present some updates to YOLO. We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell." -that well describes what we have done to DeblurGAN, too; although we consider DeblurGAN-v2 a non-incremental upgrade of DeblurGAN, with significant performance &amp; efficiency improvements. explored to restore a clean image from the blurry input directly, e.g., <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35]</ref>. The latest work by Tao et al. <ref type="bibr" target="#b44">[45]</ref> extended the Multi-Scale CNN from <ref type="bibr" target="#b32">[33]</ref> to a Scale-Recurrent CNN for blind image deblurring, with impressive results.</p><p>The success of GANs for image restoration has impacted single image deblurring as well since Ramakrishnan et al. <ref type="bibr" target="#b36">[37]</ref> first solved image deblurring by referring to the image translation idea <ref type="bibr" target="#b12">[13]</ref>. Lately, Kupyn et al. <ref type="bibr" target="#b20">[21]</ref> introduced DeblurGAN that exploited Wasserstein GAN <ref type="bibr" target="#b1">[2]</ref> with the gradient penalty <ref type="bibr" target="#b9">[10]</ref> and the perceptual loss <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Generative adversarial networks</head><p>A GAN <ref type="bibr" target="#b8">[9]</ref> consists of two models: a discriminator D and a generator G, that form a two-player minimax game. The generator learns to produce artificial samples and is trained to fool the discriminator, in a goal to capture the real data distribution. In particular, as a popular GAN variant, conditional GANs <ref type="bibr" target="#b30">[31]</ref> have been widely applied to imageto-image translation problems, with image restoration and enhancement as special cases. They take the label or an observed image in addition to the latent code as inputs.</p><p>The minimax game with the value function V (D, G) is formulated as the following <ref type="bibr" target="#b8">[9]</ref> (fake-real labels set to 0−1):</p><formula xml:id="formula_0">min G max D V (D, G) = E x∼p data (x) log D(x) + E z∼pz(z) log(1 − D(G(z)))</formula><p>Such an objective function is notoriously hard to optimize, and one needs to deal with many challenges, e.g., mode collapse and gradient vanishing/explosion, during the training process. To fix the vanishing gradients and stabilize the training, Least Squares GANs discriminator <ref type="bibr" target="#b29">[30]</ref> tried to introduce a loss function that provides smoother and nonsaturating gradient. The authors observe that the log-type loss in <ref type="bibr" target="#b8">[9]</ref> saturates quickly as it ignores the distance between x to the decision boundary. In contrast, an L2 loss provides gradients proportional to that distance, so that fake samples more far away from the boundary receive larger penalties. The proposed loss function also minimizes the Pearson χ 2 divergence that leads to the better training stability. The LSGAN objective function is written as::</p><formula xml:id="formula_1">min D V (D) = 1 2 E x∼p data (x) (D(x) − 1) 2 + 1 2 E z∼pz(z) D(G(z)) 2 (1) min G V (G) = 1 2 E z∼pz(z) (D(G(z)) − 1) 2</formula><p>Another relevant improvement to GANs is the Relativistic GAN <ref type="bibr" target="#b15">[16]</ref>. It used a relativistic discriminator to estimate the probability that the given real data is more realistic than a randomly sampled fake data. As the author advocated, such would account for a priori knowledge that half of the data in the mini-batch is fake. The relativistic discriminators show more stable and computationally efficient training in comparison to other GAN types, including WGAN-GP <ref type="bibr" target="#b9">[10]</ref> that was used in DeblurGAN-v1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DeblurGAN-v2 Architecture</head><p>The overview of DeblurGAN-v2 architecture is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. It restores a sharp image I S from a single blurred image I B , via the trained generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Pyramid Deblurring</head><p>Existing CNNs for image deblurring (and other restoration problems) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b32">33]</ref> typically refer to ResNet-like structures. Most state-of-the-art methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b44">45]</ref> dealt with different levels of blurs, utilizing multi-stream CNN s with an input image pyramid at different scales. However, processing multiple scale images is time-consuming and memorydemanding. We introduce the idea of Feature Pyramid Networks <ref type="bibr" target="#b26">[27]</ref> to image deblurring (more generally, the field of image restoration and enhancement), for the first time to our best knowledge. We treat this novel approach as a lighterweight alternative to incorporate multi-scale features.</p><p>The FPN module was originally designed for object detection <ref type="bibr" target="#b26">[27]</ref>. It generates multiple feature map layers which encode different semantics and contain better quality information. FPN comprises a bottom-up and a top-down pathway. The bottom-up pathway is the usual convolutional network for feature extraction, along which the spatial resolution is downsampled, but more semantic context information is extracted and compressed. Through the top-down pathway, FPNs reconstructs higher spatial resolution from the semantically rich layers. The lateral connections between the bottom-up and top-down pathways supplement high-resolution details and help localize objects.</p><p>Our architecture consists of an FPN backbone from which we take five final feature maps of different scales as the output. Those features are later up-sampled to the same <ref type="bibr">1 4</ref> input size and concatenated into one tensor which contains the semantic information on different levels. We additionally add two upsampling and convolutional layers at the end of the network to restore the original image size and reduce artifacts. Similar to <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29]</ref>, we introduce a direct skip connection from the input to the output, so that the learning focuses on the residue. The input images are normalized to [- <ref type="bibr">1 1]</ref>. We also use a tanh activation layer to keep the output in the same range. In addition to the multi-scale feature aggregation capability, FPN also strikes a balance between accuracy and speed: please see experiment parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Choice of Backbones: Trade-off between Performance and Efficiency</head><p>The new FPN-embeded architecture is agnostic to the choice of feature extractor backbones. With this plug-andplay property, we are entitled with the flexibility to navigate through the spectrum of accuracy and efficiency. By default, we choose ImageNet-pretrained backbones to convey more semantic-related features. As one option, we use Inception-ResNet-v2 <ref type="bibr" target="#b43">[44]</ref> to pursue strong deblurring performance, although we find other backbones such as SE-ResNeXt <ref type="bibr" target="#b11">[12]</ref> to be similarly effective.</p><p>The demands of efficient restoration model have recently drawn increasing attentions due to the prevailing need of mobile on-device image enhancement <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b46">47]</ref>. To explore this direction, we choose the MobileNet V2 backbone <ref type="bibr" target="#b39">[40]</ref> as one option. To reduce the complexity further, we try another more aggressive option on top of DeblurGAN-v2 with MobileNet V2, by replacing all normal convolutions in the full network (including those not in backbone) with Depthwise Separable Convolutions <ref type="bibr" target="#b5">[6]</ref>. The resulting model is denoted as MobileNet-DSC, and can provide extremely lightweight and efficient image deblurring.</p><p>To unleash this important flexibility to practitioners, in our codes, we have implemented the switch of backbones as a simple one-line command: it can be compatible with many state-of-the-art pre-trained networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Double-Scale RaGAN-LS Discriminator</head><p>Instead of the WGAN-GP discriminator in DeblurGAN <ref type="bibr" target="#b20">[21]</ref>, we suggest several upgrades in DeblurGAN-v2. We first adopt the relativistic "wrapping" <ref type="bibr" target="#b15">[16]</ref> on the LSGAN <ref type="bibr" target="#b29">[30]</ref> cost function, creating a new RaGAN-LS loss:</p><formula xml:id="formula_2">L RaLSGAN D = E x∼p data (x) (D(x) − E z∼pz(z) D(G(z)) − 1) 2 + E z∼pz(z) (D(G(z)) − E x∼p data (x) D(x) + 1) 2<label>(2)</label></formula><p>It is observed to make training notably faster and more stable compared to using the WGAN-GP objective. We also empirically conclude that the generated results possess higher perceptual quality and overall sharper outputs. Correspondingly, the adversarial loss L adv for the DeblurGAN-v2 generator will be optimizing (2) w.r.t. G. Extending to Both Global and Local Scales. Isola et al. <ref type="bibr" target="#b12">[13]</ref> propose to use a PatchGAN discriminator which operates on the images patches of size 70 × 70, that proves to produce sharper results than the standard "global" discriminator that operates on the full image. The PatchGAN idea was adopted in DeblurGAN <ref type="bibr" target="#b20">[21]</ref>.</p><p>However, we observed that for highly non-uniform blurred images, especially when complex object movements are involved, the "global" scales are still essential for discriminators to incorporate full spatial contexts <ref type="bibr" target="#b13">[14]</ref>. To take advantage of both global and local features, we propose to use a double-scale discriminator, consisting of one local branch that operates on patch levels like <ref type="bibr" target="#b12">[13]</ref> did, and the other global branch that feeds the full input image. We observe that to allow DeblurGAN-v2 to better handle larger and more heterogeneous real blurs.</p><p>Overall Loss Function For training image restoration GANs, one needs to compare the images on the training stage the reconstructed and the original ones, under some metric. One common option is the pixel-space loss L P , e.g., the simplest L 1 or L 2 distance. As <ref type="bibr" target="#b22">[23]</ref> suggested, using L p tends to yield oversmoothened pixel-space outputs. <ref type="bibr" target="#b20">[21]</ref> proposed to use the perceptual distance <ref type="bibr" target="#b14">[15]</ref>, as a form of "content" loss L X . In contrast to the L 2 , it computes the Euclidean loss on the VGG19 [41] conv3 3 feature maps. We incorporate those prior wisdoms and use a hybrid threeterm loss for training DeblurGAN-v2:</p><formula xml:id="formula_3">L G = 0.5 * L p + 0.006 * L X + 0.01 * L adv</formula><p>The L adv terms contains both global and local discriminator losses. Also, we choose mean-square-error (MSE) loss as L p : although DeblurGAN did not include an L p term, we find it to help correct color and texture distortions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Datasets</head><p>The GoPro dataset <ref type="bibr" target="#b32">[33]</ref> uses the GoPro Hero 4 camera to capture 240 frames per second (fps) video sequences, and generate blurred images through averaging consecutive short-exposure frames. It is a common benchmark for image motion blurring, containing 3,214 blurry/clear image pairs. We follow the same split <ref type="bibr" target="#b32">[33]</ref>, to use 2,103 pairs for training and the remaining 1,111 pairs for evaluation. The DVD dataset <ref type="bibr" target="#b41">[42]</ref> collects 71 real-world videos captured by various devices such as iPhone 6s, GoPro Hero 4 and Nexus 5x, at 240 fps. The author then generated 6708 synthetic blurry and sharp pairs by averaging consecutive short-exposure frames to approximate a longer exposure <ref type="bibr" target="#b45">[46]</ref>. The dataset was initially used for video deblurring but was later also brought to the image deblurring field.</p><p>The NFS dataset <ref type="bibr" target="#b16">[17]</ref> was initially proposed to benchmark visual object tracking. It consists of 75 videos captured with high-frame rate cameras from iPhone 6 and iPad Pro. Additionally, 25 sequences are collected from YouTube captured at 240 fps from a variety of different devices. It covers variety of scenes including sport, skydiving, underwater, wildlife, roadside, and indoor scenes.</p><p>Training data preparation: Conventionally, the blurry frames are averaged from consecutive clean frames. However, we notice unrealistic ghost effects when observing the directly averaged frames, as in <ref type="figure" target="#fig_1">Figure 3</ref>(a)(c). To alleviate that, we first use a video frame interpolation model <ref type="bibr" target="#b33">[34]</ref> to increase the original 240-fps videos to 3840 fps, then perform average pooling over the same time window (but now with more frames). It leads to smoother and more continuous blurs, as in <ref type="figure" target="#fig_1">Figure 3</ref>(b)(d). Experimentally, this data preparation did not noticeably impact PSNR/SSIM but was observed to improve the visual quality results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental evaluation 4.1. Implementation Details</head><p>We implemented all of our models using PyTorch <ref type="bibr" target="#b0">[1]</ref>. We compose our training set by selecting each second frame from the GoPro and DVD datasets, and every tenth frame from the NFS dataset, with the hope to reduce overfitting to any specific dataset. We then train DeblurGAN-v2 on the resulting set of approximately 10,000 image  All models were trained on a single Tesla-P100 GPU, with Adam <ref type="bibr" target="#b17">[18]</ref> optimizer and the learning rate of 10 −4 for 150 epochs, followed by another 150 epochs with a linear decay to 10 −7 . We freeze the pre-trained backbone weights for 3 epochs, and then we unfreeze all weights and continue the training. The un-pre-trained parts are initialized with random Gaussian. The training takes 5 days to converge. The models are fully convolutional, thus can be applied to the images of arbitrary size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative Evaluation on GoPro Dataset</head><p>We compare our models with a number of state-of-thearts: one of is a traditional method by Xu et al. <ref type="bibr" target="#b50">[51]</ref>, while the rest are deep learning-based: <ref type="bibr" target="#b42">[43]</ref> by Sun et al., Deep-Deblur <ref type="bibr" target="#b32">[33]</ref>, SRN <ref type="bibr" target="#b44">[45]</ref>, and DeblurGAN <ref type="bibr" target="#b20">[21]</ref>. We compare on both standard performance metrics (PSNR, SSIM), and inference efficiency (averaged running time per image measured on a single GPU). Results are summarized in Table1.</p><p>In terms of PSNR/SSIM, DeblurGAN-v2 (Inception-ResNet-v2) and SRN are ranked top-2: DeblurGAN-v2 (Inception-ResNet-v2) has slightly lower PSNR, which is not surprising since it was not trained under pure MSE loss; but it outperforms SRN in SSIM. However, we are very encouraged to observe that DeblurGAN-v2 (Inception-ResNet-v2) takes 78% less inference time than SRN. Moreover, two of our light-weight models, DeblurGAN-v2 (MobileNet) and DeblurGAN-v2 (MobileNet-DSC), show SSIMs (0.925 and 0.922) on par with the other two latest deep deblurring methods, DeblurGAN (0.927) and Deep-Deblur (0.916), while being up to 100 times faster.</p><p>In particular, MobileNet-DSC only costs 0.04s per image, which even enables near real-time video frame deblurring, for 25-fps videos. To our best knowledge, DeblurGAN-v2 (MobileNet-DSC) is the only deblurring method so far that can simultaneously achieve (reasonably) high performance and that high inference efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Evaluation on Kohler dataset</head><p>The Kohler dataset <ref type="bibr" target="#b18">[19]</ref> consists of 4 images, each blurred with 12 different kernels. It is a standard benchmark for evaluating blind deblurring algorithms. The dataset was generated by recording and analyzing real camera motion, which was then played back on a robot platform such that a sequence of sharp images was recorded sampling the 6D camera motion trajectory.</p><p>The comparison results are reported in <ref type="table" target="#tab_2">Table 2</ref>. Similarly to GoPro, SRN and DeblurGAN-v2 (Inception-ResNet-v2) remain to be the best two PSNR/SSIM performers, but this time SRN is marginally superior in both. However, please be reminded that, similarly to the GoPro case, this "almost tie" result was achieved while DeblurGAN-v2 (Inception-ResNet-v2) costs only 1/5 of SRN's inference complexity. Moreover, both DeblurGAN-v2 (MobileNet) and DeblurGAN-v2 (MobileNet-DSC) outperform Deblur-GAN on the Kohler dataset in both SSIM and PSNR: that is impressive given the former two's much lighter weights. <ref type="figure">Figure 4</ref> displays visual examples on the Kohler dataset. DeblurGAN-v2 effectively restores the edges and textures, without noticeable artifacts. SRN for this specific example shows some color artifacts when zoomed in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Quantitative Evaluation on DVD dataset</head><p>We next test DeblurGAN-v2 on the DVD testing set used in <ref type="bibr" target="#b41">[42]</ref>, but with a single-frame setting (treating all frames as individual images) without using multiple frames together. We compare with two strong video deblurring meth- (c) SRN <ref type="bibr" target="#b44">[45]</ref> (d) DeblurGAN <ref type="bibr" target="#b20">[21]</ref> (e) DeblurGAN-v2 (f) DeblurGAN-v2 (Inception-ResNet-v2) (MobileNet) <ref type="figure">Figure 4</ref>: Visual comparison on the Kohler dataset. ods: WFA <ref type="bibr" target="#b6">[7]</ref>, and DVD <ref type="bibr" target="#b41">[42]</ref>, For the latter, we adopt the authors' self-reported results when using a single frame as the model input (denoted as "single"), for a fair comparison. As shown in <ref type="table" target="#tab_7">Table 6</ref>, DeblurGAN-v2 (MobileNet) outperforms WFA and DVD (single), while being at least 17 times faster (DVD was tested on a reduced resolution of 960 × 540, while DeblurGAN-v2 is on 1280 x 720).</p><p>While not specifically optimized for video deblurring, DeblurGAN-v2 shows good potential, and we will extend it to video deblurring as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Subjective Evaluation on Lai dataset</head><p>The Lai dataset <ref type="bibr" target="#b21">[22]</ref> has real-world blurry images of different qualities and resolutions collected in various types of scenes. Those real images have no clean/sharp counterparts, making a full-reference quantitative evaluation impossible. Following <ref type="bibr" target="#b21">[22]</ref>, we conduct a subjective survey to compare the deblurring performance on those real images.</p><p>We fit a Bradley-Terry model <ref type="bibr" target="#b2">[3]</ref> to estimate the subjective score for each method so that they can be ranked, with the identical routine following the previous benchmark work <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref>. Each blurry image is processed with each of the following algorithms: Krishnan et al. <ref type="bibr" target="#b19">[20]</ref>, Whyte et al. <ref type="bibr" target="#b48">[49]</ref>, Xu et al. <ref type="bibr" target="#b50">[51]</ref>, Sun et al. <ref type="bibr" target="#b42">[43]</ref>, Pan et al. <ref type="bibr" target="#b35">[36]</ref>, DeepDeblur <ref type="bibr" target="#b32">[33]</ref>, SRN <ref type="bibr" target="#b44">[45]</ref>, DeblurGAN <ref type="bibr" target="#b20">[21]</ref>; and the three DeblurGAN-v2 variants (Inception-ResNet-v2, Mo-bileNet, MobileNet-DSC). The eleven deblurring results, together with the original blurry image, are sent for pairwise comparison to construct the winning matrix. We collect the pair comparison results from 22 human raters. We observed good consensus and small inter-person variances among raters, which makes scores reliable.</p><p>The subjective scores are reported in <ref type="table" target="#tab_4">Table 4</ref>. We did not normalize the scores due to the absence of groundtruth: as a result, it is the score rank rather than the absolute score value that matters here. It can be observed that deep learning-based deblurring algorithms, in general, have more favorable visual results than traditional methods (some even making visual quality worse than the blurry input). DeblurGAN <ref type="bibr" target="#b20">[21]</ref> outperforms DeepDeblur <ref type="bibr" target="#b32">[33]</ref>, but lags behind SRN <ref type="bibr" target="#b44">[45]</ref>.  version. However, both are still preferred by subjective raters, compared to DeepDeblur and DeblurGAN, while being 2-3 orders-of-magnitude faster. <ref type="figure" target="#fig_2">Figure 5</ref> displays visual comparison examples on deblurring the "face2" image. DeblurGAN-v2 (Inception-ResNet-v2) (5j) and SRN (5h) are the top-2 most favored results, both balancing well between edge-sharpness and overall smoothness. By zooming in, SRN is found to still generate some ghost artifacts on this example, e.g., the white "intrusion" from the collar to the bottom right face region. In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation Study and Analysis</head><p>We perform an ablation study on the effect of specific components of the DeblurGAN-v2 pipeline. Starting from the original DeblurGAN (ResNet G, local-scale patch D, WGAN-GP + perceptual loss), we gradually inject our modifications on the generator (adding FPN), discriminator (adding global-scale), and the loss (replacing WGAN-GP loss with RaGAN-LS, and adding an MSE term). The results are summarized in <ref type="table" target="#tab_7">Table 6</ref>. We can see that all our proposed components steadily improve both PSNR and SSIM. In particular, the FPN module contributes most significantly. Also, adding either MSE or perceptual loss benefits both training stability and final results. As an extra baseline for the efficiency of FPN, we tried to create a "compact" version of SRN, with roughly the same FLOPs (456 GFLOPs) to match DeblurGAN-v2 Inception-ResNet-v2 (411 GFLOPs). We reduced the numbers of ResBlocks by 2/3 in each EBlock/DBlock while keeping their 3-scale recurrent structure. We then compare with DeblurGAN-v2 (Inception-ResNet-v2) on GoPro, where that "compact" SRN only achieved PSNR = 28.92 dB and SSIM = 0.9324. We also tried channel pruning <ref type="bibr" target="#b10">[11]</ref> to reduce SRN FLOPs and the result was no better. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Extension to General Restoration</head><p>Real-world atural images commonly go through multiple kinds of degradations (noise, blur, compression, etc.) at once, and a few recent works were devoted to such join enhancement tasks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b54">55]</ref> We study the effect of DeblurGAN-v2 on the task of general image restoration. While NOT being the main focus of this paper, we intend to show the general architecture superiority of DeblurGAN-v2, especially for modifications made w.r.t. DeblurGAN.</p><p>We synthesize a new challenging Restore Dataset. We take 600 images from GoPRO, and 600 images from DVD, both with motion blurs already (same as above). We then use the albumentations library <ref type="bibr" target="#b3">[4]</ref> to further add Gaussian and speckle Noise, JPEG compression, and up-scaling artifacts to those images. Eventually, we split 8000 images for training and 1200 for testing. We train and compare DeblurGAN-v2 (Inception-ResNet-v2), DeblurGAN-v2 (MobileNet-DSC), and DeblurGAN. As shown in <ref type="table" target="#tab_7">Table 6</ref> and <ref type="figure" target="#fig_3">Fig. 6</ref>, DeblurGAN-v2 (Inception-ResNet-v2) achieves the best PSNR, SSIM, and visual quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper introduces DeblurGAN-v2, a powerful and efficient image deblurring framework, with promising quantitative and qualitative results. DeblurGAN-v2 enables to switch between different backbones, for flexible tradeoffs between performance and efficiency. We plan to extend DeblurGAN-v2 for real-time video enhancement, and for better handling mixed degradations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>DeblurGAN-v2 pipeline architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Visual comparison of synthesized blurry images, without interpolation (a,c) and with interpolation (b,d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative comparison on the "face2" test image of the Lai dataset<ref type="bibr" target="#b21">[22]</ref>. DeblurGAN-v2 models are artifact-free, in contrast to other neural and non-CNN algorithms, producing smoother and visually more pleasing results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Visual comparison example on the Restore Dataset. comparison, DeblurGAN-v2 (Inception-ResNet-v2) shows artifact-free deblurring. Besides, DeblurGAN-v2 (Mo-bileNet) and DeblurGAN-v2 (MobileNet-DSC) results are also smooth and visually better than DeblurGAN, though less sharper than DeblurGAN-v2 (Inception-ResNet-v2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance and efficiency comparison on the GoPro test dataset, All models were tested on the linear image subset. Sun et al. [43] Xu et al. [51] DeepDeblur [33] SRN [45] DeblurGAN [21] Inception-ResNet-v2 MobileNet MobileNet-DSC</figDesc><table><row><cell>PSNR</cell><cell>24.64</cell><cell>25.10</cell><cell>29.23</cell><cell>30.10</cell><cell>28.70</cell><cell>29.55</cell><cell>28.17</cell><cell>28.03</cell></row><row><cell>SSIM</cell><cell>0.842</cell><cell>0.890</cell><cell>0.916</cell><cell>0.932</cell><cell>0.927</cell><cell>0.934</cell><cell>0.925</cell><cell>0.922</cell></row><row><cell>Time</cell><cell>20 min</cell><cell>13.41s</cell><cell>4.33s</cell><cell>1.6s</cell><cell>0.85s</cell><cell>0.35s</cell><cell>0.06s</cell><cell>0.04s</cell></row><row><cell>FLOPS</cell><cell>N/A</cell><cell>N/A</cell><cell>1760.04G</cell><cell>1434.82G</cell><cell>678.29G</cell><cell>411.34G</cell><cell>43.75G</cell><cell>14.83G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>PSNR and SSIM comparison on the Kohler dataset.</figDesc><table><row><cell cols="8">Method Sun [43] DeepDeblur [33] SRN [45] DeblurGAN [21] Inception-ResNet-v2 MobileNet MobileNet-DSC</cell></row><row><cell>PSNR</cell><cell>25.22</cell><cell>26.48</cell><cell>26.75</cell><cell>26.10</cell><cell>26.72</cell><cell>26.36</cell><cell>26.35</cell></row><row><cell>SSIM</cell><cell>0.773</cell><cell>0.807</cell><cell>0.837</cell><cell>0.816</cell><cell>0.836</cell><cell>0.820</cell><cell>0.819</cell></row><row><cell cols="5">pairs. Three backbones are evaluated: Inception-ResNet-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">v2, MobileNet, and MobileNet-DSC. The former tar-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">gets at high-performance deblurring, while the latter two</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">are more suited for resource-constrained edge applica-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">tions. Specifically, the extremely lightweight DeblurGAN-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">v2 (MobileNet-DSC) costs 96% fewer parameters than</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">DeblurGAN-v2 (Inception-ResNet-v2).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on DVD dataset</figDesc><table><row><cell></cell><cell cols="3">PSNR SSIM Inference Time Resolution</cell></row><row><cell>WFA</cell><cell>28.35 N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell cols="2">DVD (single) 28.37 0.913</cell><cell>1.0s</cell><cell>960 x 540</cell></row><row><cell cols="2">DeblurGAN-v2 28.54 0.929 (MobileNet)</cell><cell>0.06s</cell><cell>1280 x 720</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Average subjective scores of deblurring results on the Lai dataset [22]. Blurry Krishnan et al. [20] Whyte et al. [49] Xu et al. [51] Sun et al. [43] Pan et al.</figDesc><table><row><cell>[36]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation Study on the GoPro dataset, based on DeblurGAN-v2 (Inception-ResNet-v2).</figDesc><table><row><cell></cell><cell>PSNR SSIM</cell></row><row><cell>DeblurGAN (starting point)</cell><cell>28.70 0.927</cell></row><row><cell>+ FPN</cell><cell>29.26 0.931</cell></row><row><cell>+ FPN + Global D</cell><cell>29.29 0.932</cell></row><row><cell cols="2">+ FPN + Global D + RaGAN-LS 29.37 0.933</cell></row><row><cell>DeblurGAN-v2 (FPN + Global D +</cell><cell></cell></row><row><cell>RaGAN-LS + MSE Loss)</cell><cell>29.55 0.934</cell></row><row><cell>Removing perceptual loss</cell><cell></cell></row><row><cell>(replace 0.5 with 0 in LG)</cell><cell>28.81 0.924</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>PSNR/SSIM comparison on Restore Dataset.</figDesc><table><row><cell></cell><cell>PSNR SSIM</cell></row><row><cell>Degraded</cell><cell>22.056 0.873</cell></row><row><cell>DeblurGAN</cell><cell>26.435 0.892</cell></row><row><cell cols="2">DeblurGAN-v2 (Inception-ResNet-v2) 26.916 0.894</cell></row><row><cell>DeblurGAN-v2 (MobileNet-DSC)</cell><cell>25.412 0.891</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://pytorch.org" />
		<title level="m">PyTorch</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rank analysis of incomplete block designs: I. the method of paired comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Ralph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milton</forename><forename type="middle">E</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="324" to="345" />
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Buslaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Parinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Khvedchenya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandr A</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalinin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.06839</idno>
		<title level="m">Albumentations: fast and flexible image augmentations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A new single image deblurring algorithm using hyper laplacian priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Feng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiunn-Lin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1015" to="1022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Burst deblurring: Removing camera shake through fourier burst accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2385" to="2393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Anton Van Den Hengel, and Qinfeng Shi. From Motion Blur to Motion Flow: a Deep Learning Solution for Removing Heterogeneous Motion Blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Generative Adversarial Networks</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1389" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks. arxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Enlightengan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06972</idno>
		<title level="m">Deep light enhancement without paired supervision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The relativistic discriminator: a key element missing from standard gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00734</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Need for speed: A benchmark for higher frame rate object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashton</forename><surname>Hamed Kiani Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recording and playback of camera shake: Benchmarking blind deconvolution with a real-world database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolf</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Betty</forename><surname>Mohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th European Conference on Computer Vision -Volume Part VII, ECCV&apos;12</title>
		<meeting>the 12th European Conference on Computer Vision -Volume Part VII, ECCV&apos;12<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="27" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Blind deconvolution using a normalized sparsity measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terence</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deblurgan: Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykola</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiří</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8183" to="8192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A comparative study for single image blind deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1701" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Benchmarking singleimage dehazing and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengpan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="492" to="505" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning a discriminative prior for blind image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lerenhan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Single image deraining: A comprehensive benchmark analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iago</forename><forename type="middle">Breno</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">K</forename><surname>Tokuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><forename type="middle">Hirata</forename><surname>Junior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cesar-Junior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3838" to="3847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust video super-resolution with learned temporal dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2507" to="2515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">When image denoising meets high-level vision tasks: a deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bihan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="842" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno>arxiv:1611.04076</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial nets. CoRR, abs/1411.1784</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Lsd 2 -joint denoising and deblurring of short and long exposure images with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Mustaniemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simo</forename><surname>Särkkä</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Heikkilä</surname></persName>
		</author>
		<idno>abs/1811.09485</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Kyoung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Motion Deblurring in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paramanand</forename><surname>Chandramouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Blind image deblurring using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1628" to="1636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep generative filter for motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Sainandan Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aalok</forename><surname>Pachori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanmuganathan</forename><surname>Gangopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2993" to="3000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep non-blind deconvolution via generalized low-rank approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wenqi Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="295" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04381</idno>
		<title level="m">MobileNetV2: Inverted Residuals and Linear Bottlenecks. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep video deblurring for hand-held cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuochen</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning a Convolutional Neural Network for Non-uniform Motion Blur Removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenfei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scale-recurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8174" to="8182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Synthetic shutter speed imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Telleen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabath</forename><surname>Gunawardane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Energynet: Energyefficient dynamic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS CDNNRIA Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">D3: Deep dualdomain based fast restoration of jpeg-compressed images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2764" to="2772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Non-uniform deblurring for shaken images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Whyte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="491" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep k-means: Retraining and parameter sharing with harder cluster assignments for compressing deep convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashok</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyan</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5359" to="5368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Unnatural L0 Sparse Representation for Natural Image Deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shicheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Motion blur kernel estimation via deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="194" to="205" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Semantic image inpainting with perceptual and contextual losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teck-Yian</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<idno>abs/1607.07539</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Crafting a toolchain for image restoration by deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2443" to="2452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Gated fusion network for joint image deblurring and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
