<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Learning of Landmarks by Descriptor Vector Exchange</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thewlis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
							<email>albanie@robots.ox.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
							<email>hbilen@ed.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
							<email>vedaldi@robots.ox.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Unitary</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">VGG</orgName>
								<orgName type="institution" key="instit2">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">VGG</orgName>
								<orgName type="institution" key="instit2">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Learning of Landmarks by Descriptor Vector Exchange</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: We propose Descriptor Vector Exchange (DVE), a mechanism that enables unsupervised learning of robust highdimensional dense embeddings with equivariance losses. The embeddings learned for the category of faces are visualised in the figure above with the help of a query image <ref type="bibr" target="#b7">[8]</ref>, shown in the centre of the figure. (Left): We colour the locations of pixel embeddings that form the nearest neighbours of the query reference points. (Right): The same reference points are used to retrieve patches amongst a collection of face images. The result is an approximate face mosaic, matching parts across different identities despite the fact that no landmark annotations of any kind were used during learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Equivariance to random image transformations is an effective method to learn landmarks of object categories, such as the eyes and the nose in faces, without manual supervision. However, this method does not explicitly guarantee that the learned landmarks are consistent with changes between different instances of the same object, such as different facial identities. In this paper, we develop a new perspective on the equivariance approach by noting that dense landmark detectors can be interpreted as local image descriptors equipped with invariance to intra-category variations. We then propose a direct method to enforce such an invariance in the standard equivariant loss. We do so by exchanging descriptor vectors between images of different object instances prior to matching them geometrically. In this manner, the same vectors must work regardless of the specific object identity considered. We use this approach to learn vectors that can simultaneously be interpreted as local descriptors and dense landmarks, combining the advan- * Equal Contribution. James was with the VGG during part of this work. tages of both. Experiments on standard benchmarks show that this approach can match, and in some cases surpass state-of-the-art performance amongst existing methods that learn landmarks without supervision. Code is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning without manual supervision remains an open problem in machine learning and computer vision. Even recent advances in self-supervision <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref> are often limited to learning generic feature extractors and still require some manually annotated data to solve a concrete task such as landmark detection. In this paper, we thus consider the problem of learning the landmarks of an object category, such as the eyes and nose in faces, without any manual annotation. Namely, given as input a collection of images of a certain object, such as images of faces, the goal is to learn what landmarks exist and how to detect them.</p><p>In the absence of manual annotations, an alternative supervisory signal is required. Recently, <ref type="bibr" target="#b45">[46]</ref> proposed to build on the fact that landmark detectors are equivariant to image transformations. For example, if one translates or rotates a face, then the locations of the eyes and nose follow suit. Equivariance can be used as a learning signal by applying random synthetic warps to images of an object and then requiring the landmark detector to be consistent with these transformations.</p><p>The main weakness of this approach is that equivariance can only be imposed for transformations of specific images. This means that a landmark detector can be perfectly consistent with transformation applied to a specific face and still match an eye in a person and the nose in another. In this approach, achieving consistency across object instances is left to the generalisation capabilities of the underlying learning algorithm.</p><p>In this paper, we offer a new perspective on the problem of learning landmarks, generalising prior work and addressing its shortcomings. We start by establishing a link between two apparently distinct concepts: landmarks and local image descriptors ( <ref type="figure">fig. 2</ref>). Recall that a descriptor, such as SIFT, is a vector describing the appearance of the image around a given point. Descriptors can establish correspondences between images because they are invariant to viewing effects such as viewpoint changes. However, similar to descriptors, landmarks can also establish image correspondences by matching concepts such as eyes or noses detected in different images.</p><p>Thus invariant descriptors and landmark detectors are similar, but landmarks are invariant to intra-class variations in addition to viewing effects. We can make this analogy precise if we consider dense descriptors and landmarks <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b41">42]</ref>. A dense descriptor associates to each image pixel a C-dimensional vector, whereas a dense landmark detector associates to each pixel a 2D vector, which is the index of the landmark in a (u, v) parameterisation of the object surface. Thus we can interpret a landmark as a tiny 2D descriptor. Due to its small dimensionality, a landmark loses the ability to encode instance-specific details of the appearance, but gains robustness to intra-class variations.</p><p>Generalising this idea, we note that any invariant descriptor can be turned into a landmark detector by equipping it with robustness to intra-class variations. Here we propose a new method that can do so without reducing the dimensionality of the descriptor vectors. The formulation still considers pairs of synthetically-transformed images as <ref type="bibr" target="#b44">[45]</ref> do, but this time landmarks are represented by arbitrary Cdimensional vectors. Then, before geometric consistency (equivariance) is enforced, the landmark vectors extracted from one image are exchanged with similar vectors extracted from other random images of the object. This way geometric consistency between an image and its transformations can only be achieved if vectors have an intra-class validity, and thus effectively characterise landmarks.  <ref type="figure">Figure 2</ref>: Descriptor-landmark hierarchy. A local invariant descriptor maps image pixels to distinctive vectors that are invariant to viewing conditions such as a viewpoint. A dense landmark detector maps pixels to unique points of the object's surface, such as eyes and nose in faces, to points on the surface of a sphere. Both produce invariant and distinctive vectors, but landmarks are also invariant to intra-class variations. Taken together, they represent a hierarchy of distinctive pixel embeddings of increasing invariance.</p><p>Empirically (section 4), we show that the key advantage of this formulation, which we term Descriptor Vector Exchange (DVE), is that it produces embedding vectors that simultaneously work well as instance-specific image descriptors and landmarks, capturing in a single representation the advantages of both, and validating our intuition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>General image matching. Image matching based on local features has been an extensively studied problem in the literature with applications to wide-baseline stereo matching <ref type="bibr" target="#b37">[38]</ref> and image retrieval <ref type="bibr" target="#b47">[48]</ref>. The generic pipeline contains the following steps: i) detecting a sparse set of interest points <ref type="bibr" target="#b27">[28]</ref> that are covariant with a class of transformations, ii) extracting local descriptors (e.g. <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b46">47]</ref>) at these points that are invariant to viewpoint and illumination changes, and iii) matching the nearest neighbour descriptors across images with an optional geometric verification. While the majority of the image matching methods rely on hand-crafted detectors and descriptors, recent work show that CNN-based models can successfully be trained to detect covariant detectors <ref type="bibr" target="#b22">[23]</ref> and invariant descriptors <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b35">36]</ref>. We build our method on similar principles, covariance and invariance, but with an important difference that it can learn intrinsic features for object categories in contrast to generic ones. Cross-instance object matching.</p><p>The SIFT Flow method <ref type="bibr" target="#b23">[24]</ref> extends the problem of finding dense correspondences between same object instances to different instances by matching their SIFT features <ref type="bibr" target="#b26">[27]</ref> in a variational framework. This work is further improved by using multi-scale patches <ref type="bibr" target="#b10">[11]</ref>, establishing region correspondences <ref type="bibr" target="#b9">[10]</ref> and replacing SIFT features with CNN ones <ref type="bibr" target="#b25">[26]</ref>. In addition, Learned-Miller <ref type="bibr" target="#b20">[21]</ref> generalises the dense correspondences between image pairs to an arbitrary number of images by continuously warping each image via a parametric transformation. RSA <ref type="bibr" target="#b36">[37]</ref>, Collection Flow <ref type="bibr" target="#b17">[18]</ref> and Mobahi et al. <ref type="bibr" target="#b28">[29]</ref> project a collection of images into a lower dimensional subspace and perform a joint alignment among the projected images. AnchorNet <ref type="bibr" target="#b33">[34]</ref> learns semantically meaningful parts across categories, although is trained with image labels.</p><p>Transitivity. The use of transitivity to regularise structured data has been proposed by several authors <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58]</ref> in the literature. Earlier examples <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b50">51]</ref> employ this principle to achieve forward-backward consistency in object tracking and to identify inconsistent geometric relations in structure from motion respectively. Zhou et al. <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref> enforce a geometric consistency to jointly align image sets or supervise deep neural networks in dense semantic alignment by establishing a cycle between each image pair and a 3D CAD model. DVE also builds on the same general principle of transitivity, however, it operates in the space of appearance embeddings in contrast to verification of subsequent image warps to a composition.</p><p>Unsupervised learning of object structure. Visual object characterisation (e.g. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>) has a long history in computer vision with extensive work in facial landmark detection and human body pose estimation. A recent unsupervised method that can learn geometric transformations to optimise classification accuracy is the spatial transformer network <ref type="bibr" target="#b11">[12]</ref>. However, this method does not learn any explicit object geometry. Similarly, WarpNet <ref type="bibr" target="#b15">[16]</ref> and geometric matching networks <ref type="bibr" target="#b38">[39]</ref> train neural networks to predict relative transformations between image pairs. These methods are limited to perform only on image pairs and do not learn an invariant geometric embedding for the object. Most related to our work, <ref type="bibr" target="#b45">[46]</ref> characterises objects by learning landmarks that are consistent with geometric transformations without any manual supervision, while <ref type="bibr" target="#b32">[33]</ref> similarly use such transformations for semantic matching. The authors of <ref type="bibr" target="#b45">[46]</ref> extended their approach to extract a dense set of landmarks by projecting the raw pixels on a surface of a sphere in <ref type="bibr" target="#b44">[45]</ref>. Similar work <ref type="bibr" target="#b40">[41]</ref> leverages frame-toframe correspondence using Dynamic Fusion <ref type="bibr" target="#b30">[31]</ref> as supervision to learn a dense labelling for human images. We build our method, DVE, on these approaches and further extend them in significant ways. First, we learn more versatile descriptors that can encode both generic and objectspecific landmarks and show that we can gradually learn to move from generic to specific ones. Second, we improve the cross-instance generalisation ability by better regularising the embedding space with the use of transitivity. Finally, we show that DVE both qualitatively and quantitatively outperforms <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b44">45]</ref> in facial landmark detection (section 4). Recent work <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b41">42]</ref> proposes to disentangle appearance from pose by estimating dense deformation field <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b41">42]</ref> and by learning landmark positions to reconstruct one sample from another. We compare DVE to these approaches in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We first summarise the method of <ref type="bibr" target="#b44">[45]</ref> and then introduce DVE, our extension to their approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learning dense landmarks using equivariance</head><p>Denote by x ∈ R 3×H×W an image of an object, by Ω = {0, . . . , H − 1} × {0, . . . , W − 1} its domain, and by u ∈ Ω an image pixel. Consider as in <ref type="bibr" target="#b44">[45]</ref> a spherical parameterisation of the object surface, where each point on the sphere indexes a different characteristic point of the object, i.e. a landmark. Our goal is to learn a function Φ that maps pixels u ∈ Ω to their corresponding landmark indices</p><formula xml:id="formula_0">Φ u (x) ∈ S 2 .</formula><p>The authors of <ref type="bibr" target="#b44">[45]</ref> showed that Φ can be learned without manual supervision by requiring it to be invariant with transformations of the image. Namely, consider a random warp g : Ω → Ω and denote with gx the result of applying the warp to the image. <ref type="bibr" target="#b0">1</ref> Then, if the map assigns label Φ u (x) to pixel u of image x, it must assign the same label Φ gu (gx) to pixel gu of the deformed image gx. This is because, by construction, pixels u and gu land on the same object point, and thus contain the same landmark. Hence, we obtain the equivariance constraint Φ u (x) = Φ gu (gx).</p><p>This version of the equivariance constraint is not quite sufficient to learn meaningful landmarks. In fact, the constraint can be satisfied trivially by mapping all pixels to some fixed point on the sphere. Instead, we must also require landmarks to be distinctive, i.e. to identify a unique point in the object. This is captured by the equation:</p><formula xml:id="formula_1">∀u, v ∈ Ω : v = gu ⇔ Φ u (x) = Φ v (gx).<label>(1)</label></formula><p>Probabilistic formulation. For learning, eq. (1) is relaxed probabilistically ( <ref type="figure">fig. 3</ref>). Given images x and x , define the probability of pixel u in image x matching pixel v in image x by normalising the cosine similarity Φ u (x), Φ v (x ) of the corresponding landmark vectors:</p><formula xml:id="formula_2">p(v|u; Φ, x, x ) = e Φu(x),Φv(x ) Ω e Φu(x),Φt(x ) dt .<label>(2)</label></formula><formula xml:id="formula_3">1 I.e. (gx)u = x g −1 u .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Build using Match Match</head><p>With DVE Without DVE <ref type="figure">Figure 3</ref>: We learn a dense embedding Φ u (x) ∈ R C of image pixels. The embedding is learned from pairs of images (x, x ) related by a known warp v = g(u). Note that in practice, we do not have access to pairs of pairs of images with a known correspondence-thus, throughout this work the warps are generated synthetically. Left: the approach of <ref type="bibr" target="#b44">[45]</ref> directly matches embedding Φ u (x) from the left image to embeddings Φ v (x ) in the right image. Right: DVE replaces Φ u (x) from its reconstructionΦ u (x|x α ) obtained from the embeddings in a third auxiliary image x α . Importantly, the correspondence with x α does not need to be known.</p><p>Given a warp g, and image x and its deformation x = gx, constraint eq. (1) is captured by the loss:</p><formula xml:id="formula_4">L(Φ; x, x , g) = 1 |Ω| 2 Ω Ω v−gu p(v|u; Φ, x, x ) du dv (3)</formula><p>where v − gu is a distance between pixels. In order to understand this loss, note that L(Φ; x, x , g) = 0 if, and only if, for each pixel u ∈ Ω, the probability p(v|u; Φ, x, x ) puts all its mass on the corresponding pixel gu. Thus minimising this loss encourages p(v|u; Φ, x, x ) to establish correct deterministic correspondences.</p><p>Note that the spread of probability (2) only depends on the angle between landmark vectors. In order to allow the model to modulate this spread directly, the range of function Φ is relaxed to be R 3 . In this manner, estimating longer landmark vectors causes (2) to become more concentrated, and this allows the model to express the confidence of detecting a particular landmark at a certain image location. <ref type="bibr" target="#b1">2</ref> Siamese learning with random warps. We now explain how (3) can be used to learn the landmark detector function Φ given only an unlabelled collection X = {x 1 , . . . , x n } of images of the object. The idea is to synthesise for each image a corresponding random warp from a distribution G. Denote with P the empirical distribution over the training images; then this amounts to optimising the energy</p><formula xml:id="formula_5">E(Φ) = E x∼P,g∼G [L(Φ; x, gx, g)] .<label>(4)</label></formula><p>Implemented as a neural network, this is a Siamese learning formulation because the network Φ is evaluated on both x and gx. <ref type="bibr" target="#b1">2</ref> The landmark identity is recovered by normalising the vectors to unit length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">From landmarks to descriptors</head><p>Equation <ref type="formula" target="#formula_1">(1)</ref> says that landmark vectors must be invariant to image transformations and distinctive. Remarkably, exactly the same criterion is often used to define and learn local invariant feature descriptors instead <ref type="bibr" target="#b0">[1]</ref>. In fact, if we relax the function Φ to produce embeddings in some highdimensional vector space R C , then the formulation above can be used out-of-the-box to learn descriptors instead of landmarks.</p><p>Thus the only difference is that landmarks are constrained to be tiny vectors (just points on the sphere), whereas descriptors are usually much higher-dimensional. As argued in section 1, the low dimensionality of the landmark vectors forgets instance-specific details and promotes intra-class generalisation of these descriptors.</p><p>The opposite is also true: we can start from any descriptor and turn it into a landmark detector by promoting intraclass generalisation. Using a low-dimensional embedding space is a way to do so, but not the only one, nor the most direct. We propose in the next section an alternative approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Vector exchangeability</head><p>We now propose our method, Descriptor Vector Exchange, to learn embedding vectors that are distinctive, transformation invariant, and insensitive to intra-class variations, and thus identify object landmarks. The idea is to encourage the sets of embedding vectors extracted from an image to be exchangeable with the ones extracted from another while retaining matching accuracy.</p><p>In more detail, let (x, x , g) be a warped image pair (hence x = gx). Furthermore, let x α be an auxiliary image, containing an object of the same category as the pair (x, x ), but possibly a different instance. If the embed-ding function Φ u (x) is insensitive to intra-class variations, then the set of embedding vectors {Φ u (x) : u ∈ Ω} and {Φ u (x α ) : u ∈ Ω} extracted from any two images should be approximately the same. This means that, in loss <ref type="formula">(3)</ref>, we can exchange the vectors Φ u (x) extracted from image x with corresponding vectors extracted from the auxiliary image x α .</p><p>Next, we integrate this idea in the probabilistic learning formulation given above ( <ref type="figure">fig. 3</ref>). We start by matching pixels in the source image x to pixels in the auxiliary image x α by using the probability p(w|u; Φ, x, x α ) computed according to eq. (2). Then, we reconstruct the source embedding Φ u (x) as the weighted average of the embeddings Φ w (x α ) in the auxiliary image, as follows:</p><formula xml:id="formula_6">Φ u (x|x α ) = Φ w (x α )p(w|u; Φ, x, x α ) dw.<label>(5)</label></formula><p>Once Φ u is computed, we use it to establish correspondences between x and x , using eq. (2). This results in the matching probability:</p><formula xml:id="formula_7">p(v|u; Φ, x, x , x α ) = e Φu(x|xα),Φv(x ) Ω e Φu(x|xα),Φt(x ) dt .<label>(6)</label></formula><p>This matching probability can be used in the same loss function (3) as before, with the only difference that now each sample depends on x, x as well as the auxiliary image x α .</p><p>Discussion. While this may seem a round-about way of learning correspondences, it has two key benefits: as eq. (3) encourages vectors to be invariant and distinctive; in addition to eq. (3), DVE also requires vectors to be compatible between different object instances. In fact, without such a compatibility, the reconstruction (5) would result in a distorted, unmatchable embedding vector. Note that the original formulation of <ref type="bibr" target="#b44">[45]</ref> lacks the ability to enforce this compatibility directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Using multiple auxiliary images</head><p>A potential issue with eq. (6) is that, while image x can be obtained from x by a synthetic warp so that all pixels can be matched, image x α is only weakly related to the two. For example, partial occlusions or out of plane rotations may cause some of the pixels in x to not have corresponding pixels in x α .</p><p>In order to overcome this issue, we take inspiration from the recent method of <ref type="bibr" target="#b58">[59]</ref> and consider not one, but a small set {x α : α ∈ A} of auxiliary images. Then, the summation in eq. (5) is extended not just over spatial locations, but also over images in this set. The intuition for this approach is that as long as at least one image in the auxiliary image set matches x sufficiently well, then the reconstruction will be reliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Using datasets of human faces (section 4.1), animal faces (section 4.3) and a toy robotic arm (section 4.4), we demonstrate the effectiveness of the proposed Descriptor Vector Exchange technique in two ways. First, we show that the learned embeddings work well as visual descriptors, matching reliably different views of an object instance. Second, we show that they also identify a dense family of object landmarks, valid not for one, but for all object instances in the same category. Note that, while the first property is in common with traditional and learned descriptors in the spirit of SIFT, the second clearly sets DVE embeddings apart from these. Implementation details. In order to allow for a comparison with the literature, we perform experiments with the deep neural network architecture of <ref type="bibr" target="#b44">[45]</ref> (which we refer to as SmallNet). Inspired by the success of the Hourglass model in <ref type="bibr" target="#b53">[54]</ref>, we also experiment with a more powerful hourglass design (we use the "Stacked Hourglass" design of <ref type="bibr" target="#b31">[32]</ref> with a single stack). The weights of both models are learned from scratch using the Adam optimiser <ref type="bibr" target="#b18">[19]</ref> for 100 epochs with an initial learning rate of 0.001 and without weight decay. Further details of the architectures are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Human faces</head><p>First, we consider two standard benchmark datasets of human faces: CelebA <ref type="bibr" target="#b24">[25]</ref> and MAFL <ref type="bibr" target="#b55">[56]</ref>, which is a subset of the former. The CelebA <ref type="bibr" target="#b24">[25]</ref> dataset contains over 200k faces of celebrities; we use the former for training and evaluate embedding quality on the smaller MAFL <ref type="bibr" target="#b55">[56]</ref> (19,000 train images, 1,000 test images). Annotations are provided for the eyes, nose and mouth corners. For training, we follow the same procedure used by <ref type="bibr" target="#b44">[45]</ref> and exclude any image in the CelebA training set that is also contained in the MAFL test set. Note that we use MAFL annotations only for evaluation and never for training of the embedding function.</p><p>We use formulation <ref type="bibr" target="#b5">(6)</ref> to learn a dense embedding function Φ mapping an image x to C-dimensional pixel embeddings, as explained above. Note that loss (3) requires sampling transformations g ∈ G; in order to allow a direct comparison with <ref type="bibr" target="#b44">[45]</ref>, we use the same random Thin Plate Spline (TPS) warps as they use, obtaining warped pairs (x, x = gx). We also sample at random one or more auxiliary images x α from the training set in order to implement DVE.</p><p>We consider several cases; in the first, we set C = 3 and sample no auxiliary images, using formulation <ref type="bibr" target="#b1">(2)</ref>, which is the same as <ref type="bibr" target="#b44">[45]</ref>. In the second case, we set C = 16, 32, 64 3 but still do not use DVE; in the last case, we use C = 3, 16, 32, 64 and also use DVE. Qualitative results. In <ref type="figure">fig. 4</ref> we compute 64D embeddings with SmallNet models trained with or without DVE on AFLW M images, visualising as in <ref type="figure">fig. 1 (left)</ref>. With DVE, matches are accurate despite large intra-class variations. Without DVE, embedding quality degrades significantly. This shows that, by having a category-wide validity, embeddings learned with DVE identify object landmarks rather than mere visual descriptors of local appearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4: Learning 64D descriptors without/with DVE</head><p>Matching results. Next, we explore the ability of the embeddings learned with SmallNet to match face images. We sample pairs of different identities using MAFL test (1000 pairs total) and consider two cases: First, we match images x, x of the same identity; since multiple images of the same identity are not provided, we generate them with warps as before, so that the ground-truth correspondence field g is known. We extract embeddings at the annotated keypoint positions from x and match them to their closest neighbour embedding in image x (searching all pixels in the target). Second, we match images of different identities, again using the annotations. In both cases, we report the mean pixel matching error from the ground truth. Examining the results in table 1 we note several facts. When matching the same identities, higher dimensional embeddings work better than lower (i.e. 3D), including in particular <ref type="bibr" target="#b44">[45]</ref>. This is expected as high dimensional embeddings more easily capture instance-specific details; also as expected, DVE does not change the results much as here there are no intra-class variations. When matching different identities, high-dimensional embeddings are rather poor: these descriptors are too sensitive to instance-specific   <ref type="bibr" target="#b5">[6]</ref>), † and ‡ use different training data: Vox-Celeb <ref type="bibr" target="#b29">[30]</ref> and VoxCeleb+ (the union of VoxCeleb and Vox-Celeb2 <ref type="bibr" target="#b1">[2]</ref>) respectively. details and cannot bridge intra-class variations correctly. This justifies the choice of a low dimensional embedding in <ref type="bibr" target="#b44">[45]</ref> as the latter clearly generalises better across instances. However, once DVE is applied, the performance of the high-dimensional embeddings is much improved, and is in fact better than the low-dimensional descriptors even for intra-class matching <ref type="bibr" target="#b44">[45]</ref>. Overall, the embeddings learned with DVE have both better intra-class and intra-instance matching performance than <ref type="bibr" target="#b44">[45]</ref>, validating our hypothesis and demonstrating that our method for regularising the embedding is preferable to simply constraining the embedding dimensionality.</p><p>Landmark regression. Next, as in <ref type="bibr" target="#b44">[45]</ref> and other recent papers, we assess quantitatively how well our embeddings correspond to manually-annotated landmarks in faces. For this, we follow the approach of <ref type="bibr" target="#b44">[45]</ref> and add on top of our embedding 50 filters of dimension 1 × 1 × C, converting them into the heatmaps of 50 intermediate virtual points; these heatmaps are in turn converted using a softargmax layer to 2C x-y pairs which are finally fed to a linear regressor to estimate manually annotated landmarks. The parameters of the intermediate points and linear regressor are learned using a certain number of manual annotations, but the signal is not back-propagated further so the embeddings remain fully unsupervised.</p><p>In detail, after pretraining both the SmallNet and Hourglass networks on the CelebA dataset in a unsupervised manner, we freeze its parameters and only learn the regressors for MAFL <ref type="bibr" target="#b55">[56]</ref>. We then follow the same methodology for the 68-landmark 300-W dataset <ref type="bibr" target="#b39">[40]</ref>, with 3148 training and 689 testing images. We also evaluate on the challenging AFLW <ref type="bibr" target="#b19">[20]</ref> dataset, under the 5 landmark setting. Two slightly different evaluation splits for have been used in prior work: one is the train/test partition of AFLW used in the works of <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b44">[45]</ref> which used the existing crops from MTFL <ref type="bibr" target="#b54">[55]</ref> and provides 2,995 faces for testing and 10,122 AFLW faces for training (we refer to this split as AFLW M ). The second is a set of re-cropped faces released by <ref type="bibr" target="#b53">[54]</ref>, which comprises 2991 test faces with 10,122 train faces (we refer to this split as AFLW R ). For both AFLW partitions, and similarly to <ref type="bibr" target="#b44">[45]</ref>, after training for on CelebA we continue with unsupervised pretraining on 10,122 training images from AFLW for 50 epochs (we provide an ablation study to assess the effect of this choice in section 4.2). We report the errors in percentage of inter-ocular distance in table 2 and compare our results to state-of-the-art supervised and unsupervised methods, following the protocol and data selection used in <ref type="bibr" target="#b44">[45]</ref> to allow for a direct comparison.</p><p>We first see that the proposed DVE method outperforms the prior work that either learns sparse landmarks <ref type="bibr" target="#b45">[46]</ref> or 3D dense feature descriptors <ref type="bibr" target="#b44">[45]</ref>, which is consistent with the results in table 1. Encouragingly, we also see that our method is competitive with the state-of-the-art unsupervised learning techniques across the different benchmarks, indicating that our unsupervised formulation can learn useful information for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablations</head><p>In addition to the study evaluating DVE presented in table 1, we conduct two additional experiments to investigate: (i) The sensitivity of the landmark regressor to a reduction in training annotations; (ii) the influence of additional unsupervised pretraining on a target dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limited annotation:</head><p>We evaluate how many image annotations our method requires to learn landmark localisation in the AFLW dataset, comparing to Dense3D <ref type="bibr" target="#b44">[45]</ref> (which shares the SmallNet backbone architecture). To do so, we vary the number of training images across the following range: 1, 5, 10, 20 and up to the whole training set <ref type="bibr" target="#b9">(10,</ref><ref type="bibr">122</ref> in total) and report the errors for each setting in <ref type="figure" target="#fig_2">fig. 5</ref>. For reference, we also include the supervised CNN baseline from <ref type="bibr" target="#b45">[46]</ref> (suppl. material), which consists of a slightly modified SmallNet (denoted SmallNet+ in <ref type="figure" target="#fig_2">fig. 5</ref>) to make it better suited for landmark regression. Where available, we report the mean and std. deviation over three randomly seeded runs. Further details of this experiment and the SmallNet+ architecture are provided in the suppl. material. While there is considerable variance for very small numbers of annotations, the results indicate that DVE can produce    Unsupervised finetuning: Next we assess the influence of using unsupervised finetuning of the embeddings on a given target dataset, immediately prior to learning to regress landmarks. To do so, we report the performance of several models with and without finetuning on both the AFLW M and 300W benchmarks in table 3. We see that for AFLW M , this approach (which can be achieved "for free" i.e. without collecting additional annotations) brings a boost in performance. However, it is less effective for 300W, particularly at higher dimensions, having no influence on the performance of the stronger hourglass model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Animal faces</head><p>To investigate the generalisation capabilities of our method, we consider learning landmarks in an unsupervised manner not just for humans, but for animal faces. To do this, we simply extend the set X of example image to contain images of animals as well.</p><p>In more detail, we consider the Animal Faces dataset <ref type="bibr" target="#b42">[43]</ref> with images of 20 animal classes and about 100 images per class. We exclude birds and elephants since these images have a significantly different appearance on average (birds profile, elephants include whole body). We then add additional 8609 additional cat faces from <ref type="bibr" target="#b52">[53]</ref>, 3506 cat and dog faces from <ref type="bibr" target="#b34">[35]</ref>, and 160k human faces from CelebA (but keep roughly the same distribution of animal classes per batch as the original dataset). We train SmallNet descriptors using DVE on this data. Here we also found it necessary to use the grouped attention mechanism (section 3.4) which relaxes DVE to project embeddings on a set of auxiliary images rather than just one. In order to do so, we include 16 pairs of images (x, x ) in each batch and we randomly choose a set of 5 auxiliary images for each pair from a separate pool of 16 images. Note that these images have also undergone synthetic warps. Results matching human and cat landmarks to other animals are shown in <ref type="figure" target="#fig_3">fig. 6</ref>. DVE achieves localisation of semantically-analogous parts across species, with excellent results particularly for the eyes and general facial region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Roboarm exchg</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>Dense3D Dense20D Dense20D DVE exchgexchg exchg <ref type="figure">Figure 7</ref>: An example of descriptor matching on a pair from the roboarm dataset, using blob centres in the first image to locate them in a second image. We show 3D/20D descriptors (columns 2/3) learned with the loss from <ref type="bibr" target="#b44">[45]</ref>. The high error of the 20D case is corrected by DVE (last column).</p><p>Lastly, we experimented on the animated robotic arm dataset ( <ref type="figure">fig. 7)</ref> introduced in <ref type="bibr" target="#b44">[45]</ref> to demonstrate the applicability of the approach to diverse data. This dataset contains around 24k images of resolution 90 × 90 with ground truth optical flow between frames for training. We use the same matching evaluation of section 4.1 using the centre of the robot's segments as keypoints for assessing correspondences. We compare models using 3D and 20D embeddings  using the formulation of <ref type="bibr" target="#b44">[45]</ref> with and without DVE, and finally removing transformation equivariance from the latter (by setting g = 1 in eq. <ref type="formula" target="#formula_7">(6)</ref>).</p><p>In this case there are no intra-class variations, but the high-degree of articulation makes matching non-trivial.</p><p>Without DVE, 20D descriptors are poor (10.34 error) whereas 3D are able to generalise (1.42). With DVE, however, the 20D descriptors (at 1.25 error) outperform the 3D ones (1.41). Interestingly, DVE is effective enough that even removing transformations altogether (by learning from pairs of identical images using g = 1) still results in good performance (1.42) -this is possible because matches must hop through the auxiliary image set x α which contains different frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We presented a new method that can learn landmark points in an unsupervised way. We formulated this problem in terms of finding correspondences between objects from the same or similar categories. Our method bridges the gap between two seemingly independent concepts: landmarks and local image descriptors. We showed that relatively high dimensional embeddings can be used to simultaneously match and align points by capturing instance-specific similarities as well as more abstract correspondences. We also applied this method to predict facial landmarks in standard computer vision benchmarks as well as to find correspondences across different animal species. <ref type="figure">Figure 8</ref>: Additional images querying manual annotations on a human and finding the matching descriptors on animal faces, images are selected randomly from the validation set and include some more severe failure cases (eg mouse mouth row 1 col 2, dog eye row 5 col 8).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>The effect of varying the number of annotated images used for different methods on AFLW M , incorporating the Supervised CNN baseline from<ref type="bibr" target="#b45">[46]</ref> (suppl. material).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Top: Five landmarks are manually annotated in the top-left image (human) and matched using our unsupervised embedding to a number of animals. Bottom: same process, but using a cat image (bottom left) as query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Landmark detection results on the MAFL, 300W and AFLW (AFLW M and ALFW R splits-see section 4.1 for details). The results are reported as percentage of inter- ocular distance. * report a more conservative evaluation metric (see</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The effect of unsupervised finetuning on landmark regression performance (errors reported as percentage of inter-ocular distance). Each table entry describes performance without/with finetuning. All methods use DVE.</figDesc><table /><note>effective landmark detectors with few manual annotations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Results on Roboarm, including an experiment ignoring optical flow (right).</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank Almut Sophia Koepke for helpful discussions. We are grateful to ERC StG IDIU-638009, EP/R03298X/1 and AWS Machine Learning Research Awards (MLRA) for support.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Animal Faces</head><p>Here we present additional qualitative results on the animal faces dataset in <ref type="figure">fig. 8</ref>. The leftmost column shows the human faces with their annotated landmarks (drawn as coloured circles) which are matched to a set of queried animal faces in the remaining columns. The correspondent matches are depicted in the same colour with the manual landmarks. We observe that our method achieves to find many semantically meaningful matches in spite of wide variation in the appearance across different species.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Roboarm details</head><p>We showed results for an experiment showing that the use of optical flow (ground truth flow in this dataset) may not be essential. In this setup we set x = x when training, meaning the same image is used rather than two consecutive frames, and the transformation is the identity g = 1. However we still explicitly ignore the background region (which is otherwise achieved by ignoring areas of zero flow). Our hope is that DVE, which has the effect of searching for a matching descriptor in a third image x α , will be able to stand in for explicit matches given by g. The results appear to confirm this. Surprisingly we can even obtain results matching <ref type="bibr" target="#b44">[45]</ref> without flow information, albeit using a higher dimensionality 20D descriptor. However the highest performance is still obtained by using the flow in addition to DVE, therefore we use flow (from synthetic warps) in experiments on faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Limited Annotation experiments</head><p>The numbers corresponding to the figure shown in section 4 of the paper portraying the effect of varying the number of annotated images are given in table 5. To allow the experiment to be reproduced, the list of the randomly sampled annotations is provided on the project page http://www.robots.ox.ac.uk/˜vgg/research/DVE.  <ref type="table">Table 5</ref>: Error (% inter-ocular distance) Varying the number of images used for training (AFLW M ). The errors are reported in the form (mean ± std.), where the statistics are computed three randomly seeded samples of annotations. The general indication is that most of the information has been encoded in the unsupervised stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Architecture details</head><p>The SmallNet architecture, which was used in in <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b44">45]</ref> consists of a set of seven convolutional layers with 20, 48, 64, 80, 256 before C filters are used to produce a C-dimensional embedding. The second, third and fourth layers are dilated by 2, 4 and 2 resp. Every convolutional layer except the last is followed by batch norm and a ReLU. Following the first convolutional layer, features are downsampled via a 2 × 2 max pool layer using a stride of 2. Consequently, for an input size of H × W × 3, the size of the output H 2 × W 2 × C. The SmallNet+ architecture, introduced in <ref type="bibr" target="#b45">[46]</ref>, is a slightly modified version of SmallNet, which further includes pooling layers with a stride of 2 after each of the first three convolutional layers, and operates on an input of size 64 × 64.</p><p>The Houglass architecture was introduced (in its "stacked" formulation) by <ref type="bibr" target="#b31">[32]</ref>. We use this network with a single stack, operating on inputs of size 96 × 96 and using preactivation residuals.</p><p>The code implementing the architectures used in this work can be found via the project page: http://www.robots. ox.ac.uk/˜vgg/research/DVE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Preprocessing details</head><p>For all datasets, the inputs to SmallNet are then resized to 100 × 100 pixels then centre-cropped to 70 × 70 pixels (this cropping is done after warping during training). The inputs to Hourglass are resized to 136 × 136 pixels then centre-cropped to 96 × 96 pixels. For the particular case of the CelebA face crops, which contain a good deal of surrounding context with varied backgrounds, faces are additionally preprocessed by removing the top 30 pixels and bottom 10 vertically from the 218 × 178 image before resizing. For 300-W we make the ground truth bounding box square (setting the height to equal the width) and then add more context on each side such that the original width occupies the central 52% of the resulting image.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Discriminative learning of local image descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Winder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Arsha Nagrani, and Andrew Zisserman. Voxceleb2: Deep speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><surname>Joon Son</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IN-TERSPEECH</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Active shape models: their training and application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Histograms of Oriented Gradients for Human Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Object Detection with Discriminatively Trained Part Based Models. PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wing loss for robust facial landmark localisation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Zhen-Hua Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2235" to="2245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Object class recognition by unsupervised scaleinvariant learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Average faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Grundl</surname></persName>
		</author>
		<ptr target="http://www.beautycheck.de/cmsms/index.php/durchschnittsgesichter" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Densereg: Fully convolutional dense shape regression in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Rıza Alp Güler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6799" to="6808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Proposal flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On sifts and their scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viki</forename><surname>Mayzels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1522" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial Transformer Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised learning of object landmarks through conditional image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning landmarks from unaligned data using image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02055</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-supervised feature learning by learning to spot artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Jenni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2733" to="2742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">WarpNet: Weakly supervised matching for single-view reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rotationnet: Joint object categorization and pose estimation using multiviews from unsupervised viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asako</forename><surname>Kanezaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshifumi</forename><surname>Nishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5010" to="5019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Collection flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV workshops</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Data driven image models through continuous joint alignment. PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Combined object categorization and segmentation with an implicit shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Bastian Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning covariant feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop on Geometry Meets Deep Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<title level="m">SIFT Flow: Dense correspondence across scenes and its applications. PAMI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Do convnets learn correspondence?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1601" to="1609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A comparison of affine region detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timor</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="43" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Compositional Model for Low-Dimensional Image Set Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Voxceleb: a large-scale speaker identification dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dynamicfusion: Reconstruction and tracking of non-rigid scenes in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven M</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-supervised learning of geometrically stable features through probabilistic introspection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3637" to="3645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning 3d object categories by looking around them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5218" to="5227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Local convolutional features with unsupervised training for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattis</forename><surname>Paulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Rasl: Robust alignment by sparse and low-rank decomposition for linearly correlated images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yigang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<idno>2012. 3</idno>
		<imprint>
			<publisher>PAMI</publisher>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Wide baseline stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pritchett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="754" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Convolutional neural network architecture for geometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6148" to="6157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR-W</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Self-supervised visual descriptor learning for dense correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanner</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="420" to="427" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rıza Alp Güler, Dimitris Samaras, Nikos Paragios, and Iasonas Kokkinos. Deforming autoencoders: Unsupervised disentangling of shape and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Sahasrabudhe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning hybrid image templates (HIT) by information projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangzhang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dense point trajectories by gpu-accelerated large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayanan</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised learning of object frames by dense equivariant image labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thewlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised learning of object landmarks by factorized spatial embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thewlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A fast local descriptor for dense matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engin</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Matching of affinely invariant regions for visual servoing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Luk D&amp;apos;haene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1601" to="1606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of a facial attribute embedding from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Koepke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Robust Facial Landmark Detection via Recurrent Attentive-Refinement Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengtao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjiang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashraf</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Disambiguating visual relations using loop constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Klopschitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Cat head detection -How to effectively exploit shape and texture features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of object landmarks as structural representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Learning Deep Representation for Face Alignment with Auxiliary Attributes. PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Flowweb: Joint image set alignment by weaving consistent, pixel-wise correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyosha A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1191" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning dense correspondence via 3d-guided cycle consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="117" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Attend in groups: a weakly-supervised deep learning framework for learning from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1878" to="1887" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
