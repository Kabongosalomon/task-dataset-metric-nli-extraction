<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How Robust is 3D Human Pose Estimation to Occlusion?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">István</forename><surname>Sárándi</surname></persName>
							<email>sarandi@vision.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Visual Computing Institute</orgName>
								<orgName type="institution" key="instit2">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timm</forename><surname>Linder</surname></persName>
							<email>timm.linder@de.bosch.com</email>
							<affiliation key="aff1">
								<orgName type="department">Corporate Research</orgName>
								<orgName type="institution">Robert Bosch GmbH</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Corporate Research</orgName>
								<orgName type="institution">Robert Bosch GmbH</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
							<email>leibe@vision.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Visual Computing Institute</orgName>
								<orgName type="institution" key="instit2">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">How Robust is 3D Human Pose Estimation to Occlusion?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Occlusion is commonplace in realistic human-robot shared environments, yet its effects are not considered in standard 3D human pose estimation benchmarks. This leaves the question open: how robust are state-of-the-art 3D pose estimation methods against partial occlusions? We study several types of synthetic occlusions over the Hu-man3.6M dataset and find a method with state-of-the-art benchmark performance to be sensitive even to low amounts of occlusion. Addressing this issue is key to progress in applications such as collaborative and service robotics. We take a first step in this direction by improving occlusionrobustness through training data augmentation with synthetic occlusions. This also turns out to be an effective regularizer that is beneficial even for non-occluded test cases.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>To collaborate with humans and to understand their actions, collaborative and service robots need the ability to reason about human pose in 3D space. An important challenge in realistic environments is that humans are often only seen partially, e.g., standing behind machine parts or carrying objects in front of the body (see <ref type="figure">Fig. 1</ref>). Robust robotics solutions need to handle such disturbances gracefully and make use of the visual cues still present in the scene to reason around the occlusion.</p><p>Although recent years have brought significant advances in 3D human pose estimation, as measured on standard computer vision benchmarks such as Human3.6M <ref type="bibr" target="#b10">[11]</ref> <ref type="bibr" target="#b1">[2]</ref>, the behavior of models under occlusion remains largely unexplored, as the benchmarks do not systematically model occlusion effects.</p><p>To our knowledge, we present the first systematic study of various types of test-time (synthetic) occlusions in 3D human pose estimation from a single RGB image. As we <ref type="bibr">Figure 1</ref>. Example of partial occlusions in the context of shared human-robot workspaces. Note how easily we humans can guess the rough pose of the person behind the occlusion. Can current 3D human pose estimation methods do that as well? will see, ignoring the aspect of occlusions may cause model accuracy to rapidly deteriorate, even under mild occlusion levels, despite the good benchmark performance. Such sudden and unexpected failures in the robot's perception would prevent smooth and comfortable human-robot interaction and may lead to safety hazards. Furthermore, we demonstrate that simple occlusion data augmentation during training increases model robustness. This augmentation also improves performance even for non-occluded test images. Our approach is efficient and suitable for high frame-rate applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D Human Pose Estimation. 3D human pose estimation has seen rapid progress in recent years. For a thorough overview of approaches, we refer the reader to Sarafianos et al.'s survey <ref type="bibr" target="#b19">[20]</ref>. Current state-of-the-art methods use deep neural networks, either directly on the input image or on the output of a 2D pose estimator. Based on the sweep- <ref type="figure">Figure 2</ref>. Examples of the applied geometric occlusions: circles, a single rectangle <ref type="bibr" target="#b25">[26]</ref>, rectangles, oriented bars. See <ref type="figure" target="#fig_0">Fig. 3</ref> for an example with Pascal VOC objects.</p><p>ing success of heatmap-based representations in 2D human pose estimation (e.g., <ref type="bibr" target="#b14">[15]</ref>), heatmaps have recently been also adopted in 3D methods, including volumetric <ref type="bibr" target="#b17">[18]</ref> <ref type="bibr" target="#b22">[23]</ref> and marginal heatmaps <ref type="bibr" target="#b15">[16]</ref>.</p><p>Occlusions, Erasing and Copy-Pasting. In a pre-deep learning study based on silhouettes and HOG features, Huang et al. tackled occlusions in 3D pose estimation from RGB <ref type="bibr" target="#b9">[10]</ref>, but their analysis was limited to walking actions and occlusions with two rectangles. Occlusion effects have also been studied in 3D pose estimation from depth input <ref type="bibr" target="#b18">[19]</ref>, where exploiting semantic information from the occluder itself was found to improve predictions.</p><p>Data augmentation by erasing a rectangular block from the input has recently been concurrently investigated under the names Random Erasing <ref type="bibr" target="#b25">[26]</ref> and Cutout <ref type="bibr" target="#b2">[3]</ref>, for image classification, object detection, and person re-identification. Similarly, synthetically placing objects into a scene by image-level copy-pasting has been shown to help object detection <ref type="bibr" target="#b4">[5]</ref>[4] <ref type="bibr" target="#b6">[7]</ref>. However, those methods are trained to detect these pasted objects, while in our case the task is to infer what lies behind them. Ke et al. <ref type="bibr" target="#b11">[12]</ref> augment training images for 2D human pose estimation by copying background patches over some of the body joints. Research on facial landmark localization has investigated and modeled occlusions for a long time <ref type="bibr" target="#b0">[1]</ref> <ref type="bibr" target="#b7">[8]</ref>, including augmenting training images with randomly pasted occluding objects <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this paper we study the effect of occlusion on the accuracy of 3D human pose estimation. To this end, we have devised a 3D pose estimation approach that reaches stateof-the-art benchmark performance, leading us to expect that the observations drawn from our experiments also transfer to other models. Architecture. We use a fully convolutional net to predict volumetric body joint heatmaps from the input RGB image, based on a ResNet-50 <ref type="bibr" target="#b8">[9]</ref> backbone architecture. After discarding the global average pooling layer, we adjust the number of output channels of the ResNet to be the product of the number of joints and the number of heatmapvoxels along the depth axis. Reshaping the resulting tensor yields the volumetric heatmaps. Nominal stride and depth discretization are configured to yield heatmaps of size 16 × 16 × 16 for an image of size 256 × 256. Given the volumetric heatmap, coordinate predictions are obtained using soft-argmax <ref type="bibr" target="#b12">[13]</ref>[17] <ref type="bibr" target="#b22">[23]</ref>. As in <ref type="bibr" target="#b17">[18]</ref>, the x and y coordinates are interpreted as image space coordinates, while z is the depth of the particular joint relative to the root (pelvis) joint depth, with the 16 voxels covering 2 meters. In order to concentrate on the aspect of articulated pose, as opposed to person localization, we assume that the true root joint depth is given by an oracle at test time. The coordinates are back-projected to camera space using the known camera intrinsics. Finally, the L 1 loss is computed on the predicted and ground truth 3D coordinates in camera space. Since all of the preceding operations are differentiable, the network can be trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>Dataset. Human3.6M <ref type="bibr" target="#b10">[11]</ref> is the largest public 3D pose estimation dataset. It contains 11 subjects imitating 15 actions in a controlled indoor environment while being recorded with 4 cameras and a motion capture system. Following the most common experimental protocol in the literature, we use five subjects (S1, S5, S6, S7, S8) for training and two (S9, S11) for testing. We train action-agnostic models, as opposed to action-specific ones.</p><p>Dataset Subsampling. To reduce the redundancy in training poses, we adaptively subsample the frames similarly to <ref type="bibr" target="#b13">[14]</ref>, only keeping a frame when at least one body joint moves by at least 30 mm compared to the last kept frame.</p><p>For the test set we follow prior work and use every 64 th frame.</p><p>Image Preprocessing. Before feeding an image to the network, we center and zoom it on the person, at a resolution of 256 × 256 px. To ensure correct perspective (with the principal point at the image center), we reproject the image onto a virtual camera pointed at the center of the person's bounding box, as provided in the dataset. Scaling is applied so that the larger side of the person's bounding box covers about 80% of the image side length. Common data augmentation techniques are used in training, including random rotation, scaling, translation, horizontal flipping, as well as image filtering such as color distortions and blurs.</p><p>Evaluation Metrics. Following standard practice on Hu-man3.6M, we evaluate prediction accuracy by the so-called mean per joint position error (MPJPE), which is the mean Euclidean error of all joints after skeleton alignment at the root (pelvis) joint. Procrustes alignment is not used.</p><p>Synthetic Occlusions for Robustness Analysis. We consider solid black shapes and some more realistic object segments from the Pascal VOC 2012 dataset <ref type="bibr" target="#b5">[6]</ref> as occluders in this study (see <ref type="figure" target="#fig_0">Fig. 2 and 3</ref>). The number, position and size of the objects are generated at random. We define the degree of occlusion as the percentage of occluded pixels inside the person's bounding box and vary this quantity between 0% and 70%.</p><p>Occlusion-Augmented Training. We hypothesize that synthetic occlusion data augmentation during training can improve test-time occlusion-robustness. To verify this, we use the same kinds of occlusions as described in the previous section, with an additional mixture variant, which uses one of the other types at random for each frame. We make sure to strictly separate the VOC objects used for training and testing. Furthermore, we try the RE-0 variant of Zhong et al.'s random erasing <ref type="bibr" target="#b25">[26]</ref>, generating a single occluding black rectangle of random size according to their pseudocode. We refer to this mode as single rectangle in this paper.</p><p>To make these strategies comparable, we parameterize them such that the distribution of the number of occluded pixels is similar. Notably, we only apply these augmentations with 50% probability for each frame. This was found important in prior work on occlusion augmentation <ref type="bibr" target="#b2">[3]</ref>. Implementation Details. We use the implementation of ResNet-50v1 and the corresponding ImageNet-pretrained initial weights from the TensorFlow-Slim library <ref type="bibr" target="#b20">[21]</ref>. Training is done with the Adam optimizer and a mini-batch size of 64, for 40 epochs, taking approximately 24 hours on an NVIDIA GeForce Titan X (Pascal) GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We start presenting our results by showing that our baseline model has state-of-the-art performance. We then show how performance deteriorates with test-time occlusions and that this can be mitigated using occlusion data augmentation. The augmentations are then shown to help even when the test images do not contain synthetic occlusions.</p><p>Baseline Performance. The current state-of-the-art among published methods which use no extra 2D pose datasets for training is by Pavlakos et al. <ref type="bibr" target="#b17">[18]</ref> (see <ref type="table" target="#tab_0">Table 1</ref>).  Robustness Analysis under Occlusion. We evaluate the robustness of our baseline model using different degrees and types of occlusions (see the top left plot of <ref type="figure" target="#fig_1">Fig. 4</ref>). We observe that circular occlusions cause by far the largest increase in error, the reason for which needs further investigation. Occlusions with oriented bars, VOC objects and rectangles lead to comparable performance loss. We note that rectangles are the least problematic type of occlusion, despite being a widely used test case in the literature. <ref type="figure" target="#fig_0">Fig. 3</ref> visualizes an example. The baseline network gives good predictions for the unoccluded case, but when we paste two Pascal VOC objects onto the image, prediction visibly fails for the affected limbs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Augmentation Improves Occlusion-Robustness.</head><p>We now turn to the evaluation of occlusion augmentation at training time for increased test-time occlusion-robustness. <ref type="figure" target="#fig_1">Fig. 4 and 5</ref> show the results. Erasing a single rectangle (as in <ref type="bibr" target="#b25">[26]</ref>) results in robustness against multiple rectangles at test time, but is much less effective for the other types of occlusions, being most sensitive to circles. Using several rectangles during training works slightly better than single-rectangle random erasing, but it, too, has difficulty in generalizing to other types of occlusion structures. Circular occlusion augmentation generalizes to all other simple geometric occlusion shapes, but barely helps when more realistic VOC objects are used as occluders at test time. VOCaugmentation, however, does generalize to both simple geometric shapes and other VOC objects (the objects used in training and testing are strictly separated). The qualitative difference in robustness when using this augmentation type  is illustrated in <ref type="figure" target="#fig_0">Fig. 3</ref>. The network learned to use context cues and gives good prediction even for the almost fullyoccluded lower left leg. Finally, the combination of all these strategies proves to be effective against all of the analyzed occlusion types together.</p><p>The Regularizing Effect of Occlusion Augmentation. In the previous section we have seen that training-time occlusion augmentation is helpful when evaluating on occluded test examples. Let us now look at the effect of these augmentation schemes when evaluating on the original test data without synthetic occlusions (see <ref type="table" target="#tab_0">Table 1</ref>). All occlusion augmentation strategies are found to improve upon the baseline result, with the VOC objects performing the best and bars the worst.</p><p>Runtime. Inference of the whole pipeline runs at 64, 165, and 204 fps for batch sizes of 1, 8, and 64 images, respectively, on a single NVIDIA GeForce Titan X (Pascal) GPU. This makes the method suitable for high frame rate applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented a systematic study of occlusion effects on 3D human pose estimation from a single RGB image, using an efficient ResNet-based test model. We found that despite producing state-of-the-art benchmark results, the network's performance quickly drops when synthetic occlusions are added. Circular structures turned out to be particularly problematic, the reason of which needs further study. We then showed that training-time occlusion data augmentation is effective in reducing occlusion-induced errors, while also improving the performance without test-time occlusions.</p><p>Future experiments should also target other datasets besides Human3.6M and it remains to be seen how well our findings about synthetic occlusions generalize to real ones.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Prediction change in the presence of synthetic test-time occlusion. Ground truth is shown with grey dashed lines, predictions with colorful ones. The baseline model fails to predict the pose of the occluded limbs, while the model trained with occlusion augmentation behaves more robustly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Assessing occlusion-robustness on Human3.6M. Each subplot shows the performance when training with a particular augmentation method. Within a subplot, each line shows the mean and standard deviation of MPJPE under increasing degrees of occlusion of a particular type. Sun et al.'s (unpublished) method achieves an MPJPE of 64.1 mm [23], but it is unclear whether they use the known root joint depth or resolve scale ambiguity by other means.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>n o n e s in g le re c t. re c ta n g le s c ir c le s b a rs V O C o b je c ts m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Exploring how much each type of training-time data augmentation protects against each type of test occlusions. The numbers are the MPJPE averaged for degrees of occlusion between 10% and 50%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Direct Discuss Eat Greet Phone Photo Pose Purch. Sit SitD Smoke Wait Walk WalkD WalkT Avg Zhou [28] 87.4 109.3 87.1 103.2 116.2 139.5 106.9 99.8 124.5 199.2 107.4 118.1 79.4 114.2 97.7 113.0 Tekin [24] 102.4 147.7 88.8 125.4 118.0 182.7 112.4 129.2 138.9 224.9 118.4 138.8 55.1 126.3 65.8 125.0 Zhou [27] 91.8 102.4 97.0 98.8 113.4 125.2 90.0 93.8 132.2 159.0 106.9 94.4 79.0 126.0 99.0 107.3 Sun [22] 90.2 95.5 82.3 85.0 87.1 94.5 87.9 93.4 100.3 135.4 91.4 87.3 78.0 90.4 86.5 92.4 Sun [23] (ArXiv) 63.8 64.0 56.9 64.8 62.1 70.4 59.8 60.1 71.6 91.7 60.9 65.1 51.3 Mean per joint position error on Human3.6M for methods using no extra pose datasets in training. Methods below the line have access to the ground-truth root joint depth at test-time. (No synthetic occlusions are used on the test inputs.)</figDesc><table><row><cell>63.2</cell><cell>55.4 64.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Since our evaluation assumes knowledge of the root joint depth at test time, we compare with Pavlakos et al.'s performance under the same conditions, for which the results can be found in their supplementary material. Our baseline's MPJPE of 63.3 mm is already better than Pavlakos et al.'s 64.8.</figDesc><table><row><cell>500</cell><cell>Baseline</cell><cell>Trained with 'single rectangle' aug.</cell><cell>Trained with 'circles' aug.</cell></row><row><cell>400</cell><cell></cell><cell></cell><cell></cell></row><row><cell>300</cell><cell></cell><cell></cell><cell></cell></row><row><cell>200</cell><cell></cell><cell></cell><cell></cell></row><row><cell>100</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>100 200 300 400 500</cell><cell>Trained with 'bars' aug.</cell><cell>Trained with 'VOC objects' aug.</cell><cell>Trained with 'mixture' aug. circles single rect. rectangles bars VOC objects</cell></row><row><cell>0</cell><cell>0% 20% 40% 60%</cell><cell>0% 20% 40% 60% Degree of occlusion</cell><cell>0% 20% 40% 60%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This project has been funded by a grant from the Bosch Research Foundation and by ILIAD (H2020-ICT-2016-732737).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent structured models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Catalin Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Modeling visual context is key to augmenting object detection datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07428</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cut, paste and learn: Surprisingly easy synthesis for instance detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge 2012 (voc2012) development kit. Pattern Analysis, Statistical Modelling and Computational Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Synthesizing training data for object detection in indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Georgakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07836</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Occlusion coherence: Localizing occluded faces with a hierarchical deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Estimating human pose from occluded images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multiscale structure-aware network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09894</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end training of deep visuomotor policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1334" to="1373" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prendergast</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01484</idno>
		<title level="m">3d human pose estimation with 2d marginal heatmaps</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prendergast</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07372</idno>
		<title level="m">Numerical coordinate regression with convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A semantic occlusion model for human pose estimation from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3d human pose estimation: A review of the literature and analysis of covariates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Tensorflowslim image classification model library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08229</idno>
		<title level="m">Integral human pose regression</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Direct prediction of 3d body poses from motion compensated sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An occluded stacked hourglass approach to facial landmark localization and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intel. Veh</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<title level="m">Random erasing data augmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
