<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Statistical Machine Translation Draft of Chapter 13: Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="201722-09-25">September 25, 2017 22 Sep 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Speech and Language Processing Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Statistical Machine Translation Draft of Chapter 13: Neural Machine Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="201722-09-25">September 25, 2017 22 Sep 2017</date>
						</imprint>
					</monogr>
					<note>1st public draft August 7, 2015 2nd public draft 2</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.1">A Short History</head><p>Already during the last wave of neural network research in the 1980s and 1990s, machine translation was in the sight of researchers exploring these methods <ref type="bibr" target="#b91">(Waibel et al., 1991)</ref>. In fact, the models proposed by <ref type="bibr" target="#b25">Forcada and Ñeco (1997)</ref> and <ref type="bibr" target="#b7">Castaño et al. (1997)</ref> are striking similar to the current dominant neural machine translation approaches. However, none of these models were trained on data sizes large enough to produce reasonable results for anything but toy examples. The computational complexity involved by far exceeded the computational resources of that era, and hence the idea was abandoned for almost two decades.</p><p>During this hibernation period, data-driven approaches such as phrase-based statistical machine translation rose from obscurity to dominance and made machine translation a useful tool for many applications, from information gisting to increasing the productivity of professional translators.</p><p>The modern resurrection of neural methods in machine translation started with the integration of neural language models into traditional statistical machine translation systems. The pioneering work by <ref type="bibr" target="#b66">Schwenk (2007)</ref> showed large improvements in public evaluation campaigns. However, these ideas were only slowly adopted, mainly due to computational concerns. The use of GPUs for training also posed a challenge for many research groups that simply lacked such hardware or the experience to exploit it.</p><p>Moving beyond the use in language models, neural network methods crept into other components of traditional statistical machine translation, such as providing additional scores or extending translation tables <ref type="bibr" target="#b69">(Schwenk, 2012;</ref>, reordering <ref type="bibr" target="#b44">(Kanouchi et al., 2016;</ref><ref type="bibr">Li 5 et al., 2014)</ref> and pre-ordering models <ref type="bibr" target="#b16">(de Gispert et al., 2015)</ref>, and so on. For instance, the joint translation and language model by <ref type="bibr" target="#b17">Devlin et al. (2014)</ref> was influential since it showed large quality improvements on top of a very competitive statistical machine translation system.</p><p>More ambitious efforts aimed at pure neural machine translation, abandoning existing statistical approaches completely. Early steps were the use of convolutional models <ref type="bibr" target="#b43">(Kalchbrenner and Blunsom, 2013)</ref> and sequence-to-sequence models <ref type="bibr" target="#b12">Cho et al., 2014)</ref>. These were able to produce reasonable translations for short sentences, but fell apart with increasing sentence length. The addition of the attention mechanism finally yielded competitive results <ref type="bibr" target="#b1">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b39">Jean et al., 2015b)</ref>. With a few more refinements, such as byte pair encoding and back-translation of target-side monolingual data, neural machine translation became the new state of the art.</p><p>Within a year or two, the entire research field of machine translation went neural. To give some indication of the speed of change: At the shared task for machine translation organized by the Conference on Machine Translation (WMT), only one pure neural machine translation system was submitted in 2015. It was competitive, but outperformed by traditional statistical systems. A year later, in 2016, a neural machine translation system won in almost all language pairs. In 2017, almost all submissions were neural machine translation systems.</p><p>At the time of writing, neural machine translation research is progressing at rapid pace. There are many directions that are and will be explored in the coming years, ranging from core machine learning improvements such as deeper models to more linguistically informed models. More insight into the strength and weaknesses of neural machine translation is being gathered and will inform future work.</p><p>There is an extensive proliferation of toolkits available for research, development, and deployment of neural machine translation systems. At the time of writing, the number of toolkits is multiplying, rather than consolidating. So, it is quite hard and premature to make specific recommendations. Nevertheless, some of the promising toolkits are:</p><p>• Nematus (based on Theano): https://github.com/EdinburghNLP/nematus </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.2">Introduction to Neural Networks</head><p>A neural network is a machine learning technique that takes a number of inputs and predicts outputs. In many ways, they are not very different from other machine learning methods but have distinct strengths. <ref type="figure" target="#fig_1">Figure 13</ref>.1: Graphical illustration of a linear model as a network: feature values are input nodes, arrows are weights, and the score is an output node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.2.1">Linear Models</head><p>Linear models are a core element of statistical machine translation. A potential translation x of a sentence is represented by a set of features h i (x). Each feature is weighted by a parameter λ i to obtain an overall score. Ignoring the exponential function that we used previously to turn the linear model into a log-linear model, the following formula sums up the model.</p><formula xml:id="formula_0">score(λ, x) = j λ j h j (x) (13.1)</formula><p>Graphically, a linear model can be illustrated by a network, where feature values are input nodes, arrows are weights, and the score is an output node (see <ref type="figure" target="#fig_1">Figure 13</ref>.1).</p><p>Most prominently, we use linear models to combine different components of a machine translation system, such as the language model, the phrase translation model, the reordering model, and properties such as the length of the sentence, or the accumulated jump distance between phrase translations. Training methods assign a weight value λ i to each such feature h i (x), related to their importance in contributing to scoring better translations higher. In statistical machine translation, this is called tuning.</p><p>However, linear models do not allow us to define more complex relationships between the features. Let us say that we find that for short sentences the language model is less important than the translation model, or that average phrase translation probabilities higher than 0.1 are similarly reasonable but any value below that is really terrible. The first hypothetical example implies dependence between features and the second example implies non-linear relationship between the feature value and its impact on the final score. Linear models cannot handle these cases.</p><p>A commonly cited counter-example to the use of linear models is XOR, i.e., the boolean operator ⊕ with the truth table 0 ⊕ 0 = 0, 1 ⊕ 0 = 1, 0 ⊕ 1 = 1, and 1 ⊕ 1 = 0. For a linear model with two features (representing the inputs), it is not possible to come up with weights that give the correct output in all cases. Linear models assume that all instances, represented as points in the feature space, are linearly separable. This is not the case with XOR, and may not be the case for type of features we use in machine translation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.2.2">Multiple Layers</head><p>Neural networks modify linear models in two important ways. The first is the use of multiple layers. Instead of computing the output value directly from the input values, a hidden layer is introduced. It is called hidden, because we can observe inputs and outputs in training instances, but not the mechanism that connects them -this use of the concept hidden is similar to its meaning in hidden Markov models. See <ref type="figure" target="#fig_1">Figure 13</ref>.2 for on illustration. The network is processed in two steps. First, a linear combination of weighted input node is computed to produce each hidden node value. Then a linear combination of weighted hidden nodes is computed to produce each output node value.</p><p>At this point, let us introduce mathematical notations from the neural network literature. A neural network with a hidden layer consists of Note that we snuck in the possibility of multiple output nodes y k , although our figures so far only showed one. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.2.3">Non-Linearity</head><p>If we carefully think about the addition of a hidden layer, we realize that we have not gained anything so far to model input/output relationships. We can easily do away with the hidden layer by multiplying out the weights</p><formula xml:id="formula_1">y k = j h j u kj = j i x i w ji u kj = i x i   j u kj w ji   (13.4)</formula><p>Hence, a salient element of neural networks is the use of a non-linear activation function. After computing the linear combination of weighted feature values s j = i x i w ji , we obtain the value of a node only after applying such a function h j = f (s j ).</p><p>Popular choices are the hyperbolic tangent tanh(x) and the logistic function sigmoid(x). See <ref type="figure" target="#fig_1">Figure 13</ref>.3 for more details on these functions. A good way to think about these activation functions is that they segment the range of values for the linear combination s j into • a segment where the node is turned off (values close to 0 for tanh, or -1 for sigmoid)</p><p>• a transition segment where the node is partly turned on • a segment where the node is turned on (values close to 1) A different popular choice is the activation function for the rectified linear unit (ReLU). It does not allow for negative values and floors them at 0, but does not alter the value of positive values. It is simpler and faster to compute than tanh(x) or sigmoid(x).</p><p>You could view each hidden node as a feature detector. For a certain configurations of input node values, it is turned on, for others it is turned off. Advocates of neural networks claim that the use of hidden nodes obviates (or at least drastically reduces) the need for feature engineering: Instead of manually detecting useful patterns in input values, training of the hidden nodes discovers them automatically.</p><p>We do not have to stop at a single hidden layer. The currently fashionable name deep learning for neural networks stems from the fact that often better performance can be achieved by deeply stacking together layers and layers of hidden nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.2.4">Inference</head><p>Let us walk through neural network īnference (i.e., how output values are computed from input values) with a concrete example. Consider the neural network in <ref type="figure" target="#fig_1">Figure 13</ref>.4. This network has one additional innovation that we have not presented so far: bias units. These are nodes that always have the value 1. Such bias units give the network something to work with in the case that all input values are 0. Otherwise, the weighted sum s j would be 0 no matter the weights.</p><p>Let us use this neural network to process some input, say the value 1 for the first input node x 0 and 0 for the second input node x 1 . The value of the bias input node (labelled x 2 ) is fixed to 1. To compute the value of the first hidden node h 0 , we have to carry out the following calculation. The calculations for the other nodes are summarized in <ref type="table" target="#tab_0">Table 13</ref>.1. The output value in node y 0 for the input (0,1) is 0.743. If we expect binary output, we would understand this result as the value 1, since it is over the threshold of 0.5 in the range of possible output values [0;1].</p><p>Here, the output for all possible binary inputs:</p><p>Layer Node Summation Activation hidden h 0 1 × 3 + 0 × 4 + 1 × −2 = 1 0.731 hidden h 1 1 × 2 + 0 × 3 + 1 × −4 = −2 0.119 output y 0 0.731 × 5 + 0.119 × −5 + 1 × −2 = 1.060 0.743 Our neural network computes XOR. How does it do that? If we look at the hidden nodes h 0 and h 1 , we notice that h 0 acts like the Boolean OR: Its value is high if at least of the two input values is 1 (h 0 = 0.881, 0.731, 0.993, for the three configurations), it otherwise has a low value (0.119). The other hidden node h 1 acts like the Boolean AND -it only has a high value (0.731) if both inputs are 1. XOR is effectively implemented as the subtraction of the AND from the OR hidden node.</p><p>Note that the non-linearity is key here. Since the value for the OR node h 0 is not that much higher for the input of (1,1) opposed to a single 1 in the input (0.993 vs. 0.881 and 0.731), the distinct high value for the AND node h 1 in this case (0.731) manages to push the final output y 0 below the threshold. This would not be possible if the values of the inputs would be simply summed up as in linear models.</p><p>As mentioned before, recently the use of the name deep learning for neural networks has become fashionable. It emphasizes that often higher performance can be achieved by using networks with multiple hidden layers. Our XOR example hints at where this power comes from. With a single input-output layer network it is possible to mimic basic Boolean operations such as AND and OR since they can be modeled with linear classifiers. XOR can be expressed as x AND y − x OR y, and our neural network example implements the Boolean operations AND and OR in the first layer, and the subtraction in the second layer. For functions that require more intricate computations, more operations may be chained together, and hence a neural network architecture with more hidden layers may be needed. It may be possible (with sufficient training data) to build neural networks for any computer program, if the number of hidden layers matches the depth of the computation. There is a line of research under the banner neural Turing machines that explores what kind of architectures are needed to implement basic algorithms <ref type="bibr" target="#b30">(Gemici et al., 2017)</ref>. For instance, a neural network with two hidden layers is sufficient to implement an algorithm that sorts n-bit numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.2.5">Back-Propagation Training</head><p>Training neural networks requires the optimization of weight values so that the network predicts the correct output for a set of training examples. We repeatedly feed the input from the  <ref type="figure" target="#fig_1">Figure 13</ref>.5: Gradient descent training: We compute the gradient with regard to every dimension. In this case the gradient with respect to weight w 2 smaller than the gradient with respect to the weight w 1 , so we move more to the left than down (note: arrows point in negative gradient direction, pointing to the minimum).</p><p>training examples into the network, compare the computed output of the network with the correct output from the training example, and update the weights. Typically, several passes over the training data are carried out. Each pass over the data is called an epoch.</p><p>The most common training method for neural networks is called back-propagation, since it first updates the weights to the output layer, and propagates back error information to earlier layers. Whenever a training example is processed, then for each node in the network, an error term is computed which is the basis for updating the values for incoming weights.</p><p>The formulas used to compute updated values for weights follows principles of gradient descent training. The error for a specific node is understood as a function of the incoming weights. To reduce the error given this function, we compute the gradient of the error function with respect to each of the weights, and move against the gradient to reduce the error.</p><p>Why is moving alongside the gradient a good idea? Consider that we optimize multiple dimensions at the same time. If you are looking for the lowest point in an area (maybe you are looking for water in a desert), and the ground falls off steep to the west of you, and also slightly south of you, then you would go in a direction that is mainly west -and only slightly south. In other words, you go alongside the gradient. See <ref type="figure" target="#fig_1">Figure 13</ref>.5 for an illustration.</p><p>In the following two sections, we will derive the formulae for updating weights for our example network. If you are less interested in the why and more in the how, you can skip these sections and continue reading when we summarize the update formulae on page 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weights to the output nodes</head><p>Let us first review and extend our notation. At an output node y i , we first compute a linear combination of weight and hidden node values.</p><formula xml:id="formula_2">s i = j w i←j h j (13.6)</formula><p>This sum s i is passed through an activation function such as sigmoid to compute the output value y. y i = sigmoid(s i ) (13.7)</p><p>We compare the computed output values y i against the target output values t i from the training example. There are various ways to compute an error value E from these values. Let us use the L2 norm. E = i 1 2 (t i − y i ) 2 (13.8)</p><p>As we stated above, our goal is to compute the gradient of the error E with respect to the weights w k to find out in which direction (and how strongly) we should move the weight value. We do this for each weight w k separately. We first break up the computation of the gradient into three steps, essentially unfolding the Equations 13.6 to 13.8.</p><formula xml:id="formula_3">dE dw i←j = dE dy i dy i ds i ds i dw i←j (13.9)</formula><p>Let us now work through each of these three steps.</p><p>• Since we defined the error E in terms of the output values y i , we can compute the first component as follows.</p><formula xml:id="formula_4">dE dy i = d dy i 1 2 (t i − y i ) 2 = −(t i − y i ) (13.10)</formula><p>• The derivative of the output value y i with respect to s i (the linear combination of weight and hidden node values) depends on the activation function. In the case of sigmoid, we have:</p><formula xml:id="formula_5">dy i ds i = d sigmoid(s i ) ds i = sigmoid(s i )(1 − sigmoid(s i )) = y i (1 − y i ) (13.11)</formula><p>To keep our treatment below as general as possible and not commit to the sigmoid as an activation function, we will use the shorthand y i for dy i ds i below. Note that for any given training example and any given differentiable activation function, this value can always be computed.</p><p>• Finally, we compute the derivative of s i with respect to the weight w i←j , which turns out to be quite simply the value to the hidden node h j .</p><formula xml:id="formula_6">ds dw i←j = d dw i←j j w i←j h j = h j (13.12)</formula><p>Where are we? In Equations 13.10 to 13.12, we computed the three steps needed to compute the gradient for the error function given the unfolded laid out in Equation 13.9. Putting it all together, we have</p><formula xml:id="formula_7">dE dw i←j = dE dy i dy i ds i ds dw i←j = −(t i − y i ) y i h j (13.13)</formula><p>Factoring in a learning rate µ gives us the following update formula for weight w i←j . Note that we also remove the minus sign, since we move against the gradient towards the minimum.</p><formula xml:id="formula_8">∆w i←j = µ(t i − y i ) y i h j</formula><p>It is useful to introduce the concept of an error term δ i . Note that this term is associated with a node, while the weight updates concern weights. The error term has to be computed only once for the node, and it can be then used for each of the incoming weights.</p><formula xml:id="formula_9">δ i = (t i − y i ) y i (13.14)</formula><p>This reduces the update formula to:</p><formula xml:id="formula_10">∆w i←j = µ δ i h j (13.15)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weights to the hidden nodes</head><p>The computation of the gradient and hence the update formula for hidden nodes is quite analogous. As before, we first define the linear combination z j (previously s i ) of input values x k (previously hidden values h j ) weighted by weights u j←k (previously weights w i←j ). <ref type="bibr">13.16)</ref> This leads to the computation of the value of the hidden node h j .</p><formula xml:id="formula_11">z j = k u j←k x k<label>(</label></formula><formula xml:id="formula_12">h j = sigmoid(z j ) (13.17)</formula><p>Following the principles of gradient descent, we need to compute the derivative of the error E with respect to the weights u j←k . We decompose this derivative as before.</p><formula xml:id="formula_13">dE du j←k = dE dh j dh j dz j dz j du j←k (13.18)</formula><p>However, the computation of dE dh j is more complex than in the case of output nodes, since the error is defined in terms of output values y i , not values for hidden nodes h j . The idea behind back-propagation is to track how the error caused by the hidden node contributed to the error in the next layer. Applying the chain rule gives us:</p><formula xml:id="formula_14">dE dh j = i dE dy i dy i ds i ds i dh j (13.19)</formula><p>We already encountered the first two terms dE dy i (Equation 13.10) and dy i ds i (Equation 13.11) previously. To recap:</p><formula xml:id="formula_15">dE dy i dy i ds i = d dy i i 1 2 (t i − y i ) 2 y i = d dy i 1 2 (t i − y i ) 2 y i = −(t i − y i ) y i = δ i (13.20)</formula><p>The third term in Equation 13.19 is computed straightforward.</p><formula xml:id="formula_16">ds i dh j = d dh j i w i←j h j = w i←j (13.21)</formula><p>Putting Equation 13.20 and Equation 13.21 together, Equation 13.19 can be solved as:</p><formula xml:id="formula_17">dE dh j = i δ i w i←j (13.22)</formula><p>This gives rise to a quite intuitive interpretation. The error that matters at the hidden node h j depends on the error terms δ i in the subsequent nodes y i , weighted by w i←j , i.e., the impact the hidden node h j has on the output node y i .</p><p>Let us tie up the remaining loose ends. The missing pieces from Equation 13.18 are the second term </p><formula xml:id="formula_18">dh j dz j = d sigmoid(z j ) dz j = sigmoid(z j )(1 − sigmoid(z j )) = h j (1 − h j ) = h j<label>(</label></formula><formula xml:id="formula_19">dE du j←k = dE dh j dh j dz j dz j du j←k = i (δ i w i←j ) h j x k (13.25)</formula><p>If we define an error term δ j for hidden nodes analogous to output nodes For output nodes, the error term δ i is computed from the actual output y i of the node for our current network, and the target output t i for the node.</p><formula xml:id="formula_20">δ j = i (δ i w i←j ) h j<label>(</label></formula><formula xml:id="formula_21">δ i = (t i − y i ) y i (13.28)</formula><p>For hidden nodes, the error term δ j is computed via back-propagating the error term δ i from subsequent nodes connected by weights w i←j .</p><formula xml:id="formula_22">δ j = i (δ i w i←j ) h j (13.29)</formula><p>Computing y i and h j requires the derivative of the activation function, to which the weighted sum of incoming values is passed.</p><p>Given the error terms, weights w i←j (or u j←k ) from each proceeding node h j (or x k ) are updated, tempered by a learning rate µ.</p><formula xml:id="formula_23">∆w i←j = µ δ i h j ∆u j←k = µ δ j x k (13.30)</formula><p>Once weights are updated, the next training example is processed. There are typically several passes over the training set, called epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head><p>Given the neural network in <ref type="figure" target="#fig_1">Figure 13</ref>.4, let us see how the training example (1,0) → 1 is processed.</p><p>Let us start with the calculation of the error term δ for the output node y 0 . During inference (recall <ref type="table" target="#tab_0">Table 13</ref>.1 on page 11), we computed the linear combination of weighted hidden node values s 0 = 1.060 and the node value y 0 = 0.743. The target value is t 0 = 1.</p><formula xml:id="formula_24">δ = (t 0 − y 0 ) y 0 = (1 − 0.743) × sigmoid (1.060) = 0.257 × 0.191 = 0.049 (13.31)</formula><p>With this number, we can compute weight updates, such as for weight w 0←0 .</p><formula xml:id="formula_25">∆w 0←0 = µ δ 0 h 0 = µ × 0.049 × 0.731 = µ × 0.036 (13.32)</formula><p>Since the hidden node h 0 leads only to one output node y 0 , the calculation of its error term δ 0 is not more computationally complex.</p><formula xml:id="formula_26">δ j = i (δ i u i←0 ) h 0 = (δ × w 0←0 ) × sigmoid (z 0 ) = 0.049 × 5 × 0.197 = 0.049</formula><p>(13.33) <ref type="table" target="#tab_0">Table 13</ref>.2 summarizes the updates for all weights.  </p><formula xml:id="formula_27">δ = (t 0 − y 0 ) sigmoid(s 0 ) ∆w 0←j = µ δ h j y 0 δ = (1 − 0.743) × 0.191 = 0.049 ∆w 0←0 = µ × 0.049 × 0.731 = 0.036 ∆w 0←1 = µ × 0.049 × 0.119 = 0.006 ∆w 0←2 = µ × 0.049 × 1 = 0.049 δ j = δ w i←j sigmoid(z j ) ∆u j←i = µ δ j x i h 0 δ 0 = 0.049 × 5 × 0.197 = 0.048 ∆u 0←0 = µ × 0.048 × 1 = 0.048 ∆u 0←1 = µ × 0.048 × 0 = 0 ∆u 0←2 = µ × 0.048 × 1 = 0.048 h 1 δ 1 = 0.049 × −5 × 0.105 = −0.026 ∆u 1←0 = µ × −0.026 × 1 = −0.026 ∆u 1←1 = µ × −0.026 × 0 = 0 ∆u 1←2 = µ × −0.026 × 1 = −0.026</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.2.6">Refinements</head><p>We conclude our introduction to neural networks with some basic refinements and considerations. To motivate some of the refinements, consider <ref type="figure" target="#fig_1">Figure 13</ref>.6. While gradient descent training is a fine idea, it may run into practical problems.</p><p>• Setting the learning rate too high leads to updates that overshoot the optimum. Conversely, a too low learning rate leads to slow convergence.</p><p>• Bad initialization of weights may lead to long paths of many update steps to reach the optimum. This is especially a problem with activation functions like sigmoid which only have a short interval of significant change.</p><p>• The existence of local optima lead the search to get trapped and miss the global optimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Validation Set</head><p>Neural network training proceeds for several epochs, i.e., full iterations over the training data. When to stop? When we track training progress, we see that the error on the training set continuously decreases. However, at some point over-fitting sets in, where the training data is memorized and not sufficiently generalized.</p><p>We can check this with an additional set of examples, called the validation set, that is not used during training. See <ref type="figure" target="#fig_1">Figure 13</ref>.7 for an illustration. When we measure the error on the validation set at each point of training, we see that at some point this error increases. Hence, we stop training, when the minimum on the validation set is reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weight Initialization</head><p>Before training starts, weights are initialized to random values. The values are choses from a uniform distribution. We prefer initial weights that lead to node values that are in the transition area for the activation function, and not in the low or high shallow slope where it would take a long time to push towards a change. For instance, for the sigmoid activation function, feeding values in the range of, say, [−1; 1] to the activation function leads to activation values in the range of [0.269;0.731].</p><p>For the sigmoid activation function, commonly used formula for weights to the final layer of a network are</p><formula xml:id="formula_28">− 1 √ n , 1 √ n (13.34)</formula><p>where n is the size of the previous layer. For hidden layers, we chose weights from the range − √ 6 √ n j + n j+1 ,</p><formula xml:id="formula_29">√ 6 √ n j + n j+1 (13.35)</formula><p>where n j is the size of the previous layer, n j size of next layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Momentum Term</head><p>Consider the case where a weight value is far from its optimum. Even if most training examples push the weight value in the same direction, it may still take a while for each of these small updates to accumulate until the weight reaches its optimum. A common trick is to use a momentum term to speed up training. This momentum term m t gets updated at each time step t (i.e., for each training example). We combine the previous value of the momentum term m t−1 with the current raw weight update value ∆w t and use the resulting momentum term value to update the weights.</p><p>For instance, with a decay rate of 0.9, the update formula changes to</p><formula xml:id="formula_30">m t = 0.9m t−1 + ∆w t w t = w t−1 − µ m t (13.36)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adapting Learning Rate per Parameter</head><p>A common training strategy is to reduce the learning rate µ over time. At the beginning the parameters are far away from optimal values and have to change a lot, but in later training stages we are concerned with fine tuning, and a large learning rate may cause a parameter to bounce around an optimum.</p><p>The Adagrad update formula is based on the sum of gradients of the error E with respect to the weight w at all time steps t, i.e., g t = dEt dw . We divide the learning rate µ for this weight this accumulated sum.</p><formula xml:id="formula_31">∆w t = µ t τ =1 g 2 τ g t (13.37)</formula><p>Intutively, big changes in the parameter value (corresponding to big gradients g t ), lead to a reduction of the learning rate of the weight parameter.</p><p>Combining the idea of momentum term and adjusting parameter update by their accumulated change is the inspiration of Adam, another method to transform the raw gradient into a parameter update.</p><p>First, there is the idea of momentum, which is computed as in Equation 13.36 above.</p><formula xml:id="formula_32">m t = β 1 m t−1 + (1 − β 1 )g t (13.38)</formula><p>Then, there is the idea of the squares of gradients (as in Adagrad) for adjusting the learning rate. Since raw accumulation does run the risk of becoming too large and hence permanently depressing the learning rate, Adam uses exponential decay, just like for the momentum term.</p><formula xml:id="formula_33">v t = β 2 v t−1 + (1 − β 2 )g 2 t (13.39)</formula><p>The hyper parameters β 1 and β 2 are set typically close to 1, but this also means that early in training the values for m t and v t are close to their initialization values of 0. To adjust for that, they are corrected for this bias.m</p><formula xml:id="formula_34">t = m t 1 − β t 1 ,v t = v t 1 − β t 2 (13.40)</formula><p>With increasing training time steps t, this correction goes away: lim t→∞ 1 1−β t → 1.</p><p>Having these pieces in hand (learning rate µ, momentumm t , accumulated changev t ), weight update per Adam is computed as</p><formula xml:id="formula_35">∆w t = µ √v t + m t (13.41)</formula><p>Common values for the hyper parameters are β 1 = 0.9, β 2 = 0.999 and = 10 −8 .</p><p>There are various other adaptation schemes. This is an active area of research. For instance, the second order gradient gives some useful information about the rate of change. However, it is often expensive to compute, so other shortcuts are taken.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dropout</head><p>The parameter space in which back-propagation learning and its variants are operating is littered with local optima. The hill-climbing algorithm may just climb a mole hill and be stuck there, instead of moving towards a climb of the highest mountain. Various methods have been proposed to get training out of these local optima. One currently popular method in neural machine translation is called drop-out.</p><p>It sounds a bit simplistic and wacky. During training, some of the nodes of the neural network are ignored. Their values are set to 0, and their associated parameters are not updated. These dropped-out nodes are chosen at random, and may account for as much as 10%, 20% or even more of all the nodes. Training resumes for some number of iterations without the nodes, and then a different set of drop-out nodes are selected.</p><p>The dropped-out nodes played some useful role in the model trained up to the point where they are ignored. After that, other nodes have to pick up the slack. The end result is a more robust model where several nodes share similar roles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer Normalization</head><p>Layer normalization addresses a problem that arises especially in the deep neural networks that we are using in neural machine translation, where computing proceeds through a large sequence of layers. For some training examples, average values at one layer may become very large, which feed into the following layer, also producing large output values, and so on. This is especially a problem with activation functions that do not limit the output to a narrow interval, such as rectified linear units. For other training examples the average values at the same layers may be very small. This causes a problem for training. Recall from Equation 13.30, that gradient updates are strongly effected by node values. Too large node values lead to exploding gradients and too small node values lead to diminishing gradients.</p><p>To remedy this, the idea is to normalize the values on a per-layer basis. This is done by adding additional computational steps to the neural network. Recall that a feed-forward layer consists of the the matrix multiplication of the weight matrix W with the node values from the previous layer h l−1 , resulting in a weighted sum s l , followed by an activation function such as sigmoid.</p><formula xml:id="formula_36">s l = W h l−1 h l = sigmoid( h l ) (13.42)</formula><p>We can compute the mean and variance of the values in the weighted sum vector s l by</p><formula xml:id="formula_37">µ l = 1 H H i−1 s l i σ l = 1 H H i−1 (s l i − µ l ) 2 (13.43)</formula><p>Using these values, we normalize the vector s l using two additional bias vectors g and b</p><formula xml:id="formula_38">ŝl = g σ l · ( s l − µ l ) + b (13.44)</formula><p>where · is element-wise multiplication and the difference subtracts the scalar average from each vector element.</p><p>The formula first normalizes the values in s l by shifting them against their average value, hence ensuring that their average afterwards is 0. The resulting vector is then divided by the variance σ l . The additional bias vectors give some flexibility, they may be shared across multiple layers of the same type, such as multiple time steps in a recurrent neural network (we will introduce these in Section 13.4.4 on page 39).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mini Batches</head><p>Each training example yields a set of weight updates ∆w i . We may first process all the training examples and only afterwards apply all the updates. But neural networks have the advantage that they can immediately learn from each training example. A training method that updates the model with each training example is called online learning. The online learning variant of gradient descent training is called stochastic gradient descent.</p><p>Online learning generally takes fewer passes over the training set (called epochs) for convergence. However, since training constantly changes the weights, it is hard to parallelize. So, instead, we may want to process the training data in batches, accumulate the weight updates, and then apply them collectively. These smaller sets of training examples are called mini batches to distinguish this approach from batch training where the entire training set is considered one batch.</p><p>There are other variations to organize the processing of the training set, typically motivated by restrictions of parallel processing. If we process the training data in mini batches, then we can parallelize the computation of weight update values ∆w, but have to synchronize their summation and application to the weights. If we want to distribute training over a number of machines, it is computationally more convenient to break up the training data in equally sized parts, perform online learning for each of the parts (optionally using smaller mini batches), and then average the weights. Surprisingly, breaking up training this way, often leads to better results than straightforward linear processing.</p><p>Finally, a scheme called Hogwild runs several training threads that immediately update weights, even though other threads still use the weight values to compute gradients. While this is clearly violates the safe guards typically taken in parallel programming, it does not hurt in practical experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vector and Matrix Operations</head><p>We can express the calculations needed for handling neural networks as vector and matrix operations.</p><p>• Forward computation: s = W h • Activation function: y = sigmoid( h)</p><p>• Error term: δ = ( t − y) sigmoid'( s)</p><p>• Propagation of error term: δ i = W δ i+1 · sigmoid'( s)</p><p>• Weight updates: ∆W = µ δ h T Executing these operations is computationally expensive. If our layers have, say, 200 nodes, then the matrix operation W h requires 200 × 200 = 40, 000 multiplications. Such matrix operations are also common in another highly used area of computer science: graphics processing. When rendering images on the screen, the geometric properties of 3-dimensional objects have to be processed to generate the color values of the 2-dimensional image on the screen. Since there is high demand for fast graphics processing, for instance for the use in realistic looking computer games, specialized hardware has become commonplace: graphics processing units (GPUs).</p><p>These processors have a massive number of cores (for example, the NVIDIA GTX 1080ti GPU provides 3584 thread processors) but a rather lightweight instruction set. GPUs provide instructions that are applied to many data points at once, which is exactly what is needed out the vector space computations listed above. Programming for GPUs is supported by various libraries, such as CUDA for C++, and has become an essential part of developing large scale neural network applications.</p><p>The general term for scalars, vectors, and matrices is tensors. A tensor may also have more dimensions: a sequence of matrices can be packed into a 3-dimensional tensor. Such large objects are actually frequently used in today's neural network toolkits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Further Readings A good introduction to modern neural network research is the textbook "Deep</head><p>Learning" <ref type="bibr" target="#b32">(Goodfellow et al., 2016)</ref>. There is also book on neural network methods applied to the natural language processing in general <ref type="bibr" target="#b31">(Goldberg, 2017)</ref>.</p><p>A number of key techniques that have been recently developed have entered the standard repertoire of neural machine translation research. Training is made more robust by methods such as drop-out <ref type="bibr" target="#b79">(Srivastava et al., 2014)</ref>, where during training intervals a number of nodes are randomly masked. To avoid exploding or vanishing gradients during back-propagation over several layers, gradients are typically clipped <ref type="bibr" target="#b64">(Pascanu et al., 2013)</ref>. Layer normalization (Lei <ref type="bibr" target="#b48">Ba et al., 2016)</ref> has similar motivations, by ensuring that node values are within reasonable bounds.</p><p>An active topic of research are optimization methods that adjust the learning rate of gradient descent training. Popular methods are Adagrad <ref type="bibr" target="#b19">(Duchi et al., 2011</ref><ref type="bibr">), Adadelta (Zeiler, 2012</ref>, and currently Adam (Kingma and <ref type="bibr" target="#b45">Ba, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.3">Computation Graphs</head><p>For our example neural network from Section 13.2.5, we painstakingly worked out derivates for gradient computations needed by gradient descent training. After all this hard work, it may sigmoid sum b 2 prod</p><formula xml:id="formula_39">W 2 sigmoid sum b 1 prod W 1 x 3 4 2 3 5 −5 −2 −4 −2 1.0 0.0 3 2 1 −2 .731 .119</formula><p>3.06</p><p>1.06</p><p>.743 <ref type="figure" target="#fig_1">Figure 13</ref>.8: Two layer feed-forward neural network as a computation graph, consisting of the input value x, weight parameters W 1 , W 2 , b 1 , b 2 , and computation nodes (product, sum, sigmoid). To the right of each parameter node, its value is shown. To the left of input and computation nodes, we show how the input (1, 0) T is processed by the graph.</p><p>come as surprise that you will likely never have to do this again. It can be done automatically, even for arbitrarily complex neural network architectures. There are a number of toolkits that allow you to define the network and it will take care of the rest. In this section, we will take a close look at how this works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.3.1">Neural Networks as Computation Graphs</head><p>First, we will take a different look at the networks we are building. We previously represented neural networks as graphs consisting of nodes and their connections (recall <ref type="figure" target="#fig_1">Figure 13</ref>.4 on page 10), or by mathematical equations such as</p><formula xml:id="formula_40">h = sigmoid(W 1 x + b 1 ) y = sigmoid(W 2 h + b 2 ) (13.45)</formula><p>The equations above describe the feed-forward neural network that we use as our running example. We now represent this math in form of a computation graph. See <ref type="figure" target="#fig_1">Figure 13</ref>.8 for an illustration of the computation graph for our network. The graph contains as nodes the parameters of the models (the weight matrices W 1 , W 2 and bias vectors b 1 , b 2 ), the input x and the mathematical operations that are carried out between them (product, sum, and sigmoid). Next to each parameter, we show their values.</p><p>Neural networks, viewed as computation graphs, are any arbitrary connected operations between an input and any number of parameters. Some of these operations may have little to do with any inspiration from neurons in the brain, so we are stretching the term neural networks quite a bit here. The graph does not have to have a nice tree structure as in our example, but may be any acyclical directed graph, i.e., anything goes as long there is a straightforward processing direction and no cycles. Another way to view such a graph is as a fancy way to visualize a sequence of function calls that take as arguments the input, parameters, previously computed values, or any combination thereof, but have no recursion or loops.</p><p>Processing an input with the neural network requires placing the input value into the node x and carrying out the computations. In the figure, we show this with the input vector (1, 0) T . The resulting numbers should look familiar since they are the same as when previously worked through this example in Section 13.2.4.</p><p>Before we move on, let us take stock of what each computation node in the graph has to accomplish. It consists of the following:</p><p>• a function that executes its computation operation • links to input nodes • when processing an example, the computed value</p><p>We will add two more items to each node in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.3.2">Gradient Computations</head><p>So far, we showed how the computation graph can be used process an input value. Now we will examine how it can be used to vastly simply model training. Model training requires an error function and the computation of gradients to derive update rules for parameters.</p><p>The first of these is quite straightforward. To compute the error, we need to add another computation at the end of the computation graph. This computation takes the computed output value y and the given correct output value t from the training data and produces an error value. A typical error function is the L2 norm 1 2 (t − y) 2 . From the view of training, the result of the execution of the computation graph is an error value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Calculus Refresher</head><p>In calculus, the chain rule is a formula for computing the derivative of the composition of two or more functions. That is, if f and g are functions, then the chain rule expresses the derivative of their composition f · g (the function which maps x to f (g(x))) in terms of the derivatives of f and g and the product of functions as follows:</p><formula xml:id="formula_41">(f • g) = (f • g) · g .</formula><p>This can be written more explicitly in terms of the variable. Let F = f · g, or equiva-Now, for the more difficult part -devising update rules for the parameters. Looking at the computation graph, model updates originate from the error values and propagate back to the model parameter. Hence, we call the computations needed to compute the update values also the backward pass through the graph, opposed to the forward pass that computed output and error.</p><p>Consider the chain of operations that connect the weight matrix W 2 to the error computation.</p><formula xml:id="formula_42">e = L2(y, t) y = sigmoid(s) s = sum(p, b 2 ) p = prod(h, W 2 ) (13.46)</formula><p>where h are the values of the hidden layer nodes, resulting from earlier computations.</p><p>To compute the update rule for the parameter matrix W 2 , we view the error as a function of these parameters and take the derivative with respect to them, in our case dL2(W 2 ) dW 2 . Recall that when we computed this derivate we first broke it up into steps using the chain rule. We now do the same here. dL2(W 2 ) dW 2 = dL2(sigmoid(sum(prod(h, W 2 ), b 2 )), t) dW 2 = dL2(y, t) dy dsigmoid(s) ds dsum(p, b 2 ) dp dprod(h, W 2 ) dW 2 (13.47) Note that for the purpose for computing an update rule for W 2 , we treat all the other variables in this computation (the target value t, the bias vector b 2 , the hidden node values h) as constants. This breaks up the derivative of the error with respect to the parameters W 2 into a chain of derivatives along the line of the nodes of the computation graph.</p><p>Hence, all we have to do for gradient computations is to come up with derivatives for each node in the computation graph. In our example these are dL2(y, t)</p><formula xml:id="formula_43">dy = d 1 2 (t − y) 2 dy = t − y dsigmoid(s) ds = sigmoid(s)(1 − sigmoid(s)) dsum(p, b 2 ) dp = dp + b 2 dp = 1 dprod(h, W 2 ) dW 2 = dW 2 h dW 2 = h (13.48)</formula><p>If we want to compute the gradient update for a parameter such as W 2 , we compute values in a backward pass, starting from the error term y. See <ref type="figure" target="#fig_1">Figure 13</ref>.9 for an illustration.</p><p>To give more detail on the computation of the gradients in the backward pass, starting at the bottom of the graph: .257 <ref type="figure" target="#fig_1">Figure 13</ref>.9: Computation graph with gradients computed in the backward pass for the training example (0, 1) T → 1.0. Gradients are computed with respect to the input of the nodes, so some nodes that have two inputs also have two gradients. See text for details on the computations of the values.</p><formula xml:id="formula_44">L2 t sigmoid sum b 2 prod W 2 sigmoid sum b 1 prod W 1 x 3 4 2 3 -µ 0484 0 −.0258 0 5 −5 -µ .0360 .00587 −2 −4 -µ .</formula><p>• For the L2 node, we use the formula dL2(y, t)</p><formula xml:id="formula_45">dy = d 1 2 (t − y) 2 dy = t − y (13.49)</formula><p>The given target output value given as training data is t = 1, while we computed y = 0.743 in the forward pass. Hence, the gradient for the L2 norm is 1 − 0.743 = 0.257. Note that we are using values computed in the forward pass for these gradient computations.</p><p>• For the lower sigmoid node, we use the formula dsigmoid(s) ds = sigmoid(s)(1 − sigmoid(s)) (13.50)</p><p>Recall that the formula for the sigmoid is sigmoid(s) = 1 1+e −s . Plugging in the value for s = 1.06 computed in the forward pass into this formula gives us 0.191. The chain rule requires us to multiply this with the value that we just computed for the L2 node, i.e., 0.257, which gives us 0.191 × 0.257 = 0.0492.</p><p>• For the lower sum node, we simply copy the previous value, since the derivate is 1: dsum(p, b 2 ) dp = dp + b 2 dp = 1 (13.51)</p><p>Note that there are two gradients associated with the sum node. One with respect to the output of the prod node, and one with the b 2 parameter. In both cases, the derivative is 1, so both values are the same. Hence, the gradient in both cases is 0.0492.</p><p>• For the lower prod node, we use the formula</p><formula xml:id="formula_46">dprod(h, W 2 ) dW 2 = dW 2 h dW 2 = h (13.52)</formula><p>So far, we dealt with scalar values. Here we encounter vectors for the first time: the value of the hidden nodes h = (0.731, 0.119) T . The chain rule requires us to multiply this with the previously computed scalar 0.0492:</p><formula xml:id="formula_47">0.731 0.119 × 0.0492 T = 0.0360 0.00587</formula><p>As for the sum node, there are two inputs and hence two gradients. The other gradient is with respect to the output of the upper sigmoid node.</p><formula xml:id="formula_48">dprod(h, W 2 ) dh = dW 2 h dh = W 2 (13.53)</formula><p>Similarly to above, we compute</p><formula xml:id="formula_49">(W 2 × 0.0492) T = 5 −5 × 0.0492 T = 0.246 −0.246</formula><p>Having all the gradients in place, we can now read of the relevant values for weight updates. These are the gradients associated with trainable parameters. For the W 2 weight matrix, this is the second gradient of the prod node. So the new value for W 2 at time step t + 1 is</p><formula xml:id="formula_50">W t+1 2 = W t 2 − µ dprod(x, W t 2 ) dW t 2 = 5 5 − µ 0.0360 0.00587 (13.54)</formula><p>The remaining computations are carried out in very similar fashion, since they form simply another layer of the feed-forward neural network.</p><p>Our example did not include one special case: the output of a computation may be used multiple times in subsequent steps of a computation graphs. So, there are multiple output nodes that feed back gradients in the back-propagation pass. In this case, we add up the gradients from these descendent steps to factor in their added impact.</p><p>Let us take a second look at what a node in a computation graph comprises:</p><p>• a function that computes its value • links to input nodes (to obtain argument values)</p><p>• when processing an example in the forward pass, the computed value • a function that executes its gradient computation • links to children nodes (to obtain downstream gradient values)</p><p>• when processing an example in the forward pass, the computed gradient From an object oriented programming view, a node in a computation graph provides a forward and backward function for value and gradient computations. As instantiated in an computation graph, it is connected with specific inputs and outputs, and is also aware of the dimensions of its variables its value and gradient. During forward and backward pass, these variables are filled in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.3.3">Deep Learning Frameworks</head><p>In the next sections, we will encounter various network architectures. What all of these share, however, are the need for vector and matrix operations, as well as the computation of derivatives to obtain weight update formulas. It would be quite tedious to write almost identical code to deal with each these variants. Hence, a number of frameworks have emerged to support developing neural network methods for any chosen problem. At the time of writing, the most prominent ones are Theano 1 (a Python library that dymanically generates and compiles C++ code and is build on NumPy), Torch 2 (a machine learning library and a script language based on the Lua programming language), pyTorch 3 (the Python variant of Torch), DyNet 4 (a C++ implementation by natural language processing researchers that can be used as a library in C++ or Python), and Tensorflow 5 (a more recent entry to the genre from Google).</p><p>These frameworks are less geared towards ready-to-use neural network architectures, but provide efficient implementations of the vector space operations and computation of derivatives, with seamless support of GPUs. Our example from Section 13.2 can be implemented in a few lines of Python code, as we will show in this section, using the example of Theano (other frameworks are quite similar).</p><p>You can execute the following commands on the Python command line interface if you first installed Theano (pip install Theano). &gt; import numpy &gt; import theano &gt; import theano.tensor as T</p><p>The mapping of the input layer x to the hidden layer h uses a weight matrix W, a bias vector b, and a mapping function which consists of the linear combination T.dot and the sigmoid activation function. Note that we define x as a matrix. This allows us to process several training examples at once (a sequence of vectors). A good way to think about these definitions of x and h is in term of a functional programming language. They symbolically define operations. To actually define a function that can be called, the Theano method function is used. This example call to h_function computes the values for the hidden nodes (compare to the numbers in <ref type="table" target="#tab_0">Table 13</ref>.1 on page 11).</p><p>The mapping from the hidden layer h to the output layer y is defined in the same fashion.  Model training requires the definition of a cost function (we use the L2 norm). To formulate it, we first need to define the variable for the correct output. The overall cost is computed as average over all training examples.</p><p>&gt; y = T.dvector() &gt; l2 = (y-y_pred)**2 &gt; cost = l2.mean() Gradient descent training requires the computation of the derivative of the cost function with respect to the model parameters (i.e., the values in the weight matrices W and W2 and the bias vectors b and b2. A great benefit of using Theano is that it computes the derivatives for you. The following is also an example of a function with multiple inputs and multiple outputs. We have now all we need to define training. The function updates the model parameters and returns the current predictions and cost. It uses a learning rate of 0.1.</p><formula xml:id="formula_51">&gt; train = theano.function(inputs=[x,y],outputs=[y_pred,cost], updates=((W, W-0.1*gW), (b, b-0.1*gb), (W2, W2-0.1*gW2), (b2, b2-0.1*gb2)))</formula><p>Let us define the training data. The training function returns the prediction and cost before the updates. If we call the training function again, then the predictions and cost have changed for the better. Typically, we would loop over the training function until convergence. As discussed above, we may also break up the training data into mini-batches and train on one mini-batch at a time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.4">Neural Language Models</head><p>Neural networks are a very powerful method to model conditional probability distributions with multiple inputs p(a|b, c, d). They are robust to unseen data points -say, an unobserved <ref type="bibr">(a,b,c,d)</ref> in the training data. Using traditional statistical estimation methods, we may address such a sparse data problem with back-off and clustering, which require insight into the problem (what part of the conditioning context to drop first?) and arbitrary choices (how many clusters?). Word 5 Hidden Layer <ref type="figure" target="#fig_1">Figure 13</ref>.10: Sketch of a neural language model: We predict a word w i based on its preceding words. N-gram language models which reduce the probability of a sentence to the product of word probabilities in the context of a few previous words -say, p(w i |w i−4 , w i−3 , w i−2 , w i−1 ). Such models are a prime example for a conditional probability distribution with a rich conditioning context for which we often lack data points and would like to cluster information. In statistical language models, complex discounting and back-off schemes are used to balance rich evidence from lower order models -say, the bigram model p(w i |w i−1 ) -with the sparse estimates from high order models. Now, we turn to neural networks for help. <ref type="figure" target="#fig_1">Figure 13</ref>.10 gives a basic sketch of a 5-gram neural network language model. Network nodes representing the context words have connections to a hidden layer, which connects to the output layer for the predicted word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.4.1">Feed-Forward Neural Language Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representing Words</head><p>We are immediately faced with a difficult question: How do we represent words? Nodes in a neural network carry real-numbered values, but words are discrete items out of a very large vocabulary. We cannot simply use token IDs, since the neural network will assume that token 124,321 is very similar to token 124,322 -while in practice these numbers are completely arbitrary. The same arguments applies to the idea of using bit encoding for token IDs. The words (1, 1, 1, 1, 0, 0, 0, 0) T and (1, 1, 1, 1, 0, 0, 0, 1) T have very similar encodings but may have nothing to do with each other. While the idea of using such bit vectors is occasionally explored, it does not appear to have any benefits over what we consider next.</p><p>Instead, we will represent each word with a high-dimensional vector, one dimension per word in the vocabulary, and the value 1 for the dimension that matches the word, and 0 for the rest. The type of vectors are called one hot vector. For instance:</p><p>• dog = (0, 0, 0, 0, 1, 0, 0, 0, 0, ...) T</p><p>• cat = (0, 0, 0, 0, 0, 0, 0, 1, 0, ...) T</p><p>• eat = (0, 1, 0, 0, 0, 0, 0, 0, 0, ...) T </p><formula xml:id="formula_52">w i−4 , w i−3 , w i−2 , w i−1 )</formula><p>are represented in a one-hot vector, then projected into continuous space as word embeddings (using the same weight matrix C for all words). The predicted word is computed as a one-hot vector via a hidden layer.</p><p>These are very large vectors, and we will continue to wrestle with the impact of this choice to represent words. One stopgap is to limit the vocabulary to the most frequent, say, 20,000 words, and pool all the other words in an OTHER token. We could also use word classes (either automatic clusters or linguistically motivated classes such as part-of-speech tags) to reduce the dimensionality of the vectors. We will revisit the problem of large vocabularies later.</p><p>To pool evidence between words, we introduce another layer between the input layer and the hidden layer. In this layer, each context word is individually projected into a lower dimensional space. We use the same weight matrix for each of the context words, thus generating a continuous space representation for each word, independent of its position in the conditioning context. This representation is commonly referred to as word embedding.</p><p>Words that occur in similar contexts should have similar word embeddings. For instance, if the training data for a language model frequently contains the n-grams</p><p>• but the cute dog jumped</p><p>• but the cute cat jumped</p><p>• child hugged the cat tightly • child hugged the dog tightly • like to watch cat videos • like to watch dog videos then the language model would benefit from the knowledge that dog and cat occur in similar contexts and hence are somewhat interchangeable. If we like to predict from a context where dog occurs but we have seen this context only with the word cat, then we would still like to treat this as positive evidence. Word embeddings enable generalizing between words (clustering) and hence having robust predictions in unseen contexts (back-off).</p><p>Neural Network Architecture See <ref type="figure" target="#fig_1">Figure 13</ref>.11 for a visualization of the architecture the fully fledged feed forward neural network language model, consisting of the context words as onehot-vector input layer, the word embedding layer, the hidden layer and predicted output word layer.</p><p>The context words are first encoded as one-hot vectors. These are then passed through the embedding matrix C, resulting in a vector of floating point numbers, the word embedding. This embedding vector has typically in the order of 500 or 1000 nodes. Note that we use the same embedding matrix C for all the context words.</p><p>Also note that mathematically there is not all that much going on here. Since the input to the multiplication to the matrix C is a one hot vector, most of the input values to the matrix multiplication are zeros. So, practically, we are selecting the one column in the matrix that corresponds to the input word ID. Hence, there is no use for an activation function here. In a way, the embedding matrix a lookup table C(w j ) for word embeddings, indexed by the word ID w j .</p><p>C(w j ) = C w j (13.55)</p><p>Mapping to the hidden layer in the model requires concatenation of all context word embeddings C(w j ) as input to a typical feed-forward layer, say, using tanh as activation function.</p><formula xml:id="formula_53">h = tanh b h + j H j C(w j ) (13.56)</formula><p>The output layer is interpreted as a probability distribution over words. As before, first the linear combination s i of weights w ij and hidden node values h j is computed for each node i.</p><formula xml:id="formula_54">s = W h (13.57)</formula><p>To ensure that it is indeed a proper probability distribution, we use the softmax activation function to ensure that all values add up to one.</p><formula xml:id="formula_55">p i = softmax(s i , s) = e s i j e s j (13.58)</formula><p>What we described here is close to the neural probabilistic language model proposed by <ref type="bibr" target="#b4">Bengio et al. (2003)</ref>. This model had one more twist, it added direct connections of the context word embeddings to the output word. So, Equation 13.57 is replaced by</p><formula xml:id="formula_56">s = W h + j U C(w j ) (13.59)</formula><p>Their paper reports that having such direct connections from context words to output words speeds up training, although does not ultimately improve performance. We will encounter the idea of short-cutting hidden layers again a bit later when we discuss deeper models with more hidden layers. They are also called residual connections, skip connections, or even highway connections.</p><p>Training We train the parameters of a neural language model (word embedding matrix, weight matrices, bias vectors) by processing all the n-grams in the training corpus. For each n-gram, we feed the context words into the network and match the network's output against the onehot vector of the correct word to be predicted. Weights are updated using back-propagation (we will go into details in the next section).</p><p>Language models are commonly evaluated by perplexity, which is related to the probability given to proper English text. A language model that likes proper English is a good language model. Hence, the training objective for language models is to increase the likelihood of the training data.</p><p>During training, given a context x = (w n−4 , w n−3 , w n−2 , w n−1 ), we have the correct value for the 1-hot vector y. For each training example (x, y), likelihood is defined as</p><formula xml:id="formula_57">L(x, y; W ) = − k y k log p k (13.60)</formula><p>Note that only one value y k is 1, the others are 0. So this really comes down to the probability p k given to the correct word k. Defining likelihood this way allows us to update all weights, also the one that lead to the wrong output words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.4.2">Word Embedding</head><p>Before we move on, it is worth reflecting the role of word embeddings in neural machine translation and many other natural language processing tasks. We introduced them here as compact encoding of words in relatively high-dimensional space, say 500 or 1000 floating point numbers. In the field of natural language processing, at the time of this writing, word embeddings have acquired the reputation of almost magical quality.</p><p>Consider the role they play in the neural language language that we just described. They represent context words to enable prediction the next word in a sequence.</p><p>Recall part of our earlier example:</p><p>• but the cute dog jumped</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• but the cute cat jumped</head><p>Since dog and cat occur in similar contexts, their influence on predicting the word jumped should be similar. It should be different from words such as dress which is unlikely to trigger the completion jumped. The idea that words that occur in similar contexts are semantically similar is a powerful idea in lexical semantics.</p><p>At this point in the argument, researchers love to cite John Rupert Firth:</p><p>You shall know a word by the company it keeps.</p><p>Or, as Ludwig Wittgenstein put it a bit more broadly: The meaning of a word is its use.</p><p>Meaning and semantics are quite difficult concepts with largely unresolved definition. The idea of distributional lexical semantics is to define word meaning by their distributional properties, i.e., in which contexts they occur. Words that occur in similar contexts (dog and cat) should have similar representations. In vector space models, such as the word embeddings that we use here, similarity can be measured by a distance function, e.g., the cosine distancethe angle between the vectors.</p><p>If we project the high-dimensional word embeddings down to two dimensions, we can visualize word embeddings as shown in <ref type="figure" target="#fig_1">Figure 13</ref>.12. In this figure, words that are similar (drama, theater, festival) are clustered together.</p><p>But why stop there? We would like to have semantic representations so we can carry out semantic inference such as • queen = king + (woman -man)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• queens = queen + (kings -king)</head><p>Indeed there is some evidence that word embedding allow just that . However, we better stop here and just note that word embeddings are a crucial tool in neural machine translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.4.3">Efficient Inference and Training</head><p>Training a neural language model is computationally expensive. For billion word corpora, even with the use of GPUs, training takes several days with modern compute clusters. Even using a neural language model as a scoring component in statistical machine translation decoding requires a lot of computation. We could restrict its use only to re-ranking n-best lists or lattices, or consider more efficient methods for inference and training.</p><p>Caching for Inference However, with a few considerations, it is actually possible to use this neural language model within the decoder.</p><p>• Word embeddings are fixed for the words, so do not actually need to carry out the mapping from one-hot vectors to word embeddings, but just store them beforehand.</p><p>• The computation between embeddings and the hidden layer can be also partly carried out offline. Note that each word can occur in one of the 4 slots for conditioning context (assuming a 5-gram language model). For each of the slots, we can pre-compute the matrix multiplication of word embedding vector and the corresponding submatrix of weights. So, at run time, we only have to sum up these pre-computations at the hidden layer and apply the activation function.</p><p>• Computing the value for each output node is insanely expensive, since there are as many output nodes as vocabulary items. However, we are interested only in the score for a given word that was produced by the translation model. If we only compute its node value, we have a score that we can use.</p><p>The last point requires a longer discussion. If we compute the node value only for the word that we want to score with the language model, we are missing an important step. To obtain a proper probability, we need to normalize it, which requires the computation of the values for all the other nodes.</p><p>We could simply ignore this problem and use the scores at face value. More likely words given a context will get higher scores than less likely words, and that is the main objective. But since we place no constraints on the scores, we may work with models where some contexts give high scores to many words, while some contexts do not give preference for any.</p><p>It would be great, if the node values in the final layer were already normalized probabilities. There are methods to enforce this during training. Let us first discuss training in detail, and then move to these methods in Section 13.4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noise Contrastive Estimation</head><p>We discussed earlier the problem that computing probabilities with a neural language model is very expensive due to the need to normalize the output node values y i using the softmax function. This requires computing values for all output nodes, even if we are only interested in the score for a particular n-gram. To overcome the need for this explicit normalization step, we would like to train a model that already has y i values that are normalized.</p><p>One way is to include the constraint that the normalization factor Z(x) = j e s j is close to 1 in the objective function. So, instead of the just the simple likelihood objective, we may include the L2 norm of the log of this factor. Note that if log Z(x) 0, then Z(x) 1.</p><formula xml:id="formula_58">L(x, y; W ) = − k y k log p k − α log 2 Z(x) (13.61)</formula><p>Another way to train a self-normalizing model is called noise contrastive estimation. The main idea is to optimize the model so that it can separate correct training examples from artificially created noise examples. This method needs less computation during training, since it does not require the computation of all output node values.</p><p>Formally, we are trying to learn the model distribution p m ( y|x; W ). Given a noise distribution p n ( y|x) -in our case of language modeling a unigram model p n ( y) is a good choice -we first generate a set of noise examples U n in addition to the correct training examples U t . If both sets have the same size |U n | = |U t |, then the probability that a given example (x; y) ∈ U n ∪ U t is predicted to be a correct training example is</p><formula xml:id="formula_59">p(correct|x, y) = p m ( y|x; W ) p m ( y|x; W ) + p n ( y|x) (13.62)</formula><p>The objective of noise contrastive estimation is to maximize p(correct|x, y) for correct training examples (x; y) ∈ U t and to minimize it for noise examples (x; y) ∈ U n . Using loglikelihood, we define the objective function as</p><formula xml:id="formula_60">L = 1 2|U t | (x; y)∈Ut log p(correct|x, y) + 1 2|U n | (x; y)∈Un log (1 − p(correct|x, y)) (13.63)</formula><p>Returning the the original goal of a self-normalizing model, first note that the noise distribution p n ( y|x) is normalized. Hence, the model distribution is encouraged to produce comparable values. If p m ( y|x; W ) would generally overshoot -i.e., y p m ( y|x; W ) &gt; 1 then it would also give too high values for noise examples. Conversely, generally undershooting would give too low values to correct translation examples.</p><p>Training is faster, since we only need to compute the output node value for the given training and noise examples -there is no need to compute the other values, since we do not normalize with the softmax function.</p><p>Given the definition of the training objective L, we have now a complete computation graph that we can implement using standard deep learning toolkits, as we have done before. These toolkits compute the gradients dL dW for all parameters W and use them for parameter updates via gradient descent training (or its variants).</p><p>It may not be immediately obvious why optimizing towards classifying correct against noise examples gives rise to a model that also predicts the correct probabilities for n-grams. But this is a variant of methods that are common in statistical machine translation in the tuning phase. MIRA (margin infused relaxation algorithm) and PRO (pairwise ranked optimization) follow the same principle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word 1</head><p>Word 2 E C 1 H1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word 2</head><p>Word 3 E C H2 H1 copy values</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word 3</head><p>Word 4 E C H3</p><p>H2 copy values <ref type="figure" target="#fig_1">Figure 13</ref>.13: Recurrent neural language models: After predicting Word 2 in the context of following Word 1, we re-use this hidden layer (alongside the correct Word 2) to predict Word 3. Again, the hidden layer of this prediction is re-used for the prediction of Word 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.4.4">Recurrent Neural Language Models</head><p>The feed-forward neural language model that we described above is able to use longer contexts than traditional statistical back-off models, since it has more flexible means to deal with unknown contexts. Namely, the use of word embeddings to make use of similar words, and the robust handling of unseen words in any context position. Hence, it is possible to condition on much larger contexts than traditional statistical models. In fact, large models, say, 20-gram models, have been reported to be used.</p><p>Alternatively, instead of using a fixed context word window, recurrent neural networks may condition on context sequences of any length. The trick is to re-use the hidden layer when predicting word w n as additional input to predict word w n−1 .</p><p>See <ref type="figure" target="#fig_1">Figure 13</ref>.13 for an illustration. Initially, the model does not look any different from the feed-forward neural language model that we discussed so far. The inputs to the network is the first word of the sentence w 1 and a second set of neurons which at this point indicate the start of the sentence. The word embedding of w 1 and the start-of-sentence neurons first map into a hidden layer h 1 , which is then used to predict the output word w 2 . This model uses the same architecture as before: Words (input and output) are represented with one-hot vectors; word embeddings and the hidden layer use, say, 500 real valued neurons. We use a sigmoid activation function at the hidden layer and the softmax function at the output layer.</p><p>Things get interesting when we move to predicting the third word w 3 in the sequence. One input is the directly preceding (and now known) word w 2 , as before. However, the neurons in the network that we used to represent start-of-sentence are now filled with values from the hidden layer of the previous prediction of word w 2 . In a way, these neurons encode the previous sentence context. They are enriched at each step with information about a new input word and are hence conditioned on the full history of the sentence. So, even the last word of the sentence is conditioned in part on the first word of the sentence. Moreover, the model is simpler: it has less weights than a 3-gram feed-forward neural language model. How do we train such a model with arbitrarily long contexts? One idea: At the initial stage (predicting the second word from the first), we have the same architecture and hence the same training procedure as for feed-forward neural networks. We assess the error at the output layer and propagate updates back to the input layer. We could process every training example this way -essentially by treating the hidden layer from the previous training example as fixed input the current example. However, this way, we never provide feedback to the representation of prior history in the hidden layer.</p><p>The back-propagation through time training procedure (see <ref type="figure" target="#fig_1">Figure 13</ref>.14) unfolds the recurrent neural network over a fixed number of steps, by going back over, say, 5 word predictions. Note that, despite limiting the unfolding to 5 time steps, the network is still able to learn dependencies over longer distances.</p><p>Back-propagation through time can be either applied for each training example (here called time step), but this is computationally quite expensive. Each time computations have to be carried out over several steps. Instead, we can compute and apply weight updates in minibatches (recall Section 13.2.6). First, we process a larger number of training examples (say, 10-20, or the entire sentence), and then update the weights.</p><p>Given modern compute power, fully unfolding the recurrent neural network has become more common. While recurrent neural networks have in theory arbitrary length, given a specific training example, its size is actually known and fixed, so we can fully construct the computation graph for each given training example, define the error as the sum of word prediction errors, and then carry out back-propagation over the entire sentence. This does require that we can quickly build computation graphs -so-called dynamic computation graphs -which is currently supported by some toolkits better than others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.4.5">Long Short-Term Memory Models</head><p>Consider the following step during word prediction in a sequential language model:</p><p>After much economic progress over the years, the country → has</p><p>The directly preceding word country will be the most informative for the prediction of the word has, all the previous words are much less relevant. In general, the importance of words decays with distance. The hidden state in the recurrent neural network will always be updated with the most recent word, and its memory of older words is likely to diminish over time.</p><p>But sometimes, more distant words are much more important, as the following example shows:</p><p>The country which has made much economic progress over the years still → has</p><p>In this example, the inflection of the verb have depends on the subject country which is separated by a long subordinate clause.</p><p>Recurrent neural networks allow modeling of arbitrarily long sequences. Their architecture is very simple. But this simplicity causes a number of problems.</p><p>• The hidden layer plays double duty as memory of the network and as continuous space representation used to predict output words.</p><p>• While we may sometimes want to pay more attention to the directly previous word, and sometimes pay more attention to the longer context, there is no clear mechanism to control that.</p><p>• If we train the model on long sequences, then any update needs to back propagate to the beginning of the sentence. However, propagating through so many steps raises concerns that the impact of recent information at any step drowns out older information. 6</p><p>The rather confusingly named long short-term memory (LSTM) neural network architecture addresses these issues. Its design is quite elaborate, although it is not very difficult to use in practice.</p><p>A core distinction is that the basic building block of LSTM networks, the so-called cell, contains an explicit memory state. The memory state in the cell is motivated by digital memory cells in ordinary computers. Digital memory cells offer operations to read, write, and reset. While a digital memory cell may store just a single bit, a LSTM cell stores a real number. Furthermore, the read/write/reset operations in a LSTM cell are regulated with a real numbered parameter, which are called gates (see <ref type="figure" target="#fig_1">Figure 13</ref>.15). • The input gate parameter regulates how much new input changes the memory state.</p><p>• The forget gate parameter regulates how much of the prior memory state is retained (or forgotten).</p><p>• The output gate parameter regulates how strongly the memory state is passed on to the next layer.</p><p>Formally, marking the input, memory, and output values with the time step t, we define the flow of information within a cell as follows.</p><formula xml:id="formula_61">memory t = gate input × input t + gate forget × memory t−1 output t = gate output × memory t (13.64)</formula><p>The hidden node value h t passed on to the next layer is the application of an activation function f to the output value.</p><p>h</p><formula xml:id="formula_62">t = f (output t ) (13.65)</formula><p>An LSTM layer consists of a vector of LSTM cells, just as traditional layers consist of a vector of nodes. The input to LSTM layer is computed in the same way as the input to a recurrent neural network node. Given the node values for the prior layer x t and the values for the hidden layer from the previous time step h t−1 , the input value is the typical combination of matrix multiplication with weights W x and W h and an activation function g.</p><formula xml:id="formula_63">input t = g W x x t + W h h t−1 (13.66)</formula><p>But how are the gate parameters set? They actually play a fairly important role. In particular contexts, we would like to give preference to recent input (gate input 1), rather retain past memory (gate forget 1), or pay less attention to the cell at the current point in time (gate output 0). Hence, this decision has to be informed by a broad view of the context.</p><p>How do we compute a value from such a complex conditioning context? Well, we treat it like a node in a neural network. For each gate a ∈ (input, forget, output) we define matrices W xa , W ha , and W ma to compute the gate parameter value by the multiplication of weights and node values in the previous layer x t , the hidden layer h t−1 at the previous time step, and the memory states at the previous time step memory t−1 , followed by an activation function h.</p><formula xml:id="formula_64">gate a = h W xa x t + W ha h t−1 + W ma memory t−1 (13.67)</formula><p>LSTM are trained the same way as recurrent neural networks, using back-propagation through time or fully unrolling the network. While the operations within a LSTM cell are more complex than in a recurrent neural network, all the operations are still based on matrix multiplications and differentiable activation functions. Hence, we can compute gradients for the objective function with respect to all parameters of the model and compute update functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.4.6">Gated Recurrent Units</head><p>LSTM cells add a large number of additional parameters. For each gate alone, multiple weight matrices are added. More parameters lead to longer training times and risk overfitting. As a simpler alternative, gated recurrent units (GRU) have been proposed and used in neural translation models. At the time of writing, LSTM cells seem to make a comeback in neural machine translation, but both are still commonly used.</p><p>See <ref type="figure" target="#fig_1">Figure 13</ref>.16 for an illustration for GRU cells. There is no separate memory state, just a hidden state that serves both purposes. Also, there are only two gates. These gates are predicted as before from the input and the previous state.</p><formula xml:id="formula_65">update t = g(W update input t + U update state t−1 + bias update ) reset t = g(W reset input t + U reset state t−1 + bias reset ) (13.68)</formula><p>The first gate is used in the combination of the input and previous state. This is combination is identical to traditional recurrent neural network, except that the previous states impact is scaled by the reset gate. Since the gate's value is between 0 and 1, this may give preference to the current input.</p><formula xml:id="formula_66">combination t = f (W input t + U (reset t • state t−1 )) (13.69)</formula><p>Then, the update gate is used for a interpolation of the previous state and the just computed combination. This is done as a weighted sum, where the update gate balances between the two. </p><formula xml:id="formula_67">state t =(1 − update t ) • state t−1 + update t • combination t ) + bias (13.70)</formula><p>In one extreme case, the update gate is 0, and the previous state is passed through directly. In another extreme case, the update gate is 1, and the new state is mainly determined from the input, with as much impact from the previous state as the reset gate allows.</p><p>It may seem a bit redundant to have two operations with a gate each that combine prior state and input. However, these play different roles. The first operation yielding combination t (Equation 13.69) is a classic recurrent neural network component that allows more complex computations in the combination of input and output. The second operation yielding the new hidden state and the output of the unit (Equation 13.70) allows for bypassing of the input, enabling long-distant memory that simply passes through information and, during backpropagation, passes through the gradient, thus enabling long-distance dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.4.7">Deep Models</head><p>The currently fashionable name deep learning for the latest wave of neural network research has a real motivation. Large gains have been seen in tasks such as vision and speech recognition due to stacking multiple hidden layers together.</p><p>More layers allow for more complex computations, just as having sequences of traditional computation components (Boolean gates) allows for more complex computations such as addition and multiplication of numbers. While this has been generally recognized for a long time, modern hardware finally enabled to train such deep neural networks on real world problems. Deep Transition <ref type="figure" target="#fig_1">Figure 13</ref>.17: Deep recurrent neural networks. The input is passed through a few hidden layers before an output prediction is made. In deep stacked models, the hidden layers are also connected horizontally, i.e., a layer's values at time step t depends on its value at time step t − 1 as well as the previous layer at time step t. In deep transitional models, the layers at any time step t are sequentially connected and first hidden layer is also informed by the last layer at time step t − 1.</p><p>And we learned from experiments in vision and speech that having a handful, and even dozens of layers does give increasingly better quality.</p><p>How does the idea of deep neural networks apply to the sequence prediction tasks common in language? There are several options. <ref type="figure" target="#fig_1">Figure 13</ref>.17 gives two examples. In shallow neural networks, the input is passed to a single hidden layer, from which the output is predicted. Now, a sequence of hidden layers is used. These hidden layers h t,i may be deeply stacked, so that each layer acts like the hidden layer in the shallow recurrent neural network. Its state is conditioned on its value at the previous time step h t−1,i and the value of previous layer in the sequence h t,i−1 .</p><formula xml:id="formula_68">h t,1 = f 1 (h t−1,1 , x t ) first layer h t,i = f i (h t−1,i , h t,i−1 ) for i &gt; 1 y t = f i+1 (h t,I ) prediction from last layer I (13.71)</formula><p>Or, the hidden layers may be directly connected in deep transitional networks, where the first hidden layer h t,1 is informed by the last hidden layer at the previous time step h t−1,I , but all other hidden layers are not connected to values from previous time steps.</p><formula xml:id="formula_69">h t,1 = f 1 (h t−1,I , x t ) first layer h t,i = f i (h t,i−1 ) for i &gt; 1 y t = f i+1 (h t,I ) prediction from last layer I (13.72)</formula><p>In all these equations, the function f i may be a feedforward layer (matrix multiplication plus activation function), an LSTM cell or a GRU cell.</p><p>Experiments with using neural language models in traditional statistical machine translation have shown benefits with 3-4 hidden layers <ref type="bibr" target="#b53">(Luong et al., 2015a)</ref>.</p><p>While modern hardware allows training of deep models, they do stretch computational resources to their practical limit. Not only are there more computations in the neural network, convergence of training is typically slower. Adding skip connections (linking the input directly to the output or the final hidden layer) sometimes speeds up training, but we still talking about a several times longer training times than shallow networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Further Readings</head><p>The first vanguard of neural network research tackled language models. A prominent reference for neural language model is <ref type="bibr" target="#b4">Bengio et al. (2003)</ref>, who implement an n-gram language model as a feed-forward neural network with the history words as input and the predicted word as output. <ref type="bibr" target="#b71">Schwenk et al. (2006)</ref> introduce such language models to machine translation (also called "continuous space language models"), and use them in re-ranking, similar to the earlier work in speech recognition. <ref type="bibr" target="#b66">Schwenk (2007)</ref> propose a number of speed-ups. They made their implementation available as a open source toolkit <ref type="bibr" target="#b67">(Schwenk, 2010)</ref>, which also supports training on a graphical processing unit (GPU) .</p><p>By first clustering words into classes and encoding words as pair of class and word-in-class bits, <ref type="bibr" target="#b3">Baltescu et al. (2014)</ref> reduce the computational complexity sufficiently to allow integration of the neural network language model into the decoder. Another way to reduce computational complexity to enable decoder integration is the use of noise contrastive estimation by <ref type="bibr" target="#b90">Vaswani et al. (2013)</ref>, which roughly self-normalizes the output scores of the model during training, hence removing the need to compute the values for all possible output words. <ref type="bibr" target="#b2">Baltescu and Blunsom (2015)</ref> compare the two techniquesclass-based word encoding with normalized scores vs. noise-contrastive estimation without normalized scores -and show that the letter gives better performance with much higher speed.</p><p>As another way to allow straightforward decoder integration, <ref type="bibr" target="#b92">Wang et al. (2013)</ref> convert a continuous space language model for a short list of 8192 words into a traditional n-gram language model in ARPA (SRILM) format.  present a method to merge (or "grow") a continuous space language model with a traditional n-gram language model, to take advantage of both better estimate for the words in the short list and the full coverage from the traditional model. <ref type="bibr" target="#b23">Finch et al. (2012)</ref> use a recurrent neural network language model to rescore n-best lists for a transliteration system. <ref type="bibr" target="#b80">Sundermeyer et al. (2013)</ref> compare feed-forward with long short-term neural network language models, a variant of recurrent neural network language models, showing better performance for the latter in a speech recognition re-ranking task. <ref type="bibr" target="#b62">Mikolov (2012)</ref> reports significant improvements with reranking n-best lists of machine translation systems with a recurrent neural network language model. Neural language model are not deep learning models in the sense that they use a lot of hidden layers. <ref type="bibr" target="#b53">Luong et al. (2015a)</ref> show that having 3-4 hidden layers improves over having just the typical 1 layer.</p><p>Language Models in Neural Machine Translation: Traditional statistical machine translation models have a straightforward mechanism to integrate additional knowledge sources, such as a large out of domain language model. It is harder for end-to-end neural machine translation. <ref type="bibr" target="#b35">Gülçehre et al. (2015)</ref> add a language model trained on additional monolingual data to this model, in form of a recurrently  <ref type="figure" target="#fig_1">Figure 13</ref>.18: Sequence-to-sequence encoder-decoder model: Extending the language model, we concatenate the English input sentence the house is big with the German output sentence das Haus ist groß.</p><p>The first dark green box (after processing the end-of-sentence token &lt;/s&gt;) contains the embedding of the entire input sentence .</p><p>neural network that runs in parallel. They compare the use of the language model in re-ranking (or, rescoring) against deeper integration where a gated unit regulates the relative contribution of the language model and the translation model when predicting a word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.5">Neural Translation Models</head><p>We are finally prepared to look at actual translation models. We have already done most of the work, however, since the most commonly used architecture for neural machine translation is a straightforward extension of neural language models with one refinement, an alignment model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.5.1">Encoder-Decoder Approach</head><p>Our first stab at a neural translation model is a straightforward extension of the language model. Recall the idea of a recurrent neural network to model language as a sequential process. Given all previous words, such a model predicts the next word. When we reach the end of the sentence, we now proceed to predict the translation of the sentence, one word at a time.</p><p>See <ref type="figure" target="#fig_1">Figure 13</ref>.18 for an illustration. To train such a model, we simply concatenate the input and output sentences and use the same method as to train a language model. For decoding, we feed in the input sentence, and then go through the predictions of the model until it predicts an end of sentence token.</p><p>How does such a network work? Once processing reaches the end of the input sentence (having predicted the end of sentence marker &lt;/s&gt;), the hidden state encodes its meaning. In other words, the vector holding the values of the nodes of this final hidden layer is the input sentence embedding. This is the encoder phase of the model. Then this hidden state is used to produce the translation in the decoder phase.</p><p>Clearly, we are asking a lot from the hidden state in the recurrent neural network here. During encoder phase, it needs to incorporate all information about the input sentence. It cannot forget the first words towards the end of the sentence. During the decoder phase, not only does it need to have enough information to predict each next word, there also needs to be some accounting for what part of the input sentence has been already translated, and what still needs to be covered.</p><p>In practice, the proposed models works reasonable well for short sentences (up to, say, 10-15 words), but fails for long sentences. Some minor refinements to this model have been proposed, such using the sentence embedding state as input to all hidden states of the decoder phase of the model. This makes the decoder structurally different from the encoder and reduces some of the load from the hidden state during decoding, since it does not need to remember anymore the input. Another idea is to reverse the order of the output sentence, so that the last words of the input sentences are close to the last words of the output sentence.</p><p>However, in the following section, we will embark on a more significant improvement of the model, by explicitly modelling alignment of output words to input words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.5.2">Adding an Alignment Model</head><p>At the time of writing, the state of the art in neural machine translation is a sequence-tosequence encoder-decoder model with attention. That is a mouthful, but it is essentially the model we just described in the previous section, with a explicitly alignment mechanism. In the deep learning world, this alignment is called attention, we are using the words alignment and attention interchangeable here.</p><p>Since the attention mechanism does add a bit of complexity to the model, we are now slowly building up to it, by first taking a look at the encoder, then the decoder, and finally the attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head><p>The task of the encoder is to provide a representation of the input sentence. The input sentence is a sequence of words, for which we first consult the embedding matrix. Then, as in the basic language model described previously, we process these words with a recurrent neural network. This results in hidden states that encode each word with its left context, i.e., all the preceding words. To also get the right context, we also build a recurrent neural network that runs rightto-left, or more precisely, from the end of the sentence to the beginning. <ref type="figure" target="#fig_1">Figure 13</ref>.19 illustrates the model. Having two recurrent neural networks running in two directions is called a bidirectional recurrent neural network. Mathematically, the encoder consists of the embedding lookup for each input word x j , and the mapping that steps through the hidden states</p><formula xml:id="formula_70">← − h j and ← − h j ← − h j = f ( ←−− h j+1 ,Ē x j ) (13.73) − → h j = f ( −−→ h j−1 ,Ē x j ) (13.74)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Word Embeddings</head><p>Left-to-Right Recurrent NN Right-to-Left Recurrent NN <ref type="figure" target="#fig_1">Figure 13</ref>.19: Neural machine translation model, part 1: input encoder. It consists of two recurrent neural networks, running right to left and left to right (bidrectional recurrent neural network). The encoder states are the combination of the two hidden states of the recurrent neural networks.</p><p>In the equation above, we used a generic function f for a cell in the recurrent neural network. This function may be a typical feed-forward neural network layer -such as f (x) = tanh(Ax + b) -or the more complex gated recurrent units (GRUs) or long short term memory cells (LSTMs). The original paper proposing this approached used GRUs, but lately LSTMs have become more popular.</p><p>Note that we could train these models by adding a step that predicts the next word in the sequence, but we are actually training it in the context of the full machine translation model. Limiting the description to the decoder, its output is a sequence of word representations that concatenate the two hidden states (</p><formula xml:id="formula_71">← − h j , − → h j ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoder</head><p>The decoder is also a recurrent neural network. It takes some representation of the input context (more on that in the next section on the attention mechanism) and the previous hidden state and output word prediction, and generates a new hidden decoder state and a new output word prediction. See <ref type="figure" target="#fig_1">Figure 13</ref>.20 for an illustration.</p><p>Mathematically, we start with the recurrent neural network that maintains a sequence of hidden states s i which are computed from the previous hidden state s i−1 , the embedding of the previous output word Ey i−1 , and the input context c i (which we still have to define).</p><formula xml:id="formula_72">s i = f (s i−1 , Ey i−1 , c i ) (13.75)</formula><p>Again, there are several choices for the function f that combines these inputs to generate the next hidden state: linear transforms with activation function, GRUs, LSTMs, etc. Typically, the choice here matches the encoder. So, if we use LSTMs for the encoder, then we also use LSTMs for the decoder.</p><p>From the hidden state. we now predict the output word. This prediction takes the form of a probability distribution over the entire output vocabulary. If we have a vocabulary of, say, 50,000 words, then the prediction is a 50,000 dimensional vector, each element corresponding to the probability predicted for one word in the vocabulary. The prediction vector t i is conditioned on the decoder hidden state s i−1 and, again, the embedding of the previous output word Ey i−1 and the input context c i .</p><formula xml:id="formula_73">t i = softmax W (U s i−1 + V Ey i−1 + Cc i ) (13.76)</formula><p>Note that we repeat the conditioning on Ey i−1 since we use the hidden state s i−1 and not s 1 . This separates the encoder state progression from s i−1 to s i from the prediction of the output word t i .</p><p>The softmax is used to convert the raw vector into a probability distribution, where the sum of all values is 1. Typically, the highest value in the vector indicates the output word token y i . Its word embedding Ey i−1 informs the next time step of the recurrent neural network.</p><p>During training, the correct output word y i is known, so training proceeds with that word. The training objective is to give as much probability mass as possible to the correct output word. The cost function that drives training is hence the negative log of the probability given to the correct word translation.</p><formula xml:id="formula_74">cost = −log t i [y i ] (13.77)</formula><p>Ideally, we want to give the correct word the probability 1, which would mean a negative log probability of 0, but typically it is a lower probability, hence a higher cost. Note that the cost function is tied to individual words, the overall sentence cost is the sum of all word costs.</p><p>During inference on a new test sentence, we typically chose the word y i with the highest value in t i use its embedding Ey i for the next steps. But we will also explore beam search strategies where the next likely words are selected as y i , creating a different conditioning context for the next words. More on that later. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Mechanism</head><p>We currently have two loose ends. The decoder gave us a sequence of word representations h j = ( ← − h j , − → h j ) and the decoder expects a context c i at each step i. We now describe the attention mechanism that ties these ends together.</p><p>The attention mechamism is hard to visualize using our typical neural network graphs, but <ref type="figure" target="#fig_1">Figure 13</ref>.21 gives at least an idea what the input and output relations are. The attention mechanism is informed by all input word representations ( ← − h j , − → h j ) and the previous hidden state of the decoder s i−1 , and it produces a context state c i .</p><p>The motivation is that we want to compute an association between the decoder state (which contains information where we are in the output sentence production) and each input word. Based on how strong this association is, or in other words how relevant each particular input word is to produce the next output word, we want to weight the impact of its word representation.</p><p>Mathematically, we first compute this association with a feedforward layer (using weight vectors w a , u a and bias value b a )</p><formula xml:id="formula_75">a(s i−1 , h j ) = w aT s i−1 + u aT h j + b a (13.78)</formula><p>The output of this computation is a scalar value, indicating how important input word j is to produce output word i. We normalize this attention value, so that the attention values across all input words j add up to one, using the softmax.</p><formula xml:id="formula_76">α ij = exp(a(s i−1 , h j )) k exp(a(s i−1 , h k )) (13.79)</formula><p>Now we use the normalized attention value to weigh the contribution of the input word representation h j to the context vector c i and we are done.</p><formula xml:id="formula_77">c i = j α ij h j (13.80)</formula><p>Simply adding up word representation vectors (weighted or not) may at first seem an odd and simplistic thing to do. But it is very common practice in deep learning for natural language processing. Researchers have no qualms about using sentence embeddings that are simply the sum of word embeddings and other such schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.5.3">Training</head><p>With the complete model in hand, we can now take a closer look at training. One challenge is that the number of steps in the decoder and the number of steps in the encoder varies with each training example. Sentence pairs consist of sentences of different length, so we cannot have the same computation graph for each training example but instead have to dynamically create the computation graph for each of them. This technique is called unrolling the recurrent neural networks, and we already discussed it with regard to language models (recall Section 13.4.4).</p><p>The fully unrolled computation graph for a short sentence pair is shown in <ref type="figure" target="#fig_1">Figure 13</ref>.22. Note a couple of things. The error computed from this one sentence pair is the sum of the errors computed for each word. When proceeding to the next word prediction, we use the correct word as conditioning context for the decoder hidden state and the word prediction. Hence, the training objective is based on the probability mass given to the correct word, given a perfect context. There have been some attempts to use different training objectives, such as the BLEU score, but they have not yet been shown to be superior.</p><p>Practical training of neural machine translation models requires GPUs which are well suited to the high degree of parallelism inherent in these deep learning models (just think of the many matrix multiplications). To increase parallelism even more, we process several sentence pairs (say, 100) at once. This implies that we increase the dimensionality of all the state tensors.</p><p>To given an example. We represent each input word in specific sentence pair with a vector h j . Since we already have a sequence of input words, these are lined up in a matrix. When we process a batch of sentence pairs, we again line up these matrices into a 3-dimensional tensor.</p><p>Similarly, to give another example, the decoder hidden state s i is a vector for each output word. Since we process a batch of sentences, we line up their hidden states into a matrix. Note that in this case it is not helpful to line up the states for all the output words, since the states are computed sequentially.</p><p>Recall the first computation of the attention mechanism a(s i−1 , h j ) = W a s i−1 + U a h j + b a (13.81)</p><p>We can pass this computation to the GPU with a matrix of encoder states s i−1 and a 3dimensional tensor of input encodings h j , resulting in a matrix of attention values (one dimension for the sentence pairs, one dimension for the input words). Due to the massive re-use of values in W a , U a , and b a as well as the inherent parallelism of this computation, GPUs can show their true power.</p><p>You may feel that we just created a glaring contradiction. First, we argued that we have to process one training example at a time, since sentence pairs typically have different length, and  hence computation graphs have different size. Then, we argued for batching, say, 100 sentence pairs together to better exploit parallelism. These are indeed conflicting goals. See <ref type="figure" target="#fig_1">Figure 13</ref>.23. When batching training examples together, we have to consider the maximum sizes for input and output sentences in a batch and unroll the computation graph to these maximum sizes. For shorter sentences, we fill the remaining gaps with non-words and keep track of where the valid data is with a mask. This means, for instance, that we have to ensure that no attention is given to words beyond the length of the input sentence, and no errors and gradient updates are computed from output words beyond the length of the output sentence.</p><p>To avoid wasted computations on gaps, a nice trick is to sort the sentence pairs in the batch by length and break it up into mini-batches of similar length. <ref type="bibr">7</ref> To summarize, training consists of the following steps</p><p>• Shuffle the training corpus (to avoid undue biases due to temporal or topical order)</p><p>• Break up the corpus into maxi-batches</p><p>• Break up each maxi-batch into mini-batches</p><p>• Process each mini-batch, gather gradients</p><p>• Apply all gradients for a maxi-batch to update the parameters Typically, training neural machine translation models takes about 5-15 epochs (passes through entire training corpus). A common stopping criteria is to check progress of the model on a validation set (that is not part of the training data) and halt when the error on the validation set does not improve. Training longer would not lead to any further improvements and may even degrade performance due to overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.5.4">Beam Search</head><p>Translating with neural translation models proceeds one step at a time. At each step, we predict one output word. In our model, we first compute a probability distribution over all words. <ref type="bibr">7</ref> There is a bit of confusion of the technical terms here. Sometimes, the entire training corpus is called a batch, as used in the contrast between batch updating and online updating. In that context, smaller batches with a subset of the are called mini-batches (recall Section 13.2.6 on page 22). Here, we use the term batch (or maxi-batch) for such a subset, and mini-batch for a subset of the subset. We select the most likely word (the). Its embedding is part of the conditioning context for the next word prediction (and decoder state).</p><p>We then pick the most likely word and move to the next prediction step. Since the model is conditioned on the previous output word (recall Equation 13.75), we use its word embedding in the conditioning context for the next step.</p><p>See <ref type="figure" target="#fig_1">Figure 13</ref>.24 for an illustration. At each time step, we obtain a probability distribution over words. In practice, this distribution is most often quite spiked, only few words -or maybe even just one word -amass almost all of the probability. In the example, the word the received the highest probability, so we pick it as the output word.</p><p>A real example of how a neural machine translation model translates a German sentence into English is shown in <ref type="figure" target="#fig_1">Figure 13</ref>.25. The model tends to give most, if not almost all, probability mass to the top choice, but the sentence translation also indicates word choice ambiguity, such as believe (68.4%) vs. think (28.6%) or different (41.5%) vs. various (22.7%). There is also ambiguity about grammatical structure, such as if the sentence should start with the discourse connective but (42.1%) or the subject I (20.4%).</p><p>This process suggests that we perform 1-best greedy search. This makes us vulnerable to the so-called garden-path problem. Sometimes we follow a sequence of words and realize too late that we made a mistake early on. In that case, the best sequence consists of less probable words initially which are redeemed by subsequent words in the context of the full output. Consider the case of having to produce an idiomatic phrase that is non-compositional. The first words of these phrases may be really odd word choices by themselves (e.g., piece of cake for easy). Only once the full phrase is formed, their choice is redeemed.</p><p>Note that we are faced with the same problem in traditional statistical machine translation models -arguable even more so there since we rely on sparser contexts when making predictions for the next words. Decoding algorithms for these models keep a list of the n-best candidate hypotheses, expand them and keep the n-best expanded hypotheses. We can do the same for neural translation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Sentence</head><p>ich glaube aber auch , er ist clever genug um seine Aussagen vage genug zu halten , so dass sie auf verschiedene Art und Weise interpretiert werden können .  When predicting the first word of the output sentence, we keep a beam of the top n most likely word choices. They are scored by their probability. Then, we use each of these words in the beam in the conditioning context for the next word. Due to this conditioning, we make different word predictions for each. We now multiply the score for the partial translation (at this point just the probability for the first word), and the probabilities from its word predictions. We select the highest scoring word pairs for the next beam. See <ref type="figure" target="#fig_1">Figure 13</ref>.26 for an illustration. This process continues. At each time step, we accumulate word translation probabilities, giving us scores for each hypothesis. A sentence translation is complete, when the end of sentence token is produced. At this point, we remove the completed hypothesis from the beam and reduce beam size by 1. Search terminates, when no hypotheses are left in the beam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output Word Predictions</head><p>Search produces a graph of hypotheses, as shown in <ref type="figure" target="#fig_1">Figure 13</ref>.27. It starts with the start of sentence symbol &lt;s&gt; and its paths terminate with the end of sentence symbol &lt;/s&gt;. Given the compete graph, the resulting translations can be obtained by following the back-pointers. The complete hypothesis (i.e., one that ended with a &lt;/s&gt; symbol) with the highest score points to the best translation.</p><p>When choosing among the best paths, we score each with the product of its word prediction probabilities. In practice, we get better results when we normalize the score by the output length of a translation, i.e., divide by the number of words. We carry out this normalization after search is completed. During search, all translations in a beam have the same length, so the normalization would make no difference.</p><p>Note that in traditional statistical machine translation, we were able to combine hypotheses if they share the same conditioning context for future feature functions. This not possible anymore for recurrent neural networks since we condition on the entire output word sequence from the beginning. As a consequence, the search graph is generally less diverse than search At each time step, the n = 6 best partial translations (called hypotheses) are selected. An output sentence is complete when the end of sentence token &lt;/s&gt; is predicted. We reduce the beam after that and terminate when n full sentence translations are completed. Following the back-pointers from the end of sentence tokens allows us to read them off. Empty boxes represent hypotheses that are not part of any complete path. graphs in statistical machine translation models. It is really just a search tree where the number of complete paths is the same as the size of the beam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Further Readings</head><p>The attention model has its roots in a sequence-to-sequence model. Cho et al. The seminal work by <ref type="bibr" target="#b1">Bahdanau et al. (2015)</ref> adds an alignment model (so called "attention mechanism") to link generated output words to source words, which includes conditioning on the hidden state that produced the preceding target word. Source words are represented by the two hidden states of recurrent neural networks that process the source sentence left-to-right and right-to-left. <ref type="bibr" target="#b54">Luong et al. (2015b)</ref> propose variants to the attention mechanism (which they call "global" attention model) and also a hard-constraint attention model ("local" attention model) which is restricted to a Gaussian distribution around a specific input word.</p><p>To explicitly model the trade-off between source context (the input words) and target context (the already produced target words), <ref type="bibr" target="#b85">Tu et al. (2016a)</ref> introduce an interpolation weight (called "context gate") that scales the impact of the (a) source context state and (b) the previous hidden state and the last word when predicting the next hidden state in the decoder. <ref type="bibr" target="#b86">Tu et al. (2017)</ref> augment the attention model with a reconstruction step. The generated output is translated back into the input language and the training objective is extended to not only include the likelihood of the target sentence but also the likelihood to the reconstructed input sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.6">Refinements</head><p>The previous section gave a comprehensive description of the currently most commonly used basic neural translation model architecture. It performs fairly well out of the box for many</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Checkpoint ensemble</head><p>Multi-run ensemble <ref type="figure" target="#fig_1">Figure 13</ref>.28: Two methods to generate alternative systems for ensembling: Checkpoint ensembling uses model dumps from various stages of the training process, while multi-run ensembling starts independent training runs with different initial weights and order of the training data.</p><p>language pairs. Since its conception, a number of refinements have been proposed. We will describe them in this section.</p><p>Some of the refinements are fairly general, some target particular use cases or data conditions. To given one example, the best performing system at the recent WMT 2017 evaluation campaign used ensemble decoding (Section 13.6.1), byte pair encoding to address large vocabularies (Section 13.6.2), added synthetic data derived from monolingual target side data (Section 13.6.3) , and used deeper models (Section 13.6.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.6.1">Ensemble Decoding</head><p>A common technique in machine learning is to not just build one system for your problem, but multiple ones and then combine them. This is called an ensemble of systems. It is such a successful strategy that various methods have been proposed to systematically build alternative systems, for instance by using different features or different subsets of the data. For neural networks, one straightforward way is to use different initializations or stop at different points in the training process.</p><p>Why does it work? The intuitive argument is that each system makes different mistakes. When two systems agree, then they are more likely both right, rather than both make the same mistake. One can also see the general principle at play in human behavior, such as setting up committees to make decisions or the democratic voting in elections.</p><p>Applying ensemble methods to our case of neural machine translation, we have to address two sub-problems: (1) generating alternate systems, and (2) combining their output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generating Alternative Systems</head><p>See <ref type="figure" target="#fig_1">Figure 13</ref>.28 for an illustration of two methods for the first sub-problem, generating alternative system. When training a neural translation model, we iterate through the training data until some stopping criteria is met. This is typically a lack of improvements of the cost function applied to a validation set (measured in cross-entropy), or the translation performance on that validation set (measured in BLEU).</p><p>During training, we dump out the model at fixed intervals (say, every 10,000 iteration of batch processing). Once training is completed, we can look back at the performance of the .00</p><p>.07</p><p>.20</p><p>.00</p><p>Model 4</p><p>.37</p><p>.10</p><p>.08</p><p>.02</p><p>.07</p><p>.03</p><p>.00</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Average</head><p>.06 <ref type="figure" target="#fig_1">Figure 13</ref>.29: Combining predictions from a ensemble of models: Each model independently predicts a probability distribution over output words, which are averaged into a combined distribution.</p><p>model these different stages. We then pick the, say, 4 models with the best performance (typically translation quality measured in BLEU). This is called checkpoint ensembling since we select the models at different checkpoints in the training process.</p><p>Multi-run ensembling requires building systems in completely different training runs. As mentioned before, this can be accomplished by using different random initialization of weights, which leads training to seek out different local optima. We also randomly shuffle the training data, so using different random order will also lead to different training outcomes.</p><p>Multi-run ensembling usually works a good deal better, but it is also computationally much more expensive. Note that multi-run ensembling can also build on checkpoint ensembling. Instead of combining the end points of training, we first apply checkpoint ensembling to each run, and then combine those ensembles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combine System Output</head><p>Neural translation models allow the combination of several systems fairly deeply. Recall that the model first predicts a probability distribution over possible output words, and then commits to one of the words. This is where we combine the different trained models. Each model predicts a probability distribution and we then combine their predictions. The combination is done by simple averaging over the distributions. The averaged distribution is then the basis for selecting an output word.</p><p>See <ref type="figure" target="#fig_1">Figure 13</ref>.29 for an illustration. There may be some benefit to weighing the different systems differently, although in our way of generating them, they will all have very similar quality, so this is not typically done.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reranking with Right-to-Left Decoding</head><p>One more tweak on the idea of ensembling: Instead of building multiple systems with different random initialization, we can also build one set of system as before, and then a second set of system where we reverse the order of the output sentences. The second set of systems are called right-to-left systems, although arguably this is not a good name since it makes no sense for languages such as Arabic or Hebrew where the normal writing order is right to left. The deep integration we described just above does not work anymore for the combination of left-to-right and right-to-left systems, since they produce output in different order. So, we have to resort to reranking. This involves several steps:</p><p>• Use an ensemble of left-to-right systems to generate an n-best list of candidate translations for each input sentence.</p><p>• Score each candidate translation with the individual left-to-right and right-to-left systems.</p><p>• Combine the scores (simple average) of the different models for each candidate, select the candidate with the best score for each input sentence.</p><p>Scoring a given candidate translation with a right-to-left system does require the require forced decoding, a special mode of running inference on an input sentence, but predicting a given output sentence. This mode is actually much closer to training (where also an output translation is given) the regular inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.6.2">Large Vocabularies</head><p>Zipf's law tells us that words in a language are very unevenly distribution. So, there is always a large tail of rare words. New words come into the language all the time (e.g., retweeting, website, woke), and we also have to deal with a very large inventory of names, including company names (e.g., eBay, Yahoo, Microsoft).</p><p>On the other hand, neural methods are not well equipped to deal with such large vocabularies. The ideal representations for neural networks are continuous space vectors. This is why we first convert discrete objects such as words into such word embeddings.</p><p>However, ultimately the discrete nature of words shows up. On the input side, we need to train an embedding matrix that maps each word into its embedding. On the output side we predict a probability distribution over all output words. The latter is generally the bigger concern, since the amount of computation involved is linear with the size of the vocabulary, making this a very large matrix operation.</p><p>Hence, neural translation models typically restrict the vocabulary to, say, 20,000 to 80,000 words. In initial work on neural machine translation, only the most frequent words were used, and all others represented by a unknown or other tag. The translation of these rare words was handled with a back-off dictionary.</p><p>Obama receives Net@@ any@@ ahu the relationship between Obama and Net@@ any@@ ahu is not exactly friendly . the two wanted to talk about the implementation of the international agreement and about Teheran 's destabil@@ ising activities in the Middle East . the meeting was also planned to cover the conflict with the Palestinians and the disputed two state solution . relations between Obama and Net@@ any@@ ahu have been stra@@ ined for years . Washington critic@@ ises the continuous building of settlements in Israel and acc@@ uses Net@@ any@@ ahu of a lack of initiative in the peace process . the relationship between the two has further deteriorated because of the deal that Obama negotiated on Iran 's atomic programme . in March , at the invitation of the Republic@@ ans , Net@@ any@@ ahu made a controversial speech to the US Congress , which was partly seen as an aff@@ ront to Obama . the speech had not been agreed with Obama , who had rejected a meeting with reference to the election that was at that time im@@ pending in Israel . <ref type="figure" target="#fig_1">Figure 13</ref>.30: Byte pair encoding (BPE) applied to English (model used 49,500 BPE operations). Word splits are indicated with @@. Note that the data is also tokenized and true-cased.</p><p>The more common approach today is to break up rare words into subword units. This may seem a bit crude but is actually very similar to standard approaches in statistical machine translation to handle compounds (recall website → web + site) and morphology (unfollow → un + follow, convolutions → convolution + s). It is even a decent approach to the problem of transliteration of names which are traditionally handled by a sub-modular letter translation component.</p><p>A popular method to create an inventory of subword units and legitimate words is byte pair encoding. This method is trained on the parallel corpus. First, the words in the corpus are split into characters (marking original spaces with a special space character). Then, the most frequent pair of characters is merged (in English, this may be t and h into th). This step is repeated for a fixed given number of times. Each of these steps increases the vocabulary by one, beyond the original inventory of single characters.</p><p>The example mirrors quite well the behavior of the algorithm on real-world data sets. It starts with grouping together with frequent letter combinations (e+r, t+h, c+h) and then joins frequent words (the, in, of). At the end of this process, the most frequent words will emerge as single tokens, while rare words consist of still un-merged subwords. See <ref type="figure" target="#fig_1">Figure 13</ref>.30 for an example, where subword units are indicated with two "at" symbols (@@). After 49,500 byte pair encoding operations, the vast majority of words are intact, while rarer words are broken up (e.g., critic@@ ises, destabil@@ ising). Sometimes, the split seem to be morphologically motivated (e.g., im@@ pending), but mostly they are not (e.g., stra@@ ined). Note also the decomposition of the relatively rare name Net@@ any@@ ahu.</p><p>Further Readings A significant limitation of neural machine translation models is the computational burden to support very large vocabularies. To avoid this, typically the vocabulary is reduced to a shortlist of, say, 20,000 words, and the remaining tokens are replaced with the unknown word token "UNK". To translate such an unknown word, <ref type="bibr" target="#b55">Luong et al. (2015c)</ref>; <ref type="bibr" target="#b38">Jean et al. (2015a)</ref> resort to a separate dictionary. <ref type="bibr" target="#b0">Arthur et al. (2016)</ref> argue that neural translation models are worse for rare words and interpolate a traditional probabilistic bilingual dictionary with the prediction of the neural machine translation model. They use the attention mechanism to link each target word to a distribution of source words and weigh the word translations accordingly.</p><p>Source words such as names and numbers may also be directly copied into the target. <ref type="bibr" target="#b34">Gulcehre et al. (2016)</ref> use a so-called switching network to predict either a traditional translation operation or a copying operation aided by a softmax layer over the source sentence. They preprocess the training data to change some target words into word positions of copied source words. Similarly, <ref type="bibr" target="#b33">Gu et al. (2016)</ref> augment the word prediction step of the neural translation model to either translate a word or copy a source word. They observe that the attention mechanism is mostly driven by semantics and the language model in the case of word translation, but by location in case of copying.</p><p>To speed up training, <ref type="bibr" target="#b59">Mi et al. (2016)</ref> use traditional statistical machine translation word and phrase translation models to filter the target vocabulary for mini batches. <ref type="bibr" target="#b77">Sennrich et al. (2016d)</ref> split up all words to sub-word units, using character n-gram models and a segmentation based on the byte pair encoding compression algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.6.3">Using Monolingual Data</head><p>A key feature of statistical machine translation system are language models, trained on very large monolingual data set. The larger the language models, the higher translation quality. Language models trained on up to a trillion words crawled from the general web have been used. So, it is a surprise that the basic neural translation model does not use any additional monolingual data, its language model aspect (the conditioning of the previous hidden decoder state and the previous output) is trained jointly with the translation model aspect (the conditioning on the input context).</p><p>Two main ideas have been proposed to improve neural translation models with monolingual data. One is to transform additional monolingual translation into parallel data by synthesizing the missing half of the data, and the other is to integrate a language model as a component into the neural network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Back Translation</head><p>Language models improve fluency of the output. Using larger amounts of monolingual data in the target language give the machine more evidence what are common sequences of words and what are not.</p><p>We cannot use monolingual target side data in our neural translation model training, since it is missing the source side. So, one idea is to just synthesize this data by back translation. See <ref type="figure" target="#fig_1">Figure 13</ref>.31 for an illustration of the steps involved.</p><p>• Train a reverse system that translates from the intended target language into the source language. We typically use the same neural machine translation setup for this as for our final system, just with source and target flipped. But we may use any system, even traditional phrase-based systems. reverse system final system <ref type="figure" target="#fig_1">Figure 13</ref>.31: Creating synthetic parallel data from target-side monolingual data: (1) train a system in reverse order, (2) use it to translate target-side monolingual data into the source language, (3) combine the generated synthetic parallel data with the true parallel data in final system building.</p><p>• Use the reverse system to translate target side monolingual data, creating a synthetic parallel corpus.</p><p>• Combine the generated synthetic parallel data with the true parallel data when building the final system.</p><p>There is an open question on how much synthetic parallel data should be used in relation to the amount of existing true parallel data. Typically, there are magnitudes more monolingual data available, but we also do not want to drown out the actual real data. Successful applications of this idea used equal amounts of synthetic and true data. We may also generate much more synthetic parallel data, but then ensure during training that we process equal amounts of each by over-sampling the true parallel data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adding a Language Model</head><p>The other idea is to train a language model as a separate component of the neural translation model. <ref type="bibr" target="#b35">Gülçehre et al. (2015)</ref> first train the large language model as a recurrent neural network on all available data, including the target side of the parallel corpus. Then, they add this language model to the neural translation model. Since both language model and translation model predict output words, the natural point to connect the two models is joining them at that output prediction node in the network by concatenating their conditioning contexts.</p><p>We expand Equation 13.75 to add the hidden state of the neural language model s LM i to the hidden state of the neural translation model s TM i , the source context c i and the previous English word e i−1 .</p><formula xml:id="formula_78">e i = g(c i , s TM i , s LM i , e i−1 ) (13.82)</formula><p>When training the combined model, we leave the parameters of the large neural language model unchanged, and update only the parameters of the translation model and the combination layer. The concern is that otherwise the output side of the parallel corpus would overwrite</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MT f→e</head><p>MT f←e e f <ref type="figure" target="#fig_1">Figure 13</ref>.32: Round trip training: In addition to training two models f→e and e→f, as done traditionally on parallel data, we also optimize both models to convert a sentence f into e and then restore it back into f , using on monolingual data in f . We may add a corresponding round trip starting with e. the memory of the large monolingual corpus. In other words, the language model would overfit to the parallel training data and be less general.</p><p>One final question remains: How much weight should be given to the translation model and how much weight should be given to the language model? The above equation considers them in all instances the same way. But there may be output words for which the translation model is more relevant (e.g., the translation of content words with distinct meaning) and output words where the language model is more relevant (e.g., the introduction of relevant function words for fluency).</p><p>The balance of the translation model and the language model can be achieved with the type of gated units that we encountered in our discussion of the long short-term memory neural network architecture <ref type="bibr">(Section 13.4.5)</ref>. Such a gated unit may be predicted solely from the language model state s LM i and then used as a factor that is multiplied with that language model state before it is used in the prediction of Equation 13.82.</p><formula xml:id="formula_79">gate LM i = f (s LM i ) s LM i = gate LM i × s LM i e i = g(c i , s TM i ,s LM i , e i−1 ) (13.83)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Round Trip Training</head><p>Looking at the backtranslation idea from a strict machine learning perspective, we can see two learning objectives. There is the objective to learn the transformations given by the parallel data, as done traditionally. Then, there is the goal to learn how to convert an output lamguage sentence into the input language, and then back into the output language with the objective to match the traditional sentence. A good machine translation model should be able to preserve the meaning of the output language sentence when mapped into the input language and back.</p><p>See <ref type="figure" target="#fig_1">Figure 13</ref>.32 for an illustration. There are two machine translation models. One that translates sentences in the language direction f→e, the other in the opposite direction e→f. These two systems may be trained with traditional means, using a parallel corpus. We can also round trip a sentence f first through the f→e and then back through e→f In this scenario, there are two objectives for model training.</p><p>• The translation e' of the given monolingual sentence f should be a valid sentence in the language e, as measured with a language model LM e (e').</p><p>• The reconstruction of the translation e' back into the original language f should be easy, as measured with the translation model MT e→f (f|e')</p><p>These two objectives can be used to update model parameters in both translation models MT f →e and MT e→f . Typical model update is driven by correct predictions of each word. In this round-trip scenario, the translation e' has to be computed first, before we can do the usual training of model MT e→f with the given sentence pair (e',f). To make better use of the training data, a n-best list of translations e' 1 , ..., e' n is computed and model updates are computed for each of them.</p><p>We can also update the model MT f →e with monolingual data in language f by scaling updates by the language model cost LM e (e' i ) and the forward translation cost MT f →e (e' i |f) for each of the translations e' i in the n-best list.</p><p>To use monolingual data in language e, training is done in the reverse round trip direction. For details of this idea, refer to <ref type="bibr" target="#b98">Xia et al. (2016)</ref>. <ref type="bibr" target="#b76">Sennrich et al. (2016c)</ref> back-translate the monolingual data into the input language and use the obtained synthetic parallel corpus as additional training data. <ref type="bibr" target="#b98">Xia et al. (2016)</ref> use monolingual data in a dual learning setup. Machine translation engines are trained in both directions, and in addition to regular model training from parallel data, monolingual data is translated in a round trip (e to f to e) and evaluated with a language model for language f and reconstruction match back to e as cost function to drive gradient descent updates to the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Further Readings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.6.4">Deep Models</head><p>Learning the lessons from other research fields such as vision or speech recognition, recent work in machine translation has also looked at deeper models. Simply put, this involves adding more intermediate layers into the baseline architecture.</p><p>The core components of neural machine translation are the encoder that takes input words and converts them into a sequence of contextualized representations and the decoder that generates a output sequence of words. Both are recurrent neural networks.</p><p>Recall that we already discussed how to build deeper recurrent neural networks for language modelling (refer back to Section 13.4.7 on page 44). We now extend these ideas to the recurrent neural networks in the encoder and the decoder.</p><p>What all these recurrent neural networks have in common is that they process an input sequence into an output sequence, and at each time step t information from a new input x t is combined with the hidden state from the previous time step h t−1 to predict a new hidden state h t . From that hidden state additional predictions may be made (output words y t in the case of the decoder, the next word in the sequence in the case of language models), or the hidden state is used otherwise (via the attention mechanism in case of the encoder). Decoder See <ref type="figure" target="#fig_1">Figure 13</ref>.33 for part of the decoder in neural machine translation, using a particular deeper architecture. We see that instead of a single hidden state h t for a given time step t, we now have a sequence of hidden states h t,1 , h t,2 , ..., h t,I for a given time step t.</p><p>There are various options how the hidden states may be connected. Previously, in Section 13.4.7 we presented two ideas. (1) In stacked recurrent neural networks where a hidden state h t,i is conditioned on the hidden state from a previous layer h t,i−1 and the hidden state at the same depth from a previous time step h t−1,i . (2) In deep transition recurrent neural networks, the first hidden state h t,1 is conditioned on the last hidden state from the previous time step h t−1,I and the input, while the other hidden layers h t,I (i &gt; 1) are just conditioned on the previous previous layer h t,i−1 . <ref type="figure" target="#fig_1">Figure 13</ref>.33 combines these two ideas. some layers are both stacked (conditioned on the previous time step h t−1,I and previous layer h t,i−1 ), while others are deep transitions (conditioned only on the previous layer h t,i−1 .</p><p>Mathematically, we can break this out into the stacked layers h t,i : <ref type="bibr">(13.84)</ref> and the deep transition layers  <ref type="figure" target="#fig_1">Figure 13</ref>.34: Deep alternating encoder: combination of the idea of a bidirectional recurrent neural network previously proposed for neural machine translation (recall <ref type="figure" target="#fig_1">Figure 13</ref>.19 on page 49) and the stacked recurrent neural network (recall <ref type="figure" target="#fig_1">Figure 13</ref>.19 on page 49). This architecture may be further extended with the idea of deep transitions, as shown for the decoder (previous <ref type="figure" target="#fig_1">Figure 13</ref>.33).</p><formula xml:id="formula_80">h t,1 = f 1 (x t , h t−1,1 ) h t,i = f i (h t,i−1 , h t−1,i ) for i &gt; 1</formula><formula xml:id="formula_81">v i,i,j . v t,i,1 = g i,1 (in t,i , h t−1,i ) in t,i is either x t or h t,i−1 v t,i,j = g i,j (v t,i,j−1 ) for j &gt; 1 h t,i = v t,i,J<label>(</label></formula><p>The function f i (h t,i−1 , h t−1,i ) is computed as a sequence of function calls g i,j . Each of the functions g i,j may be implemented as feed-forward neural network layer (matrix multiplication plus activation function), long-short term memory cell (LSTM), or gated recurrent unit (GRU). On either case, each function g i,j has its own set of trainable model parameters.</p><p>Encoder Deep recurrent neural networks for the encoder may draw in the same ideas as the decoder, with one addition: in the baseline neural translation model, we used bidirectional recurrent neural networks to condition on both left and right context. We want to do the same for any deep version of the encoder. <ref type="figure" target="#fig_1">Figure 13</ref>.34 shows one idea how this could be done, called alternating recurrent neural network. It looks basically like a stacked recurrent neural network, with one twist: the hidden states at each layer h t,i are alternately conditioned on the hidden state from the previous time step h t−1,i or the next time step h t+1,i .</p><p>Mathematically, we formulate this as even numbered hidden states h t,2i being conditioned on the left context h t−1,2i and odd numbered hidden states h t,2i+1 conditioned on the right context h t+1,2i .</p><formula xml:id="formula_82">h t,1 = f (x t , h t−1,1 ) h t,2i = f (h t,2i−1 , h t−1,2i ) h t,2i+1 = f (h t,2i , h t+1,2i+1 ) (13.86)</formula><p>As before in the encoder, we can extend this idea by having deep transitions.</p><p>Note that deep models are typically augmented with direct connections from the input to the output. In the case of the encoder, this may mean a direct connection from the embedding to the final encoder layer, or connections at each layer that pass the input directly to the output.  <ref type="figure" target="#fig_1">Figure 13</ref>.35: Alignment vs. Attention: In this example, alignment points from traditional word alignment methods are shown as squares, and attention states as shaded boxes depending on the alignment value (shown as percentage). They generally match up well, but note for instance that the prediction of the output auxiliary verb sind pays attention to the entire verb group have been strained.</p><p>Such residual connections help with training. In early stages, the deep architecture can be skipped. Only when a basic functioning model has been acquired, the deep architecture can be exploited to enrich it. We typically see the benefits of residual connections in early training stages (faster initial reduction of model perplexity), and less so as improvement in the final converged model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Further Readings</head><p>Recent work has shown good results with 4 stacks and 2 deep transitions each for encoder and decoder, as well as alternating networks for the encoder <ref type="bibr" target="#b61">(Miceli Barone et al., 2017)</ref>.</p><p>There are a large number of variations (including the use of skip connections, the choice of LSTM vs. GRU, number of layers of any type) that still need to be explored empirical for various data conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.6.5">Guided Alignment Training</head><p>The attention mechanism in neural machine translation models is motivated by the need to align output words to input words. <ref type="figure" target="#fig_1">Figure 13.35</ref> shows an example of attention weights given to English input words for each German output word during the translation of a sentence.</p><p>The attention values typically match up pretty well with word alignment used in traditional statistical machine translation, obtained with tools such as GIZA++ or fast-align which implement variants of the IBM Models.</p><p>There are several good uses for word alignments beyond their intrinsic value of improving the quality of translations. For instance in the next section, we will look at using the attention mechanism to explicitly track coverage of the input. We may also want to override preferences of the neural machine translation model with pre-specified translations of certain terminology or expressions such as numbers, dates, or measurements that are better handled by rule-based components; this requires to know when the neural model is about to translate a specific source word. But also the end user may be interested in alignment information, such as translators using machine translation in a computer aided translation tool may want to check where an output word originates from.</p><p>Hence, instead of trusting the attention mechanism to implicitly acquire the role as word alignmer, we may enforce this role. The idea is to provide not just the parallel corpus as training data, but also pre-computed word alignments using traditional means. Such additional information may even benefit training of models to converge faster or overcome data sparsity under low resource conditions.</p><p>A straightforward way to add such given word alignment to the training process is to not change the model at all, but to just modify the training objective. Typically, the goal of training neural machine translation models is to generate the correct output words. We can add to this goal to also match the given word alignment.</p><p>Formally, we assume to have access to an alignment matrix A that specifies alignment points A ij input words j and output words i in a way that j A ij = 1, i.e., each output word's alignment scores add up to 1. The model estimates attention scores α ij that also add up to 1 for each output word: j α ij = 1 (recall Equation 13.79 on page 51). The mismatch between given alignment scores A ij and computed attention scores α ij can be measured in several ways, such as cross entropy <ref type="bibr">13.87)</ref> or mean squared error</p><formula xml:id="formula_83">cost CE = − 1 I I i=1 J j=1 A ij log α ij<label>(</label></formula><formula xml:id="formula_84">cost MSE = − 1 I I i=1 J j=1 (A ij − α ij ) 2 (13.88)</formula><p>This cost is added to the training objective and may be weighted. <ref type="bibr" target="#b10">Chen et al. (2016b)</ref>;  add supervised word alignment information (obtained with traditional statistical word alignment methods) to training. They augment the objective function to also optimize matching of the attention mechanism to the given alignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Further Readings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.6.6">Modeling Coverage</head><p>One impressive aspect of neural machine translation models is how well they are able to translate the entire input sentence, even when a lot of reordering is involved. But this as aspect is not perfect, occasionally the model translates some input words multiple times, and sometimes it misses to translate them.  <ref type="figure" target="#fig_1">Figure 13</ref>.36: Example for over-generation and under-generation: the input tokens around Social Housing are attended too much, leading to hallucinated output words (das Unternehmen, English: the company), while the end of the sentence a fresh start is not attended and untranslated.</p><p>See <ref type="figure" target="#fig_1">Figure 13</ref>.36 for an example. The translation has two flaws related to mis-allocation of attention. The beginning of the phrase "Social Housing" alliance receives too much attention, resulting in a faulty translation with hallucinated words: das Unternehmen der Gesellschaft für soziale Bildung, or the company of the society for social education. At the end of the input sentence, the phrase a fresh start does not receive any attention and is hence untranslated in the output.</p><p>Hence, an obvious idea is to more strictly model coverage. Given the attention model, a reasonable way to define coverage is by adding up the attention states. In a complete sentence translation, we roughly expect that each input word receives a similar amount of attention. If some input words never receive attention or too much attention that signals a problem with the translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Enforcing Coverage during Inference</head><p>We may restrict the enforcing of proper coverage to the decoder. When considering multiple hypothesis in beam search, then we should discourage the ones that pay too much attention to some input words. And, once hypotheses are completed, we can penalize those that paid only little attention to some of the input. There are various ways to come up with scoring functions for over-generation and under-generation. The use of multiple scoring functions in the decoder is common practice in traditional statistical machine translation. For now, it is not in neural machine translation. A challenge is to give proper weight to the different scoring functions. If there are only two or three weights, these can be optimized with grid search over possible values. For more weights, we may borrow methods such as MERT or MIRA from statistical machine translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coverage Models</head><p>The vector that accumulates coverage of input words may be directly used to inform the attention model. Previously, the attention given to a specific input word j was conditioned on the previous state of the decoder s i−1 and the representation of the input word h j . Now, we also add as conditioning context the accumulated attention given to the word (compare to Equation 13.78 on page 51).</p><formula xml:id="formula_85">a(s i−1 , h j ) = W a s i−1 + U a h j + V a coverage(j) + b a (13.90)</formula><p>Coverage tracking may also integrated into the training objective. Taking a page from the guided alignment training (recall the previous Section 13.6.5), we augment the training objective function with a coverage penalty with some weight λ. log i P (y i |x) + λ j (1 − coverage(j)) 2 (13.91)</p><p>Note that in general, it is problematic to add such additional functions to the learning objective, since it does distract from the main goal of producing good translations.</p><p>Fertility So far, we described coverage as the need to cover all input words roughly evenly. However, even the earliest statistical machine translation models considered the fertility of words, i.e., the number of output words that are generated from each input word. Consider the English do not construction: most other language do not require an equivalent of do when negating a verb. Meanwhile, other words are translated into multiple output words. For instance, the German natürlich may be translated as of course, thus generating 2 output words.</p><p>We may augment models of coverage by adding a fertility components that predicts the number of output words for each input words. Here one example for a model that predicts the fertility Φ j for each input word, and uses it to normalize the coverage statistics.</p><formula xml:id="formula_86">Φ j = N σ(W j h j ) coverage(j) = 1 Φ j i k α i,k (13.92)</formula><p>Fertility Φ j is predicted with a neural network layer that is conditioned on the input word representation h j and uses a sigmoid activation function (thus resulting in values from 0 to 1), which is scaled to a pre-defined maximum fertility of N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Engineering versus Machine Learning</head><p>The work on modeling coverage in neural machine translation models is a nice example to contrast between the engineering approach and the belief in generic machine learning techniques. From an engineering perspective, a good way to improve a system is to analyze its performance, find weak points and consider changes to overcome them. Here, we notice over-generation and under-generation with respect to the input, and add components to the model to overcome this problem. On the other hand, proper coverage is one of the features of a good translation that machine learning should be able to get from the training data. If it is not able to do that, it may need deeper models, more robust estimation techniques, ways to fight over-fitting or under-fitting, or other adjustments to give it just the right amount of power needed for the problem.</p><p>It is hard to carry out the analysis needed to make generic machine learning adjustments, given the complexity of a task like machine translation. Still, the argument for deep learning is that it does not require feature engineering, such as adding coverage models. It remains to be seen how neural machine translation evolves over the next years, and if it moves more into a engineering or machine learning direction.</p><p>Further Readings To better model coverage, <ref type="bibr" target="#b87">Tu et al. (2016b)</ref> add coverage states for each input word by either (a) summing up attention values, scaled by a fertility value predicted from the input word in context, or (b) learning a coverage update function as a feed-forward neural network layer. This coverage state is added as additional conditioning context for the prediction of the attention state. <ref type="bibr" target="#b22">Feng et al. (2016)</ref> condition the prediction of the attention state also on the previous context state and also introduce a coverage state (initialized with the sum of input source embeddings) that aims to subtract covered words at each step. Similarly, <ref type="bibr" target="#b56">Meng et al. (2016)</ref> separate hidden states that keep track of source coverage and hidden states that keep track of produced output. <ref type="bibr" target="#b14">Cohn et al. (2016)</ref> add a number of biases to model coverage, fertility, and alignment inspired by traditional statistical machine translation models. They condition the prediction of the attention state on absolute word positions, the attention state of the previous output word in a limited window, and coverage (added attention state values) over a limited window. They also add a fertility model and add coverage in the training objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.6.7">Adaptation</head><p>Text may differ by style, topic, degree of formality, and so on. A common problem in the practical development of machine translation systems is that most of the available training data is different from the data relevant to a chosen use case. For instance, if your goal is to translate chat room dialogs, you will realize that there is very little translated chat room data available. There are massive quantities of official publications from international organizations, random translations crawled from the web, and maybe somewhat relevant movie subtitle translations. This problem is generally framed as a problem of domain adaptation. In the simplest form, you have one set of data relevant to your use case -the in-domain data -and another set that is less relevant -the out-of-domain data.</p><p>In traditional statistical machine translation, a vast number of methods for domain adaptation have been proposed. Models may be interpolated, we may back-off from in-domain to out-of-domain models, we may over-sample in-domain data during training or sub-sample out-of-domain data, etc.</p><p>For neural machine translation, a fairly straightforward method is currently the most popular (see <ref type="figure" target="#fig_1">Figure 13</ref>.37). This method divides training up into two stages. First, we train the model on all available data until convergence. Then, we run a few more iterations of training on the in-domain data only and stop training when performance on the in-domain validate set peaks. This way, the final model benefits from all the training data, but is still specialized to the in-domain data.</p><p>Practical experience with this method shows that the second in-domain training stage may converge very quickly. The amount if in-domain data is typically relatively small, and only a handful of training epochs are needed.</p><p>Another, less commonly used method draws on the idea of ensemble decoding (Section 13.6.1). If we train separate models on different sets of data, we may combine their predictions, just as we did for ensemble decoding. In this case, we do want to choose weights for each model, although how to choose these weights is not a trivial task. If there is just an in-domain and out-of-domain model, however, this may be simply done by line search over possible values.</p><p>Let us now look at a few special cases that arise in practical use.</p><p>Subsample in-domain data from large collections A common problem is that the amount of available in-domain data is very small, so just training on this data, even in a secondary adaptation stage, risks overfitting -very good performance on the seen data but poor performance on everything else.</p><p>Large random collections of parallel text often contain data that closely matches the indomain data. So, we may want to extract this in-domain data from the large collections of mainly out-of-domain data. The general idea behind a variety of methods is to build two detectors: one in-domain detector trained on in-domain data, and one out-of-domain detector trained on out-of-domain data. We then score each sentence pair in the out-of-domain data with both detectors and select sentence pairs that are preferred (or judged relatively relevant) by the in-domain detector.</p><p>The classic detectors are language models trained on the source and target side of the indomain and out-of-domain data, resulting in a total of 4 language models: the source side in-domain model LM in f , the target side in-domain model LM in e , the source side out-of-domain model LM out f , and the target side out-of-domain model LM out e . Any given sentence pair from the out-of-domain data is then scored based on these models:</p><formula xml:id="formula_87">relevance e,f = LM in e (e) − LM out e (e) + LM in f (f ) − LM out f (f ) (13.93)</formula><p>We may use traditional n-gram language models or neural recurrent language models. Some work suggests to replace open class words (nouns, verbs, adjectives, adverbs) with part-of-speech tags or word clusters. More sophisticated models not only consider domainrelevance but noisiness of the training data (e.g., misaligned or mistranslated data). We may even use in-domain and out-of-domain neural translation models to score sentence pairs instead of source and target side sentences in isolation.</p><p>The subsampled data may be used in several ways. We may only train on this data to build our domain-specific system. Or, we use it in a secondary adaptation stage as outlined above.</p><p>Only monolingual in-domain data What if we have no parallel data in the domain of our use case? Two main ideas have been explored. Firstly, we may still use the monolingual data, may it be in the source or target language or both, for subsampling parallel data from a large pile of general data, as outline above.</p><p>Another idea is to use existing parallel data to train an out-of-domain model, then backtranslate out-of-domain data (recall Section 13.6.3) to generate a synthetic in-domain corpus, and then use this data to adapt the initial model. In traditional statistical machine translation, much adaptation success has been achieved with just interpolating the language model, and this idea is the neural translation equivalent to that.</p><p>Multiple domains Sometimes, we have multiple collections of data that are clearly identified by domain -typically categories such as information technology, medical, law, etc. We can use the techniques described above to build specialized translation models for each of these domains.</p><p>For a given test sentence, we then select the appropriate model. If we do not know the domain of the test sentence, we first have to build a classifier that allows us to automatically make this determination. The classifier may be based on the methods for domain detectors described above. Given the decision of the classifier, we then select the most appropriate model.</p><p>But we do not have to commit to a single domain. The classifier may instead provide a distribution of relevance of the specific domain models (say, 50% domain A, 30% domain B, 20% domain C) which are then used as weights in an ensemble of domain-specific models.</p><p>The domain classification may done based on a whole document instead of each individual sentence, which brings in more context to make a more robust decision.</p><p>As a final remark, it is hard to give conclusive advice on how to handle adaptation challenges, since it is such a broad topic. The style of the text may be more relevant than its content. Data may differ narrowly (e.g., official publications from the United Nations vs. official announcements from the European Union) or dramatically (e.g., chat room dialogs vs. published laws). The amounts of in-domain and out-of-domain data differs. The data may be cleanly separated by domain or just come in a massive disorganized pile. Some of the data may be of higher translation quality than other, which may be polluted by noise such as mistranslations, misalignments, or even generated by some other machine translation system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Further Readings</head><p>There is often a domain mismatch between the bulk (or even all) of the training data for a translation and its test data during deployment. There is rich literature in traditional statistical machine translation on this topic. A common approach for neural models is to first train on all available training data, and then run a few iterations on in-domain data only <ref type="bibr" target="#b52">(Luong and Manning, 2015)</ref>, as already pioneered in neural language model adaption <ref type="bibr" target="#b82">(Ter-Sarkisov et al., 2015)</ref>. <ref type="bibr" target="#b78">Servan et al. (2016)</ref> demonstrate the effectiveness of this adaptation method with small in-domain sets consisting of as little as 500 sentence pairs. <ref type="bibr" target="#b13">Chu et al. (2017)</ref> argue that given small amount of in-domain data leads to overfitting and suggest to mix in-domain and out-of-domain data during adaption. Freitag and Al-Onaizan (2016) identify the same problem and suggest to use an ensemble of baseline models and adapted models to avoid overfitting. <ref type="bibr" target="#b65">Peris et al. (2017)</ref> consider alternative training methods for the adaptation phase but do not find consistently better results than the traditional gradient descent training. Inspired by domain adaptation work in statistical machine translation on sub-sampling and sentence weighting, <ref type="bibr" target="#b8">Chen et al. (2017)</ref> build an in-domain vs. out-of-domain classifier for sentence pairs in the training data, and then use its prediction score to reduce the learning rate for sentence pairs that are out of domain. A multi-domain model may be trained and informed at run-time about the domain of the input sentence.  apply an idea initially proposed by <ref type="bibr" target="#b74">Sennrich et al. (2016a)</ref> -to augment input sentences for register with a politeness feature token -to the domain adaptation problem. They add a domain token to each training and test sentence. <ref type="bibr" target="#b10">Chen et al. (2016b)</ref> report better results over the token approach to adapt to topics by encoding the given topic membership of each sentence as an additional input vector to the conditioning context of word prediction layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.6.8">Adding Linguistic Annotation</head><p>One of the big debates in machine translation research is the question if the key to progress is to develop better, relatively generic, machine learning methods that implicitly learn the important features of language, or to use linguistic insight to augment data and models.</p><p>Recent work in statistical machine translation has demonstrated the benefits of linguistically motivated models. The best statistical machine translation systems in major evaluation campaigns for language pairs such as Chinese-English and German-English are syntax-based. While they translate sentences, they also build up the syntactic structure of the output sentence. There have been serious efforts to move towards deeper semantics in machine translation.</p><p>The turn towards neural machine translation was at first hard swing back towards better machine learning while ignoring much linguistic insights. Neural machine translation views translation as a generic sequence to sequence task, which just happens to involve sequences of words in different languages. Methods such as byte pair encoding or character-based translation models even put the value of the concept of a word as a basic unit into doubt.</p><p>However, recently there have been also attempts to add linguistic annotation into neural translation models, and steps towards more linguistically motivated models. We will take a look at successful efforts to integrate (1) linguistic annotation to the input sentence, (2) linguistic annotation to the output sentence, and (3) build linguistically structured models.</p><p>Linguistic annotation of the input One of the great benefits of neural networks is their ability to cope with rich context. In the neural machine translation models we presented, each word prediction is conditioned on the entire input sentence and all previously generated output words. Even if, as it is typically the case, a specific input sequence and partially generated output sequence has never been observed before during training, the neural model is able to generalize the training data and draw from relevant knowledge. In traditional statistical models, this required carefully chosen independence assumptions and back-off schemes.</p><p>So, adding more information to the conditioning context in neural translation models can be accommodated rather straightforwardly. First, what information would be like to add? The typical linguistic treasure chest contains part-of-speech tags, lemmas, morphological properties of words, syntactic phrase structure, syntactic dependencies, and maybe even some semantic annotation.</p><p>All of these can be formatted as annotations to individual input words. Sometimes, this requires a bit more work, such as syntactic and semantic annotation that spans multiple words. See <ref type="figure" target="#fig_1">Figure 13</ref>.38 for an example. To just walk through the linguistic annotation of the word girl in the sentence:</p><p>• Part of speech is NN, a noun.</p><p>• Lemma is girl, the same as the surface form. The lemma differs for watched / watch.</p><p>• Morphology is singular.  <ref type="figure" target="#fig_1">Figure 13</ref>.38: Linguistic annotation of a sentence, formatted as word-level factored representation</p><p>• The word is the continuation (CONT) of the noun phrase that started with the.</p><p>• The word is not part of a verb phrase (OTHER).</p><p>• Its syntactic head is watched.</p><p>• The dependency relationship to the head is subject (SUBJ).</p><p>• Its semantic role is ACTOR.</p><p>• There are many schemes of semantic types. For instance girl could be classified as HU-MAN.</p><p>Note how phrasal annotations are handled. The first noun phrase is the girl. It is common to use an annotation scheme that tags individual words in a phrasal annation as <ref type="bibr">BEGIN and CONTINUATION (or INTERMEDIATE)</ref>, while labelling words outside such phrases as OTHER.</p><p>How do we encode the word-level factored representation? Recall that words are initially represented as 1-hot vectors. We can encode each factor in the factored representation as a 1hot vector. The concatenation of these vectors is then used as input to the word embedding. Note that mathematically this means, that each factor of the representation is mapped to a embedding vector, and the final word embedding is the sum of the factor embeddings.</p><p>Since the input to the neural machine translation system is still a sequence of word embeddings, we do not have to change anything in the architecture of the neural machine translation model. We just provide richer input representations and hope that the model is able to learn how to take advantage of it.</p><p>Coming back to the debate about linguistics versus machine learning. All the linguistic annotation proposed here can arguable be learned automatically as part of the word embeddings (or contextualized word embeddings in the hidden encoder states). This may or may not be true. But it does provide additional knowledge that comes from the tools that produce the annotation and that is particularly relevant if there is not enough training data to automatically induce it. Also, why make the job harder for the machine learning algorithm than needed? In other words, why force the machine learning to discover features that can be readily provided? ) ) (NP (DET the ) (JJ beautiful ) (NNS fireflies ) ) ) ) <ref type="figure" target="#fig_1">Figure 13</ref>.39: Linearization of phrase structure grammar tree into a sequence of words -e.g., girl, watched -and tags -e.g., <ref type="bibr">(S, (NP, )</ref> Ultimately, these questions will be resolved empirically by demonstrating what actually works in specific data conditions.</p><p>Linguistic annotation of the output What we have done for input words could be done also for output words. Instead of discussing the fine points about what adjustments need to made (e.g., separate softmax for each output factor), let us take a look at another annotation scheme for the output that has been successfully applied to neural machine translation.</p><p>Most syntax-based statistical machine translation models have focused on adding syntax to the output side. Traditional n-gram language models are good at promoting fluency among neighboring words, they are not powerful enough to ensure overall grammaticality of each output sentence. By designing models that also produce and evaluate the syntactic parse structure for each output sentence, syntax-based models give the means to promote grammatically correct output.</p><p>The word-level annotation of phrase structure syntax suggested in <ref type="figure" target="#fig_1">Figure 13</ref>.38 is rather crude. The nature of language is recursive, and annotating nested phrases cannot be easily handled with a BEGIN/CONT/OTHER scheme. Instead, typically tree structures are used to represent syntax. See <ref type="figure" target="#fig_1">Figure 13</ref>.39 for an example. It shows the phrase structure syntactic parse tree for our example sentence The girl watched attentively the beautiful fireflies. Generating a tree structures is generally a quite different process than generating a sequence. It is typically built recursively bottom-up with algorithms such as chart parsing.</p><p>However, we can linearize the parse tree into a sequence of words and structural tokens that indicate the beginning -e.g., "(NP" -and end -closing parenthesis ")" -of syntactic phrases. So, forcing syntactic parse tree annotations into our sequence-to-sequence neural machine translation model may be done by encoding the parse structure with additional output tokens. To be perfectly clear, the idea is to produce as the output of the neural translation system not just a sequence of words, but a sequence of a mix of output words and special tokens.</p><p>The hope is that forcing the neural machine translation model to produce syntactic structure (even in a linearized form) encourages it to produce syntactically well-formed output. There is some evidence to support this hope, despite the simplicity of the approach.</p><p>Linguistically structured models The field of syntactic parsing has not been left untouched by the recent wave of neural networks. The previous section suggests that syntactic parsing may be done as simply as framing it as a sequence to sequence with additional output tokens.</p><p>However, the best-performing syntactic parsers use model structures that take the recursive nature of language to heart. They are either inspired by convolutional networks and build parse trees bottom-up, or are neural versions of left-to-right push-down automata that maintain a stack of opened phrases that any new word may extend or close, or be pushed down the stack to start a new phrase.</p><p>There is some early work on integrating syntactic parsing and machine translation into a unified framework but no consensus on best practices has emerged yet. At the time of writing, this is clearly still a challenge for future work. <ref type="bibr" target="#b97">Wu et al. (2012)</ref> propose to use factored representations of words (using lemma, stem, and part of speech), with each factor encoded in a one-hot vector, in the input to a recurrent neural network language model.  use such representations in the input and output of neural machine translation models, demonstrating better translation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Further Readings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.6.9">Multiple Language Pairs</head><p>There are more than two languages in the world. And we also have training data for many language pairs, sometimes it is highly overlapping (e.g., European Parliament proceedings in 24 languages), sometimes it is unique (e.g. Canadian Hansards in French and English). For some language pairs, a lot of training data is available (e.g., French-English). But for most language pairs, there is only very little, including commercially interesting language pairs such as Chinese-German or Japanese-Spanish.</p><p>There is a long history of moving beyond specific languages and encode meaning languageindependent, sometimes called interlingua. In machine translation, the idea is to map the input language first into an interlingua, and then map the interlingua into the output language. In such a system, we have to build just one mapping step into and one step out of the interlingua for each language. Then we can translate between it and all the other languages for which we have done the same.</p><p>Researchers in deep learning often do not hesitate to claim that intermediate states in neural translation models encode semantics or meaning. So, can we train a neural machine translation system that accepts text in any language as input and translates it into any other language?</p><p>Multiple Input Languages Let us say, we have two parallel corpora, one for German-English, and one for French-English. We can train a neural machine translation model on both corpora at the same time by simply concatenating them. The input vocabulary contains both German and French words. Any input sentence will be quickly recognized as being either German or French, due to the sentence context, disambiguating words such as du (you in German, of in French).</p><p>The combined model trained on both data sets has one advantage over two separate models. It is exposed to both English sides of the parallel corpora and hence can learn a better language model. There may be also be general benefits to having diversity in the data, leading to more robust models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiple Output Languages</head><p>We can do the same trick for the output language, by concatenating, say, a French-English and a French-Spanish corpus. But given a French input sentence during inference, how would the system know which output language to generate? A crude but effective way to signal this to the model is by adding a tag like <ref type="bibr">[SPANISH]</ref> as first token of the input sentence.</p><p>[ENGLISH] N'y a-t-il pas ici deux poids, deux mesures?</p><p>⇒ Is this not a case of double standards?</p><p>[SPANISH] N'y a-t-il pas ici deux poids, deux mesures? ⇒ £No puede verse con toda claridad que estamos utilizando un doble rasero?</p><p>If we train a system on the three corpora mentioned (German-English, French-English, and French-Spanish) we can also use it translate a sentence from German to Spanish -without having ever presented a sentence pair as training data to the system.</p><p>[SPANISH] Messen wir hier nicht mit zweierlei Maß?</p><p>⇒ £No puede verse con toda claridad que estamos utilizando un doble rasero?</p><p>For this to work, there has to be some representation of the meaning of the input sentence that is not tied to the input language and the output language. Surprisingly, experiments show that this actually does work, somewhat. To achieve good quality, however, some parallel data in the desired language pair is needed, but much less than for a standalone model . <ref type="figure" target="#fig_1">Figure 13</ref>.40 summarizes this idea. A single neural machine translation is trained on various parallel corpora in turn, resulting in a system that may translate between any seen input and output language. It is likely that increasingly deeper models (recall Section 13.6.4) may better serve as multi-language translators, since their deeper layer compute more abstract representations of language.</p><p>The idea of marking the output language with a token such as <ref type="bibr">[SPANISH]</ref> has been explored more widely in the context of systems for a single language pair. Such tokens may represent the domain of the input sentence , or the required level of politeness of the output sentence <ref type="bibr" target="#b74">(Sennrich et al., 2016a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>French</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spanish</head><p>German MT <ref type="figure" target="#fig_1">Figure 13</ref>.40: Multi-language machine translation system trained on one language pair at a time, rotating through many of them. After training on French-English, French-Spanish, and German-English, it is even able to translate from German to Spanish.</p><p>Sharing Components Instead of just throwing data at a generic neural machine translation model, we may want to more carefully consider which components may be shared among language-pair-specific models. The idea is to train one model per language pair, but some of the components are identical in these unique models.</p><p>• The encoder may be shared in models that have the same input language.</p><p>• The decoder may be shared in models that have the same output language.</p><p>• The attention mechanism may be shared in all models for all language pairs. Sharing components means is that the same parameter values (weight matrices, etc.) are used in these separate models. Updates to them when training a model for one language pair then also changes them for in the model for the other language pairs. There is no need to mark the output language, since each model is trained for a specific language pair.</p><p>The idea of shared training of components can also be pushed further to exploit monolingual data. The encoder may be trained on monolingual input language data, but we will need to add a training objective (e.g., language model cross-entropy). Also, the decoder may be trained in isolation with monolingual language model data. However, since there are no context states available, these have to be blanked out, which may lead it to learn to ignore the input sentence and function only as a target side language model.  explore how well a single canonical neural translation model is able to learn from multiple to multiple languages, by simultaneously training on on parallel corpora for several language pairs. They show small benefits for several input languages with the same output languages, mixed results for translating into multiple output languages (indicated by an additional input language token). The most interesting result is the ability for such a model to translate in language directions for which no parallel corpus is provided, thus demonstrating that some interlingual meaning representation is learned, although less well than using traditional pivot methods. <ref type="bibr" target="#b24">Firat et al. (2016)</ref> support multi-language input and output by training language-specific encoders and decoders and a shared attention mechanism.  <ref type="figure" target="#fig_1">Figure 13</ref>.41: Encoding a sentence with a convolutional neural network. By always using two convolutional layers, the size of the convolutions differ (here K 2 and K 3 ). Decoding reverses this process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Further Readings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.7">Alternate Architectures</head><p>Most of neural network research has focused on the use of recurrent neural networks with attention. But this is by no means the only architecture for neural networks. Arguable, a disadvantage of using recurrent neural networks on the input side is that it requires a long sequential process that consumes each input word in one step. This also prohibits the ability to parallelize the processing of all words at once, thus limiting the use of the capabilities of GPUs.</p><p>There have been a few alternate suggestions for the architecture of neural machine translation models. We will briefly present some of them in this section. It remains to be seen, if they are a curiosity or conquer the field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.7.1">Convolutional Neural Networks</head><p>The first end-to-end neural machine translation model of the modern era <ref type="bibr" target="#b43">(Kalchbrenner and Blunsom, 2013)</ref> was actually not based on recurrent neural networks, but based on convolutional neural networks. These had been shown to be very successful in image processing, thus looking for other applications was a natural next step.</p><p>See <ref type="figure" target="#fig_1">Figure 13</ref>.41 for an illustration of a convolutional network that encodes an input sentence. The basic building block of these networks is a convolution. It merges the representation of i input words into a single representation by using a matrix K i . Applying the convolution to every sequence of input words reduces the length of the sentence representation by i − 1. Repeating this process leads to a sentence representation in a single vector.</p><p>The illustration shows an architecture with two convolutional K i layers, followed by a final L i layer that merges the sequence of phrasal representations into a single sentence representation. The size of the convolutional kernels K i and L i depends on the length of the sentences. The example shows a 6-word sentence and a sequence of K 2 , K 3 , and L 3 layers. For longer sentences, bigger kernels are needed.</p><p>The hierarchical process of building up a sentence representation bottom-up is well grounded in linguistic insight in the recursive nature of language. It is similar to chart parsing, except that we are not committing to a single hierarchical structure. On the other hand, we are asking an  awful lot from the resulting sentence embedding to represents the meaning of an entire sentence of arbitrary length.</p><p>Generating the output sentence translation reverses the bottom-up process. One problem for the decoder is to decide the length of the output sentence. One option to address this problem is to add a model that predicts output length from input length. This then leads to the selection of the size of the reverse convolution matrices.</p><p>See <ref type="figure" target="#fig_1">Figure 13</ref>.42 for an illustration of a variation of this idea. The shown architecture always uses a K 2 and a K 3 convolutional layer, resulting in a sequence of phrasal representations, not a single sentence embedding. There is an explicit mapping step from phrasal representations of input words to phrasal representations of output words, called transfer layer.</p><p>The decoder of the model includes a recurrent neural network on the output side. Sneaking in a recurrent neural network here does undermine a bit the argument about better parallelization. However, the claim still holds true for encoding the input, and a sequential language model is just a too powerful tool to disregard. While the just-described convolutional neural machine translation model helped to set the scene for neural network approaches for machine translation, it could not be demonstrated to achieve competitive results compared to traditional approaches. The compression of the sentence representation into a single vector is especially a problem for long sentences. However, the model was used successfully in reranking candidate translations generated by traditional statistical machine translation systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Word Embeddings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolution</head><p>Layer <ref type="formula">1</ref>   <ref type="bibr" target="#b29">Gehring et al. (2017)</ref> propose an architecture for neural networks that combines the ideas of convolutional neural networks and the attention mechanism. It is essentially the sequence-tosequence attention that we described as the canonical neural machine translation approach, but with the recurrent neural networks replaced by convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.7.2">Convolutional Neural Networks With Attention</head><p>We introduced convolutions in the previous section. The idea is to combine a short sequence of neighboring words into a single representation. To look at it in another way, a convolution encodes a word with its left and right context, in a limited window. Let us now describe in more detail what this means for the encoder and the decoder in the neural model.</p><p>Encoder See <ref type="figure" target="#fig_1">Figure 13</ref>.43 for an illustration of the convolutional layers used in the encoder. For each input word, the state at each layer is informed by the corresponding state in the previous layer and its two neighbors. Note that these convolutional layers do not shorten the sequence, because we have a convolution centered around each word, using padding (vectors with zero values) for word positions that are out of bounds.</p><p>Mathematically, we start with the input word embeddings Ex j and progress through a sequence of layer encodings h d,j at different depth d until a maximum depth D.</p><formula xml:id="formula_88">h 0,j = E x j h d,j = f (h d−1,j−k , ..., h d−1,j+k ) for d &gt; 0, d ≤ D (13.94)</formula><p>The function f is a feed-forward layer, with a residual connection from the corresponding previous layer state h d−1,j .</p><p>Note that even with a few convolutional layers, the final representation of a word h D,j may only be informed by partial sentence context -in contrast to the bi-directional recurrent neural networks in the canonical model. However, relevant context words in the input sentence that help with disambiguation may be outside this window.</p><p>On the other hand, there are significant computational advantages to this idea. All words at one depth can be processed in parallel, even combined into one massive tensor operation that can be efficiently parallelized on a GPU. Decoder The decoder in the canonical model also has at its core a recurrent neural network. Recall its state progression defined in Equation 13.75 on page 49:</p><formula xml:id="formula_89">s i = f (s i−1 , Ey i−1 , c i ) (13.95)</formula><p>where s i is the encoder state, Ey i−1 the embedding of the previous output word, and c i the input context.</p><p>The convolutional version of this does not have recurrent decoder states, i.e., the computation does not depend on the previous state s i−1 , but is conditioned on the sequence of the κ most recent previous words.</p><p>s i = f (Ey i−κ , ..., Ey i−1 , c i ) (13.96) Furthermore, these decoder convolutions may be stacked, just as the encoder convolutional layers.</p><formula xml:id="formula_90">s 1,i = f (Ey i−κ , ..., Ey i−1 , c i ) s d,i = f (s d−1,i−κ−1 , ..., s d−1,i , c i ) for d &gt; 0, d ≤D (13.97)</formula><p>See <ref type="figure" target="#fig_1">Figure 13</ref>.44 for an illustration of these equations. The main difference between the canonical neural machine translation model and this architecture is the conditioning of the states of the decoder. They are computed in a sequence of convolutional layers, and also always the input context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention</head><p>The attention mechanism is essentially unchanged from the canonical neural translation model. Recall that is is based on an association a(s i−1 , h j ) between the word representations computed by the encoder h j and the previous state of the decoder s i−1 (refer back to Equation 13.78 on page 51).</p><p>Since we still have such encoder and decoder states (h D,j and sD ,i−1 ), we use the same here. These association scores are normalized and used to compute a weighted sum of the input word embeddings (i.e., the encoder states h D,j ). A refinement is that the encoder state h D,j and the input word embedding x j is combined via addition when computing the context vector. This is the usual trick of using residual connections to assist training with deep neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.7.3">Self-Attention</head><p>The critique of the use of recurrent neural networks is that they require a lengthy walk-through, word by word, of the entire input sentence, which is time-consuming and limits parallelization. The previous sections replaced the recurrent neural networks in our canonical model with convolutions. However, these have a limited context window to enrich representations of words. What we would like is some architectural component that allows us to use wide context and can be highly parallelized. What could that be?</p><p>In fact, we already encountered it: the attention mechanism. It considers associations between every input word and any output word, and uses it to build a vector representation of the entire input sequence. The idea behind self-attention is to extend this idea to the encoder. Instead of computing the association between an input and an output word, self-attention computes the association between any input word and any other input word. One way to view it is that this mechanism refines the representation of each input word by enriching it with context words that help to disambiguate it. Let us look at this equation in detail. The association between every word representation h j any other context word h k is done via the dot product between the packed matrix H and its transpose H T , resulting in a vector of raw association values HH T . The values in this vector are first scaled by the size of the word representation vectors |h|, and then by the softmax, so that their values add up to 1. The resulting vector of normalized association values is then used to weigh the context words.</p><p>Another way to put Equation 13.98 without the matrix H notation but using word representation vectors h j : </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention Layer</head><p>The self-attention step described above is only one step in the selfattention layer used to encode the input sentence. There are four more steps that follow it.</p><p>• We combine self-attention with residual connections that pass the word representation through directly self-attention(h j ) + h j (13.100)</p><p>• Next up is a layer normalization step (described in Section 13.2.6 on page 21).</p><p>h j = layer-normalization(self-attention(h j ) + h j ) (13.101)</p><p>• A standard feed-forward step with ReLU activation function is applied.</p><formula xml:id="formula_91">relu(Wĥ j + b) (13.102)</formula><p>• This is also augmented with residual connections and layer normalization. The deep modeling is the reason behind the residual connections in the self-attention layer -such residual connections help with training since they allow a shortcut to the input which may be utilized in early stages of training, before it can take advantage of the more complex interdependencies that deep models enable. The layer normalization step is one standard training trick that also helps especially with deep models.</p><p>Attention in the Decoder Self-attention is also used in the decoder, now between output words. The decoder also has more traditional attention. In total there are 3 sub layers.</p><p>• Self attention: Output words are initially encoded by word embeddings s i = Ey i . We perform exactly the same self-attention computation as described in Equation 13.98. However, the association of a word s i is limited to words s k with k ≤ i, i.e., just the previously produced output words. Let us denote the result of this sub layer for output word i ass i</p><p>• Attention: The attention mechanism in this model follows very closely self-attention. The only difference is that, previously, we compute self attention between the hidden states H and themselves. Now, we compute attention between the decoder statesS and the final encoder states H.  Using the same more detailed exposition as above for self-attention:</p><formula xml:id="formula_92">a ik = 1 |h|s i h T k raw association S H T |h| α ik = exp(a ik ) κ exp(a iκ ) normalized association (softmax) attention(s i ) = k α jκ h k weighted sum (13.106)</formula><p>This attention computation is augmented by adding in residual connections, layer normalization, and an additional ReLU layer, just like the self-attention layer described above.</p><p>It is worth noting that, the output of the attention computation is a weighted sum over input word representations k α jκ h k . To this, we add the (self-attended) representation of the decoder states i via a residual connection. This allows skipping over the deep layers, thus speeding up training.</p><p>• Feed-forward layer: This sub layer is identical to the encoder, i.e., relu(W sŝi + b s )</p><p>Each of the sub-layers is followed by the add-and-norm step of first using residual connections and then layer normalization (as noted in the description of the attention sub layer).</p><p>The entire model is shown in <ref type="figure" target="#fig_1">Figure 13</ref>.45, Further Readings <ref type="bibr" target="#b43">Kalchbrenner and Blunsom (2013)</ref> build a comprehensive machine translation model by first encoding the source sentence with a convolutional neural network, and then generate the target sentence by reversing the process. A refinement of this was proposed by <ref type="bibr" target="#b29">Gehring et al. (2017)</ref> who use multiple convolutional layers in the encoder and the decoder that do not reduce the length of the encoded sequence but incorporate wider context with each layer. <ref type="bibr" target="#b89">Vaswani et al. (2017)</ref> replace the recurrent neural networks used in attentional sequence-to-sequence models with multiple self-attention layers, both for the encoder as well as the decoder. There are a number of additional refinements of this model: so-called multi-head attention, encoding of sentence positions of words, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.8">Current Challenges</head><p>Neural machine translation has emerged as the most promising machine translation approach in recent years, showing superior performance on public benchmarks  and rapid adoption in deployments by, e.g., Google , <ref type="bibr">Systran (Crego et al., 2016)</ref>, and WIPO <ref type="bibr" target="#b41">(Junczys-Dowmunt et al., 2016)</ref>. But there have also been reports of poor performance, such as the systems built under low-resource conditions in the DARPA LORELEI program. <ref type="bibr">8</ref> Here, we examine a number of challenges to neural machine translation and give empirical results on how well the technology currently holds up, compared to traditional statistical machine translation. We show that, despite its recent successes, neural machine translation still has to overcome various challenges, most notably performance out-of-domain and under low resource conditions. What a lot of the problems have in common is that the neural translation models do not show robust behavior when confronted with conditions that differ significantly from training conditions -may it be due to limited exposure to training data, unusual input in case of outof-domain test sentences, or unlikely initial word choices in beam search. The solution to these problems may hence lie in a more general approach of training that steps outside optimizing single word predictions given perfectly matching prior sequences.</p><p>Another challenge that we do not examine empirically: neural machine translation systems are much less interpretable. The answer to the question of why the training data leads these systems to decide on specific word choices during decoding is buried in large matrices of real-numbered values. There is a clear need to develop better analytics for neural machine translation.</p><p>We use common toolkits for neural machine translation (Nematus) and traditional phrasebased statistical machine translation (Moses) with common data sets, drawn from WMT and OPUS. Unless noted otherwise, we use default settings, such as beam search and single model decoding. The training data is processed with byte-pair encoding <ref type="bibr" target="#b76">(Sennrich et al., 2016c)</ref> into subwords to fit a 50,000 word vocabulary limit.</p><p>Our statistical machine translation systems are trained using Moses 9 <ref type="bibr" target="#b47">(Koehn et al., 2007)</ref>. We build phrase-based systems using standard features that are commonly used in recent system submissions to WMT <ref type="bibr" target="#b94">(Williams et al., 2016;</ref><ref type="bibr" target="#b18">Ding et al., 2016)</ref>. While we consider here only phrase-based systems, we note that there are other statistical machine translation approaches such as hierarchical phrase-based models <ref type="bibr" target="#b11">(Chiang, 2007)</ref> and syntax-based models <ref type="bibr" target="#b28">(Galley et al., 2004</ref><ref type="bibr" target="#b27">(Galley et al., , 2006</ref> that have been shown to give superior performance for language pairs such as Chinese-English and German-English.</p><p>We carry out our experiments on English-Spanish and German-English. For these language pairs, large training data sets are available. We use datasets from the shared translation task organized alongside the Conference on Machine Translation (WMT) 10 . For the domain experiments, we use the OPUS corpus 11 <ref type="bibr" target="#b83">(Tiedemann, 2012)</ref>.</p><p>Except for the domain experiments, we use the WMT test sets composed of news stories, which are characterized by a broad range of topic, formal language, relatively long sentences (about 30 words on average), and high standards for grammar, orthography, and style.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.8.1">Domain Mismatch</head><p>A known challenge in translation is that in different domains, 12 words have different translations and meaning is expressed in different styles. Hence, a crucial step in developing machine translation systems targeted at a specific use case is domain adaptation. We expect that methods for domain adaptation will be developed for neural machine translation. A currently popular approach is to train a general domain system, followed by training on in-domain data for a few epochs <ref type="bibr" target="#b52">(Luong and Manning, 2015;</ref><ref type="bibr" target="#b26">Freitag and Al-Onaizan, 2016)</ref>.</p><p>Often, large amounts of training data are only available out of domain, but we still seek to have robust performance. To test how well neural machine translation and statistical machine translation hold up, we trained five different systems using different corpora obtained from OPUS <ref type="bibr" target="#b83">(Tiedemann, 2012</ref>). An additional system was trained on all the training data. Statistics about corpus sizes are shown in <ref type="table" target="#tab_0">Table 13</ref>.3. Note that these domains are quite distant from each other, much more so than, say, Europarl, TED Talks, News Commentary, and Global Voices.</p><p>We trained both statistical machine translation and neural machine translation systems for all domains. All systems were trained for German-English, with tuning and test sets subsampled from the data (these were not used in training). A common byte-pair encoding is used for all training runs. See <ref type="figure" target="#fig_1">Figure 13</ref>.46 for results. While the in-domain neural and statistical machine translation systems are similar (neural machine translation is better for IT and Subtitles, statistical machine translation is better for Law, Medical, and Koran), the out-of-domain performance for the neural machine translation systems is worse in almost all cases, sometimes dramatically so. For 9 http://www.stat.org/moses/ 10 http://www.statmt.org/wmt17/ 11 http://opus.lingfil.uu.se/ 12 We use the customary definition of domain in machine translation: a domain is defined by a corpus from a specific source, and may differ from other domains in topic, genre, style, level of formality, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus</head><p>Words NMT: Take heed of your own souls. SMT: And you see.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subtitles</head><p>NMT: Look around you. SMT: Look around you . <ref type="figure" target="#fig_1">Figure 13</ref>.47: Examples for the translation of a sentence from the Subtitles corpus, when translated with systems trained on different corpora. Performance out-of-domain is dramatically worse for neural machine translation. instance the Medical system leads to a BLEU score of 3.9 (neural machine translation) vs. 10.2 (statistical machine translation) on the Law test set. <ref type="figure" target="#fig_1">Figure 13</ref>.47 displays an example. When translating the sentence Schaue um dich herum. (reference: Look around you.) from the Subtitles corpus, we see mostly non-sensical and completely unrelated output from the neural machine translation system. For instance, the translation from the IT system is Switches to paused.</p><p>Note that the output of the neural machine translation system is often quite fluent (e.g., Take heed of your own souls.) but completely unrelated to the input, while the statistical machine translation output betrays its difficulties with coping with the out-of-domain input by leaving some words untranslated (e.g., Schaue by dich around.). This is of particular concern when MT is used for information gisting -the user will be mislead by hallucinated content in the neural machine translation output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.8.2">Amount of Training Data</head><p>A well-known property of statistical systems is that increasing amounts of training data lead to better results. In statistical machine translation systems, we have previously observed that doubling the amount of training data gives a fixed increase in BLEU scores. This holds true for both parallel and monolingual data <ref type="bibr" target="#b88">(Turchi et al., 2008;</ref><ref type="bibr" target="#b37">Irvine and Callison-Burch, 2013)</ref>.</p><p>How do the data needs of statistical machine translation and neural machine translation compare? Neural machine translation promises both to generalize better (exploiting word similarity in embeddings) and condition on larger context (entire input and all prior output words). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLEU Scores with Varying Amounts of Training Data</head><p>Phrase-Based with Big LM Phrase-Based Neural <ref type="figure" target="#fig_1">Figure 13</ref>.48: BLEU scores for English-Spanish systems trained on 0.4 million to 385.7 million words of parallel data. Quality for neural machine translation starts much lower, outperforms statistical machine translation at about 15 million words, and even beats a statistical machine translation system with a big 2 billion word in-domain language model under high-resource conditions. Ratio shuffled 0% 10% 20% 50% SMT (BLEU) 32.7 32.7 (-0.0) 32.6 (-0.1) 32.0 (-0.7) NMT (BLEU) 35.4 34.8 (-0.6) 32.1 (-3.3) 30.1 (-5.3) <ref type="table" target="#tab_0">Table 13</ref>.4: Impact of noise in the training data, with parts of the training corpus shuffled to contain mis-aligned sentence pairs. Neural machine translation degrades severely, while statistical machine translation holds up fairly well.</p><p>Is this still the case for neural machine translation? <ref type="bibr" target="#b9">Chen et al. (2016a)</ref> considered one kind of noise: misaligned sentence pairs in an experiments with a large English-French parallel corpus. They shuffle the target side of part of the training corpus, so that these sentence pairs are mis-aligned. <ref type="table" target="#tab_0">Table 13</ref>.4 shows the result. Statistical machine translation systems hold up fairly well. Even with 50% of the data perturbed, the quality only drops from 32.7 to 32.0 BLEU points, about what is to be expected with half the valid training data. However, the neural machine translation system degrades severely, from 35.4 to 30.1 BLEU points, a drop of 5.3 points, compared to the 0.7 point drop for statistical systems.</p><p>A possible explanation for this poor behavior of neural machine translation models is that its prediction has to find a good balance between language model and input context as the main driver. When training observes increasing ratios of training example, for which the input sentence is a meaningless distraction, it may generally learn to rely more on the output language model aspect, hence hallucinating fluent by inadequate output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.8.4">Word Alignment</head><p>The key contribution of the attention model in neural machine translation <ref type="bibr" target="#b1">(Bahdanau et al., 2015)</ref> was the imposition of an alignment of the output words to the input words. This takes the shape of a probability distribution over the input words which is used to weigh them in a bag-of-words representation of the input sentence.</p><p>Arguably, this attention model does not functionally play the role of a word alignment between the source in the target, at least not in the same way as its analog in statistical machine translation. While in both cases, alignment is a latent variable that is used to obtain probability distributions over words or phrases, arguably the attention model has a broader role. For instance, when translating a verb, attention may also be paid to its subject and object since these may disambiguate it. To further complicate matters, the word representations are products of bidirectional gated recurrent neural networks that have the effect that each word representation is informed by the entire sentence context.</p><p>But there is a clear need for an alignment mechanism between source and target words. For instance, prior work used the alignments provided by the attention model to interpolate word translation decisions with traditional probabilistic dictionaries <ref type="bibr" target="#b0">(Arthur et al., 2016)</ref>, for the introduction of coverage and fertility models <ref type="bibr" target="#b87">(Tu et al., 2016b)</ref>  But is the attention model in fact the proper means? To examine this, we compare the soft alignment matrix (the sequence of attention vectors) with word alignments obtained by traditional word alignment methods. We use incremental fast-align <ref type="bibr" target="#b20">(Dyer et al., 2013)</ref> to align the input and output of the neural machine system. See <ref type="figure" target="#fig_1">Figure 13</ref>.50a for an illustration. We compare the word attention states (green boxes) with the word alignments obtained with fast align (blue outlines). For most words, these match up pretty well. Both attention states and fast-align alignment points are a bit fuzzy around the function words have-been/sind.</p><p>However, the attention model may settle on alignments that do not correspond with our intuition or alignment points obtained with fast-align. See <ref type="figure" target="#fig_1">Figure 13</ref>.50b for the reverse language direction, German-English. All the alignment points appear to be off by one position. We are not aware of any intuitive explanation for this divergent behavior -the translation quality is high for both systems.</p><p>We measure how well the soft alignment (attention model) of the neural machine translation system match the alignments of fast-align with two metrics:</p><p>• a match score that checks for each output if the aligned input word according to fast-align is indeed the input word that received the highest attention probability, and</p><p>• a probability mass score that sums up the probability mass given to each alignment point obtained from fast-align.  <ref type="table" target="#tab_0">Table 13</ref>.5: Scores indicating overlap between attention probabilities and alignments obtained with fastalign.</p><p>In these scores, we have to handle byte pair encoding and many-to-many alignments 14</p><p>In out experiment, we use the neural machine translation models provided by Edinburgh 15 <ref type="bibr" target="#b75">(Sennrich et al., 2016b)</ref>. We run fast-align on the same parallel data sets to obtain alignment models and used them to align the input and output of the neural machine translation system. <ref type="table" target="#tab_0">Table 13</ref>.5 shows alignment scores for the systems. The results suggest that, while drastic, the divergence for German-English is an outlier. We note, however, that we have seen such large a divergence also under different data conditions.</p><p>Note that the attention model may produce better word alignments by guided alignment training <ref type="bibr" target="#b10">(Chen et al., 2016b;</ref> where supervised word alignments (such as the ones produced by fast-align) are provided to model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.8.5">Beam Search</head><p>The task of decoding is to find the full sentence translation with the highest probability. In statistical machine translation, this problem has been addressed with heuristic search techniques that explore a subset of the space of possible translation. A common feature of these search techniques is a beam size parameter that limits the number of partial translations maintained per input word.</p><p>There is typically a straightforward relationship between this beam size parameter and the model score of resulting translations and also their quality score (e.g., BLEU). While there are diminishing returns for increasing the beam parameter, typically improvements in these scores can be expected with larger beams.</p><p>Decoding in neural translation models can be set up in similar fashion. When predicting the next output word, we may not only commit to the highest scoring word prediction but 37.2 37.5 37.5 37.6 37.637.6 37.6 37.6 37.6 37.6</p><p>Beam Size</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLEU</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>German-English</head><p>Unnormalized Normalized  <ref type="figure" target="#fig_1">Figure 13</ref>.51: Translation quality with varying beam sizes. For large beams, quality decreases, especially when not normalizing scores by sentence length. also maintain the next best scoring words in a list of partial translations. We record with each partial translation the word translation probabilities (obtained from the softmax), extend each partial translation with subsequent word predictions and accumulate these scores. Since the number of partial translation explodes exponentially with each new output word, we prune them down to a beam of highest scoring partial translations.</p><p>As in traditional statistical machine translation decoding, increasing the beam size allows us to explore a larger set of the space of possible translation and hence find translations with better model scores.</p><p>However, as <ref type="figure" target="#fig_1">Figure 13</ref>.51 illustrates, increasing the beam size does not consistently improve translation quality. In fact, in almost all cases, worse translations are found beyond an optimal beam size setting (we are using again Edinburgh's WMT 2016 systems). The optimal beam size varies from 4 (e.g., Czech-English) to around 30 (English-Romanian).</p><p>Normalizing sentence level model scores by length of the output alleviates the problem somewhat and also leads to better optimal quality in most cases (5 of the 8 language pairs investigated). Optimal beam sizes are in the range of 30-50 in almost all cases, but quality still drops with larger beams. The main cause of deteriorating quality are shorter translations under wider beams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.8.6">Further Readings</head><p>Other studies have looked at the comparable performance of neural and statistical machine translation systems. <ref type="bibr" target="#b5">Bentivogli et al. (2016)</ref> considered different linguistic categories for English-German and Toral and Sánchez-Cartagena (2017) compared different broad aspects such as fluency and reordering for nine language directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.9">Additional Topics</head><p>Especially early work on neural networks for machine translation was aimed at building neural components to be used in traditional statistical machine translation systems.</p><p>Translation Models By including aligned source words in the conditioning context, <ref type="bibr" target="#b17">Devlin et al. (2014)</ref> enrich a feed-forward neural network language model with source context <ref type="bibr" target="#b101">Zhang et al. (2015)</ref> add a sentence embedding to the conditional context of this model, which are learned using a variant of convolutional neural networks and mapping them across languages. <ref type="bibr" target="#b57">Meng et al. (2015)</ref> use a more complex convolutional neural network to encode the input sentence that uses gated layers and also incorporates information about the output context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reordering Models</head><p>Lexicalized reordering models struggle with sparse data problems when conditioned on rich context. <ref type="bibr" target="#b49">Li et al. (2014)</ref> show that a neural reordering model can be conditioned on current and previous phrase pair (encoded with a recursive neural network auto-encoder) to make the same classification decisions for orientation type.</p><p>Pre-Ordering Instead of handing reordering within the decoding process, we may pre-order the input sentence into output word order. de <ref type="bibr" target="#b16">Gispert et al. (2015)</ref> use an input dependency tree to learn a model that swaps children nodes and implement it using a feed-forward neural network. Miceli Barone and Attardi (2015) formulate a top-down left-to-right walk through the dependency tree and make reordering decisions at any node. They model this process with a recurrent neural network that includes past decisions in the conditioning context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N-Gram Translation Models</head><p>An alternative view of the phrase based translation model is to break up phrase translations into minimal translation units, and employing a n-gram model over these units to condition each minimal translation units on the previous ones.  treat each minimal translation unit as an atomic symbol and train a neural language model over it. Alternatively,  represent the minimal translation units as bag of words, <ref type="bibr" target="#b95">(Wu et al., 2014)</ref> break them even further into single input words, single output words, or single input-output word pairs, and <ref type="bibr" target="#b99">Yu and Zhu (2015)</ref> use phrase embeddings leaned with an auto-encoder. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>Marian (a C++ re-implementation of Nematus): https://marian-nmt.github.io/ • OpenNMT (based on Torch/pyTorch): http://opennmt.net/ • xnmt (based on DyNet): https://github.com/neulab/xnmt • Sockeye (based on MXNet): https://github.com/awslabs/sockeye • T2T (based on Tensorflow): https://github.com/tensorflow/tensor2tensor</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 13</head><label>13</label><figDesc>.2: A neural network with a hidden layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>•</head><label></label><figDesc>a vector of input nodes with values x = (x 1 , x 2 , x 3 , ...x n ) T • a vector of hidden nodes with values h = (h 1 , h 2 , h 3 , ...h m ) T • a vector of output nodes with values y = (y 1 , y 2 , y 3 , ...y l ) T • a matrix of weights connecting input nodes with hidden nodes W = {w ij } • a matrix of weights connecting hidden nodes with output nodes U = {u ij } The computations in a neural network with a hidden layer, as sketched out so far, are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 13 . 3 :</head><label>133</label><figDesc>Typical activation functions in neural networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 13</head><label>13</label><figDesc>.4: A simple neural network with bias nodes in input and hidden layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>b in ed G ra d ie nt</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>&gt; x = T.dmatrix() &gt; W = theano.shared(value=numpy.array([[3.0,2.0],[4.0,3.0]])) &gt; b = theano.shared(value=numpy.array([-2.0,-4.0])) &gt; h = T.nnet.sigmoid(T.dot(x,W)+b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>&gt;</head><label></label><figDesc>h_function = theano.function([x], h) &gt; h_function([[1,0]]) array([[ 0.73105858, 0.11920292]])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>W2 = theano.shared(value=numpy.array([5.0,-5.0] )) b2 = theano.shared(-2.0) y_pred = T.nnet.sigmoid(T.dot(h,W2)+b2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>&gt; gW, gb, gW2, gb2 = T.grad(cost, [W,b,W2,b2])    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13</head><label>13</label><figDesc>.11: Full architecture of a feed-forward neural network language model. Context words (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 13 .</head><label>13</label><figDesc>12: Word embeddings projected into 2D. Semantically similar words occur close to each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 13</head><label>13</label><figDesc>.14: Back-propagation through time: By unfolding the recurrent neural network over a fixed number of prediction steps (here: 3), we can derive update formulas based on the training objective of predicting all output words and back-propagation of the error via gradient descent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 13</head><label>13</label><figDesc>.15: A cell in a LSTM neural network. As recurrent neural networks, it receives input from the preceeding layer (x) and the hidden layer values from the previous time step t − 1. The memory state m is updated from the input state i and the previous time's value of the memory state m t−1 . Various gates channel information flow in the cell towards the output value o.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 13</head><label>13</label><figDesc>.16: Gated Recurrent Unit (GRU): a simplification of long short term memory (LSTM) cells.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>Figure 13.20: Neural machine translation model, part 2: output decoder. Given the context from the input sentence, and the embedding of the previously selected word, new decoder states and word predictions are computed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 13</head><label>13</label><figDesc>.21: Neural machine translation model, part 3: attention model. Associations are computed between the last hidden state of the decoder and the word representations (encoder states). These associations are used to compute a weighted sum of encoder states.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 13 .</head><label>13</label><figDesc>22: Fully unrolled computation graph for training example with 7 input tokens &lt;s&gt; the house is big &lt;/s&gt; and 6 output tokens das Haus is groß&lt;/s&gt;. The cost function (error) is computed for each output word individually, and summed up across the sentence. When walking through the deocder states, the correct previous output words are used as conditioning context.⇒ Figure 13.23: To make better use of parallelism in GPUs, we process a batch of training examples (sentence pairs) at a time. Converting a batch of training examples into a set of mini batches that have similar length. This wastes less computation on filler words (light blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 13</head><label>13</label><figDesc>.24: Elementary decoding step: The model predicts a word prediction probability distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 13</head><label>13</label><figDesc>.26: Beam search in neural machine translation. After committing to a short list of specific output words (the beam), new word predictions are made for each. These differ since the committed output word is part of the conditioning context to make predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head></head><label></label><figDesc>Figure 13.27: Search graph for beam search decoding in neural translation models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>(</head><label></label><figDesc>2014) use recurrent neural networks for the approach. Sutskever et al. (2014) use a LSTM (long shortterm memory) network and reverse the order of the source sentence before decoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head></head><label></label><figDesc>Figure 13.33: Deep Decoder: Instead of a single recurrent neural network (RNN) layer for the decoder state, in a deep model, it consists of several layers. The illustrations shows a combination of a deep transition and stacked RNNs. It omits the word prediction, word selection and output word embedding steps which are identical to the original architecture, shown inFigure 13.20 on page 50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head></head><label></label><figDesc>37: Online training of neural machine translation models allows a straightforward domain adaptation method: Having a general domain translation system trained on general-purpose data, a handful of additional training epochs on in-domain data allows for a domain-adapted system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Farajian</head><label></label><figDesc>et al. (2017)  show that traditional statistical machine translation outperforms neural machine translation when training general-purpose machine translation systems on a collection data, and then tested on niche domains. The adaptation technique allows neural machine translation to catch up.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head></head><label></label><figDesc>NP (DET the ) (NN girl ) ) (VP (VFIN watched ) (ADVP (ADV attentively</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure 13 .</head><label>13</label><figDesc>42: Refinement of the convolutional neural network model. Convolutions do not result in a single sentence embedding but a sequence. The encoder is also informed by a recurrent neural network (connections from output word embeddings to final decoding layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Figure 13</head><label>13</label><figDesc>.43: Encoder using stacked convolutional layers. Any number of layers may be used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Figure 13</head><label>13</label><figDesc>.44: Decoder in convolutional neural network with attention. The decoder state is computed as a sequence of convolutional layers (here: 2) over the already predicted output words. Each convolutional state is also informed by the input context computed from the input sentence and attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>Computing</head><label></label><figDesc>Self-Attention Vaswani et al. (2017) define self attention for a sequence of vectors h j (of size |h|), packed into a matrix H, as self-attention(H) = softmax HH T |h| H (13.98)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Figure 13 .</head><label>13</label><figDesc>45: Attention-based machine translation model: the input is encoded with several layers of self-attention. The decoder computes attention-based representations of the input in several layers, initialized with the previous word embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>Figure 13 .</head><label>13</label><figDesc>50: Word alignment for English-German: comparing the attention model states (green boxes with probability in percent if over 10) with alignments obtained from fast-align (blue outlines).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 13</head><label>13</label><figDesc>.1: Calculations for input (1,0) to the network inFigure 13.4. Input x 0 Input x 1 Hidden h 0 Hidden h 1 Output y 0</figDesc><table><row><cell>0</cell><cell>0</cell><cell>0.119</cell><cell>0.018</cell><cell>0.183 → 0</cell></row><row><cell>0</cell><cell>1</cell><cell>0.881</cell><cell>0.269</cell><cell>0.743 → 1</cell></row><row><cell>1</cell><cell>0</cell><cell>0.731</cell><cell>0.119</cell><cell>0.743 → 1</cell></row><row><cell>1</cell><cell>1</cell><cell>0.993</cell><cell>0.731</cell><cell>0.334 → 0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>SummaryWe train neural networks by processing training examples, one at a time, and update weights each time. What drives weight updates is the gradient towards a smaller error. Weight updates are computed based on error terms δ i associated with each non-input node in the network.</figDesc><table /><note>13.26) then we have an analogous update formula ∆u j←k = µ δ j x k (13.27)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 13 .</head><label>13</label><figDesc></figDesc><table><row><cell>error(λ)</cell><cell>error(λ)</cell><cell>error(λ)</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">local optimum</cell></row><row><cell>λ</cell><cell>λ</cell><cell>global optimum</cell><cell>λ</cell></row><row><cell>Too high learning rate</cell><cell>Bad initialization</cell><cell>Local optimum</cell><cell></cell></row></table><note>2: Weight updates (with unspecified learning rate µ) for the neural network in Figure 13.4 (repeated above the table) when the training example (1,0) → 1 is presented.Figure 13.6: Problems with gradient descent training that motivate some of the refinements detailed in Section 13.2.6: (a) a too high learning rate may lead to too drastic parameter updates, overshooting the optimum, (b) bad initialization may require many updates to escape a plateau, and (c) the existence of local optima which trap training.7: Training progress over time. The error on the training set continuously decreases. However, on a validation set (not used for training), at some point the error increases. Training is stopped at the validation minimum before such over-fitting sets in.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Again, we can define a callable function to test the full network.</figDesc><table><row><cell>&gt; predict = theano.function([x], y_pred)</cell></row><row><cell>&gt; predict([[1,0]])</cell></row><row><cell>array([[ 0.7425526]])</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Word predictions of the neural machine translation model. Frequently, most of the probability mass is given to the top choice, but semantically related words may rank high, e.g., believe (68.4%) vs. think (28.6%). The subword units interpre@@ are explain in Section 13.6.2 on page 61.</figDesc><table><row><cell>in</cell><cell>(96.5%) on (0.9%), differently (0.5%), as (0.3%), to (0.2%), for (0.2%), by (0.1%), ...</cell></row><row><cell>different</cell><cell>(41.5%) a (25.2%), various (22.7%), several (3.6%), ways (2.4%), some (1.7%), ...</cell></row><row><cell>ways</cell><cell>(99.3%) way (0.2%), manner (0.2%), ...</cell></row><row><cell>.</cell><cell>(99.2%) &lt;/S&gt; (0.2%), , (0.1%), ...</cell></row><row><cell>&lt;/s&gt;</cell><cell>(100.0%)</cell></row><row><cell>Figure 13.25:</cell><cell></cell></row></table><note>Best Alternatives but (42.1%) however (25.3%), I (20.4%), yet (1.9%), and (0.8%), nor (0.8%), ...I (80.4%) also (6.0%), , (4.7%), it (1.2%), in (0.7%), nor (0.5%), he (0.4%), ... also (85.2%) think (4.2%), do (3.1%), believe (2.9%), , (0.8%), too (0.5%), ... believe (68.4%) think (28.6%), feel (1.6%), do (0.8%), ... he (90.4%) that (6.7%), it (2.2%), him (0.2%), ... is (74.7%) 's (24.4%), has (0.3%), was (0.1%), ... clever (99.1%) smart (0.6%), ... enough (99.9%) to (95.5%) about (1.2%), for (1.1%), in (1.0%), of (0.3%), around (0.1%), ... keep (69.8%) maintain (4.5%), hold (4.4%), be (4.2%), have (1.1%), make (1.0%), ... his (86.2%) its (2.1%), statements (1.5%), what (1.0%), out (0.6%), the (0.6%), ... statements (91.9%) testimony (1.5%), messages (0.7%), comments (0.6%), ... vague (96.2%) v@@ (1.2%), in (0.6%), ambiguous (0.3%), ... enough (98.9%) and (0.2%), ... so (51.1%) , (44.3%), to (1.2%), in (0.6%), and (0.5%), just (0.2%), that (0.2%), ... they (55.2%) that (35.3%), it (2.5%), can (1.6%), you (0.8%), we (0.4%), to (0.3%), ... can (93.2%) may (2.7%), could (1.6%), are (0.8%), will (0.6%), might (0.5%), ... be (98.4%) have (0.3%), interpret (0.2%), get (0.2%), ... interpreted (99.1%) interpre@@ (0.1%), constru@@ (0.1%), ...</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>Figure 13.46: Quality of systems (BLEU), when trained on one domain (rows) and tested on another domain (columns). Comparably, neural machine translation systems (left bars) show more degraded performance out of domain.SourceSchaue um dich herum. Reference Look around you. In order to implement dich Schaue .Medical NMT: EMEA / MB / 049 / 01-EN-Final Work progamme for 2002 SMT: Schaue by dich around . IT NMT: Switches to paused. SMT: To Schaue by itself . \t \t Koran</figDesc><table><row><cell>All</cell><cell></cell><cell cols="3">NMT: Look around you.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Law</cell><cell></cell><cell cols="4">SMT: Look around you. Law (Acquis) NMT: Sughum gravecorn. 18,128,173 Medical (EMEA) 14,301,472 SMT:</cell><cell cols="2">Sentences W/S 715,372 25.3 1,104,752 12.9</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>IT</cell><cell></cell><cell cols="2">3,041,677</cell><cell>337,817</cell><cell>9.0</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Koran (Tanzil)</cell><cell cols="2">9,848,539</cell><cell cols="2">480,421 20.5</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Subtitles</cell><cell></cell><cell cols="3">114,371,754 13,873,398</cell><cell>8.2</cell><cell></cell><cell></cell></row><row><cell cols="11">Table 13.3: Corpora used to train domain-specific systems, taken from the OPUS repository. IT corpora</cell></row><row><cell cols="5">are GNOME, KDE, PHP, Ubuntu, and OpenOffice.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>System ↓</cell><cell cols="2">Law</cell><cell cols="2">Medical</cell><cell></cell><cell>IT</cell><cell cols="2">Koran</cell><cell cols="2">Subtitles</cell></row><row><cell>All Data</cell><cell cols="2">30.5 32.8</cell><cell cols="2">45.1 42.2</cell><cell cols="2">35.3 44.7</cell><cell cols="2">17.9 17.9</cell><cell cols="2">26.4 20.8</cell></row><row><cell>Law</cell><cell cols="2">31.1 34.4</cell><cell cols="2">12.1 18.2</cell><cell>3.5</cell><cell>6.9</cell><cell>1.3</cell><cell>2.2</cell><cell>2.8</cell><cell>6.0</cell></row><row><cell>Medical</cell><cell>3.9</cell><cell>10.2</cell><cell cols="2">39.4 43.5</cell><cell>2.0</cell><cell>8.5</cell><cell>0.6</cell><cell>2.0</cell><cell>1.4</cell><cell>5.8</cell></row><row><cell>IT</cell><cell>1.9</cell><cell>3.7</cell><cell>6.5</cell><cell>5.3</cell><cell cols="2">42.1 39.8</cell><cell>1.8</cell><cell>1.6</cell><cell>3.9</cell><cell>4.7</cell></row><row><cell>Koran</cell><cell>0.4</cell><cell>1.8</cell><cell>0.0</cell><cell>2.1</cell><cell>0.0</cell><cell>2.3</cell><cell cols="2">15.9 18.8</cell><cell>1.0</cell><cell>5.5</cell></row><row><cell>Subtitles</cell><cell>7.0</cell><cell>9.9</cell><cell>9.3</cell><cell>17.8</cell><cell>9.2</cell><cell>13.6</cell><cell>9.0</cell><cell>8.4</cell><cell cols="2">25.9 22.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>, etc.</figDesc><table><row><cell></cell><cell>relations</cell><cell>between</cell><cell>Obama</cell><cell>and</cell><cell>Netanyahu</cell><cell>have</cell><cell>been</cell><cell>strained</cell><cell>for</cell><cell>years</cell><cell>.</cell><cell>the</cell><cell>das</cell><cell>Verhältnis 47</cell><cell>zwischen</cell><cell>Obama</cell><cell>und</cell><cell>Netanyahu</cell><cell>ist</cell><cell>seit</cell><cell>Jahren 17</cell><cell>gespannt</cell><cell>.</cell></row><row><cell>die</cell><cell>56</cell><cell></cell><cell>16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>relationship</cell><cell></cell><cell></cell><cell>81</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Beziehungen</cell><cell>89</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>between</cell><cell></cell><cell></cell><cell></cell><cell>72</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>zwischen</cell><cell></cell><cell>72</cell><cell>26</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Obama</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>87</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Obama</cell><cell></cell><cell></cell><cell>96</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>93</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>und</cell><cell></cell><cell></cell><cell></cell><cell>79</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Netanyahu</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>95</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Netanjahu</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>98</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>has</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>38</cell><cell>16</cell><cell></cell><cell>26</cell></row><row><cell>sind</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>42</cell><cell>11</cell><cell>38</cell><cell></cell><cell></cell><cell></cell><cell>been</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>21</cell><cell>14</cell><cell></cell><cell>54</cell></row><row><cell>seit</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>22</cell><cell cols="2">54 10</cell><cell></cell><cell>stretched</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>77</cell></row><row><cell>Jahren</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>98</cell><cell></cell><cell>for</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>38</cell><cell>33</cell><cell>12</cell></row><row><cell>angespannt</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>84</cell><cell></cell><cell></cell><cell></cell><cell>years</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90</cell></row><row><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>11</cell><cell>14</cell><cell>23</cell><cell></cell><cell></cell><cell>49</cell><cell cols="2">. 11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>19</cell><cell>32</cell><cell>17</cell></row><row><cell cols="8">(a) Desired Alignment</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">(b) Mismatched Alignment</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://deeplearning.net/software/theano/ 2 http://torch.ch/ 3 http://pytorch.org/ 4 http://dynet.readthedocs.io/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://www.tensorflow.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Note that there is a corresponding exploding gradient problem, where over long distance gradient values become too large. This is typically suppressed by clipping gradients, i.e., limiting them to a maximum value set as a hyper parameter.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://www.nist.gov/itl/iad/mig/lorehlt16-evaluations</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">Spanish was last represented in 2013, we used data from http://statmt.org/wmt13/translation-task.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">(1) neural machine translation operates on subwords, but fast-align is run on full words. (2) If an input word is split into subwords by byte pair encoding, then we add their attention scores. (3) If an output word is split into subwords, then we take the average of their attention vectors. (4) The match scores and probability mass scores are computed as average over output word-level scores. (5) If an output word has no fast-align alignment point, it is ignored in this computation. (6) If an output word is fast-aligned to multiple input words, then (6a) for the match score: count it as correct if the n aligned words among the top n highest scoring words according to attention and (6b) for the probability mass score: add up their attention scores. 15 https://github.com/rsennrich/wmt16-scripts</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ratio</head><p>Words Source: A Republican strategy to counter the re-election of Obama 1 1024 0.4 million Un órgano de coordinación para el anuncio de libre determinación 1 512 0.8 million Lista de una estrategia para luchar contra la elección de hojas de Ohio 1 256 1.5 million Explosión realiza una estrategia divisiva de luchar contra las elecciones de autor 1 128 3.0 million Una estrategia republicana para la eliminación de la reelección de Obama 1 64 6.0 million Estrategia siria para contrarrestar la reelección del Obama . We built English-Spanish systems on WMT data, 13 about 385.7 million English words paired with Spanish. To obtain a learning curve, we used 1 1024 , 1 512 , ..., 1 2 , and all of the data. For statistical machine translation, the language model was trained on the Spanish part of each subset, respectively. In addition to a neural and statistical machine translation system trained on each subset, we also used all additionally provided monolingual data for a big language model in contrastive statistical machine translation systems.</p><p>Results are shown in <ref type="figure">Figure 13</ref>.48. Neural machine translation exhibits a much steeper learning curve, starting with abysmal results (BLEU score of 1.6 vs. 16.4 for 1 1024 of the data), outperforming statistical machine translation 25.7 vs. 24.7 with 1 16 of the data (24.1 million words), and even beating the statistical machine translation system with a big language model with the full data set (31.1 for neural machine translation, 28.4 for statistical machine translation, 30.4 for statistical with a big language model).</p><p>The contrast between the neural and statistical machine translation learning curves is quite striking. While neural machine translation is able to exploit increasing amounts of training data more effectively, it is unable to get off the ground with training corpus sizes of a few million words or less.</p><p>To illustrate this, see <ref type="figure">Figure 13</ref>.49. With 1 1024 of the training data, the output is completely unrelated to the input, some key words are properly translated with 1 512 and 1 256 of the data (estrategia for strategy, elección or elecciones for election), and starting with 1 64 the translations become respectable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.8.3">Noisy Data</head><p>Statistical machine translation is fairly robust to noisy data. The quality of systems holds up fairly well, even if large parts of the training data are corrupted in various ways, such as misaligned sentences, content in wrong languages, badly translated sentences, etc. Statistical machine translation models are built on probability distributions estimated from many occurrences of words and phrases. Any unsystematic noise in the training only affects the tail end of the distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Index</head><p>Author Index </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Incorporating discrete translation lexicons into neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1557" to="1567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/pdf/1409.0473v6.pdf" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pragmatic neural language modelling in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Baltescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N15-1083" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="820" to="829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Oxlm: A neural language modelling framework for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Baltescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<ptr target="http://ufal.mff.cuni.cz/pbml/102/art-baltescu-blunsom-hoang.pdf" />
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="81" to="92" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural versus phrasebased machine translation quality: a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-1025" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="257" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Findings of the 2016 conference on machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">Jimeno</forename><surname>Yepes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelie</forename><surname>Neveol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Rubino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><surname>Zampieri</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W/W16/W16-2301" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="131" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Machine translation using neural networks and finite-state models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asunción</forename><surname>Castaño</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Casacuberta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Vidal</surname></persName>
		</author>
		<ptr target="http://www.mt-archive.info/TMI-1997-Castano.pdf" />
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cost weighting for neural machine translation domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Larkin</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W17-3205" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Neural Machine Translation</title>
		<meeting>the First Workshop on Neural Machine Translation<address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="40" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bilingual methods for adaptive training data selection for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="http://www-labs.iro.umontreal.ca/foster/papers/bicnn-data-sel-amta16.pdf" />
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Machine Translation in the Americas (AMTA)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Guided alignment training for topic-aware neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Matusov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Khadivi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Thorsten</forename><surname>Peter</surname></persName>
		</author>
		<idno>abs/1607.01628</idno>
		<ptr target="http://arxiv.org/abs/1607.01628" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W14-4012" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<meeting>SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An empirical comparison of domain adaptation methods for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhui</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><surname>Dabre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P17-2061" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="385" to="391" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics, Vancouver</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Incorporating structural alignment biases into an attentional neural translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Duy Vu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vymolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1102" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Systran&apos;s pure neural machine translation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungi</forename><surname>Josep Maria Crego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anabel</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathy</forename><surname>Rebollo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Akhanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Brunelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Coquard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyo</forename><surname>Enoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Geiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ardas</forename><surname>Johanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raoum</forename><surname>Khalsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongil</forename><surname>Khiari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Kobus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lorieux</surname></persName>
		</author>
		<idno>abs/1610.05540</idno>
		<ptr target="http://arxiv.org/abs/1610.05540" />
	</analytic>
	<monogr>
		<title level="m">Leidiana Martins, Dang-Chuan Nguyen</title>
		<editor>Cyril Tiquet, Bo Wang, Jin Yang, Dakun Zhang, Jing Zhou, and Peter Zoldan</editor>
		<meeting><address><addrLine>Alexandra Priori, Thomas Riccardi, Natalia Segal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast and accurate preordering for smt using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonzalo</forename><surname>Adrià De Gispert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Byrne</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N15-1105" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1012" to="1017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P14-1129" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1370" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The jhu machine translation systems for wmt 2016</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuoyang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huda</forename><surname>Khayrallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W/W16/W16-2310" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="272" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of ibm model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N13-1073" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="644" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural vs. phrase-based machine translation in a multi-domain scenario</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amin Farajian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/E17-2045" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers. Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="280" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving attention modeling with implicit distortion and fertility for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Shi Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenny</forename><forename type="middle">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C16-1290" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3082" to="3092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rescoring a phrase-based machine transliteration system with recurrent neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W12-4406" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Named Entity Workshop (NEWS) 2012. Association for Computational Linguistics</title>
		<meeting>the 4th Named Entity Workshop (NEWS) 2012. Association for Computational Linguistics<address><addrLine>Jeju, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="47" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-way, multilingual neural machine translation with a shared attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1101" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="866" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recursive hetero-associative memories for translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mikel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Forcada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramón</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ñeco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biological and Artificial Computation: From Neuroscience to Technology</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="453" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fast domain adaptation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1612.06897v1.pdf" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scalable inference and training of context-rich syntactic translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Graehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Deneefe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Thayer</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P/P06/P06-1121" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="961" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">What&apos;s in a translation rule?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N04-1035.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL)</title>
		<meeting>the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno>abs/1705.03122</idno>
		<ptr target="http://arxiv.org/abs/1705.03122" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Generative temporal models with memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mevlana</forename><surname>Gemici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Chun</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">Jimenez</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno>abs/1702.04649</idno>
		<ptr target="http://arxiv.org/abs/1702.04649" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="DOI">10.2200/S00762ED1V01Y201703HLT037</idno>
		<ptr target="https://doi.org/10.2200/S00762ED1V01Y201703HLT037" />
	</analytic>
	<monogr>
		<title level="m">Neural Network Methods for Natural Language Processing</title>
		<meeting><address><addrLine>San Rafael, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1154" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pointing the unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1014" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">On using monolingual corpora in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Çaglar</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huei-Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1503.03535</idno>
		<ptr target="http://arxiv.org/abs/1503.03535" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Minimum translation modeling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/E14-1003" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="20" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Combining bilingual and comparable corpora for low resource machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W13-2233" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Statistical Machine Translation</title>
		<meeting>the Eighth Workshop on Statistical Machine Translation<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-1001" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Montreal neural machine translation systems for wmtÃćâĆňâĎć15</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W15-3014" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="134" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macduff</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1611.04558</idno>
		<ptr target="http://arxiv.org/abs/1611.04558" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Is neural machine translation ready for deployment? a case study on 30 translation directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<title level="m">Proceedings of the International Workshop on Spoken Language Translation (IWSLT)</title>
		<meeting>the International Workshop on Spoken Language Translation (IWSLT)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D13-1176" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Neural reordering model considering phrase translation and word alignment for phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Kanouchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W16-4607" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Asian Translation (WAT2016). The COLING 2016 Organizing Committee</title>
		<meeting>the 3rd Workshop on Asian Translation (WAT2016). The COLING 2016 Organizing Committee<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="94" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1412.6980.pdf" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Domain control for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Kobus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josep</forename><surname>Crego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1612.06140v1.pdf" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P/P07/P07-2045" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions. Association for Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume the Demo and Poster Sessions. Association for Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Layer Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A neural reordering model for phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Izuha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dakun</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/C14-1179" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. Dublin City University and Association for Computational Linguistics</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. Dublin City University and Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1897" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Neural machine translation with supervised attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C16-1291" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3093" to="3102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning new semi-supervised deep auto-encoder features for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P14-1012" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="122" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Stanford neural machine translation systems for spoken language domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.mt-archive.info/15/IWSLT-2015-luong.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Spoken Language Translation (IWSLT)</title>
		<meeting>the International Workshop on Spoken Language Translation (IWSLT)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="76" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep neural language models for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kayser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/K15-1031" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Nineteenth Conference on Computational Natural Language Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="305" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1166" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-1002" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Interactive attention for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C16-1205" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2174" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Encoding source language with convolutional neural network for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd</title>
		<meeting>the 53rd</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<ptr target="http://www.aclweb.org/anthology/P15-1003" />
		<title level="m">Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="20" to="30" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Vocabulary manipulation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
		<ptr target="http://anthology.aclweb.org/P16-2021" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="124" to="129" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Non-projective dependency-based prereordering with recurrent neural network for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miceli</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Attardi</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-1082" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="846" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep architectures for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Helcl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W17-4710" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Research Papers. Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="99" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Statistical Language Models based on Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<ptr target="http://www.fit.vutbr.cz/imikolov/rnnlm/thesis.pdf" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Brno University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N13-1090" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v28/pascanu13.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning, ICML</title>
		<meeting>the 30th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Online learning for neural machine translation post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Álvaro</forename><surname>Peris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Cebrián</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Casacuberta</surname></persName>
		</author>
		<idno>abs/1706.03196</idno>
		<ptr target="http://arxiv.org/abs/1706.03196" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Continuous space language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<ptr target="https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenSemester2_2009_10/sdarticle.pdf" />
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="492" to="518" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Continuous-space language models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title/>
		<ptr target="http://ufal.mff.cuni.cz/pbml/93/art-schwenk.pdf" />
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="137" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Continuous space translation models for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/C12-2104" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012: Posters. The COLING 2012 Organizing Committee</title>
		<meeting>COLING 2012: Posters. The COLING 2012 Organizing Committee<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1071" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Smooth bilingual ngram translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><forename type="middle">Ruiz</forename><surname>Costa-Jussa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">A R</forename><surname>Fonollosa</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D/D07/D07-1045" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="430" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Continuous space language models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dechelotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P/P06/P06-2093" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions</title>
		<meeting>the COLING/ACL 2006 Main Conference Poster Sessions<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="723" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Large, pruned or continuous space language models on a gpu for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Attik</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W12-2702" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT</title>
		<meeting>the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Linguistic input features improve neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W/W16/W16-2209" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation. Association for Computational Linguistics</title>
		<meeting>the First Conference on Machine Translation. Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="83" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Controlling politeness in neural machine translation via side constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1005" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Edinburgh neural machine translation systems for wmt 16</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W/W16/W16-2323" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="371" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1009" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Domain specialization: a post-training domain adaptation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Servan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Josep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Crego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Senellart</surname></persName>
		</author>
		<idno>abs/1612.06141</idno>
		<ptr target="http://arxiv.org/abs/1612.06141" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v15/srivastava14a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Comparison of feedforward and recurrent neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Oparin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Freiberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schl &amp;quot;uter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<ptr target="http://www.eu-bridge.eu/downloads/_Comparison_of_Feedforward_and_Recurrent_Neural_Network_Language_Models.pdf" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8430" to="8434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Incremental adaptation strategies for neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Ter-Sarkisov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W15-4006" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="48" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Parallel data, tools and interfaces in opus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<ptr target="http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012). European Language Resources Association (ELRA)</title>
		<editor>Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Mehmet Ugur Dogan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis</editor>
		<meeting>the Eighth International Conference on Language Resources and Evaluation (LREC-2012). European Language Resources Association (ELRA)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="12" to="1246" />
		</imprint>
	</monogr>
	<note>ACL Anthology Identifier</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">A multifaceted evaluation of neural versus phrase-based machine translation for 9 language directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Toral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Víctor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sánchez-Cartagena</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/E17-1100" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers. Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1063" to="1073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Context gates for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1608.06043</idno>
		<ptr target="http://arxiv.org/abs/1608.06043" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Neural machine translation with reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1611.01874" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st AAAI Conference on Artificial Intelligence</title>
		<meeting>the 31st AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1008" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Learning performance of a machine translation system: a statistical and computational analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nello</forename><surname>Tijl De Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cristianini</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W/W08/W08-0305" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Statistical Machine Translation</title>
		<meeting>the Third Workshop on Statistical Machine Translation<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="35" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>abs/1706.03762</idno>
		<ptr target="http://arxiv.org/abs/1706.03762" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Decoding with large-scale neural language models improves translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinggong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D13-1140" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1387" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Janus: A speechto-speech translation system using connectionist and symbolic processing strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Mcnair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tebelskis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1991 International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>the 1991 International Conference on Acoustics, Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="793" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Converting continuous-space language models into n-gram language models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D13-1082" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="845" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Neural network based bilingual language model growing for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1023" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="189" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Edinburgh&apos;s statistical machine translation systems for wmt16</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Nadejde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W/W16/W16-2327" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="399" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Improve statistical machine translation with context-sensitive bilingual semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1015" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="142" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
		<ptr target="http://arxiv.org/abs/1609.08144.pdf" />
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<editor>Greg Corrado, Macduff Hughes, and Jeffrey Dean</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Factored recurrent neural network language model in TED lecture transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xugang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeki</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiori</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Kashioka</surname></persName>
		</author>
		<ptr target="http://www.mt-archive.info/IWSLT-2012-Wu.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT)</title>
		<meeting>the seventh International Workshop on Spoken Language Translation (IWSLT)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="222" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Dual learning for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
		<idno>abs/1611.00179</idno>
		<ptr target="https://arxiv.org/pdf/1611.00179.pdf" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Recurrent neural network based rule sequence model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-2022" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="132" to="138" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<ptr target="http://arxiv.org/abs/1212.5701" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Local translation prediction with global sentence representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dakun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hao</surname></persName>
		</author>
		<ptr target="http://ijcai.org/papers15/Papers/IJCAI15-201.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1398" to="1404" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
