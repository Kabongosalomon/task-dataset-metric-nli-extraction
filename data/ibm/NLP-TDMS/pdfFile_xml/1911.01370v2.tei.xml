<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Difference Detection for Weakly-Supervised Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wataru</forename><surname>Shimoda</surname></persName>
							<email>shimoda-k@mm.inf.uec.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Artificial Intelligence eXploration Research Center</orgName>
								<orgName type="institution">The University of Electro Communications</orgName>
								<address>
									<addrLine>Tokyo 1-5-1 Chofugaoka</addrLine>
									<postCode>182-8585</postCode>
									<settlement>Chofu</settlement>
									<region>Tokyo</region>
									<country key="JP">JAPAN</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Yanai</surname></persName>
							<email>yanai@mm.inf.uec.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Artificial Intelligence eXploration Research Center</orgName>
								<orgName type="institution">The University of Electro Communications</orgName>
								<address>
									<addrLine>Tokyo 1-5-1 Chofugaoka</addrLine>
									<postCode>182-8585</postCode>
									<settlement>Chofu</settlement>
									<region>Tokyo</region>
									<country key="JP">JAPAN</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Difference Detection for Weakly-Supervised Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To minimize the annotation costs associated with the training of semantic segmentation models, researchers have extensively investigated weakly-supervised segmentation approaches. In the current weakly-supervised segmentation methods, the most widely adopted approach is based on visualization. However, the visualization results are not generally equal to semantic segmentation. Therefore, to perform accurate semantic segmentation under the weakly supervised condition, it is necessary to consider the mapping functions that convert the visualization results into semantic segmentation. For such mapping functions, the conditional random field and iterative re-training using the outputs of a segmentation model are usually used. However, these methods do not always guarantee improvements in accuracy; therefore, if we apply these mapping functions iteratively multiple times, eventually the accuracy will not improve or will decrease.</p><p>In this paper, to make the most of such mapping functions, we assume that the results of the mapping function include noise, and we improve the accuracy by removing noise. To achieve our aim, we propose the self-supervised difference detection module, which estimates noise from the results of the mapping functions by predicting the difference between the segmentation masks before and after the mapping. We verified the effectiveness of the proposed method by performing experiments on the PASCAL Visual Object Classes 2012 dataset, and we achieved 64.9% in the val set and 65.5% in the test set. Both of the results become new state-of-the-art under the same setting of weakly supervised semantic segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is a promising image recognition technology that enables the detailed analysis of images for various practical applications. However, semantic segmentation methods require training data with pixel-level annotation, which is costly to create. On the other hand, imagelevel annotation is much easier to obtain than pixel-level annotation. In recent years, various weakly-supervised se-mantic segmentation (hereinafter WSS) methods that required only image-level annotation have been proposed to resolve the annotation problems. However, there is still a large performance gap between fully-supervised and weakly-supervised methods.</p><p>In weakly-supervised segmentation methods, visualization-based approaches <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48]</ref> have been widely adopted. The visualization results highlight the regions that contributed to the classification, and we can roughly estimate the regions of the target objects by visualization. Class Activation Map (CAM) <ref type="bibr" target="#b47">[48]</ref> is a standard method to visualize the classification results. However, the visualization results do not always match actual segmentation results; therefore, it is usually necessary to consider the mapping from the visualization results to the semantic segmentation in weakly-supervised segmentation. Conditional Random Field (CRF) <ref type="bibr" target="#b18">[19]</ref> is widely used as a mapping function. CRF is a method for optimizing the probability distribution to be fitted to the edge of regions by using color and position information as features. The iterative approach for the learning segmentation models proposed by Wei et al. <ref type="bibr" target="#b42">[43]</ref> is a versatile approach for improving weakly supervised segmentation results. In this method, we generate pseudo pixel-level labels under weakly supervised conditions, and we train a segmentation model with the pseudo labels. Subsequently, we generate pseudo pixel-level labels from the outputs of the trained segmentation model, and we re-train a new segmentation model using the generated pseudo labels. Wei et al. <ref type="bibr" target="#b42">[43]</ref> showed that repeating this process absorbed outliers and gradually improved the accuracy. These methods can be regarded as mapping functions that bring inputs closer to the segmentation. However, the mapping functions of these methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b42">43]</ref> do not guarantee any improvement in the accuracy of the semantic segmentation; therefore, the mapping results contain noise. In this paper, the mapping functions that make the above inputs close to the segmentation are treated as supervision containing noise, and we propose a robust learning method for such noise.</p><p>In this paper, we denote the information used as the inputs of the mapping functions as knowledge, and we consider the supervision containing the noise as advice. The supervision for fully supervised learning that allows one-to-one mapping is teacher. We assume that the advice provides supervision, which includes some correct and incorrect information. To make effective use of the information obtained from this advice, it is necessary to select useful information. In this paper, we regard the regions where opinions differ between knowledge and advice as difference. Since difference in the two segmentation masks can be obtained by simple processing without annotation, it is a kind of selfsupervised learning to train a model, which predicts difference. Self-supervised learning is a pretext task as a form of indirect supervision. For example, as notable works, colorization <ref type="bibr" target="#b4">[5]</ref> and predicting the patch ordering <ref type="bibr" target="#b5">[6]</ref> have been proposed.</p><p>Inferring difference in knowledge and advice from knowledge leads to predicting the advisor's advice in advance. In predicting advice, there are predictable advice and unpredictable advice. Certain advice can be easily inferred because many similar samples are included during training. Here, we assumed that advice contains a sufficient number of good information, and predictable information can be considered to be useful information. Based on this idea, we propose a method for selecting information by finding the true information in advice that can be predicted from the inference results of difference detection. <ref type="figure">Fig.1</ref> shows the concept of the proposed approach.</p><p>In this paper, we demonstrate that the proposed Self-Supervised Difference Detection (SSDD) module can be used in both the seed generation stage and the training stage of fully supervised segmentation. In the seed generation stage, we refine the CRF results for pixel-level semantic affinity (PSA) <ref type="bibr" target="#b1">[2]</ref> by using the SSDD module. In the training stage, we introduce two SSDD modules inside the training loop of a fully supervised segmentation network. In the experiments, we demonstrate the effectiveness of the SSDD modules in both stages. In particular, the SSDD modules greatly boosted the performance of the WSS on the PASCAL visual object classes (VOC) 2012 dataset, and achieved new state-of-the-art. To summarize it, our contributions are as follows:</p><p>• We propose an SSDD module, which estimates the noise of the mapping functions of the weakly supervised segmentation and select useful information.</p><p>• We show that the SSDD modules can be effectively applied to both the seed generation stage and the training stage of a fully supervised segmentation model.</p><p>• We obtained the best results on the PASCAL VOC 2012 dataset with 64.9% mean IoU on the val set and 65.5% on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>In this section, we review related research on CNN-based WSS methods by classifying them into several types. Visualization In the early works of CNN-based WSS, visualization-based methods were studied. The pixels that contributed to the classification were correlated to the regions of the target objects; therefore, the visualization methods can be used as segmentation methods under weakly supervised settings. Zeiler et al. <ref type="bibr" target="#b45">[46]</ref> showed that the derivatives obtained by back-propagation from the CNN models trained for classification tasks highlight the region of a target object in an image. Simonyan et al. <ref type="bibr" target="#b37">[38]</ref> used derivatives such as the GrabCut seeds and extended the visualization method to the WSS method. They also demonstrated that the regions of multi-class objects could also be captured by the difference in class-specific derivatives <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b36">37]</ref>. Oquab et al. <ref type="bibr" target="#b25">[26]</ref> visualized the attention region by the forwarding process using activation and trained a classification model with large input images by using global max pooling. After this approach, several derived methods employing global pooling were also proposed <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b17">18]</ref>. In particular, CAM <ref type="bibr" target="#b47">[48]</ref> has been widely adopted in recent weakly supervised segmentation methods. Region refinement for WSS results using CRF In general, the segmentation results based on fully convolutional neural network (FCN) <ref type="bibr" target="#b23">[24]</ref> tend to output ambiguous outlines. CRF <ref type="bibr" target="#b18">[19]</ref> can refine the ambiguous outlines using low-level features such as the pixel colors. Chen et al. <ref type="bibr" target="#b26">[27]</ref> and Pathak et al. <ref type="bibr" target="#b27">[28]</ref> adopted CRF as a post-processing method for region refinement and demonstrated the effectiveness of the CRF for WSS. Kolesnikov et al. <ref type="bibr" target="#b17">[18]</ref> proposed the use of CRF during the training of a semantic segmentation model. Ahn et al. <ref type="bibr" target="#b1">[2]</ref> proposed a method to learn pixel-level similarity from the CRF results, and apply a random walk-based region refinement, which achieved the best results on the PASCAL VOC 2012 dataset. CRF plays an important role to improve the accuracy of weakly supervised segmentation. Furthermore, various researches employed the CRF for refining the coarse segmentation masks <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b35">36]</ref>. However, CRF does not guarantee any improvement in the mean intersection over union (IoU) score, and it often degrades the segmentation masks and the scores. Therefore, we focus on preventing a segmentation mask from being degraded by applying CRF. We estimate the confidence maps of both the initial mask and the mask after CRF post-processing, and we integrate both masks based on the estimated confidence maps.</p><p>Training fully supervised segmentation model under weakly supervised setting Certain researchers trained a fully supervised semantic segmentation (hereinafter FSS) model under a weakly supervised setting. First, Papandreou et al. <ref type="bibr" target="#b28">[29]</ref> proposed MIL-FCN, which trained a fully supervised semantic segmentation model with a global maxpooling loss using only image-level labels. Wei et al. <ref type="bibr" target="#b42">[43]</ref> proposed a novel approach to train an FSS model using pixel-level labels obtained by saliency maps <ref type="bibr" target="#b12">[13]</ref>. This method is simple, and the obtained results are impressive. Wei et al. <ref type="bibr" target="#b42">[43]</ref> also demonstrated that the outputs of the trained semantic segmentation model could be used as a new pixel-level annotation for re-training, and the re-trained FSS model achieved better results than the original model. Generating pixel-level labels during training of an FSS model Constrained convolutional neural network <ref type="figure">Figure 1</ref>. The concept of the proposed approach. (a) We denote the inputs of the mapping functions as knowledge and the outputs as advice. (b) The proposed difference detection network (DD-Net) estimates the difference between knowledge and advice. (c) In difference, the advice is divided into true advice and false advice. We assume that if the amount of true advice is larger than the amount of false advice, that is, if a set of false advice are outliers, then the predictable advice has a strong correlation with the true advice.</p><p>(CCNN) <ref type="bibr" target="#b27">[28]</ref> and EM-adopt <ref type="bibr" target="#b26">[27]</ref> generated pixel-level labels during training using class labels and outputs of the segmentation model. In both the studies similar constraints were made for generating pixel-level labels to obtain better results. They set the ratios of the foreground and the background in an image and generated pixel-level labels within the ratio. Wei et al. <ref type="bibr" target="#b41">[42]</ref> proposed an online prohibitive segmentation learning (PSL). They generated pixellevel seed labels of training samples before the first training of an FSS model and re-generated pixel-level labels using the outputs of the segmentation model and the classification results. The semantic segmentation model was trained by both the pixel-level labels, and they achieved good performance without costly manual pixel-level annotation. We expected that the pixel-level seed labels would play the role of the constraint. Huang et al. <ref type="bibr" target="#b11">[12]</ref> proposed deep seeded region growing (DSRG), which is a method to expand the seed region during training. Before training, the authors prepared pixel-level seed labels that had unlabeled regions for unconsidered pixels. In this research, we proposed new constraints for generating pixel-level labels during the training of the FSS model. We trained an FSS model and the difference detection model in an end-to-end manner. Then, we interpolated a few pixel-level seed labels, that had different regions in the newly generated pixel-level labels and these labels could also be predicted by the difference detection model. WSS methods using additional information A few recent weakly supervised approaches achieved high accuracy by using additional annotations for image-level labels. Researchers have proposed the bounding box annotation for WSS <ref type="bibr" target="#b26">[27]</ref>, and they showed that the bounding box annotation substantially boosted performance. As weaker additional annotation, point annotation and scribble annotation were also proposed <ref type="bibr" target="#b2">[3]</ref>. Saleh et al. <ref type="bibr" target="#b33">[34]</ref> proposed an approach to check the generated initial masks by minimal additional supervision by human visions. Motion segmentation of videos as additional training information for weakly supervised segmentation has also been proposed <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b9">10]</ref>. There are also reports that web images were helpful for improving the weakly supervised segmentation accuracy <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b35">36]</ref>. Recently, fully supervised saliency methods are being widely used for detecting the background regions, and certain researchers have reported that this approach could substantially boost performance <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b3">4]</ref>. Region proposal meth- ods trained with fully supervised foreground masks such as MCG <ref type="bibr" target="#b30">[31]</ref> have also been used in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref>. Hu et al. <ref type="bibr" target="#b6">[7]</ref> used instance-level saliency maps for WSS. The concept of saliency can be used and helpful in various situation; however, the fully supervised saliency model was affected by its training data domain, which may cause negative effects on applications. WSS methods without saliency maps are also beneficial. In this paper, we do not use any additional information, and we use only PASCAL VOC images with image-level labels and CNN models pre-trained with Ima-geNet images and their image-level labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>There was no supervision for the mapping functions of segmentation in the weakly supervised setting; therefore, it was necessary to consider a mapping for bringing the input close to the better segmentation results by using a method that incorporated human knowledge. In this paper, we propose a method for selecting useful information from the results of the mapping functions by treating the results as supervision containing noise. We define the inputs of the mapping functions as knowledge, and the mapped results as advice. We predict the regions of differences between knowledge and advice, and we call this as the difference detection task. Using the inference results, we select the information of the advice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Difference detection network</head><p>In this section, we formulate the difference detection task. In the proposed method, we predict the difference between knowledge and advice. Here, we define the segmentation mask of knowledge as m K , the segmentation mask of advice as m A , and their difference as M K,A ∈ R H×W .</p><formula xml:id="formula_0">M K,A u = 1 if (m K u = m A u ) 0 if (m K u = m A u ) ,<label>(1)</label></formula><p>where u ∈ {1, 2, .., n} indicates a location of pixels, and n is the number of pixels. Next, we define a network of difference detection for deducing the difference. We use feature maps extracted from a trained CNN to assist the difference detection. In particular, we use high-level features e h (x; θ e ) and low-level features e l (x; θ e ) extracted from a backbone network, such as ResNet. Here, x is an input image, and e is an embedding function parameterized by θ e . As shown in <ref type="figure" target="#fig_1">Fig.3</ref>, the confidence map of the input mask d is generated by difference detection network (DD-Net), DDnet(e h (x; θe), e l (x; θe),m; θ d ), d ∈ R H×W , wherem is a one-hot vector mask with the same number of channels to the target class number, θ d is the parameter of the DD-Net, and e(x) = (e l (x), e h (x)). The architecture of DD-Net is shown in <ref type="figure" target="#fig_0">Fig.2</ref>; it consists of three convolutional layers and one Residual block with three inputs and one output. DD-Net takes either a raw mask or a processed mask as an input, and outputs the difference mask. This network performs learning using the following losses:</p><formula xml:id="formula_1">L diff = 1 |S| u∈S (J(M K,A , d K , u; θ d ) +J(M K,A , d A , u; θ d )),<label>(2)</label></formula><p>where S is a set of pixels of the input spaces, and J() is assumed to be a function that returns a loss for the binary cross entropy.</p><formula xml:id="formula_2">J(M, d, u) = M u log d u + (1 − M u ) log(1 − d u ).</formula><p>Note that the parameters of the embedding function θ e are independent of the optimization of θ d . The training of DD-Net is self-supervised; therefore, neither special annotation nor additional data are needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Self-supervised difference detection module</head><p>In this section, we describe the details of the SSDD module shown in <ref type="figure" target="#fig_1">Fig.3</ref>, which integrates two masks adaptively according to the confidence maps. We denote a set of advice that are true in difference as S A,T , and a set of advice that are false as S A,F . The purpose of the method is to extract as many samples of S A,T as possible from the entire set of advice S A . Let d K be the inference results of advice from the given knowledge. The inference results are the probability distributions from 0 to 1, and the values have variations. The variations are caused by the difference in the difficulty of inference. The presence of similar patterns during training can have a strong influence on the difference in the difficulty of inference. Here, if there are a sufficient number of advice that are true values rather than false values, that is, if |S A,T | &gt; |S A,F |, the larger values indicate that their advice most likely belong to S A,T . However, for the values of d K at a boundary, it is not clear whether advice belongs to S A,T or not; this should probably be different from sample to sample. Therefore, it is difficult to deduce a good advice directly from the size of the value of d K . To alleviate the problem, we use the inference results about the state of knowledge for each advice. Although advices have large variations in their distribution, these variations are less than the variations in the distribution of knowledge in general. Therefore, using advice to infer knowledge is assumed to be easier than using knowledge to advice inference. In this paper, we consider the results of the inference of knowledge to advice for evaluating the difficulty of inference in each sample; we use the inferences for the thresholds for each sample. Specifically, we calculate the confidence scores of advice from the viewpoint of how close the values of d K to d A . The confidence score w u ∈ R is defined by the following expression:</p><formula xml:id="formula_3">w u = d K u − d A u + bias u<label>(3)</label></formula><p>Here, bias is a hyper parameter for a threshold of the selection obtained by the difference detection, and it is also an enhanced value for the categories in the presence labels of the input image. The refined masks m D obtained from m K and m A are defined by the following expression:</p><formula xml:id="formula_4">m D u = m A u if (w u ≥ 0) m K u if (w u &lt; 0)<label>(4)</label></formula><p>We denote this processing flow for generating new segmentation mask as an SSDD module in the after notation.</p><formula xml:id="formula_5">m D = SSDD(e(x), m K , m A ; θ d )<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Introducing SSDD modules into the processing flow of WSS</head><p>In this section, we explain how to use SSDD modules in the processing flow of WSS. The proposed method can be adapted to various cases by applying inputs of the mapping function as knowledge and the results of the mapping function as advice. The processing flow that we adopted in this paper consists of two stages: the seed generation stage with static region refinement and the training stage of a segmentation model with dynamic region refinement. In the first stage, we adapted the proposed method by applying the results of PSA as knowledge and its CRF results as advice (Sec.4.1). In the second stage, we adapted the proposed method by applying the results of the first stage (Sec.4.1) as knowledge, and the outputs of the segmentation models trained by the masks were applied as advice (Sec.4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Seed mask generation stage with static region refinement</head><p>PSA <ref type="bibr" target="#b1">[2]</ref> is a method to propagate label responses to nearby areas that belong to the same semantic entity. Though PSA employs CRF for the refinement of the segmentation mask, CRF often fails to improve the segmentation masks; in fact, it degrades the masks. In this section, we refine the outputs of CRF in PSA by using the proposed SSDD module. We illustrate the processing flow of the first seed generation stage in <ref type="figure" target="#fig_2">Fig.4</ref>. Note that we omitted the input of the given image to an SSDD module for the sake of simplifying in the figure.</p><p>We denote an input image as x; the probability maps obtained by PSA are denoted as p K0 = P SA(x; θ psa ), and its CRF results are denoted as p A0 . We obtain the segmentation masks (m K0 , m A0 ) from the probability maps (p K0 , p A0 ) by taking the argument of the maximum of the presence labels including a background category. We computed the loss of the DD-Net as follows:</p><formula xml:id="formula_6">L diff0 = 1 |S| u∈S (J(M K0,A0 , d K0 , u; θ d0 ) +J(M K0,A0 , d A0 , u; θ d0 )),<label>(6)</label></formula><p>The proposed method is not effective when either of the segmentation masks or both of them do not have the correct labels. These cases are not only meaningless for the proposed refinement approach, but they may also harm the training of the DD-Net. We define the bad training samples by simple processing based on the difference in the number of the class-specific pixels, and we exclude them from the training.</p><p>In this work, we also train the embedding function by training a segmentation network with m K0 to obtain good representation for the inputs of high-level features and lowlevel features:</p><formula xml:id="formula_7">L base = L seg (x, m K0 ; θ e0 , θ base ),<label>(7)</label></formula><formula xml:id="formula_8">L seg (x, m; θ) = − 1 k∈K |S m k | k∈K u∈|S m k | log(h k u (θ)),<label>(8)</label></formula><p>where S m k is a set of locations that belong to the class k on the mask m; h k u is the conditional probability of observing any label k at any location u ∈ {1, 2, ..., n}; and C is a set of class labels. θ e0 are parameters of embedding functions and θ base are parameters for the segmentation branch. The training of θ e0 is independent of θ d0 . The final loss function  for the static region refinement using the difference detection is as follows:</p><formula xml:id="formula_9">L static = L base + L diff0 .<label>(9)</label></formula><p>After training, we integrate the masks (m K0 , m A0 ) and obtain the integrated masks m D0 using the SSDD module with the trained parameter θ d0 as follows:</p><formula xml:id="formula_10">m D0 = SSDD(e(x), m K0 , m A0 ; θ d0 ).<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training stage of a fully supervised segmentation model with a dynamic region refinement</head><p>When we train a fully supervised semantic segmentation model with pixel-level seed labels, the accuracy of the seed labels directly effects the performance of the segmentation. The performance gain is expected by replacing the seed labels to better the pixel-level labels during training. In this study, we propose a novel approach to constrain the interpolation of the seed labels during the training of a segmentation model. The idea of the constraint is to limit the interpolation of seed labels only to predictable regions of difference detection between newly generated pixel-level labels and seed labels.</p><p>In practice, we interpolate the pixel-level seed labels in two steps of each iteration as shown in <ref type="figure" target="#fig_3">Fig.5</ref>. Note that "SegNet" in the figure does not represent a specific segmentation network; it represents any fully supervised segmentation network. In the first step, for an input image x, we obtain the outputs of the segmentation model p K1 = Seg(e(x); θ main ) and its CRF outputs p A1 . We obtain the segmentation masks (m K1 , m A1 ) from the probability maps (p K1 , p A1 ) by taking the argument of the maximum of the presence labels including a background category. Then, we obtain the refined pixel-level labels m D1 by applying the proposed refinement method as follows: m D1 = SSDD(e(x), m K1 , m A1 ; θ d1 ). In the second step, we apply the proposed method to the seed labels m D0 and to the mask m D1 obtained in the first step. The further refined mask m D2 is obtained by m D2 = SSDD(e(x), m D0 , m D1 ; θ d2 ). We generate the mask m D2 in each iteration and train the segmentation model using the generated mask m D2 . We train the semantic segmentation model with the generated mask m D2 as follows:</p><formula xml:id="formula_11">L main = L seg (x, m D2 ; θ e1 , θ main ),<label>(11)</label></formula><p>The loss of DD-Net for m A1 and m K1 is as follows:</p><formula xml:id="formula_12">L diff1 = 1 |S| u∈S (J(M K1,A1 , d K1 , u; θ d1 ) +J(M K1,A1 , d A1 , u; θ d1 )),<label>(12)</label></formula><p>In the second stage, we also exclude the bad samples (as done in Sec.static) based on the change ratio of pixels because the proposed method is not effective if the input segmentation masks do not have correct regions. We explain how to train the DD-Net for (m D0 ,m D1 ). The masks (m K1 , m A1 , m D1 ) depend on the outputs of the segmentation model Seg(e(x), θ main ). Therefore, if the learning of the segmentation model falls into a local minimum, the masks will become meaningless; all the pixels become background pixels or single foreground pixels. In this case, the inference results of the difference detection is also always constant, that is, (D K = 1, d A = 1, d A = d K ), and Eq.(3) becomes w = bias. To escape from this local minimum, we create a new branch of a segmentation model and use it for learning the difference detection between m D0 and m D1 . Assume that the mask m sub was obtained from outputs of the branch of the new segmentation model p sub = Seg(e(x); θ sub ). In the training of difference detection, we trained the network to learn the differences among (m D0 , m sub ) and (m sub , m D1 ) as follows:</p><formula xml:id="formula_13">L diff2 = 1 |S| u∈S (J(M D0,sub , d D0 , u; θ d2 ) +J(M sub,D1 , d D1 , u; θ d2 )),<label>(13)</label></formula><p>If m sub is the output, which is halfway between m D0 and m D1 , the replacement of the training samples will let the segmentation model exit from the situation (d K = 1, d A = 1, d A = d K ), and the inference results of the difference detection will predict the regions that correlate with the difference between m D0 and m D1 . We train the parameters θ sub from the following loss to achieve the outputs that are halfway between m D0 and m D1 .</p><formula xml:id="formula_14">L sub = αL seg (x, m D0 ; θ e1 , θ sub )+(1−α)L seg (x, m D1 ; θ e1 , θ sub ),<label>(14)</label></formula><p>where α is a hyper parameter of the mixing ratio of m D0 and m D1 .</p><p>The final loss function of the proposed dynamic region refinement method is calculated as follows:</p><formula xml:id="formula_15">L dynamic = L main + L sub + L diff1 + L diff2<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluated the proposed methods using the PASCAL VOC 2012 data. The PASCAL VOC 2012 segmentation dataset has 1464 training images, 1449 validation images, and 1456 test images including 20 class pixel-level labels and image-level labels. Similar to the methodology followed by <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b17">18]</ref>, we used the augmented PASCAL VOC training data provided by <ref type="bibr" target="#b8">[9]</ref> as well, wherein the training image number was 10,582. For evaluation, we used an IoU metric, which is the official evaluation metric in the PASCAL VOC segmentation task. For calculating the mean IoU on the val and test sets, we used the official evaluation server. We compared the best performance of our method with the state-of-the-art methods on both the val and test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation details</head><p>Our experiments are heavily based on the previous research <ref type="bibr" target="#b1">[2]</ref>. For the generating results of PSA results, we used implementations and trained parameters provided by the authors that are publicly available. We followed the methodology of <ref type="bibr" target="#b1">[2]</ref> and set hyperparameters that gave the best performance. For the CRF parameters, we used the default settings provided by <ref type="bibr" target="#b18">[19]</ref>. For the semantic segmentation model, we used a ResNet-38 model, which had almost the same architecture as that in <ref type="bibr" target="#b1">[2]</ref>. The only difference was in the last upsampling rate; in the paper on PSA, the authors set the upsampling rate to 8, while we set the rate to 2 for reducing the computational cost of CRF. The input image size was 448 for training, and the test images and the output feature map size before the upsampling was 56. In the DD-Net, we used features obtained from the segmentation model before the last layer as the highlevel features e h and the features obtained before the second pooling layer as the low level features e l . These feature map sizes were adjusted to 112 by 112 using the simple linear interpolation approach. We initialized the parameters of the segmentation models by using parameters trained with the PASCAL VOC images and their image-level labels with a pre-trained model using ImageNet, which was also provided in <ref type="bibr" target="#b1">[2]</ref>. The codes provided by <ref type="bibr" target="#b1">[2]</ref> did not include the training and test code for the segmentation models; therefore, we implemented our own codes. In the original paper on PSA, though the authors optimized the segmentation models by Adam; however, the performance was unstable in our re-implementation, and there were several unclear settings. Therefore, we used SGD for training the entire net- works. We set an initial learning rate to 1e-3 (1e-2 for initialization without the pre-trained model), and we decreased learning rate with cosine LR ramp down <ref type="bibr" target="#b24">[25]</ref>. For the static region refinement, we trained the network with batch sizes of 16 and 10 epochs. For the dynamic region refinement, we trained the network with batch sizes of 8 and 30 epochs. For the data augmentation and inference technique, we carefully followed the methodology used in <ref type="bibr" target="#b1">[2]</ref>. We implemented the proposed method using PyTorch. All the networks are trained using four NVIDIA Titan X PASCAL. We will open the results of the proposed method and training codes 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Analysis of static region refinement</head><p>In the proposed method, we used fully connected CRF <ref type="bibr" target="#b18">[19]</ref> with the same parameter settings as those for PSA <ref type="bibr" target="#b1">[2]</ref>, (w g = 3, w rgb = 10,θ α = 80, θ β = 13, θ γ = 3) in the following kernel potentials:</p><formula xml:id="formula_16">k(f i , f j ) = w g exp − |pi−pj | 2θ 2 α − |Ii−Ij | 2θ 2 β + w rbg exp − |pi−pj| 2 2θ 2 γ .</formula><p>To examine the relationship between the CRF params and results, we changed the values of (w g , w rgb ) and evaluated the accuracy. <ref type="figure" target="#fig_4">Fig.6</ref> shows a comparison of the proposed static region refinement with the PSA <ref type="bibr" target="#b1">[2]</ref> and its CRF results on the training set. The weakening of w rgb decreases the difference only between the CRF and the SSDD+CRF results; therefore the effectiveness of the proposed method reduces. However, the proposed method always indicates a high accuracy. The optimal weights are different for each image, and it is expected to be difficult to search them for each image. We consider that the proposed method realized the improvement of CRF by correcting the partial failure of CRF. <ref type="figure">Fig.7</ref> shows the difference detection results and their refined segmentation masks. In the fourth and fifth rows of <ref type="figure">Fig.7</ref>, we show the typical failure cases of the proposed method. The regions of small objects tend to vanish in the CRF, and the DD-Net also learns such tendencies, which causes the failure of the proposed re-refinement method. In the fifth row, both of the input segmentation masks fail to provide segmentation. In such cases, the proposed method is also not effective. <ref type="figure">Figure 7</ref>. Each row shows (a) input images, (b) raw PSA segmentation masks, (c) difference detection maps of (b), (d) CRF masks of (b), (e) difference detection maps of (d), (f) refined segmentation masks by the proposed method, and (g) ground truth masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Analysis of the whole proposed method</head><p>We denote the dynamic region refinement as "SSDD" in all the tables. The score of the SSDD is with the CRF with parameters (w g = 3, w rgb = 10) that are default values from the author's public implementation. We also used the parameters for the CRF during training.</p><p>Comparison with PSA <ref type="table" target="#tab_0">Table 1</ref> shows the comparison of the dynamic region refinement method with the PSA. We observe that the proposed method outperforms PSA by more than 3.2 point margins. This clearly proves the effectiveness of the interpolation for the seed labels with the novel constraint by difference detection. The accuracy is greatly improved as compared with the results of the static region refinement because of the increase in the number of good advice by end-to-end learning of the segmentation model, that is, |S A1,T | &gt; |S A0,T |.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, we also show the gains between the proposed method and PSA for detailed analysis. We obtain over 10% gain on the cat, cow, horse, and sheep classes. Interestingly, all the classes that gave the large gain belonged to the animal category. However, in the potted plant, airplane, and person class objects, it was hard to improve the segmentation mask by using the proposed method. In the proposed method, we considered the precondition that advise, which is a true value, was larger than the value that was not a true value(|S A,T | &gt; |S A,F |). When this precondition was satisfied, the accuracy of the classes improved. If the precondition was not satisfied, the accuracy did not improve or the accuracy decreased. <ref type="figure">Fig.8</ref> shows the examples of the results of reimplementation of PSA, the static region refinement, and the dynamic region refinement. Dynamic region refinement shows more accurate predictions on object location and boundary. The results of the static region refinement are outputs of a segmentation model re-trained with the masks in case of (w g = 3, w rgb = 10) in <ref type="figure" target="#fig_4">Fig.6</ref>. Note that we show the results of before the CRF for detailed comparisons. Comparison with the state-of-the-art methods <ref type="table" target="#tab_1">Table 2</ref> shows the results of the proposed method and the recent   weakly supervised segmentation methods that do not use additional supervisions on the PASCAL VOC 2012 validation data and PASCAL VOC 2012 test data. We observed that our method achieves the highest score as compared with all the existing methods, which use the same types of supervision <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b1">2]</ref>. The proposed method outperforms the recent previous works on MEFF and TPL by large margins. As discussed earlier, the proposed method also outperforms the current state-of-the-art methods <ref type="bibr" target="#b1">[2]</ref>. This result clearly indicates the effectiveness of the proposed method. <ref type="table" target="#tab_2">Table 3</ref> shows the comparison of the proposed method with a few weakly supervised segmentation methods that employ relatively cheap additional information. Surprisingly, the proposed method also outperforms all the listed weakly supervised segmentation methods. The proposed methods outperformed the following methods: SeeNet <ref type="bibr" target="#b33">[34]</ref>, DSRG <ref type="bibr" target="#b42">[43]</ref>, MDC <ref type="bibr" target="#b17">[18]</ref>, GAIN <ref type="bibr" target="#b21">[22]</ref>, and MCOF <ref type="bibr" target="#b40">[41]</ref> that employed fully supervised saliency methods. In addition, the score of the proposed method was also better than the results of AISC <ref type="bibr" target="#b6">[7]</ref>, which used instance-level saliency map methods. Note that AISC achieved 64.5% on the val set and 65.6% on the test set using an additional 24,000 ImageNet images for training. The score of the proposed method was also higher than the score of Shen et al. <ref type="bibr" target="#b35">[36]</ref>, which used 76.7k web images for training. It is not possible to have a completely fair comparison for them because of the difference of the network model, the augmentation technique, the number of iteration epochs, and so on. However, the proposed method demonstrates comparable performance or better performance without any additional training information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we proposed a novel method to refine a segmentation mask from a pair of segmentation masks before and after the refinement process such as the CRF by using the proposed SSDD module. We demonstrated that the proposed method could be used effectively in two stages: the static region refinement in the seed generation stage and the dynamic region refinement in the training stage. In the first stage, we refined the CRF results of PSA <ref type="bibr" target="#b1">[2]</ref> by using the SSDD module. In the second stage, we refined the generated semantic segmentation masks by using a fully supervised segmentation model and CRF during the training. We demonstrated that three SSDD modules could greatly boost the performance of WSS and achieve the best results on the PASCAL VOC 2012 dataset over all the weakly supervised methods with and without additional supervision.</p><p>Supplementary Material for "Self-Supervised Difference Detectionfor Weakly-Supervised Semantic Segmentation"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Details of the simple decision</head><p>In the proposed method, we select advice by inference results of difference detection. The confidence score is calculated from the viewpoint of how close the value of d K to d A . In the proposed method, if this difference is large enough, we ignore the advice. Therefore, if the inferences of the difference detection are too easy, the values of d K for advice that is not true become close to d A , and the proposed method does not work effectively. In particular, if the inference results of the difference detection are (d K = 1, d A = 1, d A = d K ), we cannot distinguish whether the advice belongs to the set of true values |S A,T | or the set of false values |S A,F | based on the results of the difference detection. Therefore, we judge the typical failure examples of advice and excluded them from the training sample so that the differences between d K and d A were large in the inference of the bad advice. To be concrete, when the number of differences in the pixels in each class of mask is obviously large, we assume that the advice has failed. We define the bad training samples as the pair of the masks for the difference detection that satisfies the following equation:</p><formula xml:id="formula_17">∀c ∈ C, |S m A c | |S m K c | &lt; 0.5,<label>(16)</label></formula><p>where C is a set of image-level label of the input image. We decide the threshold 0.5 empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Details of the bias in Eq.(3)</head><p>In Eq.(3), we use bias, which is a kind of hyperparameter. In this section, we discuss this bias. We define the bias as follows:</p><formula xml:id="formula_18">biasu = b dd ± b class if m A u or m k u belongs toĈ b dd if otherwise ,<label>(17)</label></formula><p>where ∀c ∈Ĉ satisfy |S m A c | |S m K c | &lt; 0.5 and c ∈ C. b dd is a bias for the difference between knowledge and advice, and b class is a bias for the class category. When the number of differences in the pixels in each class of mask is obviously large, it is assumed that the advice has failed, and to prioritize the label of that class over the results of the difference detection, we use the bias b class . We defined the values of b dd and b class by using the grid search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Values of hyperparameters</head><p>We explore good hyperparameters by a grid search and verify the effect of the hyperparameters. We change the values of the hyperparameters and measure the mean IoU scores.  <ref type="formula" target="#formula_0">(17)</ref> as the bias values. In b dd = 0.4, the mean IoU score becomes the maximum value. We also set the bias b class for the missing categories. We observe that the setting b class = 1.0 achieved a maximum mean IoU. It is expected that the class biases for the missing categories help to the train for robustness. In addition, we also verify the effect of hyperparameters for coefficients of losses in Eq.(11). Though we had expected that the value of α would affect the performance, the hyper parameter was not critical for the change of the mean IoU. The balanced setting, that is, α = 0.5 showed the best score.  <ref type="table" target="#tab_3">methods  bg  aero  bike  bird  boat  bottle  bus  car  cat  chair  cow  table  dog  horse  motor  person  plant  sheep  sofa  train</ref> tv mIoU - -</p><formula xml:id="formula_19">MIL-FCN [29] - - - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_20">- - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_21">- - - - - - - - - - - - - - - - - - - - - PSA [2]</formula><p>88.  S -- </p><formula xml:id="formula_22">- - - - - - - - - - - - - - - - - - - 63.1 AISI [7] IS - - - - - - - - - - - - - - - - - - - - -</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Difference Detection Network (DD-Net).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Overview of the DD-Net. The figure on the left shows the training of the DD-Net, and the right figure shows the processing of the integration using the results of difference detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Processing flow at the seed mask generation stage with static region refinement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Illustration of the processing flow for the dynamic region refinement. ("SegNet" does not represent any specific network but represents any kind of network for fully supervised semantic segmentation.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>mIoU of the seed masks of the training images with different params values with only CRF and with SSDD and CRF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>2 68.2 30.6 81.1 49.6 61.0 77.8 66.1 75.1 29.0 66.0 40.2 80.4 62.0 70.4 73.7 42.5 70.7 42.6 68.1 51.6 61.7 SSDD (ours) 89.0 62.5 28.9 83.7 52.9 59.5 77.6 73.7 87.0 34.0 83.7 47.6 84.1 77.0 73.9 69.6 29.8 84.0 43.2 68.0 53.4 64.9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>.5 28.9 83.7 52.9 59.5 77.6 73.7 87.0 34.0 83.7 47.6 84.1 77.0 73.9 69.6 29.8 84.0 43.2 68.0 53.4 64.9 ( † AS:Saliency mask, WV:web videos. WI Web images. IS Instance saliency mask.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results on PASCAL VOC 2012 val set. 68.2 30.6 81.1 49.6 61.0 77.8 66.1 75.1 29.0 66.0 40.2 80.4 62.0 70.4 73.7 42.5 70.7 42.6 68.1 51.6 61.7 SSDD 89.0 62.5 28.9 83.7 52.9 59.5 77.6 73.7 87.0 34.0 83.7 47.6 84.1 77.0 73.9 69.6 29.8 84.0 43.2 68.0 53.4 64.9 Gain +0.8 -5.7 -1.7 +2.6 +3.3 -1.5 -0.2 +7.6 +11.9 +5.0 +17.7 +7.4 +3.7 +15.0 +3.5 -4.1 -12.7 +13.3 +0.6 -0.1 +1.8 +3.2</figDesc><table><row><cell>Methods</cell><cell>Bg</cell><cell>Aero</cell><cell>Bike</cell><cell>Bird</cell><cell>Boat</cell><cell>Bottle</cell><cell>Bus</cell><cell>Car</cell><cell>Cat</cell><cell>Chair</cell><cell>Cow</cell><cell>Table</cell><cell>Dog</cell><cell>Horse</cell><cell>Motor</cell><cell>Person</cell><cell>Plant</cell><cell>Sheep</cell><cell>Sofa</cell><cell>Train</cell><cell>Tv</cell><cell>mIoU</cell></row><row><cell>PSA [2]</cell><cell>88.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison with the WSS methods without additional supervision.</figDesc><table><row><cell>Method</cell><cell cols="2">Val Test</cell></row><row><cell cols="3">FCN-MIL [29]ICLR2015 25.7 24.9</cell></row><row><cell>CCNN [28]ICCV2015</cell><cell cols="2">35.3 35.6</cell></row><row><cell cols="3">EM-Adapt [27]ICCV2015 38.2 39.6</cell></row><row><cell>DCSM [37]ECCV2016</cell><cell cols="2">44.1 45.1</cell></row><row><cell>BFBP [34]ECCV2016</cell><cell cols="2">46.6 48.0</cell></row><row><cell>SEC [18]ECCV2016</cell><cell cols="2">50.7 51.7</cell></row><row><cell>CBTS [33]CVPR2017</cell><cell cols="2">52.8 53.7</cell></row><row><cell>TPL [17]ICCV2017</cell><cell cols="2">53.1 53.8</cell></row><row><cell>MEFF [8]CVPR2018</cell><cell>-</cell><cell>55.6</cell></row><row><cell>PSA [2]CVPR2018</cell><cell cols="2">61.7 63.7</cell></row><row><cell>IRN [1]CVPR2019</cell><cell cols="2">63.5 64.8</cell></row><row><cell>SSDDICCV2019</cell><cell cols="2">64.9 65.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of the WSS methods with additional supervision.</figDesc><table><row><cell>Method</cell><cell>Additional supervision</cell><cell>Val Test</cell></row><row><cell>MIL-seg [30]CVPR2015</cell><cell>Saliency mask + Imagenet images</cell><cell>42.0 40.6</cell></row><row><cell>MCNN [39]ICCV2015</cell><cell>Web videos</cell><cell>38.1 39.8</cell></row><row><cell>AFF [32]ECCV2016</cell><cell>Saliency mask</cell><cell>54.3 55.5</cell></row><row><cell>STC [43]PAMI2017</cell><cell>Saliency mask + Web images</cell><cell>49.8 51.2</cell></row><row><cell>Oh et al. [35]CVPR2017</cell><cell>Saliency mask</cell><cell>55.7 56.7</cell></row><row><cell>AE-PSL [42]CVPR2017</cell><cell>Saliency mask</cell><cell>55.0 55.7</cell></row><row><cell>Hong et al. [10]CVPR2017</cell><cell>Web videos</cell><cell>58.1 58.7</cell></row><row><cell>WebS-i2 [16]CVPR2017</cell><cell>Web images</cell><cell>53.4 55.3</cell></row><row><cell>DCSP [4]BMVC2017</cell><cell>Saliency mask</cell><cell>60.8 61.9</cell></row><row><cell>GAIN [22]CVPR2018</cell><cell>Saliency mask</cell><cell>55.3 56.8</cell></row><row><cell>MDC [44]CVPR2018</cell><cell>Saliency mask</cell><cell>60.4 60.8</cell></row><row><cell>MCOF [41]CVPR2018</cell><cell>Saliency mask</cell><cell>60.3 61.2</cell></row><row><cell>DSRG [12]CVPR2018</cell><cell>Saliency mask</cell><cell>61.4 63.2</cell></row><row><cell>Shen et al. [36]CVPR2018</cell><cell>Web images</cell><cell>63.0 63.9</cell></row><row><cell>SeeNet [11]NIPS2018</cell><cell>Saliency mask</cell><cell>63.1 62.8</cell></row><row><cell>AISI [7]ECCV2018</cell><cell>Instance saliency mask</cell><cell>63.6 64.5</cell></row><row><cell>FickleNet [20]CVPR2019</cell><cell>Saliency mask</cell><cell>64.9 65.3</cell></row><row><cell>DSRG+EP. [40]ICCV2019</cell><cell>Saliency mask</cell><cell>61.5 62.7</cell></row><row><cell>AttnBN. [23]ICCV2019</cell><cell cols="2">Saliency mask + Single-label images 62.1 63.0</cell></row><row><cell>Zeng et al. [47]ICCV2019</cell><cell>Saliency mask</cell><cell>63.3 64.3</cell></row><row><cell>OAA+. [14]ICCV2019</cell><cell>Saliency mask</cell><cell>65.2 66.4</cell></row><row><cell>Lee et al. [21]ICCV2019</cell><cell>Web videos</cell><cell>66.5 67.4</cell></row><row><cell>SSDDICCV2019</cell><cell>-</cell><cell>64.9 65.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table A</head><label>A</label><figDesc>-1 shows the hyper parameter values and the mean IoU scores. The hyperparameters (b dd , b class ) are used in Eq.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table A -</head><label>A</label><figDesc>1. Experimental results with different parameters. Detailed comparison with existing works on the PASCAL VOC 2012 val and test sets Table A-2. Results on PASCAL VOC 2012 val set without additional supervision.</figDesc><table><row><cell>A.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>b dd</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell cols="7">mIoU 62.2 63.9 64.6 64.2 64.9 62.7</cell></row><row><cell>b class</cell><cell>0.0</cell><cell>0.5</cell><cell>1.0</cell><cell>1.5</cell><cell>2.0</cell><cell></cell></row><row><cell cols="6">mIoU 64.3 63.0 64.9 64.5 63.7</cell><cell></cell></row><row><cell>α</cell><cell cols="5">1.0 0.75 0.5 0.25 0.0</cell><cell></cell></row><row><cell cols="6">mIoU 63.1 64.4 64.9 64.3 63.2</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>45.1 24.6 40.8 23.0 34.8 61.0 51.9 52.4 15.5 45.9 32.7 54.9 48.6 57.4 51.8 38.2 55.4 32.2 42.6 39.6 44.1 BFBP [34] 79.2 60.1 20.4 50.7 41.2 46.3 62.6 49.2 62.3 13.3 49.7 38.1 58.4 49.0 57.0 48.2 27.8 55.1 29.6 54.6 26.6 46.6 SEC [18] 82.4 62.9 26.4 61.6 27.6 38.1 66.6 62.7 75.2 22.1 53.5 28.3 65.8 57.8 62.3 52.5 32.5 62.6 32.1 45.4 45.3 50.7 CBTS [33] 85.8 65.2 29.4 63.8 31.2 37.2 69.6 64.3 76.2 21.4 56.3 29.8 68.2 60.6 66.2 55.8 30.8 66.1 34.9 48.8 47.1 52.8 TPL [17] 82.8 62.2 23.1 65.8 21.1 43.1 71.1 66.2 76.1 21.3 59.6 35.1 70.2 58.8 62.3 66.1 35.8 69.9 33.4 45.9 45.6 53.1 MEFF [8]</figDesc><table><row><cell></cell><cell>38.2</cell></row><row><cell>DCSM [37]</cell><cell>76.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table A</head><label>A</label><figDesc>-3. Results on PASCAL VOC 2012 test set without additional supervision. 25.5 18.0 25.4 20.2 36.3 46.8 47.1 48.0 15.8 37.9 21.0 44.5 34.5 46.2 40.7 30.4 36.3 22.2 38.8 36.43.8 26.3 49.8 19.5 40.3 61.6 53.9 52.7 13.7 47.3 34.8 50.3 48.9 69.0 49.7 38.4 57.1 34.0 38.0 40.0 45.1 BFBP [34] 80.3 57.5 24.1 66.9 31.7 43.0 67.5 48.6 56.7 12.6 50.9 42.6 59.4 52.9 65.0 44.8 41.3 51.1 33.7 44.4 33.2 48.0 SEC [18] 83.5 56.4 28.5 64.1 23.6 46.5 70.6 58.5 71.3 23.2 54.0 28.0 68.1 62.1 70.0 55.0 38.4 58.0 39.9 38.4 48.3 51.7 CBTS [33] 85.7 58.8 30.5 67.6 24.7 44.7 74.8 61.8 73.7 22.9 57.4 27.5 71.3 64.8 72.4 57.3 37.0 60.4 42.8 42.2 50.6 53.7 TPL [17] 83.4 62.2 26.4 71.8 18.2 49.5 66.5 63.8 73.4 19.0 56.6 35.7 69.3 61.3 71.7 69.2 39.1 66.3 44.8 35.9 45.5 53.8 MEFF [8] 86.6 72.0 30.6 68.0 44.8 46.2 73.4 56.6 73.0 18.9 63.3 32.0 70.1 72.2 68.2 56.1 34.5 67.5 29.6 60.2 43.6 55.6 PSA [2] 89.1 70.6 31.6 77.2 42.2 68.9 79.1 66.5 74.9 29.6 68.7 56.1 82.1 64.8 78.6 73.5 50.8 70.7 47.7 63.9 51.1 63.7 SSDD (ours) 89.5 71.8 31.4 79.3 47.3 64.2 79.9 74.6 84.9 30.8 73.5 58.2 82.7 73.4 76.4 69.9 37.4 80.5 54.5 65.7 50.3 65.5 Table A-4. Results on PASCAL VOC 2012 val set with additional supervision. 50.2 21.6 40.6 34.9 40.5 45.9 51.5 60.6 12.6 51.2 11.6 56.8 52.9 44.8 42.7 31.2 55.4 21.5 38.8 36.9 42.0 MCNN [39] WV 77.5 47.9 17.2 39.4 28.0 25.6 52.7 47.0 57.8 10.4 38.0 24.3 49.9 40.8 48.2 42.0 21.6 35.2 19.6 52.5 24.7 38.68.0 19.5 60.5 42.5 44.8 68.4 64.0 64.8 14.5 52.0 22.8 58.0 55.3 57.8 60.5 40.6 56.7 23.0 57.1 31.2 49.8 71.1 30.5 72.9 41.6 55.9 63.1 60.2 74.0 18.0 66.5 32.4 71.7 56.3 64.8 52.4 37.4 69.1 31.4 58.9 43.9 55.0 Hong et al. [10] WV 87.0 69.3 32.2 70.2 31.2 58.4 73.6 68.5 76.5 26.8 63.8 29.1 73.5 69.5 66.5 70.4 46.8 72.1 27.3 57.4 50.2 58.1 WebS-i2 [16] WI 84.3 65.3 27.4 65.4 53.9 46.3 70.1 69.8 79.4 13.8 61.1 17.4 73.8 58.1 57.8 56.2 35.7 66.5 22.0 50.1 46.2 53.4 DCSP [4] S 88.9 77.7 31.3 73.2 59.8 71.0 79.2 74.5 80.0 15.1 73.3 10.2 76.1 72.2 69.1 72.1 39.9 73.9 14.6 70.3 53.85.6 34.6 75.8 61.9 65.8 67.1 73.3 80.2 15.1 69.9 8.1 75.0 68.4 70.9 71.5 32.6 74.9 24.8 73.2 50.8 60.4 MCOF [41] S 87.0 78.4 29.4 68.0 44.0 67.3 80.3 74.1 82.2 21.1 70.7 28.2 73.2 71.5 67.2 53.0 47.7 74.5 32.4 71.0 45.Shen et al. [36] WI 86.8 71.2 32.4 77.0 24.4 69.8 85.3 71.9 86.5 27.6 78.9 40.7 78.5 79.1 72.7 73.1 49.6 74.8 36.1 48.1 59.2 63.0 SeeNet [11]</figDesc><table><row><cell>methods</cell><cell>bg</cell><cell>aero</cell><cell>bike</cell><cell>bird</cell><cell>boat</cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell><cell>table</cell><cell>dog</cell><cell>horse</cell><cell>motor</cell><cell>person</cell><cell>plant</cell><cell>sheep</cell><cell>sofa</cell><cell>train</cell><cell>tv</cell><cell>mIoU</cell><cell></cell></row><row><cell>MIL-FCN [29]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>25.7</cell><cell></cell></row><row><cell>CCNN [28]</cell><cell cols="22">68.5 9 35.3</cell><cell></cell></row><row><cell>EM-Adapt [27]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>39.6</cell><cell></cell></row><row><cell cols="2">DCSM [37] 78.1 methods info type †</cell><cell>bg</cell><cell>aero</cell><cell>bike</cell><cell>bird</cell><cell>boat</cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell><cell>table</cell><cell>dog</cell><cell>horse</cell><cell>motor</cell><cell>person</cell><cell>plant</cell><cell>sheep</cell><cell>sofa</cell><cell>train</cell><cell>tv</cell><cell>mIoU</cell></row><row><cell>MIL-seg [30]</cell><cell>S</cell><cell cols="22">79.6 1</cell></row><row><cell>AFF [32]</cell><cell>S</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>54.3</cell></row><row><cell cols="3">STC [43] 84.5 Oh et al. [35] S S -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>55.7</cell></row><row><cell>AE-PSL [42]</cell><cell>S</cell><cell cols="22">83.4 1 60.8</cell></row><row><cell>GAIN [22]</cell><cell>S</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>56.8</cell></row><row><cell>MDC [44]</cell><cell>S</cell><cell cols="22">89.5 8 60.3</cell></row><row><cell>DSRG [12]</cell><cell>S</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>61.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/shimoda-uec/ssdd</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by JSPS KAKENHI Grant Number 17J10261, 15H05915, 17H01745, 17H06100 and 19H04929. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of instance segmentation with inter-pixel relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discovering class-specific pixels for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Puneet</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of British Machine Vision Conference</title>
		<meeting>of British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zezhou</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingxiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Alexei</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Associating inter-image salient instances for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Ralph</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-evidence filtering and fusion for multi-label classification, object detection and semantic segmentation based on weakly supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sibei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation using web-crawled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghun</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-erasing network for integral object attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Xinggang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Jiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Jingdong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Integral object mining via online attention accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Kai</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Top-down neural attention by excitation backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Jianming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandt</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Xiaouhui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Webly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">V</forename><surname>Bin Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Ortiz Segovia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Donggeun Yoo, and In So Kweon. Two-phase learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ficklenet: Weakly and semi-supervised semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Frame-to-frame aggregation of active regions in web videos for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tell me where to look: Guided attention inference network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuan-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ernest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention bridging network for knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a dcnn for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Constrained convolutional neural networks for weakly supervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully convolutional multi-class multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">From image-level to pixel-level labeling with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Augmented feedback in semantic segmentation under image level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Combining bottom-up, top-down, and smoothness cues for weakly supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Built-in foreground/background prior for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatemehsadat</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Sadegh Ali</forename><surname>Akbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploiting saliency for object segmentation from image level labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><surname>Oh Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benenson</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoreva</forename><surname>Anna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akata</forename><surname>Zeynep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fritz</forename><surname>Mario</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bootstrapping the performance of webly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Ian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Distinct class saliency maps for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wataru</forename><surname>Shimoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiji</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR WS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation using motion cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Information entropy based feature pooling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weitao</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiansheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youze</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Weaklysupervised semantic segmentation by iteratively mining common object features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">STC: A simple to complex framework for weaklysupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Trans. on PAMI</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semisupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Joint learning of saliency detection and weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhi</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
