<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Shape Robust Text Detection with Progressive Scale Expansion Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Key Lab for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Momenta 5 Megvii (Face++) Technology Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Comuter Science and Technology</orgName>
								<orgName type="institution">Tongji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Momenta 5 Megvii (Face++) Technology Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Key Lab for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
							<email>lutong@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Key Lab for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
							<email>shaoshuai@megvii.com</email>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security</orgName>
								<orgName type="institution">Nanjing University of Sci-ence and Technology</orgName>
								<address>
									<postCode>210094</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Shape Robust Text Detection with Progressive Scale Expansion Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scene text detection has witnessed rapid progress especially with the recent development of convolutional neural networks. However, there still exists two challenges which prevent the algorithm into industry applications. On the one hand, most of the state-of-art algorithms require quadrangle bounding box which is in-accurate to locate the texts with arbitrary shape.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene text detection in the wild is a fundamental problem with numerous applications such as scene understanding, product identification, and autonomous driving. Many progress has been made in recent years with the rapid development of Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b30">31]</ref>. We can roughly divide the existing CNN based algorithm into two categories: regression-based approaches and segmentation-based approaches.</p><p>For the regression-based approaches <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27]</ref>, the text targets are usually represented in the forms of rectangles or quadrangles with certain orientations. However, the regression-based approaches fail to deal with the text instance with arbitrary shapes, e.g., the curve texts as shown in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>. Segmentation-based approaches, on the other hand, locate the text instance based on pixel-level classification. However, it is difficult to separate the text instances which are close with each other. Usually, a false detection which covers all the text instances close to each other may be predicted based on the segmentation-based approach. One example is shown in <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>.</p><p>To address these problems, in this paper, we propose a novel kernel-based framework, namely, Progressive Scale Expansion Network (PSENet). Our PSENet has the following two benefits. First, as a segmentation-based method, PSENet performs pixel-level segmentation, which is able to precisely locate the text instance with arbitrary shape. Second, we propose a progressive scale expansion algorithm, with which the adjacent text instances can be successfully identified as shown in <ref type="figure" target="#fig_0">Fig. 1 (d)</ref>. More specifically, we assign each text instance with multiple predicted segmentation areas, which are denoted as "kernels" for simplicity. Each kernel has the similar shape to the original text instance but different scales. To obtain the final detections, we adopt a progressive scale expansion algorithm based on Breadth-First-Search (BFS). Generally, there are 3 steps: 1) starting from the kernels with minimal scales (instances can be distinguished in this step); 2) expanding their areas by involving more pixels in larger kernels gradually; 3) finishing until the complete text instances (the largest kernels) are explored.</p><p>There are three potential reasons for the design of the progressive scale expansion algorithm. First, the kernels with minimal scales are quite easy to be separated as their boundaries are far away from each other. Second, the minimal scale kernels can not cover the complete areas of text instances (see <ref type="figure" target="#fig_1">Fig. 2</ref> (b)). Therefore, it is necessary to recover the complete text instances from the minimal scale kernels. Third, the progressive scale expansion algorithm is a simple and efficient method to expand the small kernels to complete text instances, which ensures the accurate locations of text instances.</p><p>To show the effectiveness of our proposed PSENet, we conduct extensive experiments on four competitive benchmark datasets including ICDAR 2015 <ref type="bibr" target="#b16">[17]</ref>, ICDAR 2017 MLT <ref type="bibr" target="#b0">[1]</ref> ,CTW1500 <ref type="bibr" target="#b23">[24]</ref> and Total-Text <ref type="bibr" target="#b1">[2]</ref>. Among these datasets, CTW1500 and Total-Text are explicitly designed for curve text detection. Specifically, on CTW1500, a dataset with long curve texts, we outperform state-of-the-art results by absolute 6.6%, and our real-time model achieves a comparable performance (74.3%) at 27 FPS. Furthermore, the proposed PSENet also achieves promising performance . Visualization of complete text instance and kernel of text instance. It can be seen that CRNN <ref type="bibr" target="#b32">[33]</ref> recognizes complete text instance correctly but fail to recognize the kernel, because the kernel can not cover the complete areas of text instances. on multi-oriented and multi-lingual text datasets: ICDAR 2015 and ICDAR 2017 MLT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Scene text detection based on deep learning methods have achieved remarkable results over the past few years. A major of modern text detectors are based on CNN framework, in which scene text detection is roughly formulated as two categories: regression-based methods and segmentation-based methods.</p><p>Regression-based methods often based on general object detection frameworks, such Faster R-CNN <ref type="bibr" target="#b30">[31]</ref> and SSD <ref type="bibr" target="#b21">[22]</ref>. TextBoxes <ref type="bibr" target="#b18">[19]</ref> modified the anchor scales and shape of convolution kernels to adjust to the various aspect ratios of the text. EAST <ref type="bibr" target="#b41">[42]</ref> use FCN <ref type="bibr" target="#b24">[25]</ref> to directly predict score map, rotation angle and text boxes for each pixel. RRPN <ref type="bibr" target="#b27">[28]</ref> adopted Faster R-CNN and developed rotation proposals of RPN part to detect arbitrary oriented text. RRD <ref type="bibr" target="#b19">[20]</ref> extracted feature maps for text classification and regression from two separately branches to better long text detection.</p><p>However, most of the regression-based methods often require complex anchor design and cumbersome multiple stages, which might require exhaustive tuning and lead to sub-optimal performance. Moreover, the above works were specially designed for multiple oriented text detection and may fall short when handling curve texts, which are actually widely distributed in real-world scenarios.</p><p>Segmentation-based methods are mainly inspired by fully convolutional networks(FCN) <ref type="bibr" target="#b24">[25]</ref>. Zhang et al . <ref type="bibr" target="#b38">[39]</ref> first adopted FCN to extract text blocks and detect character candidates from those text blocks via MSER. Yao et al . <ref type="bibr" target="#b37">[38]</ref> formulated one text region as various properties, such as text region and orientation, then utilized FCN to predict the corresponding heatmaps. Lyu et al . <ref type="bibr" target="#b26">[27]</ref> utilized corner localization to find suitable irregular quadrangles for text instances. PixelLink <ref type="bibr" target="#b3">[4]</ref> separated texts lying close to each other by predicting pixel connections between different text instances. Recently, TextSnake <ref type="bibr" target="#b25">[26]</ref> used ordered disks to represent curve text for curve text detection. SPC-Net [?] used instance segmentation framework and utilize context information to detect text of arbitrary shape while suppressing false positives.</p><p>The above works have achieved excellent performances over several horizontal and multi-oriented text benchmarks. Similarly, most of the above approaches have not paid special attention to curve text, except for TextSnake <ref type="bibr" target="#b25">[26]</ref>. However, TextSnake still needs time-consuming and complicated post-processing steps (Centralizing, Striding and Sliding) during inference, while our proposed Progressive Scale Expansion needs only one clean and efficient step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section, we first introduce the overall pipeline of the proposed Progressive Scale Expansion Network (PSENet). Next, we present the details of progressive scale expansion algorithm, and show how it can effectively distinguish the text instances lying closely. At last, we introduce the way of generating label and the design of loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Pipeline</head><p>A high-level overview of our proposed PSENet is illustrated in <ref type="figure">Fig. 3</ref>. We use ResNet <ref type="bibr" target="#b9">[10]</ref> as the backbone of PSENet. We concatenate low-level texture feature with high-level semantic feature. These maps are further fused in F to encode information with various receptive views. Intuitively, such fusion is very likely to facilitate the generations of the kernels with various scales. Then the feature map F is projected into n branches to produce multiple segmentation results S 1 , S 2 , ..., S n . Each S i would be one segmentation mask for all the text instances at a certain scale. The scales of different segmentation mask are decided by the hyper-parameters which will be discussed in Sec. 3.4. Among these masks, S 1 gives the segmentation result for the text instances with smallest scales (i.e., the minimal kernels) and S n denotes for the original segmentation mask (i.e., the maximal kernels). After obtaining these segmentation masks, we use progressive scale expansion algorithm to gradually expand all the instances' kernels in S 1 , to their complete shapes in S n , and obtain the final detection results as R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Design</head><p>The basic framework of PSENet is implemented from FPN <ref type="bibr" target="#b20">[21]</ref>. We firstly get four 256 channels feature maps (i.e. P 2 , P 3 , P 4 , P 5 ) from the backbone. To further combine the semantic features from low to high levels, we fuse the four feature maps to get feature map F with 1024 channels via the function C(·) as:</p><formula xml:id="formula_0">F = C(P 2 , P 3 , P 4 , P 5 ) = P 2 Up ×2 (P 3 ) Up ×4 (P 4 ) Up ×8 (P 5 ),<label>(1)</label></formula><p>where " " refers to the concatenation and Up ×2 (·), Up ×4 (·), Up ×8 (·) refer to 2, 4, 8 times upsampling, respectively. Subsequently, F is fed into Conv(3, 3)-BN-ReLU layers and is reduced to 256 channels. Next, it passes through n Conv(1, 1)-Up-Sigmoid layers and produces n segmentation results S 1 , S 2 , ..., S n . Here, Conv, BN, ReLU and Up refer to convolution <ref type="bibr" target="#b17">[18]</ref>, batch normalization <ref type="bibr" target="#b14">[15]</ref>, rectified linear units <ref type="bibr" target="#b5">[6]</ref> and upsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Progressive Scale Expansion Algorithm</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>, it is hard for the segmentationbased method to separate the text instances that are close to each other. To solve this problem, we propose a progressive scale expansion algorithm.</p><p>Here is a vivid example (see <ref type="figure" target="#fig_3">Fig. 4</ref>) to explain the procedure of progressive scale expansion algorithm, whose central idea is brought from the Breadth-First-Search (BFS) algorithm. In the example, we have 3 segmentation results S = {S 1 , S 2 , S 3 } (see <ref type="figure" target="#fig_3">Fig. 4</ref> (a), (e), (f)). At first, based on the minimal kernels' map S 1 (see <ref type="figure" target="#fig_3">Fig. 4</ref> (a)), 4 distinct connected components C = {c 1 , c 2 , c 3 , c 4 } can be found as initializations. The regions with different colors in <ref type="figure" target="#fig_3">Fig. 4</ref> (b) represent these different connected components, respectively. By now we have all the text instances' central parts (i.e., the minimal kernels) detected. Then, we progressively expand the detected kernels by merging the pixels in S 2 , and then in S 3 . The results of the two scale expansions are shown in <ref type="figure" target="#fig_3">Fig. 4</ref> (c) and <ref type="figure" target="#fig_3">Fig. 4 (d)</ref>, respectively. Finally, we extract the connected components which are marked with different colors in <ref type="figure" target="#fig_3">Fig. 4 (d)</ref> as the final predictions for text instances.</p><formula xml:id="formula_1">(e) 2 (f) 3 (g) Scale Expansion (a) 1 (b) (c) (d) CC EX EX</formula><p>The procedure of scale expansion is illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref> (g). The expansion is based on Breadth-First-Search algorithm which starts from the pixels of multiple kernels and iteratively merges the adjacent text pixels. Note that there may be conflicted pixels during expansion, as shown in the red box in <ref type="figure" target="#fig_3">Fig. 4 (g)</ref>. The principle to deal with the conflict in our practice is that the confusing pixel can only be merged by one single kernel on a first-come-firstserved basis. Thanks to the "progressive" expansion procedure, these boundary conflicts will not affect the final detections and the performances. The detail of scale expansion algorithm is summarized in Algorithm 1. In the pseudocode, T, P are the intermediate results. Q is a queue. Neighbor(·) represents the neighbor pixels (4-ways) of p. GroupByLabel(·) is the function of grouping the intermediate result by label. "S i [q] = True" means that the predicted value of pixel q in S i belongs to the text part. C and E are </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Label Generation</head><p>As illustrated in <ref type="figure">Fig. 3</ref>, PSENet produces segmentation results (e.g. S 1 , S 2 , ..., S n ) with different kernel scales. Therefore, it requires the corresponding ground truths with different kernel scales during training. In our practice, these ground truth labels can be conducted simply and effectively by shrinking the original text instance. The polygon with blue border in <ref type="figure" target="#fig_5">Fig. 5</ref> (b) denotes the original text instance and it corresponds to the largest segmentation label mask (see the rightmost map in <ref type="figure" target="#fig_5">Fig. 5 (c)</ref>). To obtain the shrunk masks sequentially in <ref type="figure" target="#fig_5">Fig. 5 (c)</ref>, we utilize the Vatti clipping algorithm <ref type="bibr" target="#b36">[37]</ref> to shrink the original polygon p n by d i pixels and get shrunk polygon p i (see <ref type="figure" target="#fig_5">Fig. 5 (a)</ref>). Subsequently, each shrunk polygon p i is transferred into a 0/1 binary mask for segmentation label ground truth. We denote these ground truth maps as G 1 , G 2 , ..., G n respectively. Mathematically, if we consider the scale ratio as r i , the margin d i between p n and p i can be calculated as:</p><formula xml:id="formula_2">d i = Area(p n ) × (1 − r 2 i ) Perimeter(p n ) ,<label>(2)</label></formula><p>where Area(·) is the function of computing the polygon area, Perimeter(·) is the function of computing the polygon perimeter. Further, we define the scale ratio r i for ground truth map G i as:</p><formula xml:id="formula_3">r i = 1 − (1 − m) × (n − i) n − 1 ,<label>(3)</label></formula><p>where m is the minimal scale ratio, which is a value in (0, 1]. Based on the definition in Eqn. <ref type="formula" target="#formula_3">(3)</ref>, the values of scale ratios (i.e., r 1 , r 2 , ..., r n ) are decided by two hyperparameters n and m, and they increase linearly from m to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss Function</head><p>For learning PSENet, the loss function can be formulated as:</p><formula xml:id="formula_4">L = λL c + (1 − λ)L s ,<label>(4)</label></formula><p>where L c and L s represent the losses for the complete text instances and the shrunk ones respectively, and λ balances the importance between L c and L s . It is common that the text instances usually occupy only an extremely small region in natural images, which makes the predictions of network bias to the non-text region, when binary cross entropy <ref type="bibr" target="#b2">[3]</ref> is used. Inspired by <ref type="bibr" target="#b28">[29]</ref>, we adopt dice coefficient in our experiment. The dice coefficient D(S i , G i ) is formulated as in Eqn. <ref type="bibr" target="#b4">(5)</ref>:</p><formula xml:id="formula_5">D(S i , G i ) = 2 x,y (S i,x,y × G i,x,y ) x,y S 2 i,x,y + x,y G 2 i,x,y ,<label>(5)</label></formula><p>where S i,x,y and G i,x,y refer to the value of pixel (x, y) in segmentation result S i and ground truth G i , respectively. Furthermore, there are many patterns similar to text strokes, such as fences, lattices, etc. Therefore, we adopt Online Hard Example Mining (OHEM) <ref type="bibr" target="#b33">[34]</ref> to L c during training to better distinguish these patterns.</p><p>L c focuses on segmenting the text and non-text region. Let us consider the training mask given by OHEM as M , and thus L c can be formulated as Eqn. <ref type="bibr" target="#b5">(6)</ref>.</p><formula xml:id="formula_6">L c = 1 − D(S n · M, G n · M ),<label>(6)</label></formula><p>L s is the loss for shrunk text instances. Since they are encircled by the original areas of the complete text instances, we ignore the pixels of the non-text region in the segmentation result S n to avoid a certain redundancy. Therefore, L s can be formulated as follows:</p><formula xml:id="formula_7">L s = 1 − n−1 i=1 D(S i · W, G i · W ) n − 1 , W x,y = 1, if S n,x,y ≥ 0.5; 0, otherwise.<label>(7)</label></formula><p>Here, W is a mask which ignores the pixels of the nontext region in S n , and S n,x,y refers to the value of pixel (x, y) in S n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>In this section, we first briefly introduce datasets and present the details of implementation. Then, we conduct ablation studies for PSENet. At last, we evaluate the proposed PSENet on four recent challenging public benchmarks: CTW1500, Total-Text, ICDAR 2015 and ICDAR 2017 MLT, and compare PSENet with state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>CTW1500 <ref type="bibr" target="#b23">[24]</ref> is a challenging dataset for long curve text detection, which is constructed by Yuliang et al. <ref type="bibr" target="#b23">[24]</ref>. It consists of 1000 training images and 500 testing images. Different from traditional text datasets (e.g. ICDAR 2015, ICDAR 2017 MLT), the text instances in CTW1500 are labelled by a polygon with 14 points which can describe the shape of an arbitrarily curve text.</p><p>Total-Text [2] is a newly-released dataset for curve text detection. Horizontal, multi-Oriented and curve text instances are contained in Total-Text. The benchmark consists of 1255 training images and 300 testing images.</p><p>ICDAR 2015 (IC15) <ref type="bibr" target="#b16">[17]</ref> is a commonly used dataset for text detection. It contains a total of 1500 pictures, 1000 of which are used for training and the remaining are for testing. The text regions are annotated by 4 vertices of the quadrangle.</p><p>ICDAR 2017 MLT (IC17-MLT) <ref type="bibr" target="#b0">[1]</ref> is a large scale multi-lingual text dataset, which includes 7200 training images, 1800 validation images and 9000 testing images. The dataset is composed of complete scene images which come from 9 languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We use the ResNet <ref type="bibr" target="#b9">[10]</ref> pre-trained on ImageNet <ref type="bibr" target="#b4">[5]</ref> as our backbone. All the networks are optimized by using stochastic gradient descent (SGD). We use 7200 IC17-MLT training images and 1800 IC17-MLT validation images to train the model and report the result on IC17-MLT. Note that no extra data, e.g. SynthText <ref type="bibr" target="#b6">[7]</ref>, is adopted to train IC17-MLT. We train PSENet on IC17-MLT with batch size 16 on 4 GPUs for 180K iterations. The initial learning rate is set to 1 × 10 −3 , and is divided by 10 at 60K and 120K iterations.</p><p>Two training strategies are adopted in the rest of all datasets:(1) Training from scratch. (2) Fine-tuning on IC17-MLT model. When training from scratch, we train PSENet with batch size 16 on 4 GPUs for 36K iterations, and the initial learning rate is set to 1 × 10 −3 and is divided by 10 at 12K and 24K iterations. When fine-tuning on IC17-MLT model, the number of iterations is 24K, and the initial learning rate is 1×10 −4 which is divided by 10 at 12K iterations.</p><p>We use a weight decay of 5 × 10 −4 and a Nesterov momentum <ref type="bibr" target="#b34">[35]</ref> of 0.99 without dampening. We adopt the weight initialization introduced by <ref type="bibr" target="#b7">[8]</ref>.</p><p>During training, we ignore the blurred text regions labeled as DO NOT CARE in all datasets. The λ of loss balance is set to 0.7. The negative-positive ratio of OHEM is set to 3. The data augmentation for training data is listed as follows: 1) the images are rescaled with ratio {0.5, 1.0, 2.0, 3.0} randomly; 2) the images are horizontally flipped and rotated in the range [−10 • , 10 • ] randomly; 3) 640 × 640 random samples are cropped from the transformed images. For quadrangular text, we calculate the minimal area rectangle to extract the bounding boxes. For curve text dataset, the output of PSE is applied to produce the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Can kernels be used as the final result? The aim of kernels is to roughly locate the text instance and separate the text instances standing closely to each other. However, the minimal scale kernels can not cover the complete areas of text instances, which does harm to the text detection and recognition. In <ref type="figure">Fig. 6 (a)</ref>, the F-measures of the detector used minimal scale kernel only (the dash curves) is terrible on ICDAR 2015 and CTW1500 datasets. In addition, we use a modern text recognizer CRNN <ref type="bibr" target="#b32">[33]</ref> to recognize the text in complete text instance and kernel, and find that CRNN failed to recognize the text in the kernel (see <ref type="figure" target="#fig_1">Fig. 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Thus, the kernel can not be used as the final detection result.</head><p>Influence of the minimal kernel scale. We study the effect of the minimal scale m by setting the number of kernels n to 2 and let the minimal scale m vary from 1 to 0.1. The models are evaluated on ICDAR 2015 and CTW1500 two datasets. We can find from <ref type="figure">Fig. 6 (a)</ref> that the F-measures on the test sets drops when m is too large or too small. Note that when setting kernel scale 1, we only use text segmentation map as the final result and without progressive scale expansion algorithm. Obviously, without PSE the baseline's performance is unsatisfactory because the network fails to separate the text lying closely to each other. When m is too large, it is hard for PSENet to separate the text instances lying closely to each other. When m is too small, PSENet often splits a whole text line into different parts incorrectly and the training can not converge very well.</p><p>Influence of the kernel numbers. We investigate the effect of the number of kernels n on the performance of PSENet. Specifically, we hold the minimal scale m constant and train PSENet with different number of kernels n. In details, we set m start from 0.4 for ICDAR 2015 and 0.6 for CTW1500 and let n increase from 2 to 10. The models are evaluated on ICDAR 2015 and CTW1500 datasets. <ref type="figure">Fig. 6 (b)</ref> shows the experimental results, from which we can find that with the growing of n, the F-measure on the test set keeps rising and begins to level off when n ≥ 5. The advantage of multiple kernels is that it can accurate reconstruct two text instances with large gaps of size where they lying closely to each other.</p><p>Influence of the backbone. Deeper neural networks have been proven to boost the performance of large scale image classification and object detection. To better analyze the capability of proposed PSENet, we adopt ResNet as our backbone with three different depths of {50, 101, 152} and test on the large scale dataset IC17-MLT. As shown in Table 1, under the same setting, improving the depth of backbone from 50 to 152 can clearly improve the performance from 70.8% to 72.2%, with 1.4% absolute improvement.  <ref type="figure">Figure 6</ref>. Ablation study on minimal kernel scale (m) and kernel number (n) (Eqn. <ref type="formula" target="#formula_3">(3)</ref>). There results are based on PSENet-1s (Resnet 50) trained from scratch. "1s" means the shape of output map is 1/1 of the input image.  <ref type="table">Table 2</ref>. The single-scale results on CTW1500. "P", "R" and "F" represent the precision, recall and F-measure respectively. "1s" and "4s" means the width and height of output map is 1/1 and 1/4 of the input test image. * indicates the results from <ref type="bibr" target="#b23">[24]</ref>. "Ext" indicates external data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparisons with State-of-the-Art Methods</head><p>Detecting Curve Text. To test the ability of curve text detection, we evaluate our method on CTW1500 and Total-Text, which mainly contains the curve texts. In the test stage, we scale the longer side of images to 1280 and evaluate the results using the same evaluation method with <ref type="bibr" target="#b23">[24]</ref>. We report the single-scale performance of PSENet on CTW1500 and Total-Text in <ref type="table">Table 2 and Table 3</ref>, respectively. Note that we only use ResNet50 as the backbone.</p><p>On CTW1500, PSENet surpasses all the counterparts even without external data. Notably, we can find that the Fmeasure (82.2%) achieved by PSENet is 8.8% higher than CTD+TLOC and 6.6% higher than TextSnake on the Fmeasure. To our best knowledge, this is the best reported result in literature.</p><p>On Total-Text, the proposed PSENet achieves 84.02%, 77.96% and 80.87% in the precision, recall and F-measure, outperforming state-of-the-art methods over 2.47%. Note that our PSENet extremely surpasses the baseline on Total-Text by more than 40% in the F-measure.</p><p>The performance on CTW1500 and Total-Text demonstrates the solid superiority of PSENet when handling curve texts or the texts with arbitrary shapes. We also illustrate several challenging results and make some visual comparisons to the state-of-the-art CTD+TLOC <ref type="bibr" target="#b23">[24]</ref> in <ref type="figure" target="#fig_7">Fig. 7 (d)</ref>.</p><p>The comparisons clearly demonstrate that PSENet can elegantly distinguish very complex curve text instances and separate them in a compelling manner.</p><p>Detecting Oriented Text. We evaluate the proposed PSENet on the IC15 to test its ability for oriented text detection. Only ResNet50 is adopted as the backbone of PSENet. During inference, we scale the long side of input images to  <ref type="table">Table 3</ref>. The single-scale results on Total-Text. "P", "R" and "F" represent the precision, recall and F-measure respectively. "1s" and "4s" means the width and height of output map is 1/1 and 1/4 of the input test image. "Ext" indicates external data. Note that EAST and SegLink were not fine-tuned on Total-Text. Therefore their results are included only for reference.  <ref type="table">Table 4</ref>. The single-scale results on IC15. "P", "R" and "F" represent the precision, recall and F-measure respectively. "1s" and "4s" means the width and height of output map is 1/1 and 1/4 of the input test image. "Ext" indicates external data.</p><p>2240. We compare our method with other state-of-the-art methods in <ref type="table">Table 4</ref>. With only single scale setting, our method achieves a F-measure of 85.69%, surpassing the state of the art results by more than 3%. In addition, we demonstrate some test examples in <ref type="figure" target="#fig_7">Fig. 7 (a)</ref>, and PSENet can accurately locate the text instances with various orientations.</p><p>Detecting MultiLingual Text. To test the robustness of PSENet to multiple languages, we evaluate PSENet on IC17-MLT benchmark. Due to the large scale of the dataset, in order to fully exploit the potential of the PSENet, we adopt Res50 and Res152 as the backbone. We enlarge the original image by 2 times, the proposed PSENet achieve a F-measure of 72.13%, outperforming state of the art methods by absolute 5.3%. In addition, we demonstrate some test examples in <ref type="figure" target="#fig_7">Fig. 7 (b)</ref>, and PSENet can accurately locate the text instances with multiple languages. This proves that PSENet is robust for multi-lingual and multi-oriented detection and can indeed be deployed in complex natural scenarios. The result is shown in <ref type="table">Table 5</ref>.</p><p>Note that, We use the high resolution to test IC15 and IC17-MLT because there are so many small texts in these two datasets.   <ref type="table">Table 5</ref>. The single-scale results on IC17-MLT. "P", "R" and "F" represent the precision, recall and F-measure respectively. "Ext" indicates external data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Speed Analyze</head><p>As shown in <ref type="table" target="#tab_6">Table 6</ref>, PSENet can fast detect curve text instance. ResNet50 and ResNet18 are adopted as the backbone to trade off the speed and accuracy. We specially analyze the time consumption of PSENet in different stages. When the output feature map is 1/1 of the input image, PSENet obtains the best performance, while the time consumption of PSE is more than half of the total inference time because of the larger feature map. If the size of output feature map is 1/4 of the input images, the FPS of PSENet can be boosted from 3.9 to 8.4, while the performance slightly decrease from 82.2% to 79.9%, which is shown in <ref type="table">Table 2</ref>. We can see the time consumption of PSE is less than 1/10 of total inference time. Furthermore, when we scale the long edge of 640, the FPS is further pushed to 22 and the detector still has good performance (75.6%).</p><p>When we use ResNet 18 as the backbone, the speed of PSENet is nearly real-time <ref type="bibr">(27 FPS)</ref>, while the performance is still competitive. Note that the PSENet(ResNet18) does not use external data to pretrain. Combined with <ref type="table">Table 2</ref>  input to test the speed. All results in <ref type="table" target="#tab_6">Table 6</ref> are tested by PyTorch <ref type="bibr" target="#b29">[30]</ref> and one 1080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>We propose a novel Progressive Scale Expansion Network (PSENet) to successfully detect the text instances with arbitrary shapes in the natural scene images. By gradually expanding the detected areas from small kernels to large and complete instances via multiple semantic segmentation maps, our method is robust to shapes and can easily separate those text instances which are very close or even partially intersected. The experiments on scene text detection benchmarks demonstrate the superior performance of the proposed method.</p><p>There are multiple directions to explore in the future. Firstly, we will investigate whether the expansion algorithm can be trained along with the network end-to-end. Secondly, the progressive scale expansion algorithm can be introduced to the general instance-level segmentation tasks, especially in those benchmarks with many crowded object instances. We are cleaning our codes and will release them soon. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Supplementary Materials for Shape Robust</head><p>Text Detection with Progressive Scale Expansion Network</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">The Advantage of Multiple Kernels</head><p>Multiple kernels is used to reconstruct the closely text instances smoothly when their sizes have large gap. As shown in <ref type="figure" target="#fig_9">Fig. 8</ref>, there are some flaws in 2-kerenl reconstruction, and this problem is alleviated when the number of kernels increasing (see 3-kernel reconstruction). In addition, the time complexity of progressive scale expansion algorithm (PSE) is O(W × H) 1 , where W × H is the size of output. Thus, the increasement of kernel number have no influence on the time cost of PSE. Consequently, it is a good manner to use multiple kernels to reconstruct the text instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Applying PSENet on Other Semantic Segmentation Framework</head><p>The proposed PSENet consists of two key points: kernel mechanism and PSE. Both of them are easy to be applied on other semantic segmentation frameworks. Here, we implement PSENet-like method based on a widely used semantic segmentation framework PSPNet <ref type="bibr" target="#b39">[40]</ref>, and evaluate it on CTW1500. We detailly compare the PSENet-like method based on PSPNet with the original PSENet in <ref type="table" target="#tab_8">Table 7</ref>. We can find the method based on PSPNet can also achieve competitive performance on the curve text dataset. However, compared with the original PSENet, the PSPNetbased method need more GPU memory (3.7G vs 2.9G) and have lower forward speed (289ms vs 118ms), which indicates that the original PSENet is more suitable to text detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">More Comparisons on CTW1500</head><p>To demonstrate the power of PSENet on complex curve text detection, we implement the state-of-the-art and open source 2 method CTD-TLOC, and make detail comparisons between PSENet and CTD-TLOC on CTW1500. The comparisons are shown in <ref type="figure" target="#fig_0">Fig. 9, 10</ref>. It is interesting and amazing to find that in <ref type="figure">Fig. 9</ref>, our proposed PSENet is able to locate several text instances where the groundtruth labels are even unmarked. This highly proves that our method is quite robust due to its strong learning representation and distinguishing ability. <ref type="figure" target="#fig_0">Fig. 10</ref> demonstrate more examples where PSENet can not only detect the curve text instances <ref type="bibr" target="#b0">1</ref> To reduce time complexity, we set the end pixels of the ith expansion as the start pixels of the (i + 1)th expansion.    <ref type="figure" target="#fig_0">Fig. 11</ref> (Total Text), <ref type="figure" target="#fig_0">Fig. 12 (ICDAR 2015)</ref> and <ref type="figure" target="#fig_0">Fig. 13</ref> (ICDAR 2017 MLT). From these results, it can be easily observed that with the proposed kernel-based framework and PSE, our method is able to archive the following points: 1) locating the arbitrary-shaped text instances precisely; 2) separating the closely adjacent text instances well; 3) detecting the text instances with various orientations; 4) detecting the multi-Lingual text. Meanwhile, thanks to the strong feature representation, PSENet can as well locate the text instances with complex and unstable illumination, different colors and variable scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CTD+TLOC Original Image</head><p>Ground Truth PSENet (ours) <ref type="figure">Figure 9</ref>. Comparisons on CTW1500. The proposed PSENet produces several detections that are even missed by the groundtruth labels.</p><p>PSENet (ours) Original Image CTD+TLOC Ground Truth <ref type="figure" target="#fig_0">Figure 10</ref>. Comparisons on CTW1500.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The results of different methods, best viewed in color. (a) is the original image. (b) refers to the result of regressionbased method, which displays disappointing detections as the red box covers nearly more than half of the context in the green box. (c) is the result of naive semantic segmentation, which mistakes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2. Visualization of complete text instance and kernel of text instance. It can be seen that CRNN [33] recognizes complete text instance correctly but fail to recognize the kernel, because the kernel can not cover the complete areas of text instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 …Figure 3 .</head><label>13</label><figDesc>Illustration of our overall pipeline. The left part of pipeline is implemented from FPN<ref type="bibr" target="#b20">[21]</ref>. The right part denotes the feature fusion and the progressive scale expansion algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The procedure of progressive scale expansion algorithm. CC refers to the function of finding connected components. EX represents the scale expansion algorithm. (a), (e) and (f) refer to S1, S2 and S3, respectively. (b) is the initial connected components. (c) and (d) is the results of expansion. (g) is the illustration of expansion. The blue and orange areas represent the kernels of different text instances. The gray girds represent the pixels need to be involved. The red box in (g) refers to the conflicted pixel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1</head><label>1</label><figDesc>Scale Expansion Algorithm Require: Kernels: C, Segmentation Result: Si Ensure: Scale Expanded Kernels: E 1: function EXPANSION(C, Si) 2: T ← ∅; P ← ∅; Q ← ∅ 3: for each ci ∈ C do 4: T ← T ∪ {(p, label) | (p, label) ∈ ci} 5: P ← P ∪ {p | (p, label) ∈ ci} 6: Enqueue(Q, ci) // push all the elements in ci into Q label) ← Dequeue(Q) // pop the first element of Q 10: if ∃q ∈ Neighbor(p) and q / ∈ P and Si[q] = True then 11: T ← T ∪ {(q, label)}; P ← P ∪ {q}12:Enqueue(Q, (q, label)) // push the element (q, label) into Q 13: end function used to keep the kernels before and after expansion respectively;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>The illustration of label generation. (a) contains the annotations for d, pi and pn. (b) shows the original text instances. (c) shows the segmentation masks with different kernel scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Detection results on three benchmarks and several representative comparisons of curve texts on CTW1500. More examples are provided in the supplementary materials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>This work is supported by the Natural Science Foundation of China under Grant 61672273 and Grant 61832008, the Science Foundation for Distinguished Young Scholars of Jiangsu under Grant BK20160021, and Scientific Foun-dation of State Grid Corporation of China (Research on Icewind Disaster Feature Recognition and Prediction by Fewshot Machine Learning in Transmission Lines).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>2 https://github.com/Yuliang-Liu/Curve-Text-Detector The difference of 2-kernel reconstruction and 3-kernel reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>Test examples on Total Text produced by PSENet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 .</head><label>12</label><figDesc>Test examples on ICDAR 2015 produced by PSENet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 .</head><label>13</label><figDesc>Test examples on ICDAR 2017 MLT produced by PSENet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Xiang Li is with PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, 210094, China. Xiang Li is also a visiting scholar in Momenta.</figDesc><table /><note>* Authors contributed equally.†‡ Corresponding author.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Performance grows with deeper backbones on IC17-MLT. "P", "R" and "F" represent the precision, recall and F-measure respectively.</figDesc><table><row><cell cols="2">Methods</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell></cell></row><row><cell cols="2">PSENet (ResNet50)</cell><cell>73.7</cell><cell>68.2</cell><cell>70.8</cell><cell></cell></row><row><cell cols="2">PSENet (ResNet101)</cell><cell>74.8</cell><cell>68.9</cell><cell>71.7</cell><cell></cell></row><row><cell cols="2">PSENet (ResNet152)</cell><cell>75.3</cell><cell>69.2</cell><cell>72.2</cell><cell></cell></row><row><cell>Method</cell><cell>Ext</cell><cell>P</cell><cell cols="2">CTW1500 R F</cell><cell>FPS</cell></row><row><cell>CTPN [36]</cell><cell>-</cell><cell>60.4*</cell><cell>53.8*</cell><cell>56.9*</cell><cell>7.14</cell></row><row><cell>SegLink [32]</cell><cell>-</cell><cell>42.3*</cell><cell>40.0*</cell><cell>40.8*</cell><cell>10.7</cell></row><row><cell>EAST [42]</cell><cell>-</cell><cell>78.7*</cell><cell>49.1*</cell><cell>60.4*</cell><cell>21.2</cell></row><row><cell>CTD+TLOC [24]</cell><cell>-</cell><cell>77.4</cell><cell>69.8</cell><cell>73.4</cell><cell>13.3</cell></row><row><cell>TextSnake [26]</cell><cell></cell><cell>67.9</cell><cell>85.3</cell><cell>75.6</cell><cell>-</cell></row><row><cell>PSENet-1s</cell><cell>-</cell><cell>80.6</cell><cell>75.6</cell><cell>78.0</cell><cell>3.9</cell></row><row><cell>PSENet-1s</cell><cell></cell><cell>84.8</cell><cell>79.7</cell><cell>82.2</cell><cell>3.9</cell></row><row><cell>PSENet-4s</cell><cell></cell><cell>82.1</cell><cell>77.8</cell><cell>79.9</cell><cell>8.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>, we can find PSENet surpasses EAST and CTD+TLOC in both speed and performance.All of the above experiments are tested on CTW1500 test set. We evaluate all test images and calculate the average speed. We scale the long edge of {1280, 960, 640} as</figDesc><table><row><cell>Method</cell><cell>Res</cell><cell>F</cell><cell cols="2">Time consumption backbone(ms) head(ms)</cell><cell>PSE(ms)</cell><cell>FPS</cell></row><row><cell>PSENet-1s (ResNet50)</cell><cell>1280</cell><cell>82.2</cell><cell>50</cell><cell>68</cell><cell>145</cell><cell>3.9</cell></row><row><cell>PSENet-4s (ResNet50)</cell><cell>1280</cell><cell>79.9</cell><cell>50</cell><cell>60</cell><cell>10</cell><cell>8.4</cell></row><row><cell>PSENet-4s (ResNet50)</cell><cell>960</cell><cell>78.3</cell><cell>33</cell><cell>35</cell><cell>9</cell><cell>13</cell></row><row><cell>PSENet-4s (ResNet50)</cell><cell>640</cell><cell>75.6</cell><cell>18</cell><cell>20</cell><cell>8</cell><cell>21.65</cell></row><row><cell>PSENet-4s  † (ResNet18)</cell><cell>960</cell><cell>74.3</cell><cell>10</cell><cell>17</cell><cell>10</cell><cell>26.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Time consumption of PSENet on CTW-1500. The total time is consist of backbone, head of segmentation and PSE part. † indicates training from scratch. "Res" represents the resolution of the input image. "F" represent the F-measure.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Time consumption of PSENet-like method based on PSP-Net and original PSENet. Both of them are trained from scratch. Mem means GPU memory. F means F-measure. 1s means the size of output map is equal to input image. even with extreme curvature, but also separate those close text instances in a good manner. 7.4. More Detected Examples on Total Text, ICDAR 2015 and ICDAR 2017 MLT In this section, we demonstrate more test examples produced by the proposed PSENet in</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">text instances as 1 instance since their boundary pixels are partially connected. (d) is the result of our proposed PSENet, which successfully distinguishs and detects the 4 unique text instances.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Icdar2017 competition on multi-lingual scene text detection and script identification</title>
		<ptr target="http://rrc.cvc.uab.es/?ch=8&amp;com=introduction" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Total-text: A comprehensive dataset for scene text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Ch&amp;apos;ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A tutorial on the cross-entropy method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T. De</forename><surname>Boer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kroese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Operations Research</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pixellink: Detecting scene text via instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAIS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Single shot text detector with regional attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep direct regression for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wordsup: Exploiting word annotations for character based text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">R2cnn: rotational region cnn for orientation robust scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09579</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rotationsensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fots: Fast oriented text spotting with a unified network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01671</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Detecting curve text in the wild: New dataset and new solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Textsnake: A flexible representation for detecting text of arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Multi-oriented scene text detection via corner localization and region segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08948</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A generic solution to polygon clipping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Vatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Scene text detection via holistic, multi-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.09002</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-oriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deeptext: A unified framework for text proposal generation and text detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07314</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">East: an efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03155</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
