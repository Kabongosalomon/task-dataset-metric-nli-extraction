<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MICRONETS: NEURAL NETWORK ARCHITECTURES FOR DEPLOYING TINYML APPLICATIONS ON COMMODITY MICROCONTROLLERS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colby</forename><surname>Banbury</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuteng</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Fedorov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><forename type="middle">Matas</forename><surname>Navarro</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urmish</forename><surname>Thakker</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dibakar</forename><surname>Gope</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Janapa</forename><surname>Reddi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Mattina</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Whatmough</surname></persName>
						</author>
						<title level="a" type="main">MICRONETS: NEURAL NETWORK ARCHITECTURES FOR DEPLOYING TINYML APPLICATIONS ON COMMODITY MICROCONTROLLERS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Executing machine learning workloads locally on resource constrained microcontrollers (MCUs) promises to drastically expand the application space of IoT. However, so-called TinyML presents severe technical challenges, as deep neural network inference demands a large compute and memory budget. To address this challenge, neural architecture search (NAS) promises to help design accurate ML models that meet the tight MCU memory, latency, and energy constraints. A key component of NAS algorithms is their latency/energy model, i.e., the mapping from a given neural network architecture to its inference latency/energy on an MCU. In this paper, we observe an intriguing property of NAS search spaces for MCU model design: on average, model latency varies linearly with model operation (op) count under a uniform prior over models in the search space. Exploiting this insight, we employ differentiable NAS (DNAS) to search for models with low memory usage and low op count, where op count is treated as a viable proxy to latency. Experimental results validate our methodology, yielding our MicroNet models, which we deploy on MCUs using Tensorflow Lite Micro, a standard open-source neural network (NN) inference runtime widely used in the TinyML community. MicroNets demonstrate state-of-the-art results for all three TinyMLperf industry-standard benchmark tasks: visual wake words, audio keyword spotting, and anomaly detection. Models and training scripts can be found at https://github.com/ARM-software/ML-zoo.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Machine learning (ML) methods play an increasingly central role in a myriad of internet-of-things (IoT) applications. Using ML, we can interpret the wealth of sensor data that IoT devices generate. Prototypical uses in IoT include tasks such as monitoring environmental conditions such as temperature and atmosphere (e.g., carbon monoxide levels), monitoring mechanical vibrations from machinery to predict failure, or visual tasks such as detecting people or animals. User interfaces based on speech recognition and synthesis are also very common, as many IoT devices have limited user input features and small displays. In mobile applications, ML inference is often offloaded to the cloud, where compute resources are more abundant. However, offloading introduces overheads in terms of latency, energy and privacy. It also requires access to communications, such as WiFi or cellular access. For the proliferating class of IoT devices, offloading can be prohibitively expensive, in terms of both the radio chips which increase the bill of materials, as well as the network access costs.  TinyML is an alternative paradigm, where we execute ML tasks locally on IoT devices. This allows for real time analysis and interpretation of data at the point of collection, which translates to huge advantages in terms of cost and privacy. Microcontroller units (MCUs) are the ideal hardware platform for TinyML, as they are typically small (∼1cm 3 ), cheap (∼$1) and low-power (∼1mW) compared to mobile and cloud platforms <ref type="table" target="#tab_0">(Table 1)</ref>. MCUs typically integrate a CPU, digital and analog peripherals, on-chip embedded flash (eFlash) memory for program storage and Static Random-Access Memory (SRAM) for intermediate data. However, deploying deep neural networks on MCUs is extremely challenging; the most severe limitation being the small and flat memory system <ref type="figure" target="#fig_1">(Figure 1</ref>) within which the model weights and activations must be stored. Therefore, to arXiv:2010.11267v6 <ref type="bibr">[cs.</ref>LG] 12 Apr 2021 achieve the promise of TinyML, we must aggressively optimize models to best exploit the limited resources provided by an MCU hardware and software stack.</p><p>Mounting interest in TinyML has led to some maturity in both software stacks and benchmarks. The open source TensorFlow Lite for Microcontrollers (TFLM) inference runtime <ref type="bibr">(David et al., 2020)</ref> allows for straightforward and portable deployment of NNs. TFLM uses an interpreter to execute an NN graph, which means the same model graph can be deployed across different hardware platforms. When compared to code generation based methods (uTensor), TFLM provides portability across MCU vendors, at the cost of a fairly minimal memory overhead. Recently, the ML performance (MLPerf) benchmarking organization has outlined a suite of benchmarks for TinyML called TinyMLPerf <ref type="bibr" target="#b2">(Banbury et al., 2020)</ref>, which consists of three TinyML tasks of visual wake words (VWW), audio keyword spotting (KWS), and anomaly detection (AD). Standardizing TinyML research results around a common open-source runtime and benchmark suite makes comparing research results easier and fairer, hopefully driving research progress.</p><p>Previous work on TinyML has largely considered model design without consideration for the real deployment scenario (e.g. SpArSe <ref type="bibr">(Fedorov et al., 2019)</ref>, Structured Matrices <ref type="bibr" target="#b54">(Thakker et al., 2019b;</ref><ref type="bibr" target="#b52">Thakker et al., 2019)</ref>), or has used closed-source software stacks which make deployment and comparison impossible (e.g. MCUNet <ref type="bibr" target="#b37">(Lin et al., 2020)</ref>). In this paper, we describe MicroNets, a family of models which can be deployed with publicly available TFLM, for the three TinyMLperf tasks of VWW, KWS and AD. In contrast to previous TinyML work that uses black-box optimizations, such as Bayesian optimization <ref type="bibr">(Fedorov et al., 2019)</ref>, and evolutionary search <ref type="bibr" target="#b37">(Lin et al., 2020)</ref>, MicroNets are optimized for MCU inference performance using differentiable neural architecture search (DNAS).</p><p>The contributions of this work are summarized below.</p><p>• Using an extensive characterization of NN inference performance on three representative MCUs, we demonstrate that the number of operations is a viable proxy for inference latency and energy.</p><p>• We show that differentiable neural architecture search (DNAS) with appropriate constraints can be used to automatically construct models that fit the MCU resources, while maximizing performance and accuracy.</p><p>• We provide state of the art models for all three TinyML tasks, deployable on standard MCUs using TFLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Since its inception, deep learning has been synonymous with expensive, power-hungry GPUs <ref type="bibr" target="#b30">(Krizhevsky et al., 2012)</ref>. However, the current interest in deploying deep learning models on MCUs is reflected in a small number of papers that have begun to explore this promising space. In this section, we briefly survey the literature related to TinyML, divided between hardware, software and machine learning.</p><p>Hardware The current interest in ML has led to a growing demand for arithmetic compute performance in MCU platforms, which was previously driven by digital signal processing workloads. Single-instruction multiple-data (SIMD) extensions <ref type="bibr" target="#b24">(Hennessy &amp; Patterson, 2011)</ref> are one of the most effective approaches to achieving this in the CPU context, but increase silicon area and power consumption. The Arm Helium extensions (Helium) address this using a lightweight SIMD implementation targeted to MCUs. Beyond CPUs, various accelerators <ref type="bibr" target="#b16">(Flamand et al., 2018;</ref><ref type="bibr" target="#b28">Kodali et al., 2017;</ref><ref type="bibr" target="#b58">Whatmough et al., 2018;</ref><ref type="bibr" target="#b59">Whatmough et al., 2019)</ref> and co-processors such as digital signal processors (DSPs) <ref type="bibr" target="#b11">(Efland et al., 2016)</ref> and micro neural processing units (uNPUs) (Ethos-U55) typically offer greater performance and energy efficiency, at the cost of a more complex and less portable programming model . Finally, optimized memory technologies  and subthreshold circuit techniques <ref type="bibr">(Ambiq)</ref> can be used to reduce the power consumption at the circuit level. In this work we specifically target commodity MCUs <ref type="table" target="#tab_0">(Table 1)</ref> takes a model definition and automatically generates C code directly. In general, this approach typically gives the best results, but the generated code is not portable between different platforms. Examples include uTensor, tinyEngine <ref type="bibr" target="#b37">(Lin et al., 2020)</ref>, and embedded learning library (ELL). In contrast, TensorFlow Lite for Microcontrollers TFLM is an interpreter based runtime for executing TensorFlow Lite graphs on MCUs. TFLM supports most common NN layers, with the notable exception of recurrent networks. It is widely supported by hardware vendors and supports many optimized kernels on the back end for specific platforms. Compared to code generation based methods, TFLM is more portable but has some overheads. We use TFLM due to its portability, ease of deployment and open-source nature.</p><p>Machine Learning The challenges with implementing CNNs on MCUs were discussed in Bonsai <ref type="bibr" target="#b20">(Kumar et al., 2017)</ref>, namely that the feature maps of typical NNs require prohibitively large SRAM buffers. As a more storage efficient alternative to CNNs, pruned decision trees were proposed to suit the smallest MCUs with as little as 2KB of SRAM. <ref type="bibr" target="#b20">Gupta et al. (2017)</ref> propose a variant of k-nearest neighbors tailored for MCUs. <ref type="bibr" target="#b21">Gural &amp; Murmann (2019)</ref> propose a novel convolution kernel, reducing activation memory and enabling inference on low-end MCUs. SpArSe <ref type="bibr">(Fedorov et al., 2019)</ref> demonstrated that by optimizing the model architecture, CNNs in fact can be deployed on MCUs with SRAM down to 2KB. This was achieved using NAS, which has emerged as a vibrant area of re-search, whereby ML algorithms construct application specific NNs to meet very specific constraints <ref type="bibr" target="#b13">(Elsken et al., 2019)</ref>. SpArSe employs a Bayesian optimization framework that jointly selects model architecture and optimizations such as pruning. Similarly, MCUNet <ref type="bibr" target="#b37">(Lin et al., 2020)</ref> uses evolutionary search to design NNs for larger MCUs (2MB eFlash / 512KB SRAM) and larger datasets including visual wakewords (VWW) <ref type="bibr" target="#b8">(Chowdhery et al., 2019)</ref> and keyword spotting (KWS) <ref type="bibr" target="#b57">(Warden, 2018)</ref>). Reinforcement learning (RL) has also been used to choose quantization options in order to help fit an ImageNet model onto a larger MCU (2MB eFlash) <ref type="bibr" target="#b49">(Rusci et al., 2020b)</ref>. As well as images, audio tasks are an important driver for TinyML. TinyLSTMs <ref type="bibr" target="#b15">(Fedorov et al., 2020)</ref> shows that LSTMs for speech enhancement in smart hearing aids are similarly amenable to deployment on MCUs, after targeted optimization.</p><p>In this paper, we use differentiable NAS (DNAS)  to design specialized MCU models to target the three TinyMLperf tasks. Unlike black-box optimization methods that have previously been applied to TinyML problems, like Bayesian optimization <ref type="bibr">(Fedorov et al., 2019)</ref> and evolutionary search <ref type="bibr" target="#b37">(Lin et al., 2020)</ref>, DNAS uses gradient descent and lends itself to straightforward implementation in modern auto-differentiation software like Tensorflow with acceleration on GPUs. Our work provides experimental evidence that DNAS is capable of satisfying MCU-specific model constraints, including eFlash, SRAM, and latency. In contrast to <ref type="bibr" target="#b37">(Lin et al., 2020)</ref>, our work uses a standard deployment framework (TFLM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HARDWARE CHARACTERIZATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System Overview</head><p>In this section we characterize the performance of NN inference workloads on MCUs. The MCUs we use <ref type="table" target="#tab_0">(Table 1)</ref> are fairly self-contained, consisting of an Arm Cortex-M processor, SRAM for working memory, embedded flash for non-volatile program storage, and a variety of digital and analog peripherals. Unlike their mobile, desktop and datacenter counterparts, MCUs have a rather flat memory system, as illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>. Mobile and cloud computer systems universally employ a large off-chip main memory (usually DRAM). However, MCUs are typically equipped with only on-chip memory, which is relatively small to keep the die size reasonable. <ref type="figure">Figure 2</ref> gives an example memory map showing how a KWS model is mapped onto the STM32F746ZG devices by TFLM. Activation buffers are allocated in the SRAM, while the model weights and biases and graph definition are allocated in the eFlash memory. Alternatively, weights can be stored in SRAM, but we found experimentally that this results in only about a 1% speedup in end-to-end latency, while significantly reducing the space available for activations, which cannot be stored in eFlash. In terms of throughput, this flat memory system coupled with the lower clock frequencies and simple (cheap) microarchitectures used in MCUs results in a predominately compute-bound system. The Cortex-M7 can dual issue load and ALU instructions, which the Cortex-M4 cannot. This gives higher IPC, which, combined with a 20% higher clock rate, makes the STM32F646ZG and the STM32F767ZI approximately twice as fast as the STM32F446RE.</p><p>Note that the runtime overhead for the TFLM interpreter is fairly minimal, requiring just 4KB of SRAM and 37 KB of eFlash. The 34KB SRAM block labeled as persistent buffers in <ref type="figure">Figure 2</ref> scales with the size of the model and contains buffered quantization parameters and the structs that hold pointers to the intermediate tensors and to the operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Layer Latency</head><p>In this section, we examine the hardware performance of typical NN layers. To do this, we generate a large number of layer types 1 and sizes and characterize them on the hardware. <ref type="figure" target="#fig_2">Figure 3</ref> shows the measured latency of each layer in TFLM and CMSIS-NN kernels, as a function of the number of operations 2 (ops). We observe that different layer types and sizes result in some spread in throughput, which was previously observed by <ref type="bibr" target="#b35">Lai et al. (2018b)</ref>. 2D convolutions and fully connected layers exhibit lower latency per op than depth-wise convolutions. This is likely due to depthwise convolutions having less operations relative to their IM2COL overhead. We also note some variability in ops/s between 2D convolution layers. This is primarily caused by the sensitivity of the CMSIS-NN kernel to input and output channel sizes. The CMSIS-NN CONV 2D kernel is substantially faster when the number of input and output channels are divisible by four. As an example, we observe that increasing the input/output channels of a convolution layer from 138/138 to 140/140 decreases the latency from 37.5ms to 21.5ms (57% speedup).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Latency</head><p>Next, we characterize whole end-to-end models. To do this, we setup a parameterized supernet backbone that we randomly sample. This allows us to automatically generate a large number of random models with different layer types and dimensions, which we then characterize on the hardware in terms of latency and, in the next subsection, energy. Figure 4 shows measured model latency on the STMF446RE and the STMF746ZG. Measurements are shown for random models sampled from backbones tailored to two different tasks viz. image classification and audio classification.</p><p>Interestingly, the measured latency for the end-to-end models is linear with op count (0.95 &lt; r 2 &lt; 0.99). This is perhaps surprising given the variation seen with the layer-wise latency measurements ( <ref type="figure" target="#fig_2">Figure 3</ref>). Also, we observe that models sampled from the two different backbones results in a different slope. The explanation for this is that although single layers exhibit variation in latency as a function of ops, in a whole model this is averaged across many layers. Since a given search space will typically be dominated by a particular layer type, in terms of ops, the result is that we see a linear latency for models sampled from the same backbone.</p><p>The KWS backbone has ∼40% higher throughput (Mops/S) than the CIFAR10 backbone, which is due to the mix in layer types and sizes. Finally, STM32F746ZG is around twice as fast as STM32F446RE (Section 3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Energy</head><p>Energy consumption is obviously a critical metric for TinyML. Following the same random model sampling methodology used to characterize latency, we measured the current consumption of 400 models from the CIFAR10 backbone. We use the Qoitech Otii Arc (Otii) to power the MCU boards and measure the current draw with the inference workload looping. <ref type="figure" target="#fig_4">Figure 5</ref> shows the average power consumption versus the op count of each model on two MCUs. Clearly, there is little variance in power consumption between models (σ/µ = 0.00731), i.e. power is essentially independent of model size or architecture. Additionally, <ref type="figure" target="#fig_4">Figure 5</ref> shows the energy consumption versus the op count of each model. We observe that executing the same model on a smaller MCU reduces the total energy consumption despite an increase in latency. This decreased energy consumption motivates the design of models that can fit within the tighter constraints of smaller devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Summary</head><p>Below is a summary of the findings of this section:</p><p>• Although ops is not a good predictor for the latency of a single layer, it is a viable proxy for the latency of an entire model sampled from a given backbone.</p><p>• For a given MCU, power is largely independent of model size and design. Therefore, energy per inference is a function of the size of the MCU, which determines power, and the number of ops, which dictates latency.</p><p>Therefore, when designing a model from within a backbone for a given task, ops is a viable proxy for both latency and energy, as measured on the target hardware and software.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TINYMLPERF BENCHMARK TASKS</head><p>This section describes the TinyMLPerf benchmark tasks: Visual Wake Words (VWW), Keyword Spotting (KWS), and Anomaly Detection (AD). These were selected by a committee from industry and academia, to represent common TinyML application domains <ref type="bibr" target="#b2">(Banbury et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visual Wake Words</head><p>The VWW dataset used in TinyMLperf is a visual classification task, where each image is labeled as 1 when a person occupies at least 0.5% of the frame and 0 when no person is present <ref type="bibr" target="#b8">(Chowdhery et al., 2019)</ref>. The dataset contains 82,783 train and 40,504 test images, which we resize to a common resolution of 224 × 224. We use the standard ImageNet data prepropecessing pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Audio Keyword Spotting</head><p>Audio KWS (a.k.a. wake words), finds application in a plethora of use cases in commercial IoT products (e.g. Google Assistant, Amazon Alexa, etc.). Recent research has explored various model architectures suitable for resource constrained devices <ref type="bibr" target="#b60">Wong et al., 2020;</ref><ref type="bibr" target="#b33">Kusupati et al., 2018;</ref><ref type="bibr" target="#b53">Thakker et al., 2019a)</ref>. Among these, CNNs achieve good accuracy <ref type="bibr" target="#b7">(Choi et al., 2019;</ref><ref type="bibr" target="#b1">Anderson et al., 2020;</ref><ref type="bibr" target="#b63">Zhang et al., 2017c;</ref><ref type="bibr" target="#b18">Gope et al., 2019)</ref> and have the advantage of being deployable on commodity hardware using existing software stacks. The KWS dataset in TinyMLperf is Google Speech Commands (V2) <ref type="bibr" target="#b57">(Warden, 2018)</ref>. A model trained on this dataset is required to classify an 1-second long incoming audio clip from a vocabulary of 35 words into one of the 12 classes-10 keyword classes along with "silence" (i.e. no word spoken), and an "unknown" class, which is the remaining 25 keywords from the dataset. The raw time-domain speech signal is converted to 2-D MFCC (Mel-frequency cepstral coefficients). 40 MFCC features are then obtained from a speech frame of length 40ms with a stride of 20ms, yielding an input dimension of 49 × 10 × 1 features for 1 second of audio. Training samples are augmented by applying background noise and random timing jitter to provide robustness against noise and alignment errors. We follow the same input data processing procedure described in <ref type="bibr" target="#b62">Zhang et al. (2017b)</ref> and <ref type="bibr" target="#b42">Mo et al. (2020)</ref> for training the baselines and other DNAS variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Anomaly Detection</head><p>Anomaly detection is a task that classifies temporal signals as "normal" or "abnormal". Anomaly detection finds nu-merous application, including industrial factories, where it is deployed on smart sensors to monitor equipment and detect problems. The dataset used for anomaly detection in TinyMLperf is MIMII(Slide Rail) <ref type="bibr" target="#b46">(Purohit et al., 2019)</ref>. It is a dataset of industrial machine sounds operating under normal or anomalous conditions recorded in real factory environments. The original dataset contains different machine types, but we focus on the Slide Rail task as selected in TinyMLPerf benchmarks.</p><p>Anomaly detection is an unsupervised learning problem. The model only sees "normal" samples at training time and is expected to make predictions on a mix of normal and abnormal cases at test time. Many unsupervised learning methods can be applied, however, inspired by state-of-theart solutions <ref type="bibr" target="#b17">(Giri et al., 2020)</ref>, we reformulate the problem as a self-supervised <ref type="bibr" target="#b23">(Hendrycks et al., 2019)</ref> learning problem, so that it can be handled in a similar way as the other two tasks. The essential idea is to leverage machine ID metadata provided in this dataset. The training dataset contains 4 different machine IDs, each corresponding to a different slide machine for which the audio is recorded. We train a classifier in a supervised way to identify the machine ID given the audio as input. The classifier needs to learn useful information about the normal operating sound of these machines to tell them apart, which can then be used to detect anomaly. At testing time, we use the softmax score for the test sample machine ID as an index of how confident the classifier is about the test sample falling into the normal operating regime data on which it has been trained. Therefore its negative can be used as an anomaly score (higher meaning more likely to be abnormal). The area under the curve (AUC) metric from the receiver operating characteristic (ROC) is calculated using this anomaly score.</p><p>Data preprocessing is done in a similar way as for KWS: the audio signal is transformed into log-Mel spectrograms, which are then input to a CNN classifier. An audio clip of length 10s is split into overlapping frames of length 64ms with a stride (hop length) of 32ms between frames. 64 MFCC features are extracted for each frame. The preprocessed dataset is available on Kaggle (Kaggle AD). We then stack 64 frames together to get 64 by 64 images and the next image has an overlap of 44 frames. We found that CNNs can tolerate even lower resolution spectrograms so the image is further down-sampled to 32×32 using bilinear interpolation. This is the input to our CNN classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">MICRONET MODELS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Optimizations</head><p>We use DNAS to discover models which are highly accurate, while also satisfying SRAM, eFlash, and latency constraints.</p><p>In the following, we briefly review DNAS and how it can be applied to ML model design for MCUs. For further information, we refer the reader to <ref type="bibr" target="#b3">Cai et al., 2019;</ref><ref type="bibr" target="#b10">Dong &amp; Yang, 2019;</ref><ref type="bibr" target="#b56">Wan et al., 2020)</ref>. The search begins with the definition of a supernet consisting of decision nodes. The output of a decision node expresses a choice between K options</p><formula xml:id="formula_0">y = K k=1 z k f k (x, θ k ), K k=1 z k = 1<label>(1)</label></formula><p>where x is the input tensor, f k () is the operation executed by choice k and parameterized by θ k , K is the total number of options for the decision node, and z k ∈ {0, 1} represents the selection of one of K options. The goal of the search is to select z = z 1 · · · z K for all of the decision nodes in the supernet. In the present work, we restrict our search to the width for each layer and the overall depth. In this case, each option f k () represents an operation with a different number of channels <ref type="bibr" target="#b56">(Wan et al., 2020)</ref> or the identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Optimizing for MCU Memory</head><p>Without any model constraints, DNAS may produce models which violate one or more MCU hardware limits. Given the model size and the size of the intermediate activations produced by modern NNs, eFlash and SRAM play an important role in model design. We incorporate appropriate regularization terms in our DNAS experiments such that the selected models both fit in eFlash memory and produce activations which can fit in available SRAM. For model size considerations, we express the size of a particular selection from the supernet using</p><formula xml:id="formula_1">K k=1 z k |θ k |<label>(2)</label></formula><p>where |θ k | denotes the cardinality of θ k . Summing the size of each node, we obtain the size of the supernet as a function of decision parameters z for each decision node, which we use to regularize the DNAS such that the selected architecture meets the MCU eFlash constraint.</p><p>To ensure that the selected architecture satisfies SRAM constraints, we adopt the working memory model from <ref type="bibr">Fedorov et al. (2019)</ref>, which states that the working memory required for a particular node with inputs {x 1 , · · · , x N } and outputs {y 1 , · · · , y M } is given by N n=1 |x n | + M m=1 |y m |. For tensors which are outputs of decision nodes, we replace |x n | by (2). The total model working memory is then defined as the maximum over the working memory of every network node, which we include in the DNAS objective function such that the discovered architecture meets the MCU SRAM constraint. We define the constraint as the available SRAM minus the expected TFLM overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Optimizing for Latency</head><p>In addition to making sure that discovered models are deployable, we also incorporated a latency constraint into our DNAS experiments. Due to the (almost) linear relationship between latency and number of operations for ML inference on MCUs, we treat the operation count as a strong proxy for latency during optimization. As with memory, we begin by defining the operation count of each decision node as a function of the decision vector z : K k=1 z k c k , where c k is the number of ops required to execute option k. Note that the number of operations for a particular option typically depends on the input and output tensor sizes, which are a function of decision parameters z <ref type="bibr" target="#b56">(Wan et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Sub-Byte Quantization</head><p>The predominant datatype for NN inference on microcontrollers is 8-bit integer. The use of smaller 4-bit datatypes <ref type="bibr" target="#b14">(Esser et al., 2020;</ref><ref type="bibr" target="#b45">Park &amp; Yoo, 2020;</ref><ref type="bibr">Thakker et al., 2021;</ref><ref type="bibr" target="#b26">Huang et al., 2020)</ref> for weights (activations) allows for more parameters (feature maps), potentially realizing higher accuracy in the same memory footprint. However, current MCUs do not natively support sub-byte datatypes, so this must be emulated using 8-bit types. We investigated the benefit of 4-bit quantization on the KWS task.</p><p>Currently, the CMSIS-NN <ref type="bibr" target="#b34">(Lai et al., 2018a)</ref>, does not provide convolution operators for 4−bit values. Therefore, we developed optimized kernels for 4−bit datatypes, and incorporated them into CMSIS-NN for use in our experiments. This allows our DNAS to expand the search space to fit models with more weights and/or activations, potentially achieving higher accuracy in the same memory footprint. The unpacking and packing routines required to emulate hardware support for 4−bit using native 8− or 16−bit operations add modest latency overhead. These optimized kernels can efficiently support sub-byte quantization on either weights or activations or both. Prior work on mixed-precision inference (CMix-NN <ref type="bibr" target="#b4">(Capotondi et al., 2020)</ref>) does not support operations on signed sub-byte weight and activation values, nor non-modulo-4 feature-map channel numbers, and therefore is not compatible with current CMSIS-NN software and TFLM runtime stack. We anticipate that future MCUs may provide native hardware support for 4−bit datatypes, further increasing the value of this research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">DNAS Backbones and Training Recipes</head><p>DNAS requires a backbone supernet to be defined as the starting point for the search. The design of the backbone is an important step which requires human experience of network operators and connectivity patterns that work well for a given task. If the backbone is too large, the supernet will not fit in GPU memory. On the other hand, if the backbone is too small, it may not provide a rich enough search space within which to find models that satisfy the constraints. In this section, we describe the backbones used for the TinMLperf tasks and the training methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Visual Wake Words (VWW)</head><p>We use a MobilenetV2 <ref type="bibr" target="#b50">(Sandler et al., 2018)</ref> backbone, consisting of a series of inverted bottleneck (IBN) blocks. Each IBN block includes the sequence: 1 × 1 conv, 3 × 3 depthwise conv, 1 × 1 conv. We restrict our search space to the width of the first and last convolutions in each IBN, as well the convolutions preceding and following the sequence of IBN blocks. For each convolution, we choose between 10% and 100% of the width of the corresponding layer in MobilenetV2, in increments of 10%. In order to ensure that the input itself does not violate the SRAM constraint, we resize the input images to 50 × 50 × 1 and 160 × 160 × 1 for the small (STMF446RE) and medium (STM32F746ZG) sized MCUs, respectively. Note that we convert the RGB images to grayscale, such that the input only has 1 channel, in order to trade off color resolution for spatial resolution <ref type="bibr">(Fedorov et al., 2019;</ref><ref type="bibr" target="#b8">Chowdhery et al., 2019)</ref>.</p><p>We run DNAS for 200 epochs, batch size 768, decaying the learning rate from 0.36 to 0.0008 with a cosine schedule. We use quantization aware training <ref type="bibr" target="#b29">(Krishnamoorthi, 2018)</ref> to emulate 8−bit quantization of both weights and activations during training. Discovered architectures are finetuned for 200 epochs with the same learning rate schedule, weight decay of 0.00004, and knowledge distillation using MobilenetV2 as the teacher, knowledge distillation coefficient 0.5, and a temperature of 4 <ref type="bibr" target="#b25">(Hinton et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Keyword Spotting (KWS)</head><p>After experimenting with different architectures on the KWS task, we settled on an enlarged DS-CNN(L) <ref type="bibr" target="#b62">(Zhang et al., 2017b)</ref> model as the backbone for DNAS. The backbone is built by adding four more depthwise-separable blocks of output channels 276 to the largest variant of DS-CNN. A skip connection branch (average pooling if the parallel convolutional block downsamples the input) is also added in parallel to each depthwise-separable block in the backbone to create shortcuts for choosing the number of layers. We use DNAS to choose the number of channels and the number of layers in this backbone network, while trying to satisfy the hardware constraints. The number of channels are restricted to multiples of 4 for good performance on hardware. For the small and medium models, the constrains were set to achieve 10FPS and 5FPS on the medium (STM32F746ZG) board while also suiting the smallest (STMF446RE) board. It is thus a combination of latency and working memory constraints. For the large model, we target latency of less than one second, in order to achieve real-time throughput.</p><p>DNAS is run for 100 epochs, with a batch size of 512, de-  <ref type="table" target="#tab_8">Table 4</ref>, respectively. The two numbers following IBN denote the number of expansion and compression filters. Tensor dimensions are provided in black text.</p><p>caying the learning rate from 0.01 to 0.00001 with a cosine schedule. A weight decay coefficient of 0.001 is used. Additionally, we quantize weights, activations and input to 8-bit using fake quantization nodes to emulate deployment. The ranges of quantizers are learnt with gradient descent. We train the final models for another 100 epochs, with a batch size of 256, decaying the learning rate from 0.02 to 0.00008 with a cosine schedule and a weight decay coefficient of 0.002. SpecAugment <ref type="bibr" target="#b44">(Park et al., 2019)</ref> is used during training to further avoid overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Anomaly Detection (AD)</head><p>For AD, the model operates on spectrograms of audio signals in a similar way as for KWS, so it makes sense for the two tasks to share similar backbone networks. Hence, the backbone network we used for AD was DSCNN-L, with parallel skip connections (or average pooling if downsampling) to skip layers. The strides of the last two depthwiseseparable blocks are increased to 2 to downsample the input patch down to 4×4 before applying the final pooling. DNAS searches for channel numbers and the total number of layers to meet the hardware deployment constraints. An anomaly detection system is expected to run in real-time for continuous monitoring, and should therefore take less time than the increment between two successive spectrogram images (considering overlapping). In our setting, this latency cutoff can be calculated as 32 × 20ms = 640ms. This latency constraint together with the SRAM limits for each board are used as constraints in our DNAS runs.</p><p>We use the same DNAS hyperparameters as for KWS, except we only train for 50 epochs, as convergence is faster. We also apply a mixup <ref type="bibr" target="#b61">(Zhang et al., 2017a</ref>) augmentation coefficient of 0.3 to avoid overfitting. We experimented with spectral warping augmentations <ref type="bibr" target="#b17">Giri et al. (2020)</ref>, but did not observe benefits in our setting. This may be because our models are relatively compact and use quantization aware training, and therefore require less data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Methodology</head><p>To deploy our models, we convert them to TFlite format and then execute them on each MCU using the TFLM runtime. The eFlash occupancy is determined using the Mbed compiler <ref type="bibr">(Mbed OS)</ref> and the SRAM consumption is obtained using the TFLM recording memory APIs. We measure latency on the MCU using the Mbed Timer API.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Visual Wake Words (VWW)</head><p>Figure 8 compares our DNAS results (MicroNets) to three state of the art results, including ProxylessNAS <ref type="bibr" target="#b3">(Cai et al., 2019)</ref>, MSNet <ref type="bibr" target="#b6">(Cheng et al., 2019)</ref>, and the TFLM example model <ref type="bibr" target="#b8">(Chowdhery et al., 2019)</ref>. The largest network in our search space is MobileNetV2, which achieves 88.75% accuracy. The MicroNet models are visualized in <ref type="figure" target="#fig_5">Figure  6</ref>. We found that the model produced by targeting the medium MCU (88.03%) nearly matched the accuracy of MobileNetV2, obviating the need to search for a large-MCU specific model. MicroNets are pareto-optimal for the small and medium sized MCUs. For the small MCU, our Mi-croNet is 3.1% more accurate than the TFLM reference, the only network considered which can be deployed on the small MCU with TFLM, while being 21ms faster. For the medium MCU, our MicroNet model was the only model considered that could be deployed on that MCU. <ref type="figure">Figure 8</ref>: they do not fully exploit the precious memory resources. For instance, the ProxylessNAS model easily fits the flash memory on the smallest MCU (STM32F446RE), but requires the largest MCU (STM32F767ZI) to fit the activations in SRAM. Therefore, ProxylessNAS will only run on the large MCU. MSNet shows similar characteristics. These limitations underline the motivation for DNAS optimized models to target a specific MCU size.  <ref type="figure">Figure 7</ref>. KWS results. The small and medium MicroNets both target the smallest MCU. MicroNet medium model is more accurate than DS-CNN(L) and is 2.7× faster. The largest MobileNetV2 variant does not fit and is omitted. Latency measured on the STM32F746ZG. SRAM and Flash refer to the overall measured usage of the model, without the TFLM overheads ( <ref type="figure">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The limitations of previous TinyML models is clear in</head><p>MicroNet models are not tight against the SRAM and flash constraints for a number of reasons. TFLM has to schedule every graph and perform memory management, which leads to some variability in the resulting model size. Therefore, we estimate the maximum model size and activation footprint possible for a given hardware platform by performing some experiments with TFLM. However, the final binary size is still somewhat dependent on the graph itself, so this prevents us from tightly meeting the constraints. In a real application context there will also be application logic and potentially even a real-time operating system (RTOS) (Mbed OS), which will take additional eFlash memory resources that must be budgeted into the model constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Keyword Spotting (KWS)</head><p>KWS results are shown in <ref type="figure">Figure 7</ref>, where we compare our results (MicroNet) to DS-CNNs <ref type="bibr" target="#b62">(Zhang et al., 2017b)</ref> and models built by stacking MobileNetV2 <ref type="bibr" target="#b50">(Sandler et al., 2018)</ref> inverted bottleneck blocks. Few results are currently available for version 2 of the Google speech commands dataset, therefore we train baseline models for comparison. We trained all the models with exactly the same training recipe and quantized them to 8−bit weights and activations (including input) before measuring their accuracy. MicroNet models are the Pareto optimal for latency, SRAM usage and model size. MicroNet small and medium models also come very close to the latency constraints set for them, achieving  <ref type="table" target="#tab_11">Table 5</ref> of Appendix A.</p><p>We can further leverage sub-byte quantization to make bigger but more accurate models deployable on smaller MCUs. <ref type="table" target="#tab_4">Table 2</ref> demonstrates the accuracy, latency, and SRAM memory trade-off of the 4−bit MicroNet-KWS-Large model. The 4−bit MicroNet-KWS-Large model outperforms the 8−bit medium-sized model by 0.5%, because it is able to have more weights and activations on the same MCU. Table 2 reports the model latency as measured on the medium MCU (STM32F746ZG). The increase in latency of the 4bit model is primarily attributed to increase in ops due to larger feature maps. When compared to the sub-byte kernels of CMix-NN <ref type="bibr" target="#b4">(Capotondi et al., 2020)</ref>, our 4−bit kernels can substantially hide the latency overhead due to softwareemulation of 4−bit operations, by fully exploiting the available instruction-level-parallelism bandwidth on Cortex-M microcontrollers. Furthermore, we believe the accuracy of the 4−bit KWS MicroNet can be further improved by selectively quantizing lightweight depthwise layers to 8−bits, while quantizing remaining memory-and latency-heavy pointwise and standard convolutional layers to 4-bits <ref type="bibr" target="#b48">(Rusci et al., 2020a;</ref><ref type="bibr" target="#b19">Gope et al., 2020)</ref>. Latency results for 4-bit quantized MicroNet-KWS models and their comparison against 8-bit models can be found in the Appendix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Anomaly Detection (AD)</head><p>Results for AD are given in <ref type="table" target="#tab_5">Table 3</ref>. To aid comparison, we use the "Uptime" metric defined as model latency divided by stride time between two successive inputs. For real-time AD, this ratio is the duty cycle of the MCU workload. For example, our models expect a stride of 640ms while some other models expect 32ms and need to be run more often. This metric is also relates directly to power consumption.</p><p>The models obtained from DNAS are called MicroNet-AD. The three different sized models each targets a different sized MCU. Our solutions are compared with a baseline fully-connected auto-encoder (FC-AE) for anomaly detection <ref type="bibr" target="#b46">(Purohit et al., 2019)</ref>, which has a 640 dimensional input, followed by 4 fully-connected hidden layers of 128 neurons each, a bottleneck layer of 8 neurons, 4 fully-connected hidden layers of 128 neurons again and the output. This baseline model achieves 84.76% AUC and runs fast. However, once we try to scale it up for better anomaly detection performance, the size of the model quickly exceeds the flash limit of all MCUs used in this work. The wide FC-AE model which scales up all the hidden neuron number from 128 to 512 in the baseline, achieves only 87.1% AUC but its size exceeds 2MB in 8-bit, making it undeployable on our MCUs. An alternative to fully-connected AE that is more parameter efficient is convolutional AE, which we have also included in the comparison. Convolutional AEs require the transposed convolution operator, not supported in TFLM.</p><p>We also compare with the MobileNetV2-0.5AD model trained in a similar self-supervised way as ours, which is a component of the winning solution at DCASE2020 challenge (DCASE) presented in <ref type="bibr" target="#b17">Giri et al. (2020)</ref>. Since the authors submitted ensembles of multiple classifiers, we take the average of AUCs reported where the MobileNetV2-0.5AD is a component as an estimate of its accuracy. This model can only be deployed on the largest MCU because of it relatively large size (close to 1MB). The model is light in number of operations but its uptime requirement is worse than our solutions since it expects a time stride of 256ms. Our large MicroNet model is equally performing in terms of AUC, requires less than half the Flash size and consumes less compute resources in terms of uptime requirement. The smallest MicroNet-AD model can be deployed on the small MCU performing real-time AD with &gt; 95% AUC performance. As we have shown previously, the small MCU only draws about 1/3 the power of the medium, which is attrac-  <ref type="bibr" target="#b47">(Ribeiro et al., 2020)</ref> and MBNETV2-0.5AD <ref type="bibr" target="#b17">(Giri et al., 2020)</ref>  tive since most of these tiny IoT devices run on batteries or need to be energy self-sufficient. MicroNet-AD model architectures can be found in <ref type="table" target="#tab_11">Table 5</ref> of Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Comparison with State-of-the-Art</head><p>A key previous TinyML work is SpArSe <ref type="bibr">(Fedorov et al., 2019)</ref>. This work is focused on even smaller MCUs, with memory down to 2KBs. However, it also targets smaller datasets with smaller input dimensions than the tinyMLperf tasks that we use in our work. In a parallel line of work, MCUNet <ref type="bibr" target="#b37">(Lin et al., 2020)</ref> demonstrated SOTA MCU models using a framework that jointly designs the model architecture and the lightweight code-generation inference engine. Their latency and SRAM measurement relies on a closed-source software stack that is not available to us, so it is difficult to make comparison with their results. However, our models are pareto optimal compared to MCUNet on the KWS task even with a readily available, open source software stack (see <ref type="figure" target="#fig_1">Figure 11</ref> in Appendix E).</p><p>In support of this paper, we have open sourced our models at https://github.com/ARM-software/ ML-zoo. We hope these will be useful for MCU vendors and researchers, as a set of standard models for benchmarking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>TinyML promises to enable a broad array of IoT applications, but is technically challenging. This is primarily due to the memory demands of deep neural network inference, which are at conflict with the limitations of MCUs. We start by analyzing measured MCU inference performance. Measurements demonstrate that for models sampled from a given network search space, the inference latency of the model is, in fact, linear with the total operation count. Since MCU power is largely independent of workload, operation count is also a strong proxy for energy per inference. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A MODEL ARCHITECTURES FOR KWS AND AD MODELS</head><p>In <ref type="table" target="#tab_11">Table 5</ref>, we report the detailed architectures of MicroNet models for keyword spotting and anomaly detection found through DNAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B RESULTS TABLE</head><p>In <ref type="table" target="#tab_8">Table 4</ref> we provide a table of our results and baselines for easy comparison with future work. Binary refers to the size of the compiled binary that is loaded onto the MCU. Flash is the flash consumption of the model and SRAM is the total SRAM consumption of the model. We also report latency on the STM32F446RE (S), STM32F746ZG (M) and SRM32F767ZI (L) as well as energy consumption on the STM32F446RE (S) and STM32F746ZG (M). All of models are deployed using the TFLM inference framework. The <ref type="figure">Figure 9</ref>. Current consumption of a small and medium model on the STM32f446RE and STMF746ZG. We report the average power consumption measured over one second, including active and idle power.</p><p>Flash consumption is determined by the size of the tflite flatbuffer file and the SRAM consumption is obtained using the TFLM recording micro interpreter. We measure latency on the MCU in microseconds using the MBED Timer API. Finally we use the Qoitech Otii Arc (Otii) to measure energy consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C POWER TRACE</head><p>We plot the current vs. time for a small model and a medium model on the STM32f446RE and STMF746ZG in <ref type="figure">Figure 9</ref>. We also report the average power consumption over 1 second to illustrate the impact of the deep sleep power consumption on the overall energy consumption of a tinyML application with a duty cycle of one frame per second. We show that the current consumption varies little between models but the smaller model consume significantly less energy due to its reduced latency. <ref type="figure">Figure 9</ref> also demonstrates that the smaller mcu consumes less power on average despite being active for longer. <ref type="figure" target="#fig_1">Figure 10</ref> reports the percentage increase in latency of the MicroNet-KWS-M and MicroNet-KWS-L models as 4-bit quantization is applied to either weights or activations or both. 4-bit A/4-bit W refers to our optimized kernels that emulate 4-bit support on both weights and activations, while 4-bit A/8-bit W denotes kernels that emulate 4-bit datatypes support only on activations. It is important to note that the latency increase using our 4-bit optimized kernels is marginal even for MicroNet-KWS-M and MicroNet-KWS-L like  <ref type="table">Table.</ref> (*) Estimated (-) Unable to measure do to SRAM or eFlash constraints. ProxylessNas <ref type="bibr" target="#b3">(Cai et al., 2019)</ref>, MSNet <ref type="bibr" target="#b6">(Cheng et al., 2019)</ref>, Person Detection (TFLM), and MBNetV2-0.5 <ref type="bibr" target="#b17">(Giri et al., 2020)</ref> are all previous work.   <ref type="figure" target="#fig_1">Figure 10</ref>. Percentage increase in latency of the different models with 4-bit quantization on the medium MCU (STM32F746ZG) board in comparison to their 8-bit counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D LATENCY MEASUREMENT FOR 4-BIT MICRONET MODELS</head><p>deep networks (19.28% and 28.8% increase for MicroNet-KWS-M and MicroNet-KWS-L respectively over their 8-bit quantized models). Any smaller network than these (e.g. MicroNet-KWS-S, etc.) should observe even lower increase in latency with our 4-bit quantized kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E COMPARISON WITH MCUNET ON KWS</head><p>A Pareto front comparison between MicroNets and MCUNet models on the KWS task is shown in <ref type="figure" target="#fig_1">Figure 11</ref>, the data points for the MCUNet KWS models are our best estimates from figures published in <ref type="bibr" target="#b37">Lin et al. (2020)</ref>.  <ref type="figure" target="#fig_1">Figure 11</ref>. Comparison of different KWS models running on the medium MCU (STM32F746ZG). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>*</head><label></label><figDesc>Equal contribution 1 Arm ML Research 2 Harvard University 3 SambaNova Systems. Correspondence to: Colby Banbury &lt;cban-bury@g.harvard.edu&gt;. Proceedings of the 4 th MLSys Conference, San Jose, CA, USA, 2021. Copyright 2021 by the author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of memory hierarchies for (a) a mobile SoC which has a deep memory hierarchy with many levels of on-chip cache and a large off-chip DRAM main memory, and (b) an MCU with a flat on-chip memory system with no off-chip main memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Measured latency of a range of different individual layer types and sizes on the STM32F767ZI using TFLM. Different layers can exhibit a spread in latencies for the same ops count, due to variations in, for example, data reuse and IM2COL overheads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Measured latency of whole models randonly sampled from two backbones, on STM32F446RE and STM32F746ZG. Models sampled from a given search space exhibit latency linear with ops, despite the variation seen with individual layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Measured energy and power of models randomly sampled from an image classification CNN backbone. MCUs have simple microarchitectures and memory systems and hency power is fairly constant. Therefore, energy is largely determined by latency, which is in turn a linear function of model ops.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>VWW architectures discovered by DNAS targeting (a) medium (STM32F446RE) and (b) small (STM32F446RE) MCUs, labeled VWW-1 and VWW-2 in Appendix</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Illustrative comparison of hardware for CloudML, Mo-bileML and TinyML, including the MCUs targeted in this work.</figDesc><table><row><cell>Platform</cell><cell>Architecture</cell><cell>Memory</cell><cell>Storage</cell><cell>Power</cell><cell>Price</cell></row><row><cell>CloudML</cell><cell>GPU</cell><cell>HBM</cell><cell>SSD/Disk</cell><cell></cell><cell></cell></row><row><cell>Nvidia V100</cell><cell>Nvidia Volta</cell><cell>16GB</cell><cell>TB∼PB</cell><cell>250W</cell><cell>$9K</cell></row><row><cell>MobileML</cell><cell>CPU</cell><cell>DRAM</cell><cell>Flash</cell><cell></cell><cell></cell></row><row><cell>Cell Phone</cell><cell>Mobile CPU</cell><cell>4GB</cell><cell>64GB</cell><cell>∼8W</cell><cell>∼$750</cell></row><row><cell>TinyML</cell><cell>MCU</cell><cell>SRAM</cell><cell>eFlash</cell><cell></cell><cell></cell></row><row><cell>F446RE</cell><cell>Arm M4</cell><cell>128KB</cell><cell>0.5MB</cell><cell>0.1W</cell><cell>$3</cell></row><row><cell>F746ZG</cell><cell>Arm M7</cell><cell>320KB</cell><cell>1MB</cell><cell>0.3W</cell><cell>$5</cell></row><row><cell>F767ZI</cell><cell>Arm M7</cell><cell>512KB</cell><cell>2MB</cell><cell>0.3W</cell><cell>$8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>VWW results targeting the small and medium MCUs. Our search yields pareto optimal MicroNets. Latency measured on the STM32F746ZG. SRAM and Flash refer to the overall measured usage of the model, without the TFLM overheads Figure 2. 9.2FPS and 5.4FPS on the medium sized MCU while having accuracy of 95.3% and 95.8% and being deployable on the smallest MCU. Detailed description of MicroNet KWS model architectures can be found in</figDesc><table><row><cell>Test Accuracy</cell><cell>80 90</cell><cell></cell><cell></cell><cell></cell><cell>MicroNet s TFLM Proxylessnas</cell><cell>MSNet MobileNet V2</cell></row><row><cell></cell><cell>10 2</cell><cell></cell><cell></cell><cell>10 3</cell><cell>10 4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Lat ency (m s)</cell></row><row><cell>Test Accuracy</cell><cell>80 90</cell><cell>STMF446RE</cell><cell></cell><cell>STMF746ZG</cell><cell>STMF767ZI</cell></row><row><cell></cell><cell>100</cell><cell></cell><cell>200</cell><cell>300 SRAM (KB)</cell><cell>400</cell><cell>500</cell></row><row><cell>Test Accuracy</cell><cell>80 90</cell><cell>STMF446RE</cell><cell>STMF746ZG</cell><cell></cell><cell>STMF767ZI</cell></row><row><cell></cell><cell cols="2">500</cell><cell>1000</cell><cell>1500 Flash (KB)</cell><cell>2000</cell><cell>2500</cell></row><row><cell cols="2">Figure 8.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>KWS results for 4-bit quantized MicroNet models.</figDesc><table><row><cell></cell><cell>MN-KWS-L</cell><cell>MN-KWS-M</cell><cell>MN-KWS-L</cell></row><row><cell></cell><cell>(8-b W/8-b A)</cell><cell>(8-b W/8-b A)</cell><cell>(4-b W/4-b A)</cell></row><row><cell>Accuracy (%)</cell><cell>96.5</cell><cell>95.8</cell><cell>96.3</cell></row><row><cell>Latency (s) (M)</cell><cell>0.59</cell><cell>0.18</cell><cell>0.76</cell></row><row><cell>Model Size (KB)</cell><cell>612</cell><cell>163</cell><cell>375*</cell></row><row><cell>SRAM (KB)</cell><cell>208</cell><cell>103</cell><cell>121*</cell></row><row><cell></cell><cell cols="2">* denotes estimated value.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>AD results. AUC for MicroNets is with 8-bit weights/activations; Conv-AE</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>are unquantized FP32. S, M and L denote small, medium and large MCU targets.</figDesc><table><row><cell>Models</cell><cell>AUC(%)</cell><cell>Ops(M)</cell><cell>Size</cell><cell>Mem</cell><cell>Uptime(%)</cell></row><row><cell>MicroNet-AD(L)</cell><cell>97.28</cell><cell>129</cell><cell>442KB</cell><cell>383KB</cell><cell>95.9 (L)</cell></row><row><cell>MicroNet-AD(M)</cell><cell>96.22</cell><cell>124.7</cell><cell>464KB</cell><cell>274KB</cell><cell>94.8 (M)</cell></row><row><cell>MicroNet-AD(S)</cell><cell>95.35</cell><cell>37.5</cell><cell>253KB</cell><cell>114KB</cell><cell>71.4 (S)</cell></row><row><cell>FC-AE(Baseline)</cell><cell>84.76</cell><cell>0.52</cell><cell>270KB</cell><cell>4.7KB</cell><cell>10.3 (M)</cell></row><row><cell>FC-AE(Wide)</cell><cell>87.1</cell><cell>4.47</cell><cell>2.2MB</cell><cell>4.7KB*</cell><cell>ND</cell></row><row><cell>Conv-AE</cell><cell>91.77</cell><cell>578</cell><cell>4.1MB*</cell><cell>160KB*</cell><cell>ND</cell></row><row><cell>MBNETV2-0.5AD</cell><cell>97.24*</cell><cell>31.1</cell><cell>965KB</cell><cell>206KB</cell><cell>98.8 (L)</cell></row><row><cell cols="6">* denotes estimated value. ND denotes not deployable on MCU.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Fedorov, I., Adams, R. P., Mattina, M., and Whatmough, P.</figDesc><table><row><cell>Ethos-U55.</cell><cell cols="2">Arm ethos-u55.</cell><cell>URL https:</cell></row><row><cell cols="4">//developer.arm.com/ip-products/</cell></row><row><cell cols="4">processors/machine-learning/</cell></row><row><cell cols="2">ethos-u55.</cell><cell></cell></row><row><cell>F446RE.</cell><cell></cell><cell cols="2">STM32F446RE Microcontroller</cell></row><row><cell>.</cell><cell>URL</cell><cell cols="2">https://www.st.com/en/</cell></row><row><cell cols="4">microcontrollers-microprocessors/</cell></row><row><cell cols="3">stm32f446re.html.</cell></row><row><cell>F746ZG.</cell><cell></cell><cell cols="2">STM32F746ZG</cell><cell>Microcon-</cell></row><row><cell>troller .</cell><cell></cell><cell cols="2">URL https://www.st.com/</cell></row><row><cell cols="4">content/st_com/en/products/</cell></row><row><cell cols="4">microcontrollers-microprocessors/</cell></row><row><cell cols="4">stm32-32-bit-arm-cortex-mcus/</cell></row><row><cell cols="4">stm32-high-performance-mcus/</cell></row><row><cell cols="4">stm32f7-series/stm32f7x6/stm32f746zg.</cell></row><row><cell>html.</cell><cell></cell><cell></cell></row><row><cell>F767ZI.</cell><cell></cell><cell>STM32F767ZI</cell><cell>Microcontroller</cell></row><row><cell>.</cell><cell>URL</cell><cell cols="2">https://www.st.com/en/</cell></row><row><cell cols="4">microcontrollers-microprocessors/</cell></row><row><cell cols="3">stm32f767zi.html.</cell><cell>There-</cell></row><row><cell cols="4">fore, we use operation count as a proxy for both latency</cell></row><row><cell cols="4">and energy, and setup a differentiable NAS search to design</cell></row><row><cell cols="4">a family of models called MicroNets. MicroNet models</cell></row><row><cell cols="4">optimized for multiple MCUs demonstrate state-of-the-art</cell></row><row><cell cols="4">performance on all three tinyMLperf tasks: visual wake</cell></row></table><note>SpArSe: Sparse architecture search for cnns on resource- constrained microcontrollers. In Advances in Neural Information Processing Systems, pp. 4977-4989, 2019.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 .</head><label>4</label><figDesc>Results</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 .</head><label>5</label><figDesc>Model architectures for keyword spotting and anomaly detection MicroNet models, h and w are height and width of the convolutional filters, c is the number of output channels and s is the stride.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Architecture</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Excluding RNN layers, not currently supported in TFLM. 2 A single multiply-accumulate is defined as two operations.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was sponsored in part by the ADA (Applications Driving Architectures) Center.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Ambiq Micro Subthreshold Power Optimized Technology (SPOT)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ambiq</surname></persName>
		</author>
		<ptr target="https://ambiq.com/technology/spot/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Performance-oriented neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dahyot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregg</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Banbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fazel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Holleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hurtado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kanter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lokhmotov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04821</idno>
		<title level="m">Benchmarking tinyml systems: Challenges and direction</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1812.00332.pdf" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cmix-nn: Mixed low-precision cnn library for memoryconstrained edge devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Capotondi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rusci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fariselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems II: Express Briefs</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="871" to="875" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Small-footprint keyword spotting with graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Structural wired neural architecture search for internet of things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Teague</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Msnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Temporal convolution for real-time keyword spotting on mobile devices. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Byun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kersner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ha</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1904.03814" />
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rhodes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05721</idno>
		<title level="m">Visual wake words dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tensorflow lite micro: Embedded machine learning on tinyml systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jeffries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kreeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Nappier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Natraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Regev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08678</idno>
		<ptr target="http://dcase.community/challenge2020/task-unsupervised-detection-of-anomalous-sounds" />
	</analytic>
	<monogr>
		<title level="m">2020. DCASE. Dcase2020 challenge task 2</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Network pruning via transformable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="759" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">High performance dsp for vision, imaging and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Efland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sanghavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farooqui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hot Chips Symposium</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Embedded learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ell</surname></persName>
		</author>
		<ptr target="https://microsoft.github.io/ELL/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v20/18-598.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">55</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learned step size quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mckinstry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bablani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appuswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkgO66VKDS" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient Neural Speech Enhancement for Hearing Aids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mandell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tinylstms</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2020-1864</idno>
		<idno>doi: 10.21437/ Interspeech.2020-1864</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2020-1864" />
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2020</title>
		<meeting>Interspeech 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4054" to="4058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gap-8: A risc-v soc for ai at the edge of the iot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Flamand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Conti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pullini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rotenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 29th International Conference on Application-specific Systems, Architectures and Processors (ASAP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unsupervised anomalous sound detection using self-supervised classification and group masked autoencoder for density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Tenneti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Helwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnaswamy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-07" />
			<pubPlace>DCASE2020 Challenge</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ternary hybrid neural-tree networks for highly constrained iot applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dasika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ternary mobilenets via per-layer hybrid filter banks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Protonn: Compressed and accurate knn for resource-scarce devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G S</forename><surname>Suggala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Simhadri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Udupa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<idno>PMLR 70</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Memory-optimal direct convolutions for maximizing classification accuracy in embedded applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gural</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Murmann</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">of Proceedings of Machine Learning Research</title>
		<meeting><address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Arm Helium Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Helium</surname></persName>
		</author>
		<ptr target="https://developer.arm.com/architectures/instruction-sets/simd-isas/helium" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15663" to="15674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Computer Architecture, Fifth Edition: A Quantitative Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Hennessy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<idno>012383872X</idno>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>5th edition</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pushing the envelope of dynamic spatial gating technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3417313.3429380</idno>
		<ptr target="https://doi.org/10.1145/3417313.3429380" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Workshop on Challenges in Artificial Intelligence and Machine Learning for Internet of Things, AIChallengeIoT &apos;20</title>
		<meeting>the 2nd International Workshop on Challenges in Artificial Intelligence and Machine Learning for Internet of Things, AIChallengeIoT &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Preprocessed Anomaly Detection Dataset hosted on Kaggle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Kaggle</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/daisukelab/dc2020task2prep" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Applications of Deep Neural Networks for Ultra Low Power IoT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kodali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mulholland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCD.2017.102</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Design (ICCD)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="589" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Quantizing deep convolutional networks for efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08342</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">A whitepaper. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Neural Information Processing Systems</title>
		<meeting>the 25th International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Resource-efficient machine learning in 2 kb ram for the internet of things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1935" to="1944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jmlr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Org</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fastgrnn: A fast, accurate, stable and tiny kilobyte sized gated recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<ptr target="URLall_papers/KusupatiSBKJV18.pdf.slides/fastgrnn.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-first Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Thirty-first Annual Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9031" to="9042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cmsis-Nn</surname></persName>
		</author>
		<idno>abs/1801.06601</idno>
		<ptr target="http://arxiv.org/abs/1801.06601" />
		<title level="m">efficient neural network kernels for arm cortex-m cpus. CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Not all ops are created equal! CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
		<idno>abs/1801.04326</idno>
		<ptr target="http://arxiv.org/abs/1801.04326" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On-Chip Memory Technology Design Space Explorations for Mobile Deep Neural Network Accelerators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bhargav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">56th ACM/IEEE Design Automation Conference (DAC)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcunet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10319</idno>
		<title level="m">Tiny deep learning on iot devices</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1eYHoC5FX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<ptr target="https://developer.arm.com/ip-products/processors/cortex-m/cortex-m7" />
		<title level="m">M7. Arm Cortex-M7</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">S</forename><surname>Mbed</surname></persName>
		</author>
		<ptr target="https://os.mbed.com/mbed-os/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Microtvm</surname></persName>
		</author>
		<ptr target="https://tvm.apache.org/docs/api/python/micro.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salameh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00165</idno>
		<title level="m">Neural architecture search for keyword spotting</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Otii</surname></persName>
		</author>
		<ptr target="https://www.qoitech.com/otii/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Specaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">PROFIT: A novel training method for sub-4-bit mobilenet models. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2008" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ichige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Endo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nikaido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Suefusa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mimii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dataset</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09347</idno>
		<ptr target="https://pytorch.org/" />
		<title level="m">Sound dataset for malfunctioning industrial machine investigation and inspection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep dense and convolutional autoencoders for unsupervised anomaly detection in machine condition sounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Matos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Nunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pilastri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10417</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Leveraging automated mixed-low-precision quantization for tiny edge microcontrollers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rusci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fariselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Capotondi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
		<idno>abs/2008.05124</idno>
		<ptr target="https://arxiv.org/abs/2008" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Leveraging automated mixed-low-precision quantization for tiny edge microcontrollers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rusci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fariselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Capotondi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Tensorflow lite for microcontrollers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tflm</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/lite/microcontrollers" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Run-time efficient rnn compression for inference on edge devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dasika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<idno type="DOI">10.1109/EMC249363.2019.00013</idno>
	</analytic>
	<monogr>
		<title level="m">2019 2nd Workshop on Energy Efficient Machine Learning and Cognitive Computing for Embedded Applications (EMC2)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="26" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Compressing rnns for iot devices by 15-38x using kronecker products. CoRR, abs/1906.02876</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Beu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dasika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1906.02876" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Pushing the limits of RNN compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Beu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dasika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<idno>abs/1910.02558</idno>
		<ptr target="http://arxiv.org/abs/1910.02558" />
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Doping: A technique for efficient compression of lstm models using sparse structured additive matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/2102.07071.pdf.uTensor.URLhttps://utensor.github.io/website/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>To Appear</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Fbnetv2: Differentiable neural architecture search for spatial and channel dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A 28-nm Timing-Error Tolerant Sparse Deep Neural Network Processor for IoT Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Engine</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSSC.2018.2841824</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2722" to="2731" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Efficient Hardware for Mobile Computer Vision via Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Venkataramanaiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fixynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd Conference on Systems and Machine Learning (SysML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Attention condensers for deep speech recognition neural networks on edge devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Famouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavlova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Surana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tinyspeech</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
		<idno>abs/1711.07128</idno>
		<ptr target="http://arxiv.org/abs/1711.07128" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07128</idno>
		<title level="m">Keyword spotting on microcontrollers</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Mobile Machine Learning Hardware at ARM: A Systems-on-Chip (SoC) Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Whatmough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st Conference on Systems and Machine Learning (SysML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">GSC MicroNet-KWS-L</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">248,s:2)-Depthwise Separable Block (h:3,w:3,c:276,s:1) -Depthwise Separable Block (h:3,w:3,c:276,s:1)-Depthwise Separable Block (h:3,w:3,c:248,s:1)-Depthwise Separable Block</title>
	</analytic>
	<monogr>
		<title level="m">Conv2D (h:10,w:4,c:276,s:1)-Depthwise Separable Block</title>
		<meeting><address><addrLine>c</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>:12) GSC MicroNet-KWS-M</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">s:1)-Depthwise Separable Block (h:3,w:3,c:140,s:2)-Depthwise Separable Block (h:3,w:3,c:140,s:1) -Depthwise Separable Block (h:3,w:3,c:140,s:1)-Depthwise Separable Block</title>
		<idno>Conv2D (h:10,w:4,c:140</idno>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<pubPlace>c</pubPlace>
		</imprint>
	</monogr>
	<note>h:3,w:3,c:112,s:1)-Depthwise Separable Block (h:3,w:3,c:196,s:1) -AvgPool. h:25, w:5,s:1)-FC</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Kws-S</forename><surname>Gsc Micronet</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">h:3,w:3,c:112,s:2)-Depthwise Separable Block (h:3,w:3,c:84,s:1) -Depthwise Separable Block (h:3,w:3,c:84,s:1)-Depthwise Separable Block</title>
	</analytic>
	<monogr>
		<title level="m">Conv2D (h:10,w:4,c:84,s:1)-Depthwise Separable Block</title>
		<meeting><address><addrLine>c</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>h:3,w:3,c:84,s:1)-Depthwise Separable Block (h:3,w:3,c:196,s:1) -AvgPool. h:25, w:5,s:1)-FC</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Ad-L</forename><surname>Mimii Micronet</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">s:1)-Depthwise Separable Block (h:3,w:3,c:248,s:2)-Depthwise Separable Block (h:3,w:3,c:276,s:1) -Depthwise Separable Block (h:3,w:3,c:276,s:1)-Depthwise Separable Block</title>
	</analytic>
	<monogr>
		<title level="m">Conv2D</title>
		<imprint/>
	</monogr>
	<note>h:3,w:3,c:276,. h:3,w:3,c:248,s:2)-Depthwise Separable Block (h:3,w:3,c:248,s:2) -AvgPool. h:4, w:4,s:1)-FC(c:4</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<title level="m">MIMII MicroNet-AD-M</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">s:1)-Depthwise Separable Block (h:3,w:3,c:276,s:2)-Depthwise Separable Block (h:3,w:3,c:276,s:1) -Depthwise Separable Block (h:3,w:3,c:276,s:1)-Depthwise Separable Block</title>
	</analytic>
	<monogr>
		<title level="m">Conv2D</title>
		<imprint/>
	</monogr>
	<note>h:3,w:3,c:192,. h:3,w:3,c:276,s:2)-Depthwise Separable Block (h:3,w:3,c:276,s:2) -AvgPool. h:4, w:4,s:1)-FC(c:4</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Ad-S</forename><surname>Mimii Micronet</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">s:1)-Depthwise Separable Block (h:3,w:3,c:164,s:2)-Depthwise Separable Block (h:3,w:3,c:220,s:1) -Depthwise Separable Block</title>
		<idno>Conv2D (h:3,w:3,c:72</idno>
		<imprint/>
	</monogr>
	<note>h:3,w:3,c:276,s:2)-Depthwise Separable Block (h:3,w:3,c:276,s:2) -AvgPool. h:4, w:4,s:1)-FC(c:4</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
