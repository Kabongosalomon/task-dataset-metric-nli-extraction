<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Text Classification with Reinforced Label Assignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Tian</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<email>hanj@illinois.edu2tianjj97@pku.edu.cn3xiangren@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Text Classification with Reinforced Label Assignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While existing hierarchical text classification (HTC) methods attempt to capture label hierarchies for model training, they either make local decisions regarding each label or completely ignore the hierarchy information during inference. To solve the mismatch between training and inference as well as modeling label dependencies in a more principled way, we formulate HTC as a Markov decision process and propose to learn a Label Assignment Policy via deep reinforcement learning to determine where to place an object and when to stop the assignment process. The proposed method, HiLAP, explores the hierarchy during both training and inference time in a consistent manner and makes inter-dependent decisions. As a general framework, HiLAP can incorporate different neural encoders as base models for end-to-end training. Experiments on five public datasets and four base models show that HiLAP yields an average improvement of 33.4% in Macro-F1 over flat classifiers and outperforms state-of-the-art HTC methods by a large margin. 1 1 Data and code can be found at https://github.com/ morningmoni/HiLAP. STOP is taken t = 0 t = 1 t = 2 t = 6 0 ROOT</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years there has been a surge of interest in leveraging hierarchies (taxonomies) to organize objects (e.g., documents), leading to the development of hierarchical text classification (HTC)-a task that aims to predict for an object multiple appropriate labels in a given label hierarchy, which together constitute a sub-tree. HTC methods have found a wide range of applications such as question answering <ref type="bibr" target="#b25">(Qu et al., 2012)</ref>, online advertising <ref type="bibr" target="#b0">(Agrawal et al., 2013)</ref>, and scientific literature organization <ref type="bibr" target="#b22">(Peng et al., 2016)</ref>. In contrast to "flat" classification, the key challenges of HTC <ref type="figure">Figure 1</ref>: We aim at consistent, multi-path, and nonmandatory leaf node prediction. For a Caribbean restaurant with a beer bar, inconsistent prediction may place it to node "Beer Bars" but not "Bars", which contradicts with each other; Single-path prediction may only recognize that it is a beer bar; Mandatory leaf node prediction would have to assign a leaf node "Dominican" even if the nation of the cuisine is uncertain. lie in modeling the large-scale, imbalanced, and in particular, structured label space.</p><p>Based on how the hierarchy is explored, HTC methods can be summarized into flat, local, and global approaches <ref type="bibr" target="#b29">(Silla and Freitas, 2011)</ref>. Flat approaches <ref type="bibr" target="#b10">(Hayete and Bienkowska, 2005;</ref><ref type="bibr" target="#b11">Johnson and Zhang, 2014</ref>) assume all the labels in the given hierarchy are independent. Some predict labels at the leaf nodes and heuristically add their ancestor labels, which is problematic as the labels of some objects may not be at the leaf nodes (nonmandatory leaf node prediction, see <ref type="figure">Fig. 1</ref>) and all the non-leaf nodes are completely neglected. Some simply ignore the hierarchy and perform standard multi-label classification, in which label inconsistencies (one label is predicted positive but its ancestors are not) may occur and post-processing is needed to correct such contradictions. Local approaches <ref type="bibr" target="#b13">(Koller and Sahami, 1997;</ref><ref type="bibr" target="#b7">Cesa-Bianchi et al., 2006)</ref> train a set of local classifiers that function independently and predictions are usually made in a top-down order: one node is visited if and only if its ancestors have <ref type="figure">Figure 2</ref>: An illustrative example of the label assignment policy. At t = 0, x i is placed at the root label and the policy would decide if x i should be placed to its two children <ref type="bibr">(red)</ref>. At t = 1, x i is placed at label "Restaurants", which adds its three children as the candidates. At t = 6, the stop action is taken and the label assignment is thus terminated. We then take all the labels where x i has been placed (blue) as x i 's labels. been predicted positive. One critical issue is that the number of local classifiers depends on the size of the label hierarchy, making local approaches infeasible to scale.</p><p>Global approaches use one single classifier and model the label hierarchy more explicitly. Traditional global approaches <ref type="bibr" target="#b34">(Wang et al., 2001;</ref><ref type="bibr" target="#b30">Silla Jr and Freitas, 2009</ref>) are largely based on specific flat models and often make unrealistic assumptions <ref type="bibr" target="#b4">(Cai and Hofmann, 2004)</ref> as in flat approaches. Recent neural approaches <ref type="bibr" target="#b12">(Kim, 2014;</ref><ref type="bibr" target="#b38">Yang et al., 2016)</ref> mainly focus on flat classification while their performance in HTC is relatively less studied. Even if the classification is supposed to be hierarchical, prior work <ref type="bibr" target="#b9">(Gopal and Yang, 2013;</ref><ref type="bibr" target="#b11">Johnson and Zhang, 2014;</ref><ref type="bibr" target="#b21">Peng et al., 2018)</ref> still make flat and independent predictions or utilize simple constraints without considering the holistic quality of label assignment. One recent framework <ref type="bibr" target="#b35">(Wehrmann et al., 2018)</ref> attempts to leverage both local and global information but it uses static features as input and its inference process is still flat.</p><p>In this paper, we formulate HTC as a Markov decision process to better capture label dependencies and measure the holistic quality of label assignment. We present HiLAP, a global framework that learns a label assignment policy to determine where to place the objects and when to stop the assignment process. HiLAP explores the label hierarchy during both training and inference in a consistent manner, which alleviates the exposure bias often found in prior local and global approaches. By learning when to stop, HiLAP is more flexible than approaches that only support mandatory leaf node prediction or require thresholding. In addition, HiLAP supports multi-path prediction and its predictions of one object on different paths are inter-dependent, which not only guarantees label consistency but matches the nature of HTC. Furthermore, HiLAP estimates the holistic quality of all the labels assigned to one object via reinforcement learning instead of evaluating each label independently via maximum likelihood as in prior studies. To summarize, HiLAP achieves better effectiveness compared to flat and local approaches as it examines the label hierarchy during both training and inference. HiLAP has more flexibility and generalization capacity than previous global approaches in that it has no constraints on the structure of the hierarchy or the labels of the objects <ref type="bibr" target="#b4">(Cai and Hofmann, 2004)</ref>, generalizes to neural representation learning models <ref type="bibr" target="#b9">(Gopal and Yang, 2013)</ref>, and makes inter-dependent predictions while ensuring label consistency <ref type="bibr" target="#b35">(Wehrmann et al., 2018;</ref><ref type="bibr" target="#b21">Peng et al., 2018)</ref>.</p><p>HiLAP can be combined with various neural encoding models and trained in an end-to-end fashion. In our experiments, we select four representative encoding models as the base models to evaluate the effectiveness of HiLAP. Experimental results on five public datasets from different domains show that combining the base models with HiLAP yields an average performance improvement of 33.4% in Macro-F1 over corresponding flat classifiers and outperforms state-ofthe-art HTC methods by a large margin. In particular, ablation study shows that HiLAP is especially beneficial to those unpopular labels at the bottom levels.</p><p>2 Hierarchical Label Assignment 2.1 Overview Problem Formulation. We define a label hierarchy H = (L, E) as a tree or DAG (directed acyclic graph)-structured hierarchy with a set of nodes (labels) L and a set of edges E indicating the parent-child relation between the labels. Taking a set of objects X = {x 1 , x 2 , ..., x N } and their labels L = {L 1 , L 2 , ..., L N } as input, we aim to <ref type="figure">Figure 3</ref>: The architecture of the proposed framework HiLAP. One CNN model <ref type="bibr" target="#b12">(Kim, 2014)</ref> is used as the base model for illustration. The object embedding e d generated by the base model is combined with the embedding of currently assigned label l t and used as the state representation s t , based on which actions are taken by the policy network. The time corresponds to t = 1 in <ref type="figure">Fig. 2.</ref> learn a label assignment policy P to place each object x i to its labels L i on the label hierarchy H. The label assignment is supposed to be consistent, multi-path, and non-mandatory leaf node prediction (refer to Figs. 1 and 2). We define one base model B as a mapping f that converts raw object x i to a finite dimensional vector, i.e., the object embedding e d ∈ R D . B can be any neural representation learning model and its output e d is used as the input of P for policy learning. The major challenge, compared to standard classification setup, is that we need to model E, i.e., the relation between labels. Our Framework. Prior studies either have a mismatch between training and inference as different routines are followed in the two phases, or compute losses with respect to each individual label and make flat predictions during inference time. In contrast, we learn a policy that (1) makes consistent, inter-dependent predictions by traversing the label hierarchy and maintaining state representation; (2) measures the holistic quality of label assignment via reinforcement learning. Specifically, the policy P puts x i at the root label in the beginning. At each time step, P decides which label x i should be further placed to, among all the children labels of where x i has been placed, until a special stop action is taken. An illustration of how HiLAP labels one object is shown in <ref type="figure">Fig. 2</ref> and the overall architecture of HiLAP is shown in <ref type="figure">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Reinforcement Learning for Hierarchical Label Assignment</head><p>We describe the details of policy learning including its actions, rewards, states, and the policy network in this section. We formulate HTC as a Markov decision process (MDP): at each time step, the agent observes current state, takes an action, and receives a reward. The end goal is to train a policy network to determine where to place the objects and when to stop.</p><p>Actions. Specifically, we regard the process of placing an object x i to the right positions on the label hierarchy as making a sequence of actions, where an action a t at time step t is to select one label l t+1 from the action space A t and place x i to that label l t+1 . We denote the children of label l t as C(l t ). At the beginning of each episode, x i is placed at the root label l 0 and the action space A 0 = C(l 0 ), i.e., all the labels at level 1. When x i is placed at another label l 1 , its children C(l 1 ) are then added to the action space A 1 while l 1 itself is removed. In addition, one stop action with embedding e stop ∈ R C is included in the action space so that the model can automatically learn when to stop placing object x i to new labels. Intuitively, when the confidence of placing x i to another label is lower than the stop action, the label assignment process would be terminated. In short, the action space A t consists of all the unvisited children labels of where the object x i has been placed and the stop action. One distinction of HiLAP is that it takes the inter-dependencies of labels across different paths and levels into consideration while previous approaches make independent predictions on different paths. For example, HiLAP can first place x i to a label at level 3 if the probability of that label is high and then place it to another label at level 1 on another path.</p><p>Rewards. The agent receives scalar rewards as feedback for its actions. Different from exist-ing work where each label of one example 2 is treated independently, HiLAP measures the quality of all the labels assigned to each example x i by rewarding the agent with the Example-based F1 (see Sec. 4.1 for details of this metric). Intuitively, the agent would realize how similar the assigned and the ground-truth labels of one example are. Instead of waiting until the end of the label assignment process and comparing the predicted labels with the gold labels, we use reward shaping <ref type="bibr" target="#b18">(Mao et al., 2018)</ref>, i.e., giving intermediate rewards at each time step, to accelerate the learning process. Specifically, we set the reward r of x i at time step t to be the difference of Example-based F1 scores between current and the last time step:</p><formula xml:id="formula_0">r x i t = F1 x i t − F1 x i t−1 .</formula><p>If current F1 is better than that at the last time step, the reward would be positive, and vice versa. The cumulative reward from current time step to the end of an episode would cancel the intermediate rewards and thus reflect whether the current action improves the holistic label assignment or not. As a result, the learned policy would not focus on the current placement but have a long-term view that takes following actions into account.</p><p>States and Policy Network. We parameterize action a t by a policy network π(a | s; W). For each object, its representation e d is generated by the base model B. For each label, a label embedding l ∈ R C is randomly initialized and updated during training. The embeddings of the object e d and currently assigned label l t are concatenated and projected to a vector s t ∈ R C via a two-layer feedforward network. s t has the same size as the label embedding l and is used as the state representation at time step t. By stacking the action embeddings (i.e., the embeddings of candidate labels and stop action), we obtain an action matrix A t with size |A| × C. A t is multiplied with the state embedding s t , which outputs the probability distribution of actions. Finally, an action a t is sampled based on the probability distribution of the action space.</p><formula xml:id="formula_1">s t = ReLU(W 1 l ReLU(W 2 l [e d ; l t ])), π(a t | s; W) = softmax(A t s t ), a t ∼ π(a t | s; W).</formula><p>We use policy gradient <ref type="bibr" target="#b36">(Williams, 1992)</ref> as the optimization algorithm. In addition, we adopt a selfcritical training approach <ref type="bibr" target="#b26">(Rennie et al., 2017)</ref>.</p><p>For each object x i , two label assignments are generated:L x i is sampled from the probability distribution, andL x i , the baseline label assignment, is greedily obtained by choosing the action with the highest probability at each time step. We usẽ</p><formula xml:id="formula_2">r x i t = rL x i t − rL x i t</formula><p>as the actual reward, which ensures that the policy network learns to place the object to positions with higher F1 score than the greedy baseline. Formally, we measure the global loss O g as follows.</p><formula xml:id="formula_3">O g = − N i=1 T t=1 logπ x i (a t | s; W) × v x i t ,</formula><p>where v x i j = T t=j γ t−jr x i t is the cumulative future reward at time j and γ ∈ [0, 1] is the discount factor. At the time of inference, we greedily select labels with the highest probability asL x i .</p><p>3 End-to-End Model Learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Top-Down Supervised Pre-Training</head><p>Instead of learning from scratch, we use supervised learning to pre-train HiLAP. We denote the supervised variant as HiLAP-SL. While most parameters of HiLAP-SL are shared and used to initialize HiLAP (except that e stop is randomly initialized), its way of exploring the label hierarchy H is dissimilar.</p><p>The major difference is that HiLAP-SL explores the label hierarchy H in a top-down manner independently. At each time step t, the object goes down one level on the hierarchy and the labels under the same parent are discriminated locally. Specifically, the local per-parent label probability distribution p Local t is estimated as p Local t = σ(C t s t ), where σ denotes the sigmoid function, and C t ∈ R |C(lt)|×C denotes the candidate embeddings of HiLAP-SL, i.e., an embedding matrix consisting of the children of current label l t , rather than all the labels where x i has been placed.</p><p>Another difference is that in HiLAP the actions are sampled and thus might place the objects to incorrect labels, while in HiLAP-SL only the ground-truth positions are traversed during training. Specifically, if there are K(≥ 1) groundtruth labels at the same level, the object embedding e d would be copied K times and losses on the K different paths would be measured independently (see <ref type="figure" target="#fig_2">Fig. 6</ref> in Appendix for illustration). The local loss of HiLAP-SL is defined as</p><formula xml:id="formula_4">O l = T t=0 O t ,</formula><p>where T is the lowest label's level of one example and O t estimates the binary cross entropy over the candidate labels C(l t ):</p><formula xml:id="formula_5">O t = − N i=1 l∈C(l t,i ) L i (l) × logp Local t,i (l) + (1 − L i (l)) × log(1 − p Local t,i (l)), where L i (l) and p Local t,i (l) evaluate label l of x i .</formula><p>Intuitively, HiLAP-SL works as if there were a set of local classifiers, although most of its parameters (except for the label embedding l) are shared by all the labels so that there is no need to train multiple classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Combining Flat, Local, and Global Information for Policy Learning</head><p>We further add a flat component to HiLAP as a regularization of the base model. Specifically, the flat component is a feed-forward network that projects the object embedding e d to a label probability distribution p Flat of all the labels on the hierarchy:</p><formula xml:id="formula_6">p Flat = σ(W Flat e d ).</formula><p>The combination of the base model and the flat component functions the same as a flat model and ensures that the object representation e d has the capability of flat classification. We denote the flat loss that measures the binary cross entropy over all the labels</p><formula xml:id="formula_7">by O f = − N i=1 l∈L L i (l) × logp Flat i (l) + (1 − L i (l)) × log(1 − p Flat i (l)).</formula><p>Combining the flat and local losses, the supervised loss in HiLAP-SL is defined as</p><formula xml:id="formula_8">O SL = λO f + (1 − λ)O l , where λ ∈ [0, 1]</formula><p>is the mixing ratio. Similar to <ref type="bibr" target="#b5">Celikyilmaz et al. (2018)</ref>, we also found that mixing a proportion of the supervised loss is beneficial to the learning process of HiLAP. Further combining the global information O g (i.e., O RL ), the total loss of HiLAP is defined as O = O RL + αO SL , where α is a scaling factor accounting for the difference in magnitude between O RL and O SL . While we do not use the flat component during inference, it helps the representation learning of the base model and improves the performance of both HiLAP-SL and HiLAP (see Sec. 4.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>Datasets. We conduct extensive experiments on five public datasets from various domains (summarized in <ref type="table" target="#tab_0">Table 1</ref> and detailed in Appendix A). The first two datasets are related to news categorization, including <ref type="bibr">RCV1 (Lewis et al., 2004)</ref> and the NYT annotated corpus <ref type="bibr" target="#b28">(Sandhaus, 2008)</ref>. The third dataset is the Yelp Dataset Challenge 2018 3 . We hypothesize that one business can be represented by its reviews and use the reviews to predict business categories. The last two datasets are related to protein functional catalogue (FunCat) and gene ontology (GO) prediction <ref type="bibr" target="#b33">(Vens et al., 2008)</ref>, which are used to test the generalization ability of HiLAP to non-textual data. For all the datasets, the lowest labels of one example may not be at the leaf nodes and there could be multiple labels at each level, making them harder and more realistic than mandatory-leaf or single-path datasets such as IPC (WIPO, 2014) and LSHTC <ref type="bibr" target="#b20">(Partalas et al., 2015)</ref>.</p><p>Evaluation Metrics. We use standard metrics <ref type="bibr" target="#b11">(Johnson and Zhang, 2014;</ref><ref type="bibr" target="#b19">Meng et al., 2018;</ref><ref type="bibr" target="#b21">Peng et al., 2018)</ref> for HTC, including Micro-F1, Macro-F1, and Example-based F1 (EBF) <ref type="bibr" target="#b20">(Partalas et al., 2015;</ref><ref type="bibr" target="#b22">Peng et al., 2016)</ref>. Let T P i , F P i , F N i denote the true positive, false positive, and false negative for the i-th example x i in object set X, respectively. EBF calculates the F1 scores of all the examples independently and averages them. P</p><formula xml:id="formula_9">x i = T P i T P i +F P i , R x i = T P i T P i +F N i , F1 x i = 2P x i ×R x i P x i +R x i , and EBF = 1 N N i=1 F1 x i .</formula><p>Recall that F1 x i is used as the reward in HiLAP.</p><p>Base Models for Feature Encoding. Different from most of existing global HTC methods that rely on pre-specified features <ref type="bibr" target="#b9">(Gopal and Yang, 2013)</ref> as input or build on specific models <ref type="bibr" target="#b4">(Cai and Hofmann, 2004;</ref><ref type="bibr" target="#b33">Vens et al., 2008;</ref><ref type="bibr" target="#b30">Silla Jr and Freitas, 2009</ref>), our framework is trained in an end-to-end manner by leveraging a differentiable feature representation learning model as the base model. Specifically, we use TextCNN <ref type="bibr" target="#b12">(Kim, 2014)</ref>, HAN <ref type="bibr" target="#b38">(Yang et al., 2016)</ref>, bow-CNN (Johnson and Zhang, 2014) on the three textual datasets, and a feed-forward network on the two nontextual datasets. The details of the base models  <ref type="bibr" target="#b21">Peng et al. (2018)</ref> on the same dataset split. Note that the results of HR-SVM reported in <ref type="bibr" target="#b9">Gopal and Yang (2013)</ref> are not comparable as they use a different hierarchy with 137 labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Micro-F1 Macro-F1 EBF are provided in Appendix C due to limited space.</p><p>To incorporate one base model into our framework, we remove its final feed-forward layer that projects the object representation e d to a flat probability distribution of all labels (p Flat ), and use e d directly as the input of HiLAP. As one will see in the later experiments, HiLAP consistently improves the base model by modeling the label hierarchy in an effective manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Compared Methods</head><p>1. Traditional HTC Methods. A major line of work for HTC is Support Vector Machines (SVM) and its hierarchical variants. Specifically, SVM performs standard multi-label classification using one-vs-the-rest (OvR) strategy. Leaf-SVM treats each leaf node as a label and adds the ancestors of predicted leaf nodes. Variants such as HSVM <ref type="bibr" target="#b32">(Tsochantaridis et al., 2005)</ref>, Top-Down SVM (TD-SVM) <ref type="bibr" target="#b17">(Liu et al., 2005)</ref>, and Hierarchically Regularized SVM (HR-SVM) <ref type="bibr" target="#b9">(Gopal and Yang, 2013)</ref> are also tested. Other state-of-theart HTC methods that we compare with include Clus-HMC <ref type="bibr" target="#b33">(Vens et al., 2008)</ref> and CSSA <ref type="bibr" target="#b3">(Bi and Kwok, 2011)</ref>.</p><p>2. Neural HTC Methods. There are not many neural methods that specifically target HTC. We mainly compare with two latest neural models: HR-DGCNN <ref type="bibr" target="#b21">(Peng et al., 2018)</ref>, which extends hierarchical regularization <ref type="bibr" target="#b9">(Gopal and Yang, 2013)</ref> to Graph-CNN and compares favorably to flat models like RCNN <ref type="bibr">(Lai et al.,</ref>  2015) and XML-CNN <ref type="bibr" target="#b16">(Liu et al., 2017)</ref>, and HMCN <ref type="bibr" target="#b35">(Wehrmann et al., 2018)</ref>, which outperforms state-of-the-art HTC methods such as HMC-LMLP <ref type="bibr" target="#b6">(Cerri et al., 2016)</ref>. We also compare with the base models that we use for feature encoding. The main aim is to see how much gain they could obtain by combining each one of them with HiLAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>For datasets without held-out set, we randomly sample 10% from the training set as the validation set following Johnson and Zhang (2014); <ref type="bibr" target="#b21">Peng et al. (2018)</ref>. We only use the first 256 tokens of each document for representation learning. All the models are trained using an Adam optimizer with initial learning rate 1e-3 and weight decay 1e-6. We use GloVe <ref type="bibr" target="#b23">(Pennington et al., 2014)</ref> with size 50 as word embeddings for TextCNN <ref type="bibr" target="#b12">(Kim, 2014)</ref> and HAN <ref type="bibr" target="#b38">(Yang et al., 2016)</ref>. We create a vocabulary of the most frequent 30,000 words in the training data and generate multi-hot vectors as the input of bow-CNN <ref type="bibr" target="#b11">(Johnson and Zhang, 2014)</ref>. For our framework, since the parameter updates are performed after T steps, we cache the object representation e d and reuse it at each step for better efficiency. More details are provided in Appendix D for reproducibility. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Micro-F1</head><p>Macro-F1 Samples-F1 0 2. Comparison using Same Base Models. We compare the performance of different frameworks that support the use of exactly the same base models and summarize the results in <ref type="figure" target="#fig_0">Fig. 4</ref>. 5 Due to the extreme imbalance of the data, directly applying a flat model may suffer from low Macro-F1, i.e., the predictions of flat models are inevitably biased to the most popular labels. HMCN also has the same issue, resulting in Macro-F1 lower than 10 when combining with some base models. In contrast, HiLAP outperforms the baselines significantly in Macro-F1, which implies that our method is bet- <ref type="bibr">4</ref> The results are not comparable with <ref type="bibr" target="#b11">Johnson and Zhang (2014)</ref> due to implementation details and the fact that they tune the threshold for each label using k-fold crossvalidation. See Appendix B for more discussions. <ref type="bibr">5</ref> For HMCN, we replace its static features with the same base model for fair comparison. <ref type="table">Table 4</ref>: Performance comparison on Functional Catalogue and Gene Ontology. We compare with state-of-the-art hierarchical classification methods that take exactly the same raw features as input (i.e., we exclude models designed specifically for text objects).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>FunCat GO  Kwok, 2011), CLUS-HMC <ref type="bibr" target="#b33">(Vens et al., 2008)</ref>, and HMCN <ref type="bibr" target="#b35">(Wehrmann et al., 2018)</ref> on the Fun-Cat and GO datasets, as they represent the stateof-the-art on these datasets. An SVM classifier is also evaluated to better understand the difficulties of the task. We use the same raw features as the input of all the methods for apples-to-apples comparison and list the results in <ref type="table">Table 4</ref>. Note that the metric area under the average precision-recall curve (AUPRC) <ref type="bibr" target="#b35">(Wehrmann et al., 2018)</ref> is not applicable because HiLAP does not use a flat probability distribution of all the labels. As one can see, HiLAP outperforms all the baselines on both datasets by a large margin. In particular, we observe significant improvement on Macro-F1 over the best baseline (47.9% and 53.9%, respectively), which shows that our method is especially better at classifying sparse labels than previous approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Performance Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Ablation Study on Different Framework</head><p>Components. We show the ablation analysis of HiLAP in <ref type="table" target="#tab_6">Table 5</ref>. Using Flat-Only degenerates HiLAP to the flat baseline. By comparing the results of Flat-Only and HiLAP-SL-NoFlat (a variant of HiLAP-SL without flat loss), we further confirm that flat approaches are likely to neglect sparse labels, which results in low Macro-F1. Local approaches (HiLAP-SL-NoFlat), on the other hand, are slightly worse in terms of Micro-F1 and EBF but significantly better on Macro-F1.</p><p>By combining flat and local information, HiLAP-SL achieves performance close to Flat-Only on Micro-F1 and EBF, and even higher Macro-F1 than HiLAP-SL-NoFlat. HiLAP-NoSL is initialized by the pre-trained HiLAP-SL model without mixing the supervised loss during its training. We can see that using the reinforced loss alone still improves the performance on all the three metrics. After removing the flat loss during the training of</p><p>HiLAP, HiLAP-NoFlat shows slightly lower performance than the full HiLAP model, indicating that the flat component serves as a regularization of the base model and is beneficial to the overall performance.</p><p>2. Performance Study on Label Granularity and Popularity. We analyze the sources of performance gains by dividing the labels based on their levels and number of supporting examples. <ref type="figure" target="#fig_1">Fig. 5</ref> shows the absolute Macro-F1 differences between several methods and the base model. We observe similar results for other setups and omit them for a clearer view. As depicted in <ref type="figure" target="#fig_1">Fig. 5</ref>, HiLAP and HiLAP-SL are especially beneficial to unpopular labels (P3) at the bottom levels (L3). 3. Analysis of Label Inconsistency. Label inconsistencies often happen in approaches that perform flat inference, but they are not measured by standard evaluation metrics like F1 scores. To provide a picture of how severe the issue is, we further conduct experiments to check the percentage of objects that are predicted with inconsistent labels <ref type="table" target="#tab_7">(Table 6</ref>). We found, for example, 29,186/781,265 (3.74%) predictions of TextCNN have inconsistent on RCV1. In contrast, HiLAP ensures 0% label inconsistency without the need of post-processing, because its predictions are always valid sub-trees of the label hierarchy (refer to <ref type="figure">Fig. 2</ref>). Hierarchical classification approaches have been developed for many applications. For text classification, both traditional methods <ref type="bibr" target="#b15">(Lewis et al., 2004;</ref><ref type="bibr" target="#b9">Gopal and Yang, 2013)</ref> and neural methods <ref type="bibr" target="#b11">(Johnson and Zhang, 2014;</ref><ref type="bibr" target="#b21">Peng et al., 2018)</ref> have been proposed to classify, e.g., the topics of newswire and web content <ref type="bibr" target="#b31">(Sun and Lim, 2001)</ref> or categories of laws and patents <ref type="bibr" target="#b2">(Bi and Kwok, 2015;</ref><ref type="bibr" target="#b4">Cai and Hofmann, 2004;</ref><ref type="bibr" target="#b27">Rousu et al., 2005)</ref>. Many previous studies <ref type="bibr" target="#b17">(Liu et al., 2005;</ref><ref type="bibr" target="#b31">Sun and Lim, 2001</ref>) train a set of local classifiers and make predictions in a top-down manner. In particular, <ref type="bibr" target="#b2">Bi and Kwok (2015)</ref> develop Bayesoptimal predictions that minimize the global risks but their model is still locally trained. Such local approaches are not popularly used among recent neural-based HTC models <ref type="bibr" target="#b11">(Johnson and Zhang, 2014;</ref><ref type="bibr" target="#b21">Peng et al., 2018)</ref> since it is usually infeasible to train many neural classifiers locally. Global methods, on the other hand, train only one classifier. Although global methods are desirable, they are relatively less studied due to the complexity of the problem. Existing global models are generally modified based on specific flat models. Hierarchical-SVM <ref type="bibr" target="#b4">(Cai and Hofmann, 2004;</ref><ref type="bibr" target="#b24">Qiu et al., 2009)</ref> generalizes Support Vector Machine (SVM) learning based on discriminant functions that are structured in a way that mirrors the label hierarchy. One limitation is that Hierarchical-SVM only supports balanced tree (all possible labels are presumed to be at the same level in their experiments). Hierarchical naive Bayes <ref type="bibr" target="#b30">(Silla Jr and Freitas, 2009)</ref> modifies naive Bayes by updating weights of one's ancestors as well whenever one label's weights are updated. There are other global methods that are based on association rules <ref type="bibr" target="#b34">(Wang et al., 2001)</ref>, C4.5 <ref type="bibr" target="#b8">(Clare and King, 2003)</ref>, kernel machines <ref type="bibr" target="#b27">(Rousu et al., 2005)</ref>, and decision tree <ref type="bibr" target="#b33">(Vens et al., 2008)</ref>. Constraints such as the regularization that enforces the parameters of one node and its parent to be similar <ref type="bibr" target="#b9">(Gopal and Yang, 2013)</ref> are also proposed to leverage the label hierarchy while maintaining scalability. However, their use of the label hierarchies is somewhat limited compared with HiLAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We proposed an end-to-end reinforcement learning approach to hierarchical text classification (HTC) where objects are labeled by placing them at the proper positions in the label hierarchy. The proposed framework makes consistent and inter-dependent predictions, in which any neuralbased representation learning model can be used as a base model and a label assignment policy is learned to determine where to place the objects and when to stop the assignment process. Experiments on five public datasets and four base models showed that our approach outperforms stateof-the-art HTC methods significantly. For future work, we will explore the effectiveness of the proposed framework on other base models and forms of data (e.g., images). We will introduce more losses covering other aspects in the objective function to further improve the performance of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Reproducibility Details of Datasets</head><p>In this section, we describe the details of the datasets used in our experiments.</p><p>The RCV1 dataset <ref type="bibr" target="#b15">(Lewis et al., 2004</ref>) is a manually labeled newswire collection of Reuters News from 1996 to 1997. Its news documents are categorized with three aspects: industries, topics, and regions. We follow the original training/test split for RCV1 and use its topic-based label hierarchy for classification as it has been well used in prior work <ref type="bibr" target="#b9">(Gopal and Yang, 2013;</ref><ref type="bibr" target="#b11">Johnson and Zhang, 2014;</ref><ref type="bibr" target="#b21">Peng et al., 2018;</ref><ref type="bibr" target="#b35">Wehrmann et al., 2018)</ref>. There are 103 categories and four levels in total including all labels except for the root label in the hierarchy.</p><p>The NYT annotated corpus <ref type="bibr" target="#b28">(Sandhaus, 2008</ref>) is a collection of New York Times news from 1987 to 2007. Due to its large size, we randomly sampled 36,107 documents from all the news documents, and further split them into training and test set of 25,279 and 10,828 examples, respectively. We use the first three levels in the hierarchy and keep the labels with at least 40 supporting examples.</p><p>For the Yelp dataset, the label hierarchy is taken from the Yelp Business Categories 6 , which <ref type="figure">Fig. 2</ref> is a subset of. For preprocessing, we first removed categories that have fewer than 100 businesses and then businesses that have fewer than 5 reviews. We concatenated (at most) the first 10 reviews of each business as its representation. We set the training/test ratio to 70%/30%, which results in a training set of 87,375 examples and a test set of 37,517 examples. This is an even more challenging task because the reviews are usually written in an informal way and it is more imbalanced than the RCV1 or NYT datasets. For example, label Restaurants has 32,357 businesses in the training set while Retirement Homes has 23.</p><p>For the FunCat and GO datasets, we take the cellcycle data from <ref type="bibr">(Vens et al., 2008) 7</ref> . Compared with the text datasets above, raw features are provided as input for all compared methods. Furthermore, their training data is rather limited while the label space is much larger (4,125 vs. 539). Since there are many labels that do not have any example in either training set or test set, we exclude such labels when calculating Macro-F1. Note that it does not have any effect on the ratio of results from two different methods as the F1 scores of those labels without supporting examples are always zero. The features provided by the datasets are taken as input as they are except that the missing values are replaced with the mean value of corresponding features. All the compared methods take the same raw features for fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Performance Analysis of Baselines</head><p>There are several things to note in terms of the performance of the baselines. First, our results are not comparable to <ref type="bibr" target="#b15">Lewis et al. (2004)</ref>; <ref type="bibr" target="#b11">Johnson and Zhang (2014)</ref> due to implementation details (e.g., we only take the first 256 tokens) and the fact that they tune the threshold for each label using scutfbr <ref type="bibr" target="#b15">(Lewis et al., 2004)</ref>. According to the implementation in LibSVM 8 , the scutfbr threshold tuning algorithm uses two nested 3-fold cross validation for each of the 103 labels and the classifier is trained 3 × 3 × 103 = 927 times, which is infeasible in our case.</p><p>Secondly, we found that the original performance of HMCN <ref type="bibr" target="#b35">(Wehrmann et al., 2018)</ref> is sometimes much lower than expected. After tuning their model, we observed that if we first conduct a weighted sum of the local and global outputs and then apply the sigmoid function, the performance of HMCN becomes much better (see Table 7) than doing them in the opposite order as in <ref type="bibr" target="#b35">Wehrmann et al. (2018)</ref>. In addition, we found that HMCN + HAN <ref type="bibr" target="#b38">(Yang et al., 2016)</ref> would result in extremely low performance. We had to remove HMCN's batch normalization to make it compatible with HAN. Combining HMCN with other base models did not encounter similar issues.</p><p>Thirdly, our implementation of TextCNN <ref type="bibr" target="#b12">(Kim, 2014)</ref> and HAN <ref type="bibr" target="#b38">(Yang et al., 2016)</ref> shows better performance than those reported in <ref type="bibr" target="#b21">Peng et al. (2018)</ref> due to implementation details. A comparison can be found in <ref type="table" target="#tab_9">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details of Base Models</head><p>1. Base Models for Encoding Text Objects. For the text classification datasets, three representative text encoding models with different characteristics are selected as the base models to prove the robustness and versatility of HiLAP. We briefly describe  the base models and the reasons we choose them as follows.</p><p>TextCNN <ref type="bibr" target="#b12">(Kim, 2014</ref>) is a classic convolutional neural network for text classification. In our implementation, TextCNN is composed of one convolutional layer with three kernels of different sizes (3, 4, 5), followed by max pooling, dropout, and fully-connected layers. We choose TextCNN because it is one of the first successful and well used neural-based models for text classification.</p><p>HAN <ref type="bibr" target="#b38">(Yang et al., 2016)</ref> first learns the representation of sentences by feeding words in each sentence to a GRU-based sequence encoder <ref type="bibr" target="#b1">(Bahdanau et al., 2014)</ref> and then feeds the representation of the encoded sentences into another GRUbased sequence encoder, which generates the representation of the whole document. Attention mechanism such as word attention and sentence attention is also used. We choose HAN because it uses RNNs instead of CNNs and is shown to be effective on the flat Yelp Review datasets.</p><p>bow-CNN (Johnson and Zhang, 2014) employs bag of words (multi-hot zero-one vectors) as input to represent text objects and directly applies CNNs to the high-dimensional multi-hot vectors encoding. It learns the representation of small text regions (rather than single words) for use in classification. We choose bow-CNN since it does not use any word embeddings as in TextCNN and HAN. In addition, bow-CNN achieved state-of-the-art performance RCV1 <ref type="bibr" target="#b15">(Lewis et al., 2004)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Base Model for Encoding Raw Features.</head><p>For functional genomics prediction, one feed-forward neural network is used for simplicity as raw features are already provided in the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Reproducibility Details of Implementation</head><p>We implement the base models and HMCN <ref type="bibr" target="#b35">(Wehrmann et al., 2018)</ref> according to the original papers and existing implementations. We use the official implementation of Clus-HMC <ref type="bibr">(Vens et al., 2008) 9</ref> and one opensource implementation of CSSA <ref type="bibr">(Bi and Kwok, 2011) 10</ref> . We use scikit-learn for SVM-based methods. TF-IDF features are used for text classification when raw features are needed as input.</p><p>For our framework, we specify the number of steps in HiLAP-SL to be the number of levels in the label hierarchy. We set the maximum number of steps in HiLAP to be reasonably large (depending on the average number of labels of one object) so that it could explore the hierarchy and learn when to stop by itself. For the purpose of batch training, we convert the original indefinite-horizon MDPs to finite-horizon by adding an absorbing state, i.e., after visiting the most fine-grained label in HiLAP-SL or entering the stop state in HiLAP, it would loop in the current state until the maximum number of steps, waiting for other objects in the same batch to finish.</p><p>We set the size of W 2 l to 500 and the sizes of W 1 l and label embedding l t to 50 in all the text classification datasets and set them to 1,000 in the other datasets. We did not observe clear performance changes when varying the probability of dropout in base models like TextCNN. We set batch size to 32 as it performs well on the validation set and a batch size as large as 128 may cause performance losses. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Figure Illustration</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Performance comparison of different classification frameworks using the same base models. We compare HiLAP with its flat, supervised variants, and HMCN. Results show that HiLAP exhibits consistent improvement over flat classifiers and larger gains than HMCN. SVM uses a set of classifiers. Among all compared methods, HiLAP (bow-CNN) achieves the best performance on all the three metrics. 4 On NYT, similar results are observed: TextCNN and HAN are both improved when combining with HiLAP and HiLAP (bow-CNN) again achieves the best performance. On Yelp, HiLAP (HAN) achieves the best Micro-F1 and EBF, while HiLAP (bow-CNN) obtains the highest Macro-F1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Performance Study on Label Granularity and Popularity. We compute level-based and popularity-based Macro-F1 gains on NYT with bow-CNN as base model. We denote the levels of the hierarchy with L1, L2, and L3 (left) and divide the labels into three equal sized categories (P1, P2, and P3) in a descending order by their number of examples (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>9Figure 6 :</head><label>6</label><figDesc>https://dtai.cs.kuleuven.be/clus/ 10 https://github.com/sushobhannayak/ cssag One time step in HiLAP-SL. At t = 1, two (K = 2) local per-parent probabilities p Local 1 are measured independently and aggregated in the loss function O 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets. |L| denotes the number of labels in the label hierarchy. Avg(|L i |) and Max(|L i |) denote the average and maximum number of labels of one object, respectively.</figDesc><table><row><cell cols="2">Dataset Hierarchy</cell><cell>|L|</cell><cell cols="4">Avg(|Li|) Max(|Li|) Training Validation</cell><cell>Test</cell></row><row><cell>RCV1</cell><cell>Tree</cell><cell>103</cell><cell>3.24</cell><cell>17</cell><cell>23,149</cell><cell>2,315</cell><cell>781,265</cell></row><row><cell>NYT</cell><cell>Tree</cell><cell>115</cell><cell>2.52</cell><cell>14</cell><cell>25,279</cell><cell>2,528</cell><cell>10,828</cell></row><row><cell>Yelp</cell><cell>DAG</cell><cell>539</cell><cell>3.77</cell><cell>32</cell><cell>87,375</cell><cell>8,737</cell><cell>37,265</cell></row><row><cell>FunCat</cell><cell>Tree</cell><cell>499</cell><cell>8.76</cell><cell>45</cell><cell>1,628</cell><cell>848</cell><cell>1,281</cell></row><row><cell>GO</cell><cell>DAG</cell><cell>4,125</cell><cell>34.9</cell><cell>141</cell><cell>1,625</cell><cell>848</cell><cell>1,278</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison on RCV1. * denotes the results reported in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison on the NYT and Yelp datasets. We mainly compare with competitive baselines that perform well on RCV1.</figDesc><table><row><cell>Method</cell><cell></cell><cell>NYT</cell><cell></cell><cell></cell><cell>Yelp</cell><cell></cell></row><row><cell></cell><cell cols="6">Micro-F1 Macro-F1 EBF Micro-F1 Macro-F1 EBF</cell></row><row><cell>SVM</cell><cell>72.4</cell><cell>37.1</cell><cell>74.0</cell><cell>66.9</cell><cell>36.3</cell><cell>68.0</cell></row><row><cell>TextCNN</cell><cell>69.5</cell><cell>39.5</cell><cell>71.6</cell><cell>62.8</cell><cell>27.3</cell><cell>63.1</cell></row><row><cell>HAN</cell><cell>62.8</cell><cell>22.8</cell><cell>65.5</cell><cell>66.7</cell><cell>29.0</cell><cell>67.9</cell></row><row><cell>bow-CNN</cell><cell>72.9</cell><cell>33.4</cell><cell>74.1</cell><cell>63.6</cell><cell>23.9</cell><cell>63.9</cell></row><row><cell>TD-SVM</cell><cell>73.7</cell><cell>43.7</cell><cell>75.0</cell><cell>67.2</cell><cell>40.5</cell><cell>67.8</cell></row><row><cell>HMCN</cell><cell>72.2</cell><cell>47.4</cell><cell>74.2</cell><cell>66.4</cell><cell>42.7</cell><cell>67.6</cell></row><row><cell>HiLAP (TextCNN)</cell><cell>69.9</cell><cell>43.2</cell><cell>72.8</cell><cell>65.5</cell><cell>37.3</cell><cell>68.4</cell></row><row><cell>HiLAP (HAN)</cell><cell>65.2</cell><cell>28.7</cell><cell>68.0</cell><cell>69.7</cell><cell>38.1</cell><cell>72.4</cell></row><row><cell>HiLAP (bow-CNN)</cell><cell>74.6</cell><cell>51.6</cell><cell>76.6</cell><cell>68.9</cell><cell>42.8</cell><cell>71.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of HiLAP. We evaluate variants of HiLAP using bow-CNN<ref type="bibr" target="#b11">(Johnson and Zhang, 2014)</ref> onRCV1 (Lewis et al., 2004).</figDesc><table><row><cell>Method</cell><cell cols="3">Micro-F1 Macro-F1 EBF</cell></row><row><cell>Flat-Only</cell><cell>82.7</cell><cell>44.7</cell><cell>83.3</cell></row><row><cell>HiLAP-SL-NoFlat</cell><cell>81.0</cell><cell>52.1</cell><cell>81.7</cell></row><row><cell>HiLAP-SL</cell><cell>82.5</cell><cell>55.3</cell><cell>83.0</cell></row><row><cell>HiLAP-NoSL</cell><cell>83.2</cell><cell>59.3</cell><cell>85.0</cell></row><row><cell>HiLAP-NoFlat</cell><cell>83.0</cell><cell>59.8</cell><cell>84.7</cell></row><row><cell>HiLAP</cell><cell>83.3</cell><cell>60.1</cell><cell>85.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Analysis of Label Inconsistency. We compare various methods by the percentage of predictions with inconsistent labels on RCV1<ref type="bibr" target="#b15">(Lewis et al., 2004)</ref>.</figDesc><table><row><cell>SVM</cell><cell cols="3">TextCNN HMCN HiLAP</cell></row><row><cell>4.83%</cell><cell>3.74%</cell><cell>3.84%</cell><cell>0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Comparison of different implementations of HMCN.</figDesc><table><row><cell>Model</cell><cell cols="9">RCV1 Micro-F1 Macro-F1 EBF Micro-F1 Macro-F1 EBF Micro-F1 Macro-F1 EBF Yelp NYT</cell></row><row><cell>HMCN (original)</cell><cell>78.2</cell><cell>33.2</cell><cell>78.9</cell><cell>56.3</cell><cell>8.5</cell><cell>57.3</cell><cell>62.1</cell><cell>32.4</cell><cell>62.7</cell></row><row><cell>HMCN (ours)</cell><cell>80.8</cell><cell>54.6</cell><cell>82.2</cell><cell>66.4</cell><cell>42.7</cell><cell>67.6</cell><cell>72.2</cell><cell>47.4</cell><cell>74.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Comparison of different implementations of HAN and TextCNN on the RCV1 dataset.</figDesc><table><row><cell>Model</cell><cell cols="2">Micro-F1 Macro-F1</cell></row><row><cell>TextCNN (in Peng et al. (2018))</cell><cell>73.2</cell><cell>39.9</cell></row><row><cell>TextCNN (ours)</cell><cell>76.6</cell><cell>43.0</cell></row><row><cell>HAN (in Peng et al. (2018))</cell><cell>69.6</cell><cell>32.7</cell></row><row><cell>HAN (ours)</cell><cell>75.3</cell><cell>40.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use "example" and "object" interchangeably.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.yelp.com/dataset/ challenge</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. Results on Functional Genomics Prediction. We compare HiLAP with CSSA(Bi and   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://www.yelp.com/developers/ documentation/v3/all_category_list 7 https://dtai.cs.kuleuven.be/clus/ hmcdatasets/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://www.csie.ntu.edu.tw/˜cjlin/ libsvmtools/multilabel/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Research was sponsored in part by U.S. Army Research Lab under Cooperative Agreement No. W911NF-09-2-0053 (NSCTA), DARPA under Agreement No. W911NF-17-C-0099, National Science Foundation IIS 16-18481, IIS 17-04532, and IIS-17-41317, grant 1U54GM-114838 awarded by NIGMS, National Science Foundation SMA 18-29268, DARPA MCS and GAILA, IARPA BETTER, Schmidt Family Foundation, Amazon Faculty Award, Google Research Award, Snapchat Gift, and JP Morgan AI Research Award. We thank Chao Zhang, Xiao-Yang Liu, Qingrong Chen, Jun Yan, collaborators in the INK research lab, and anonymous reviewers for their help and valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Archit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashoteja</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bayes-optimal hierarchical multilabel classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jame</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2907" to="2918" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-label classification on tree-and dag-structured hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML-11</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical document categorization with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="78" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep communicating agents for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1662" to="1675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reduction strategies for hierarchical multi-label classification in protein function prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Cerri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André Cplf De</forename><surname>Barros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaochu</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">373</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical classification: combining bayes with svm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolò</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Gentile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Zaniboni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting gene function in saccharomyces cerevisiae</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Clare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="42" to="49" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recursive regularization for large-scale classification with hierarchical and graphical dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="257" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gotrees: predicting go associations from protein domain composition using decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Hayete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jadwiga</forename><forename type="middle">R</forename><surname>Bienkowska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biocomputing 2005</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="127" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1058</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchically classifying documents using very few words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Sahami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="170" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">333</biblScope>
			<biblScope unit="page" from="2267" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rcv1: A new benchmark collection for text categorization research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning for extreme multi-label text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingzhou</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Support vector machines classification with a very large-scale taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua-Jun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Sigkdd Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="43" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end reinforcement learning for automatic taxonomy induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2462" to="2472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weakly-supervised neural text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">LSHTC: A benchmark for large-scale text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aris</forename><surname>Kosmopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Baskiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Artières</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massih-Reza</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
		<idno>abs/1503.08581</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large-scale hierarchical text classification with recursively regularized deep graph-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaopeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjiao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1063" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepmesh: deep semantic representation for improving large-scale mesh indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghui</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Mamitsuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanfeng</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="70" to="79" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Hierarchical multi-class text categorization with global margin maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="165" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An evaluation of classification models for question topic categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JASIST</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="889" to="903" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning hierarchical multi-category text classification models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Rousu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandor</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="744" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The new york times annotated corpus. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">26752</biblScope>
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A survey of hierarchical classification across different application domains. Data Mining and Knowledge Discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">A</forename><surname>Silla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freitas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="31" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A globalmodel naive bayes approach to the hierarchical prediction of protein functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">A</forename><surname>Carlos N Silla</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM&apos;09</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="992" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical text classification and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1453" to="1484" />
			<date type="published" when="2005-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Decision trees for hierarchical multi-label classification. Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Celine</forename><surname>Vens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Struyf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leander</forename><surname>Schietgat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sašo</forename><surname>Džeroski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Blockeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">185</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hierarchical classification of real life documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senqiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hierarchical multi-label classification networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonatas</forename><surname>Wehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Cerri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Barros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5225" to="5234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">International patent classification (ipc)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ipc Wipo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>Geneve</pubPlace>
		</imprint>
	</monogr>
	<note>World Intellectual Property Organization</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
	<note>Xiaodong He, Alex Smola, and Eduard Hovy</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
