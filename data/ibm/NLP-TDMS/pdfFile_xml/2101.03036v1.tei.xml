<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contextual Non-Local Alignment over Full-Scale Representation for Text-Based Person Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Southern University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanyu</forename><surname>Cai</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Jiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Southern University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Gong</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pai</forename><surname>Peng</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Contextual Non-Local Alignment over Full-Scale Representation for Text-Based Person Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>The code is available at https : / / github . com / TencentYoutuResearch/PersonReID-NAFS</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text-based person search aims at retrieving target person in an image gallery using a descriptive sentence of that person. It is very challenging since modality gap makes effectively extracting discriminative features more difficult. Moreover, the inter-class variance of both pedestrian images and descriptions is small. Hence, comprehensive information is needed to align visual and textual clues across all scales. Most existing methods merely consider the local alignment between images and texts within a single scale (e.g. only global scale or only partial scale) or simply construct alignment at each scale separately. To address this problem, we propose a method that is able to adaptively align image and textual features across all scales, called NAFS (i.e. Non-local Alignment over Full-Scale representations). Firstly, a novel staircase network structure is proposed to extract full-scale image features with better locality. Secondly, a BERT with localityconstrained attention is proposed to obtain representations of descriptions at different scales. Then, instead of separately aligning features at each scale, a novel contextual non-local attention mechanism is applied to simultaneously discover latent alignments across all scales. The experimental results show that our method outperforms the stateof-the-art methods by 5.53% in terms of top-1 and 5.35% in terms of top-5 on text-based person search dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Text-based person search aims at retrieving target person in an image gallery using a descriptive sentence of that person. Compared to classical person re-identification (Reid), it does not need an image of the target person as query which could be difficult to obtain. In addition, text-based person search is more user-friendly since it can support open-form natural language queries. Thus it has the potential to have much broader applications.</p><p>Compared with general image text matching task where an image may contain several objects, text-based person search is a much more challenging task since the high-level semantics among different pedestrian images are very similar, causing small inter-class variance of both pedestrian images and textual descriptions. Thus, in order to explore more distinctive and comprehensive information, text-based person search requires an algorithm to extract image and textual features from all scales. For example, both of the image and textual description in <ref type="figure" target="#fig_0">Figure 1</ref> can be decomposed into representations at different scales. The sentence can be represented as short phrases, such as "black shorts" at scale 3, or longer sub-sentences at scale 2. Similarly, the image can also be partitioned into sub-regions with different sizes at scale 3 and scale 2. Since correct alignment between these image representations and textual representations are the basis of image text matching task, it is essential to represent the image and textual description at all scales. In this paper, we call it full-scale representation. However, the complex relevance at different scales makes it difficult to build a reasonable scheme of alignment. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, in most cases, the alignment occur at similar scales, such as the sub-sentence "a black shirt with a gray cross on it" and the image region in scale 2, and the short phrase "gray cross" and the smaller image region at scale 3. But occasionally alignment could also occur across different scales. For instance, as shown with the red arrows in <ref type="figure" target="#fig_0">Figure 1</ref>, a single word "woman" in scale 3 aligns with the whole image in scale 1. These phenomena illustrate the importance to jointly align image and description both within similar scales and across different scales. Therefore, a reasonable text-based person search method generally contains two key components. One is to learn image and textual representations at all scales in a coarse-to-fine fashion, the other is to explore an appropriate alignment to automatically and adaptively match these representations of different scales.</p><p>Most of the existing works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19]</ref> are unable to fully satisfy the aforementioned two perspectives. On one hand, for multi-scale representations, most methods merely learn representations for images and textual descriptions at a certain scale. Several coarse-grained methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref> focus on learning representations at the global scale, i.e. the whole image and sentence as shown in <ref type="figure" target="#fig_0">Figure 1</ref> Scale 1. Fine-grained methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19]</ref> model the images and textual descriptions at the finest scale, e.g. image regions and short phrases as shown in <ref type="figure" target="#fig_0">Figure 1</ref> Scale 3. Although some fine-grained methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref> consider combining the finest scale with the global scale, they still lack mid-scale information causing some description segments (image regions) fail to correctly align with proper image regions (description segments).</p><p>On the other hand, for the cross-scale alignment, existing methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19]</ref> try to employ pre-defined rules to align images and textual descriptions of different scales . Zhang et al. and Zheng et al. <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30]</ref> only consider the global matching of images and textual descriptions. Some other methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b6">7]</ref> add alignments between short phrases and image regions as shown in <ref type="figure" target="#fig_0">Figure 1</ref> Scale 3, but ignore alignments across different scales. Recently, Niu et al. <ref type="bibr" target="#b18">[19]</ref> further add extra alignments between the whole image and short phrases, as well as small image stripes and the whole sentence. These methods show that utilizing multiscale features can significantly improve performance. However, all of them pre-define several alignment rules among image representations and textual representations of different scales (e.g. global-global, local-local), and build alignment within these fixed scale pairs separately. Hence, it limits the alignment to a certain scope, causing the alignment between image representations and textual representations outside the scale pairs are completely ignored.</p><p>To address above problems, in this paper, we propose a novel text-based person search method that builds fullscale representations for both images and textual representations, and adaptively aligns them across all scales, called NAFS (Non-local Alignment over Full-Scale representations). First, we propose a staircase network with a novel stripe shuffling operation that incorporates better locality to the learned full scale image features. Then a modified BERT language model by adding a locality-constrained attention is adopted to extract full-scale textual features. Next, instead of aligning features under several pre-defined scales (e.g., local-local, global-global), we develop a much more flexible alignment mechanism called contextual nonlocal attention, which is able to jointly take image representations and textual representations from all scales as input then adaptively build the alignment across all scales. Finally, a novel re-ranking algorithm based on the nearest visual neighbors is proposed to further improve the ranking quality.</p><p>The main contributions of this paper can be summarized as follows: (1) A novel staircase CNN network and a local constrained BERT model are specially developed to extract full-scale image and textual representations. (2) A contextual non-local attention mechanism is proposed to adaptively align the learned representations across all scales. <ref type="formula" target="#formula_3">(3)</ref> The proposed framework achieves state-of-the-art results on the challenging dataset CUHK-PEDES <ref type="bibr" target="#b12">[13]</ref>. Extensive ablation studies clearly demonstrate the effectiveness of each component in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Person Re-identification (ReID).</head><p>Generally, most of the ReID methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref> use deep CNNs to extract a global discriminative representation for each person image, while some part-based models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref> try to exploit local information. For example, PCB <ref type="bibr" target="#b21">[22]</ref> horizontally cuts the output feature map into six parts to learn six different local features. MGN <ref type="bibr" target="#b23">[24]</ref> and Pyramid Network <ref type="bibr" target="#b27">[28]</ref> propose a pyramid structured network to extract features in a coarse-to-fine manner. Moreover, some approaches propose to learn local features from local regions with semantic meanings like human part segmentation or pose <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref>. However such methods highly rely on the accuracy of pose estimation and semantic parsing algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Text-Based Person Search.</head><p>Li et al. <ref type="bibr" target="#b12">[13]</ref> first introduce the text-based person search task and propose a GNA-RNN model to learn an affinity score between the query description and the image in the gallery. Later, Li et al. <ref type="bibr" target="#b11">[12]</ref> propose an identity-aware twostage network to efficiently locate simple incorrect matchings and make the result insensitive to changes in sentence structure. In <ref type="bibr" target="#b1">[2]</ref>, a patch-wise word matching model is introduced to exploit the local matching information and obtain the proper affinity between the text and image. Zhang et al. <ref type="bibr" target="#b25">[26]</ref> design cross-modal objective functions for learning discriminative image-text embeddings. Moreover, a new method CMAAM <ref type="bibr" target="#b0">[1]</ref> treats the task as a multi-task training framework that significantly boosts the performance of global features by introducing extra attribute annotation and prediction. Recently, Niu et al. <ref type="bibr" target="#b18">[19]</ref> propose to define three types of alignment, namely global-global, global-local and local-local, and learn separate alignment within these three scale pairs. In addition, Wang et al. <ref type="bibr" target="#b24">[25]</ref> exploit an extra segmentation model to align person partial features and textual attribute features with a k-reciprocal sampling align loss. While, a pose-guided multi-granularity attention network is explored in <ref type="bibr" target="#b6">[7]</ref>, which contains a fine-grained alignment component and a coarse alignment component to exploit multi-granularity cross-modal relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Image Text Matching.</head><p>The goal of general image text matching is to learn a joint latent space where the embeddings of visual inputs and textual annotations can be compared directly. Besides global representations, some state-of-the-art methods including SCAN <ref type="bibr" target="#b9">[10]</ref> and BFAN <ref type="bibr" target="#b15">[16]</ref> also exploit alignment between images and textual fragments such as objects and words. Recently, methods like Unicoder and OS-CAR <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref> propose to use BERT <ref type="bibr" target="#b2">[3]</ref> or transformer <ref type="bibr" target="#b22">[23]</ref> like network to model the text-image matching problem as a binary classification task, improving the retrieval performance at the cost of much longer inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Method</head><p>In this section, we explain the proposed NAFS in detail. First, we introduce the procedures of extracting the visual and textual representations. Then we describe our contextual non-local attention mechanism. Finally, we introduce proposed re-ranking by visual neighbors to further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Extracting Visual Representation</head><p>Staircase Backbone Structure. Firstly, we elaborate on the implementation details of the proposed staircase network. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, it contains three branches, each of which is responsible for extracting visual features at different scales, from coarse to fine, namely the global branch (colored in blue), the region branch (colored in yellow) and the patch branch (colored in green). A general ResNet <ref type="bibr" target="#b3">[4]</ref> network is used as the backbone. 1) The global branch is used to extract global and coarsest features. 2) The region branch extracts finer features from large sub-regions in the image. It takes the feature map at the second stage of global branch as its input, then fed into two consecutive res-blocks to extract features at region scales. The output feature map of the region branch is then horizontally partitioned into n 1 stripes, each of which is further encoded as a local feature of a certain region. 3) The patch branch extracts the finest features from small patches in the image. It takes the feature map at the third stage of global branch as its input, which is then fed into one res-block to extract features at small patch scales. Then we horizontally partition the output feature map into n 2 stripes to extract n 2 feature vectors for local patches.</p><p>Split and Shuffle Operation. A challenge of stripebased ReID models is that due to the large perception field of CNN models, the stripe of feature maps in the deep layers may contain global information as well. Thus, to guarantee a better locality for the multi-scale image features, we introduce a novel split&amp;shuffle operation. It takes the intermediate feature map as input then equally partitions the feature map into several horizontal stripes denoted as a list F ={f 1 , f 2 , · · · , f n }, where f i is the i-th stripe starting from the top of the feature map. Then, such set of the partitioned stripes are randomly shuffled and re-concatenated along the vertical axis to form a complete feature map as the output. Both feature maps at stage 2 and stage 3 will be first split and shuffled before feeding into the range and patch branches, respectively. By shuffling the partitioned stripes randomly, it enables to break the inter-relationship between consecutive stripes so that the model can focus on the information within each stripe. Since our contextual non-local attention does not rely on the order of feature map fragments, it is not necessary to re-organize the partitioned stripes to the original order.</p><p>The visual representation extraction module takes a pedestrian image as input, and then a list of image features of different scales can be obtained and notated as</p><formula xml:id="formula_0">I={i p1 , i p2 , · · · , i pn } where i pi ∈ R D .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Extracting Textual Representation</head><p>Given a textual description E, we add a locality constrain to BERT to extract different scale representations of E. In our method, a textual description will be represented in three scales, separately. 1) At the sentence-level, we add a special classification token ([CLS]) to the beginning of sentence E. The final hidden state corresponding to this token can be used as the sentence-level representation of the whole sentence E in a global view. 2) At the middlelevel, we separate the sentence E by commas resulting in a list of shorter sub-sentences. For every sub-sentence in the list, the [CLS] token is also attached to the beginning of the sub-sentence, whose final hidden state is used as the representation of each sub-sentence as well. 3) At the finest word-level, the final hidden state of each word is directly used as the word-level representation.</p><p>For a common BERT-based model <ref type="bibr" target="#b2">[3]</ref>, the hidden variables of all tokens have the same global perception field. Each token can attend to any tokens in the entire input sentence. To provide locality to representations of sub-regions in a sentence (the [CLS] token for the sub-sentence), we propose a locality-constrained attention module to attend tokens within a certain range. Similar to the original BERT, given the query of a [CLS] token that corresponds to a subsentece, denoted as q CLS , the locality-constrained attention is computed as follows:</p><formula xml:id="formula_1">Attention (q CLS ) = i e q CLS ki i e q CLS ki1(i∈U ) v i 1(i ∈ U ),<label>(1)</label></formula><p>where k i and v i denote keys and values corresponding to all tokens in a sentence, respectively. U is the set of tokens within the range of this sub-sentence, and 1(·) is an indication function that returns 1 when i-th token is in U .</p><p>The textual representation extraction module takes a pedestrian description as input, and then a list of textual embeddings of different scales can be obtained and denoted as T = {t p1 , t p2 , · · · , t pn } where t pi ∈ R D .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Contextual Non-Local Attention Mechanism</head><p>As shown in <ref type="figure" target="#fig_3">Figure 3</ref>, the contextual non-local attention expects two inputs: a set of visual features I = {i p1 , i p2 , · · · , i pm } and a set of textual features T = {t p1 , t p2 , · · · , t pn }. The output of the attention module is a similarity score that measures the relevance of a image-text pair. In a nutshell, the contextual non-local attention enables cross-modal features to align with each other in a coarse-tofine fashion according to their semantics, instead of merely using pre-defined and fixed rules (e.g., local-local, globalglobal).</p><p>Inspired by the spirit of self attention <ref type="bibr" target="#b22">[23]</ref>, we can explain our proposed attention mechanism as mapping a query and a set of key-value pairs to an output. For visual features, two learned linear projections are used to map I to visual queries I Q = {I q1 , I q2 , · · · , I qm } and visual values I V = {i v1 , i v2 , · · · , i vm }. Similarly, two linear projections are explored to map T to textual keys T K = {t k1 , t k2 , · · · , t kn } and textual values T V = {t v1 , t v2 , · · · , t vn }. Based on I Q , I V , T K and T V , we introduce our proposed attention mechanism in both Image-Text and Text-Image ways.</p><p>Image-Text Contextual Non-Local Attention. The proposed image-Text attention module includes two stages. First, each visual query attend to textual keys to get a corresponding attended textual value. Then, considering all visual values and their attended textual values, similarity between a image-text pair can be determined. In detail, to obtain attended textual values, we first compute the cosine similarity matrix of I Q and T K to obtain the weights on T V as follows:</p><formula xml:id="formula_2">s a,b = [ i T qa t kb ||i qa ||||t kb || ] + , a ∈ m, b ∈ n, [x] + = max(x, 0)<label>(2)</label></formula><p>where s a,b denotes the similarity between the a-th visual query and b-th textual key. Further, we normalize it aŝ s a,b = s a,b m a=1 s a,b . Moreover, to filter out irrelevant textual values, a focal attention trick which is similar to <ref type="bibr" target="#b14">[15]</ref> is used, wheres a,b = [ n c=1ŝ a,b −ŝ a,c ] +ŝa,b . Then, we compute the weighted textual values as:</p><formula xml:id="formula_3">r va = n b=1 α a,b t vb , α a,b = exp(λ 1sa,b ) n b=1 exp(λ 1sa,b )<label>(3)</label></formula><p>where λ 1 is the inverse temperature of the softmax function.</p><p>In the second stage, we define the relevance between ath visual value and its corresponding textual context using the cosine similarity between i va and r va :</p><formula xml:id="formula_4">R(i va , r va ) = i T va r va ||i va ||||r va ||<label>(4)</label></formula><p>By averaging all R(i va , r va ), we obtain the similarity of a image-text pair as</p><formula xml:id="formula_5">S(I, T ) = m a=1 R(i va , r va ) m<label>(5)</label></formula><p>As illustrated in our proposed attention mechanism, each visual feature pays more attention to relevant textual features. The relevant textual features may come from a word, a short phrase or a whole sentence, merely depending on whether the visual and textual features share similar semantics. While, instead, previous methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b6">7]</ref> tend to build the correspondence in a fixed way. We relax such constraints by enabling a semantic-based attention mechanism to build correspondence across different scales, which helps us align a image-text pair more adaptively and correctly.</p><p>Text-Image Contextual Non-Local Attention. Similar to the Image-Text contextual non-local attention, we regard textual keys as queries and visual queries as keys, respectively, and attend textual keys with respect to visual queries. Then, with textual values and attended visual values, we compute the similarity between a image-text pair. Specifically, the weight of b-th visual value with respect to a-th textual value is defined as s a,b = [ . Alignment Objective. We introduce an objective function named Cross-Scale Alignment Loss (CSAL) to optimize the proposed algorithm. Given a mini-batch of im-</p><formula xml:id="formula_6">ages {I i } B i=1 , captions {T j } B j=1 and all image-text pairs {(I i , T j ), y i,j } B×B i=1,j=1 where y i,j = 1 if (I i , T j )</formula><p>is a matched pair otherwise 0, we define the image-to-text similarity of (I i , T j ) as S(I i , T j ) and text-to-image similarity as S (T j , I i ). To maximize similarities between the matched pairs and restrain correspondences of unmatched pairs, we define CSAL as:</p><formula xml:id="formula_7">L CSAL =L i + L t , where<label>(6)</label></formula><formula xml:id="formula_8">L i = 1 B B i=1 B j=1 S(I i , T j )log S(I i , T j ) q i,j + , q i,j = y i,j B k=1 y i,k L t = 1 B B j=1 B i=1 S (T j , I i )log S (T j , I i ) q i,j + , q i,j = y k,j B k=1 y k,j</formula><p>where denotes a small number to avoid numerical problems.</p><p>Considering that the backbone is essential to features from multiple scales, we use Cross-Modal Projection Matching (CMPM) L CM P M and Cross-Modal Projection Classification L CM P C (CMPC) proposed by Zhang et al. <ref type="bibr" target="#b25">[26]</ref> to stabilize the training procedure by adding CMPM and CMPC loss on features extracted from the global branch. Thus, the final objective function is:</p><formula xml:id="formula_9">L = λ 2 L CM P M + λ 3 L CM P C + λ 4 L CSAL (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Re-Ranking by Visual Neighbors</head><p>We propose a multi-modal re-ranking algorithm to further improve the performance by comparing the visual neighbors of the query to the gallery (RVN). Given a textual query T , the initial ranking list is obtained by sorting the images based on their similarities to the query obtained by Eq.5. Then, for each image I in the initial list, we obtain its l-nearest neighboring images based on the similarity of their visual representations, denoted as N i2i <ref type="figure">(I, l)</ref>. Similarly, the nearest neighbors of the textual query can be obtained based on the similarity between its textual representations and the visual representation of images, denoted as N t2i (T, l). Here, to accelerate the computation, only the global feature is used for finding nearest neighbors. Then, we re-calculate the pair-wise similarity between the textual query and each image in the gallery by comparing the lnearest neighbors with Jaccard Distance:</p><formula xml:id="formula_10">D J (I, T ) = 1 − N i2i (I, l) N t2i (T, l) N i2i (I, l) N t2i (T, l)<label>(8)</label></formula><p>Finally, the gallery is re-sorted based on the averaged scores of the original similarity and the Jaccard Distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate our proposed NAFS by comparing the person search performance with state-of-theart methods. Furthermore, we conduct ablation studies to demonstrate the effectiveness of each component. Finally, the attentions between images and textual descriptions are visualized to demonstrate NAFS's ability to discover joint alignment across multiple scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Dataset and Evaluation Protocol. We evaluate our proposed model on the CUHK-PEDES dataset, which is currently the only benchmark for text-based person search. It contains 40206 images from 13003 unique person IDs in total. The training set has 34054 images, 11003 person IDs and 68126 textual descriptions. The validation set has 3078 images, 1000 person IDs and 6158 textual descriptions. The test set has 3074 images, 1000 person IDs and 6156 textual descriptions. On average, each image contains 2 different textual descriptions and each textual description is generally longer than 23 words. The vocabulary of the dataset contains 9408 different words. Following the standard evaluation setting, the performance is measured by top-k accuracy (K = 1, 5, 10). Specially, given a person description, if top-k images contain any person corresponding to the given description, the search is successful. Top-k accuracy is the percentage of successful searches among all searches.</p><p>Implementation Details. For the visual representation extraction module, we use ResNet-50 as our backbone for fair comparisons with previous methods. The region branch splits the feature map into two stripes equally and the patch branch splits the feature map into three stripes equally. The number of output strides of the convolution layer at the last stage of the backbone is set to 1. The dimension D of the image features at different scales is 768. We use horizontally flipping (50% probability) as data augmenting. All images are normalized and resized to 384 × 128 before sending into the network. For the textual representation extraction module, we use BERT-Base-Uncased model as our backbone. The dimension D of different scale textual features is set to 768 as well.</p><p>We initialize ResNet-50 with the weights pre-trained on the ImageNet classification task. And we initialize the weights of BERT-Base-Uncased model with the weights pre-trained using a combination of masked language modeling objective and next sentence prediction on a large corpus including the Toronto Book Corpus and Wikipedia. The model is optimized with the Adam <ref type="bibr" target="#b8">[9]</ref> optimizer and the importance hyperparameters of each loss function λ 2 , λ 3 and λ 4 in 7 are 1, 1 and 0.1 respectively. The learning rate <ref type="table">Table 1</ref>. Comparison with state-of-the-art methods. Top-1, top-5 and top-10 accuracies (%) are reported. The best performance is bold. In the second column, "g" stands for global scale, "g+l" stands for global scale and local scale, "m" stands for full-scale representations from coarse to fine. "RVN" stands for our proposed re-ranking by visual neighbors.</p><p>Method Scale Top1 Top5 Top10 GNA-RNN <ref type="bibr">[</ref> for the visual and textual feature extraction branch is set to 0.00011 and for the rest of the network layers is set to 0.0011. The batch size is 64. <ref type="table">Table 1</ref> demonstrates our results compared with stateof-the-art methods on CUHK-PEDES. GNA-RNN, CMCE and PWM+ATH use VGG-16 <ref type="bibr" target="#b19">[20]</ref> as visual representation extraction backbone. While Dual Path, CMPM+CMPC, MIA, PMA, ViTAA and our NAFS use ResNet-50 as visual representation extraction backbone. Overall, our NAFS achieves the highest performance both with and without RVN. In <ref type="table">Table 1</ref>, it can be observed that methods utilizing global and local information achieve better performance than those merely use global information. This verifies the effectiveness of adapting representations at finer scale. Compared with ViTTA, which is the state-of-the-art method using both global and local features, NAFS gains 5.53%, 5.35% and 3.99% performance improvement in terms of top1, top5 and top10 metrics respectively. This clearly verifies the effectiveness of introducing full-scale representation and contextual non-local attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-The-Art Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Extensive Ablation Studies</head><p>Full-Scale Representation and Joint Alignment. We conduct ablation studies to compare the performance of using image and textual representations at different scales. The results of the following three methods are shown in Ta-   . Apart from missing the mid-scale representations, the other components are exactly the same as our proposed method. We select this method to verify the effectiveness of adding finest image and textual representations to text-based person search tasks.</p><p>• Full Scale Features. This is the full implementation of our proposed NAFS, with images and textual representations at three different scales, from coarse to fine. This comparison is to verify the effectiveness of adding representations of mid-level scale (scale 2). <ref type="table">Table 2</ref> shows the performance of using representations under different scales. It is observed that the Top1 performance increases from 55.47 to 56.90 after adding local information. After adding mid-scale information, the top1 performance increases from 56.90 to 59.94. This implies different scale information is beneficial to the alignment procedure.</p><p>In order to verify the effectiveness of introducing joint alignment to the representations across different scales, we compare our joint alignment with methods using pre-define alignment. As shown in <ref type="table" target="#tab_2">Table 3</ref>, separate alignment refers to separately aligning image and text within three scale pairs: the whole image to the whole sentence, large image regions (n1-stripe partition) to sub-sentences and small image regions (n2-stripe partition) to words. From table 3, we observe that our proposed joint alignment outperforms the separate alignment. This verifies that jointly aligning images and textual descriptions across different scales effec- tively boosts the performance of text-based person search. Model Components. We divide our proposed methods into 5 different components and observe the performance improvement by adding each component, as shown in <ref type="table" target="#tab_3">Table  4</ref>:</p><p>• Baseline. The first row of <ref type="table" target="#tab_3">Table 4</ref> is a baseline model without any NAFS components. A standard bi-LSTM <ref type="bibr" target="#b5">[6]</ref> and ResNet-50 is used for feature extraction. CMPM and CMPC loss is used for training.</p><p>• BERT. We replace the bi-LSTM in the baseline with our proposed locality-constrained BERT (denoted as 'BERT' in <ref type="table" target="#tab_3">Table 4</ref>). The features from different scales are concatenated together to obtain one unified feature representation for image text matching. The localityconstrained BERT gains 0.71 performance improvement in terms of top1 accuracy.</p><p>• Staircase Network. The normal ResNet-50 backbone is replaced with the proposed staircase backbone structure that extracts representations at multiple scales. The features from different scales are concatenated together to obtain one unified feature vector for image text matching. The staircase network brings 2.12 performance improvement in terms of top1 accuracy.</p><p>• Contextual Non-Local. Instead of concatenating the multi-scale features, the joint alignment by the contextual non-local attention mechanism is applied, which gives 2.04 performance improvement in terms of top1 accuracy.</p><p>• Split&amp;shuffle. The split and shuffle operation is added to the staircase backbone structure.</p><p>• RVN. Our proposed re-ranking method by visual neighbors is applied after the initial ranking, improving the top1 accuracy by 1.56.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualization Analysis</head><p>To demonstrate NAFS's ability to discover joint alignment across different scales, we visualize the alignment results between textual descriptions and image regions at different scales, which is shown in <ref type="figure" target="#fig_5">Figure 4</ref>. For better visualization of proposed contextual non-Local attention mechanism, we horizontally partition the output feature map into three stripes in region branch and six stripes in patch branch respectively. The image regions highlighted with red and yellow colors have the highest and the second highest attention weights to the corresponding textual descriptions. In the case of two sub-regions with similar attention weights, both of them will be highlighted.</p><p>From <ref type="figure" target="#fig_5">Figure 4</ref>, we observe that NAFS is able to align textual descriptions with image regions across different scales, from coarse to fine. As shown in the top half of <ref type="figure" target="#fig_5">Figure 4</ref>, the whole description aligns with the whole image and the sub-sentence "A girl wearing a sleeveless white shirt" aligns with the image part at scale 2. Word "girl" aligns with the whole image at scale 1, because a person's gender is determined by the clues from the entire image. Words "white" and "shirt" align with the image regions at scale 3 because a small part of the images contains the white shirt. Words like "skirt" and "shoes" align with both scale 3 and scale 2 image regions because the object skirt and shoes exists in both small and mid-level image regions. Similarly, in the bottom half of <ref type="figure" target="#fig_5">Figure 4</ref>, the whole description aligns with the whole image and the sub-sentence "He is wearing a dark jacket with a scarf hanging down" aligns with the image part at scale 2. Word "older" matches the top image stripe at scale 3, because we can tell this man's age by his face, while the word "slim" matches the whole image because we need to example the full body of the person to tell if he is slim. The visualization results verify the effectiveness of proposed joint alignment and the necessity of full-scale representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a novel text-based person search method that conducts joint alignment over full-scale representations, called NAFS. A novel staircase CNN network and a locality-constrained BERT model are proposed to extract multi-scale image and textual representations. A contextual non-local attention mechanism adaptively aligns the learned representations across different scales. Extensive ablation studies on the CUHK-PEDES dataset demonstrate that our approach outperforms state-of-the-art methods by a large margin.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of image text alignment both within similar scales and across different scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The overall framework consists of a stair-case network for visual representation extraction, a locality-constrained BERT for textual representation extraction and a contextual non-local attention module for join alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>The illustration of proposed contextual non-local attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>||t ka ||||i qb || ] + , a ∈ n, b ∈ m. The normalized and focal attended weight is defined as s a,b = [ m c=1ŝ a,b −ŝ a,c ] +ŝ a,b , whereŝ a,b = s a,b n a=1 s a,b . Then, we define the weighted visual value as r va = m b=1 α a,b i vb , where α a,b = exp(λ2s a,b ) m b=1 exp(λ2s a,b ) . Using the weighted visual value r va and textual value, we compute the similarity of them as R(t va , r va ) = t T va r va ||tva||||r va || . The final similarity of a image-text pair is obtained by averaging operation S (T, I) = n a=1 R(tva,r va ) n</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of the joint alignment between words and image regions across different scales. The image regions highlighted in red and yellow color are the ones have the highest and the second highest attention weights to the corresponding words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>A girl wearing a sleeveless white shirt, a short, black skirt, a pair of white shoes and a pair of white socks.</figDesc><table><row><cell cols="2">The whole description</cell><cell cols="2">A girl wearing a sleeveless white shirt</cell><cell>Girl</cell><cell></cell><cell>White</cell><cell>Shirt</cell><cell>Skirt</cell><cell></cell><cell>Shoes</cell><cell></cell></row><row><cell>Scale 1</cell><cell>Scale 2</cell><cell>Scale 2</cell><cell>Scale 3</cell><cell>Scale 1</cell><cell>Scale 3</cell><cell>Scale 3</cell><cell>Scale 3</cell><cell>Scale 2</cell><cell>Scale 3</cell><cell>Scale 2</cell><cell>Scale 3</cell></row><row><cell></cell><cell></cell><cell cols="8">Older slim guy with glasses. He is wearing a dark jacket with a scarf hanging down.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">He is wearing a dark</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">The whole description</cell><cell cols="2">jacket with a scarf</cell><cell>Older</cell><cell></cell><cell>Slim</cell><cell></cell><cell>Jacket</cell><cell>Scarf</cell><cell cols="2">hanging</cell></row><row><cell></cell><cell></cell><cell cols="2">hanging down.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Scale 1</cell><cell>Scale 2</cell><cell>Scale 2</cell><cell>Scale 3</cell><cell>Scale 3</cell><cell>Scale 2</cell><cell>Scale 1</cell><cell>Scale 3</cell><cell>Scale 3</cell><cell>Scale 2</cell><cell>Scale 3</cell><cell>Scale 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell cols="2">Performance Comparison of separate alignment and joint</cell></row><row><cell cols="2">alignment. Top-1, top-5 and top-10 accuracies (%) are reported.</cell></row><row><cell>Method</cell><cell>Top1 Top5 Top10</cell></row><row><cell cols="2">Separate Alignment 57.98 78.22 85.43</cell></row><row><cell>Joint Alignment</cell><cell>59.94 79.86 86.70</cell></row><row><cell>ble 2.</cell><cell></cell></row><row><cell cols="2">• Global Features. This method only extracts global</cell></row><row><cell cols="2">features for the entire images and descriptions. Since</cell></row><row><cell cols="2">contextual non-local attention is not suitable for meth-</cell></row><row><cell cols="2">ods with only global representations, only CMPM and</cell></row><row><cell cols="2">CMPC loss is applied to train the model.</cell></row><row><cell cols="2">• Local + Global Features. Besides the global features,</cell></row><row><cell cols="2">this method further adds the image and text represen-</cell></row><row><cell cols="2">tations at the finest scale (scale 3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Performance comparison of different components in our methods. Top-1, top-5 and top-10 accuracies (%) are reported.</figDesc><table><row><cell cols="6">BERT Staircase Network Contextual Non-local Split&amp;Shuffle RVN Top1 Top5 Top10</cell></row><row><cell>× √ √ √ √ √</cell><cell>× × √ √ √ √</cell><cell>× × × √ √ √</cell><cell>× × × × √ √</cell><cell>× × × × × √</cell><cell>54.76 77.10 84.86 55.47 77.29 84.36 57.59 78.22 85.71 59.63 79.53 86.42 59.94 79.86 86.70 61.50 81.19 87.51</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Text-based person search via attribute-aided matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surbhi</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrish-Nan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2617" to="2625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving text-based person search by spatial matching and adaptive threshold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pose-guided multi-granularity attention network for text-based person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="11189" to="11196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human semantic parsing for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emrah</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhittin</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gökmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Kamasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1062" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stacked cross attention for imagetext matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="201" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unicoder-vl: A universal encoder for vision and language by cross-modal pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11336" to="11344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Identity-aware textual-visual matching with latent co-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Person search with natural language description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1970" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06165</idno>
		<title level="m">Object-semantics aligned pre-training for vision-language tasks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Focus your attention: A bidirectional focal attention network for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Focus your attention: A bidirectional focal attention network for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An-An</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pose transferrable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4099" to="4108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Hao Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving description-based person re-identification by multi-granularity image-text alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3960" to="3969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Vitaa: Visual-textual attributes alignment in person search by natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07327</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep cross-modal projection learning for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spindle net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoqing</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1077" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pyramidal person re-identification via multiloss dynamic training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongqiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8514" to="8522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pose-invariant embedding for deep person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4500" to="4509" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dualpath convolutional image-text embeddings with instance loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Dong</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
