<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SegSort: Segmentation by Discriminative Sorting of Segments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Hwang</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Berkeley</settlement>
									<country>ICSI</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<addrLine>3 Google Research 4 MIT</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
							<email>stellayu@berkeley.edu</email>
							<affiliation key="aff0">
								<address>
									<settlement>Berkeley</settlement>
									<country>ICSI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
							<email>jshi@seas.upenn.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<addrLine>3 Google Research 4 MIT</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
							<email>maxwellcollins@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
							<email>lcchen@google.com</email>
						</author>
						<title level="a" type="main">SegSort: Segmentation by Discriminative Sorting of Segments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Almost all existing deep learning approaches for semantic segmentation tackle this task as a pixel-wise classification problem. Yet humans understand a scene not in terms of pixels, but by decomposing it into perceptual groups and structures that are the basic building blocks of recognition. This motivates us to propose an end-to-end pixelwise metric learning approach that mimics this process. In our approach, the optimal visual representation determines the right segmentation within individual images and associates segments with the same semantic classes across images. The core visual learning problem is therefore to maximize the similarity within segments and minimize the similarity between segments. Given a model trained this way, inference is performed consistently by extracting pixel-wise embeddings and clustering, with the semantic label determined by the majority vote of its nearest neighbors from an annotated set. As a result, we present the SegSort, as a first attempt using deep learning for unsupervised semantic segmentation, achieving 76% performance of its supervised counterpart. When supervision is available, Seg-Sort shows consistent improvements over conventional approaches based on pixel-wise softmax training. Additionally, our approach produces more precise boundaries and consistent region predictions. The proposed SegSort further produces an interpretable result, as each choice of label can be easily understood from the retrieved nearest segments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is usually approached by extending image-wise classification <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b37">38]</ref> to pixel-wise classification, deployed in a fully convolutional fashion <ref type="bibr" target="#b46">[47]</ref>. In contrast, we study the semantic segmentation task in terms of perceiving an image in groups of pixels and associating objects from a large set of images. Particularly, we take the perceptual organization view <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b5">6]</ref> that pixels group by visual similarity and objects form by visual familiarity; con- <ref type="bibr">Figure 1</ref>. Top: Our proposed approach partitions an image in the embedding space into aligned segments (framed in red) and assign the majority labels from retrieved segments (framed in green or pink). Bottom: Our approach presents the first deep learning based unsupervised semantic segmentation (right). If supervised, our approach produces more consistent region predictions and precise boundaries in the supervised setting (middle) compared to its parametric counterpart (left). sequently a representation is developed to best relate pixels and segments to each other in the visual world. Our method, such motivated, not only achieves better supervised semantic segmentation but also presents the first attempt using deep learning for unsupervised semantic segmentation.</p><p>We formulate this intuition as an end-to-end metric learning problem. Each pixel in an image is mapped via a CNN to a point in some visual embedding space, and nearby points in that space indicate pixels belonging to the same segments. From all the segments collected across images, clusters in the embedding space form semantic concepts. In other words, we sort segments with respect to their visual and semantic attributes. The optimal visual representation delivers the right segmentation within individual images and associates segments with the same semantic classes across images, yielding a non-parametric model as its complexity scales with number of segments (exemplars).</p><p>We derive our method based on maximum likelihood estimation of a single equation, resulting in a two-stage Expectation-Maximization (EM) framework. The first stage performs a spherical (von Mises-Fisher) K-Means clustering <ref type="bibr" target="#b3">[4]</ref> for image segmentation. The second stage adapts the E-step for a pixel-to-segment loss to optimize the metric learning CNN.</p><p>As a result, we present the SegSort (Segment Sorting) as a first attempt to apply deep learning for semantic segmentation from the unsupervised perspective. Specifically, we create pseudo segmentation masks aligned with visual cues using a contour detector <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b72">73]</ref> and train the pixelwise embedding network to separate all the segments. The unsupervised SegSort achieves 76% performance of its supervised counterpart. We further show that various visual groups are automatically discovered in our framework.</p><p>When supervision is available (i.e., supervised semantic segmentation), we segment each image with the spherical K-Means clustering and train the network following the same optimization, but incorporated with Neighborhood Components Analysis criterion <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b70">71]</ref> for semantic labels.</p><p>To summarize our major contributions: 1. We present the first end-to-end trained non-parametric approach for supervised semantic segmentation, with performance exceeding its parametric counterparts that are trained with pixel-wise softmax loss. 2. We propose the first unsupervised deep learning approach for semantic segmentation, which achieves 76% performance of its supervised counterpart. 3. Our segmentation results can be easily understood from retrieved nearest segments and readily interpretable. 4. Our approach produces more precise boundaries and more consistent region segmentations compared with parametric pixel-wise prediction approaches. 5. We demonstrate the effectiveness of our method on two challenging datasets, PASCAL VOC 2012 <ref type="bibr" target="#b15">[16]</ref> and Cityscapes <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Segmentation and Clustering. Segmentation involves extracting representations from local patches and clustering them based on different criteria, e.g., fitting mixture models <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b4">5]</ref>, mode-finding <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b3">4]</ref>, or graph partitioning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b77">78]</ref>. The mode-finding algorithms, e.g., mean shift <ref type="bibr" target="#b12">[13]</ref> or K-Means <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b3">4]</ref>, are mostly related. Traditionally, pixels are encoded in a joint spatial-range domain by a single vector with their spatial coordinates and visual features concatenated. Applying mean shift or K-Means filtering can thus converge for each pixel. Spectral graph theory <ref type="bibr" target="#b11">[12]</ref>, and in particular the Normalized Cut <ref type="bibr" target="#b61">[62]</ref> criterion provides a way to further integrate global image information for better segmentation. More recently, superpixel approaches <ref type="bibr" target="#b0">[1]</ref> emerge to be a popular pre-processing step that helps reduce the computation, or can be used to refine the semantic segmentation predictions <ref type="bibr" target="#b19">[20]</ref>. However, the challenge of perceptual organization is to process information from different levels together to form consensus segmentation. Hence, our proposed approach aims to integrate image segmentation and clustering into end-to-end embedding learning for semantic segmentation. Semantic Segmentation. Current state-of-the-art semantic segmentation models are based on Fully Convolutional Networks <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b46">47]</ref>, tackling the problem via pixelwise classification. Given limited local context, it may be ambiguous to correctly classify a single pixel, and thus it is common to resort to multi-scale context information <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b30">31]</ref>. Typical approaches include image pyramids <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b7">8]</ref> and encoderdecoder structures <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b10">11]</ref>. Notably, to better capture multi-scale context, PSPNet <ref type="bibr" target="#b79">[80]</ref> performs spatial pyramid pooling <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b45">46]</ref> at several grid scales, while DeepLab <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b74">75]</ref> applies the ASPP module (Atrous Spatial Pyramid Pooling) consisting of several parallel atrous convolution <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b52">53]</ref> with different rates.</p><p>In this work, we experiment with applying our proposed training algorithm to PSPNet and DeepLabv3+, and show consistent improvements. Before deep learning takes a leap, non-parametric methods for semantic segmentation are explored. In the unsupervised setting, <ref type="bibr" target="#b56">[57]</ref> proposes data-driven boundary and image grouping, formulated with MRF to enhance semantic boundaries; <ref type="bibr" target="#b66">[67]</ref> extracts superpixels before nearest neighbor search; <ref type="bibr" target="#b44">[45]</ref> performs dense SIFT to find dense deformation fields between images to segment and recognize a query image. With supervision, <ref type="bibr" target="#b49">[50]</ref> learns semantic object exemplars for detection and segmentation.</p><p>It is worth noting Kong and Fowlkes <ref type="bibr" target="#b36">[37]</ref> also integrate vMF mean-shift clustering into the semantic segmentation pipeline. However, the clustering with contrastive loss is used for regularizing features and the whole system still relies on softmax loss to produce the final segmentation.</p><p>Our work also bears a similarity to the work Scene Collaging <ref type="bibr" target="#b32">[33]</ref>, which presents a nonparametric scene grammar for parsing the images into segments for which object labels are retrieved from a large dataset of example images. Metric Learning. Metric learning approaches <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b21">22]</ref> have achieved remarkable performance on different vision tasks, such as image retrieval <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b70">71]</ref> and face recognition <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b59">60]</ref>. Such tasks usually involve open world recognition, since classes during testing might be disjoint from the ones in the training set. Metric learning minimizes intraclass variations and maximizes inter-class variations with pairwise losses e.g., contrastive loss <ref type="bibr" target="#b6">[7]</ref> and triplet loss <ref type="bibr" target="#b28">[29]</ref>. Recently, Wu et al. <ref type="bibr" target="#b71">[72]</ref> propose a non-parametric softmax formulation for training feature embeddings to separate every image for unsupervised image recognition and retrieval. <ref type="figure">Figure 2</ref>. The overall training diagram for our proposed framework, Segment Sorting (SegSort), with the vMF clustering <ref type="bibr" target="#b3">[4]</ref>. Given a batch of images (leftmost), we compute pixel-wise embeddings (middle left) from a metric learning segmentation network. Then we segment each image with the vMF clustering (middle right), dubbed pixel sorting. We train the network via the maximum likelihood estimation derived from a mixture of vMF distributions, dubbed segment sorting. In between, we also illustrate how to process pixel-wise features on a hyper-sphere for pixel and segment sorting. A segment (rightmost) is color-framed with its corresponding vMF clustering color if in the displayed images. Unframed segments from different images are associated in the embedding space. The inference is done with the same procedure but using the k-nearest neighbor search to associate segments in the training set.</p><p>The non-parametric softmax is further incorporated with Neighborhood Components Analysis <ref type="bibr" target="#b21">[22]</ref> to improve generalization for supervised image recognition <ref type="bibr" target="#b70">[71]</ref>. An important technical point on metric learning is normalization <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b59">60]</ref> so that features lie on a hypersphere, which is why the vMF distribution is of particular interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our end-to-end learning framework consists of three sequential components: 1) A CNN, e.g., DeepLab <ref type="bibr" target="#b10">[11]</ref>, FCN <ref type="bibr" target="#b46">[47]</ref>, or PSPNet <ref type="bibr" target="#b79">[80]</ref>, that generates pixel-wise embeddings from an image. 2) A clustering method that partitions the pixel-wise embeddings into a fine segmentation, dubbed pixel sorting. 3) A metric learning formulation for separating and grouping the segments into semantic clusters, dubbed segment sorting.</p><p>We start with an assumption that the pixel-wise normalized embeddings from the CNN within a segment follow a von Mises-Fisher (vMF) distribution. We thus formulate the pixel sorting with spherical K-Means clustering and the segment sorting with corresponding maximum likelihood estimation. During inference, the segment sorting is replaced with k-nearest neighbor search. We then apply to each query segment the majority label of retrieved segments.</p><p>We now give a high level mathematical explanation of the entire optimization process. Let V = {v v v i } = {φ(x i )} be the set of pixel embeddings where v v v i is produced by a CNN φ centered at pixel x i . Let Z = {z i } be the image segmentation with k segments, or z i = s indicates if a pixel i belongs to a segment s. Let Θ = {θ zi } be the set of parameters that capture the representative feature of a segment through a predefined distribution f (mixture of vMF here).</p><p>Our main optimization objective can be concluded as:</p><formula xml:id="formula_0">min φ,Z,Θ − log P (V, Z | Θ) = min φ,Z,Θ − i log 1 k f zi (v v v i | θ zi ).</formula><p>(1) In pixel sorting, we use a standard EM framework to find the optimal Z and Θ, with φ fixed. In segment sorting, we adapt the previous E step for loss calculation through a set of images to optimize φ, with Z and Θ fixed. Performing pixel sorting and segment sorting can thus be viewed as a two-stage EM framework.</p><p>This section is organized as follows. We first describe the pixel sorting in Sec. 3.1, which includes a brief review of spherical K-Means clustering and creation of aligned segments. We then derive two forms of the segment sorting loss for segment sorting in Sec. 3.2. Finally, we describe the inference procedure in Sec. 3.3. The overall training diagram is illustrated in <ref type="figure">Fig. 2</ref> and the summarized algorithm can be found in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pixel Sorting</head><p>We briefly review the vMF distribution and its corresponding spherical K-Means clustering algorithm <ref type="bibr" target="#b3">[4]</ref>, which is used to segment an image as pixel sorting.</p><p>We assume the pixel-wise d-dimensional embeddings v v v ∈ S d−1 (CNN's last layer features after normalization) within a segment follow a vMF distribution. vMF distributions are of particular interest as it is one of the simplest distributions with properties analogous to those of the multivariate Gaussian for directional data. Its probability density function is given by</p><formula xml:id="formula_1">f (v v v | µ µ µ, κ) = C d (κ) exp(κµ µ µ v v v),<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">C d (κ) = κ d/2−1 (2π) d/2 I d/2−1 (κ)</formula><p>is the normalizing constant where I r (·) represents the modified Bessel function of the first kind and order r. µ µ µ = i v v v i /|| i v v v i || is the mean direction and κ ≥ 0 is the concentration parameter. Larger κ indicates stronger concentration about µ. In our particular case, we assume a constant κ for all vMF distributions to circumvent the expensive calculation of C d (κ).</p><p>The embeddings of an image with k segments can thus be considered as a mixture of k vMF distributions with a uniform prior, or</p><formula xml:id="formula_3">f (v v v | Θ) = k s=1 1 k f s (v v v | µ µ µ s , κ),<label>(3)</label></formula><p>where Θ = {µ µ µ 1 , · · · , µ µ µ k , κ}. Let z i be the hidden variable that indicates a pixel embedding v v v i belongs to a particular segment s, or</p><formula xml:id="formula_4">z i = s. Let V = {v v v 1 , · · · , v v v k }</formula><p>be the set of pixel embeddings and Z = {z 1 , · · · , z k } be the set of corresponding hidden variables. The log-likelihood of the observed data is thus given by</p><formula xml:id="formula_5">log P (V, Z | Θ) = i log 1 k f zi (v v v i | µ µ µ zi , κ).<label>(4)</label></formula><p>Since Z is unknown, the EM framework is used to estimate this otherwise intractable maximum likelihood, resulting in the spherical K-Means algorithm with an assumption of κ → ∞. This assumption holds if all the embeddings within a region are the same (homogeneous), which will be our training objective described in Sec. 3.2.</p><p>The E-step that maximizes the likelihood of Eqn. 4 is to assign z i = s with a posterior probability <ref type="bibr" target="#b51">[52]</ref>:</p><formula xml:id="formula_6">p(z i = s|v v v i , Θ) = f s (v v v i | Θ) k l=1 f l (v v v l |Θ) .<label>(5)</label></formula><p>In the setting of K-Means, we use hard assignments to</p><formula xml:id="formula_7">update z i , or z i = argmax s p(z i = s | v v v i , Θ) = argmax s µ µ µ s v v v i .</formula><p>We further denote the set of pixels within a segment c as</p><formula xml:id="formula_8">R c ; hence p(z i = c | v v v i , Θ) = 1 if i ∈ R c or 0 otherwise after hard assignments.</formula><p>The M-step that maximizes the expectation of Eqn. 4 can be derived <ref type="bibr" target="#b3">[4]</ref> aŝ</p><formula xml:id="formula_9">µ µ µ c = i v v v i p(z i = c | v v v i , Θ) || i v v v i p(z i = c | v v v i , Θ)|| = i∈Rc v v v i || i∈Rc v v v i || ,<label>(6)</label></formula><p>which is the mean direction of pixel embeddings within segment c. The spherical K-Means clustering is thus done through alternating updates of Z (E-step) and Θ (M-step). One problem of K-Means clustering is the dynamic number of EM steps, which would cause uncertain memory consumption during training. However, we find in practice a small fixed number of EM steps, i.e., 10 iterations, can already produce good segmentations. <ref type="figure">Figure 3</ref>. During supervised training, we partition the proposed segments (left) given the ground truth mask (middle). The yielded segments (right) are thus aligned with ground truth mask. Each aligned segment is labeled (0 or 1) according to ground truth mask. Note that the purple and yellow segments become, respectively, false positive and false negative that help regularize predicted boundaries.</p><p>If we only use embedding features for K-Means clustering, each resulted cluster is often disconnected and scattered. As our goal is to spatially segment an image, we concatenate pixel coordinates with the embeddings so that the K-Means clustering is guided by spatiality.</p><p>Creating Aligned Segments. Segments that are aligned with different visual cues are critical for producing coherent boundaries. However, segments produced by K-Means clustering do not always conform to the ground truth boundaries. If one segment contains different semantic labels, it clearly contradicts our assumption of homogeneous embeddings within a segment. Therefore, we partition a segment given the ground truth mask in the supervised setting so that each segment contains exactly a single semantic label as illustrated in <ref type="figure">Fig. 3</ref>.</p><p>It is easy to see that the segments after partition are always aligned with semantic boundaries. Furthermore, this partition creates small segments of false positives and false negatives which can naturally serve as hard negative examples during loss calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Segment Sorting</head><p>Following our assumption of homogeneous embeddings per segment, the training is therefore to enforce this criterion, which is done by optimizing the CNN parameters for better feature extraction.</p><p>We first define a prototype as the most representative embedding feature of a segment. Since the embeddings in a segment follow a vMF distribution, the mean direction vector µ µ µ c in Eqn. 6 can naturally be used as the prototype.</p><p>In Sec. 3.1, we consider the posterior probability of a pixel embedding v v v i belonging to a segment s with fixed CNN parameters in Eqn. 5. Now we revisit it with free CNN parameters φ and a constant hyperparameter κ:</p><formula xml:id="formula_10">p φ (z i = s | v v v i , Θ) = f s (v v v i |Θ) k l=1 f l (v v v l |Θ) = exp(κµ µ µ s v v v i ) k l=1 exp(κµ µ µ l v v v i ) .<label>(7)</label></formula><p>As both the embedding v v v and prototype µ µ µ are of unit length,</p><formula xml:id="formula_11">the dot product v v v µ µ µ = v v v µ µ µ</formula><p>||v v v|| ||µ µ µ|| becomes the cosine similarity. The numerator indicates the exponential cosine similarity between a pixel embedding v v v i and a particular segment prototype µ µ µ s . The denominator includes the exponential cosine similarities w.r.t. all the segment prototypes. The value of p φ indicates the ratio of pixel embedding v v v i close to segment s compared to all the other segments.</p><p>The training objective is thus to maximize the posterior probability of a pixel embedding belonging to its corresponding segment c obtained from the K-Means clustering. In other words, we want to minimize the following negative log-likelihood, or the vMF loss:</p><formula xml:id="formula_12">L i vMF = − log p φ (c | v v v i , Θ) = − log exp(κµ µ µ c v v v i ) k l=1 exp(κµ µ µ l v v v i )</formula><p>.</p><p>(8) The total loss is the average over all pixels. As a result, minimizing L vMF has two effects: One is expressed by the numerator, where it encourages each pixel embedding to be close to its own segment prototype. The other is from the denominator, where it encourages each embedding feature to be far away from all other segment prototypes.</p><p>Note that this vMF loss does not require any ground truth semantic labels. We can therefore use this loss to train the CNN in an unsupervised setting. As the loss pushes every segment as far away as possible, visually similar segments are forced to stay closer on the hypersphere.</p><p>To make use of ground truth semantic information, we consider soft neighborhood assignments in the Neighborhood Components Analysis <ref type="bibr" target="#b21">[22]</ref>. The idea of soft neighborhood assignments is to encourage the probability of one example selecting its neighbors (excluding itself) of the same category. In our case, we want to encourage the probability of a pixel embedding v v v i selecting any other segment in the same category, denoted as c + , as its neighbors. We can define such probability as follows, adapted from Eqn. 7:</p><formula xml:id="formula_13">p φ (z i = c + | v v v i , Θ) = f c + (v v v i |Θ) l =c f l (v v v l |Θ) = exp(κµ µ µ c + v v v i ) l =c exp(κµ µ µ l v v v i ) , p φ (z i = c | v v v i , Θ) = 0.<label>(9)</label></formula><p>We denote the set of segments {c + } w.r.t. pixel i as C + i . Our final loss function is therefore the negative log total probability of pixel i selecting a neighbor prototype in the same category:</p><formula xml:id="formula_14">L i vMF-N = − log s∈C + i p φ (z i = s | v v v i , Θ) = − log s∈C + i exp(κµ µ µ s v v v i ) l =c exp(κµ µ µ l v v v i ) .<label>(10)</label></formula><p>The total loss is the average over all pixels. Minimizing this loss is to maximize the expected number of pixels correctly classified by associating the right neighbor prototypes. The ground truth labels are thus used for finding the set of sameclass segments C + i w.r.t. pixel i within a mini-batch (and memory banks). If there is no other segment in the same category, we fall back to the previous vMF loss. Since both vMF and vMF-N losses serve the same purpose for grouping and separating segments by optimizing the CNN feature extraction, we dub them segment sorting losses.</p><p>Understandably, an essential component of the segment sorting loss is the existence of semantic neighbor segments (in the numerator) and the abundance of alien segments (in the denominator). That is, the more examples presented at once, the better the optimization. We thus leverage two strategies: 1) We calculate the loss w.r.t. all the segments in the batch as opposed to traditionally image-wise loss function. 2) We use additional memory banks that cache the segment prototypes from previous batches. In our experiments, we cache up to 2 batches. These two strategies help the fragmented segments (produced by segment partition in <ref type="figure">Fig. 3</ref>) connect to other similar segments between different images, or even between different batches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Inference via K-Nearest Neighbor Retrieval</head><p>After training, we calculate and save all the segment prototypes in the training set. We calculate the prototypes using pixels with majority labels within the segments, ignoring other unresolved noisy pixels.</p><p>During inference, we again conduct the K-Means clustering and then perform k-nearest neighbor search for each segment to retrieve the labels from segments in the training set. The ablation study on inference runtime and memory can be found in the supplementary.</p><p>Our overall framework is non-parametric. We use vMF clustering to organize pixel embeddings into segment exemplars, whose number is proportional to number of images in the training set. The embeddings of exemplars are trained with a nearest neighbor criterion such that the inference can be done consistently, resulting in a non-parametric model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we demonstrate the efficacy of our Segment Sorting (SegSort) through experiments and visual analyses. We first describe the experimental setup in Section 4.1. Then we summarize all the quantitative and qualitative results of fully supervised semantic segmentation in Section 4.2. Lastly, we present results of the proposed approach for unsupervised semantic segmentation in Section 4.3. Additional experiments including ablation studies, t-SNE embedding visualization, and qualitative results on Cityscapes can be found in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets. We mainly use two datasets in the experiments, i.e., PASCAL VOC 2012 <ref type="bibr" target="#b15">[16]</ref> and Cityscapes <ref type="bibr" target="#b13">[14]</ref>.</p><p>PASCAL VOC 2012 <ref type="bibr" target="#b15">[16]</ref> segmentation dataset contains 20 object categories and one background class. The original dataset contains 1, 464 (train) / 1, 449 (val) / 1, 456 (test) images. Following the procedure of <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b79">80]</ref>, we augment the training data with the annotations of <ref type="bibr" target="#b24">[25]</ref>, resulting in 10, 582 (train aug) images.</p><p>Cityscapes <ref type="bibr" target="#b13">[14]</ref> is a dataset for semantic urban street scene understanding. 5, 000 high quality pixel-level finely annotated images are divided into training, validation, and testing sets with 2, 975 / 500 / 1, 525 images, respectively. It defines 19 categories containing flat, human, vehicle, construction, object, nature, etc.</p><p>Segmentation Architectures. We use DeepLabv3+ <ref type="bibr" target="#b10">[11]</ref> and PSPNet <ref type="bibr" target="#b79">[80]</ref> as the segmentation architectures, powered by MobileNetV2 <ref type="bibr" target="#b57">[58]</ref> and ResNet101 <ref type="bibr" target="#b26">[27]</ref>, respectively, both of which are pre-trained on ImageNet <ref type="bibr" target="#b37">[38]</ref>.</p><p>We follow closely the training procedures of the base architectures when training the baseline model with the standard pixel-wise softmax cross-entropy loss. The performance of the final model might be slightly worse from what is reported in the original papers mainly due to two reasons: 1) We do not pre-train on any other segmentation dataset, such as MS COCO <ref type="bibr" target="#b43">[44]</ref> dataset. 2) We do not adopt any additional training tricks, such as balance sampling or fine-tuning specific categories.</p><p>Hyper-parameters of SegSort. For all the experiments, we use the following hyper-parameters for training Seg-Sort: The dimension of embeddings is 32. The number of clustering in K-Means are set to 25 and 64 for VOC and Cityscapes, respectively. The EM steps in K-Means are set to 10 and 15 for VOC and Cityscapes, respectively. The concentration constant is set to 10. During inference, we use the same hyper-parameters for K-Means segmentation and 21 nearest neighbors for predicting categories.</p><p>We use different learning rates and iterations for supervised training with SegSort. For VOC 2012, we train the network with initial learning rate 0.002 for 100k iterations on train aug set and with initial learning rate 0.0002 for 30k iterations on train set. For Cityscapes, we train the network with initial learning rate 0.005 for the same 90k iterations as the softmax baseline.</p><p>Training on VOC 2012 requires more iterations than the baseline procedure mainly because most images only contain very few categories while the network can only compare segments in 3 batches (2 batches were cached). We find that enlarging the batch size or increasing memory banks might reduce the training iterations. As a comparison, images from Cityscapes contain ample categories, so the training iterations remain the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Fully Supervised Semantic Segmentation</head><p>VOC 2012: We summarize the quantitative results of fully supervised semantic segmentation on Pascal VOC 2012 <ref type="bibr" target="#b15">[16]</ref> in <ref type="table" target="#tab_0">Table 1</ref>, evaluated using mIoU and boundary evaluation following <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34]</ref> on both validation and testing set.</p><p>We conclude that networks trained with SegSort consistently outperform their parametric counterpart (Softmax) by 1.63 to 2.43% in mIoU and by 4.07 to 7.97% in mean boundary f-measure. (Per-class results can be found in the supplementary.) We notice that SegSort with DeepLabv3+ / MNV2 captures better fine structures, such as in 'bike' and 'mbike' while with PSPNet / ResNet-101 enhances more towards detecting small objects, such as in 'boat' and 'plant'.</p><p>We present the visual comparison in <ref type="figure" target="#fig_0">Fig. 4</ref>  <ref type="table">Table 2</ref>. Per-class results on Cityscapes validation set. We conclude that network trained with SegSort outperforms Softmax consistently. <ref type="figure">Figure 5</ref>. Two examples, correct and incorrect predictions, for segment retrieval for supervised semantic segmentation on VOC 2012 validation set. Query segments (leftmost) are framed by the same color in clustering. (Top) The query segments of rider, horse, and horse outlines can retrieve corresponding semantically relevant segments in the training set. (Bottom) For the failure case, it can be inferred from the retrieved segments that the number tag on the front of bikes is confused by the other number tags or front lights on motorbikes, resulting in false predictions. prominent improvements on thin structures, such as human legs and chair legs. Also, more consistent region predictions can be found when context is critical, such as wheels in motorcycles and big trunk of buses.</p><p>One of the most important features of SegSort is the self-explanatory predictions via nearest neighbor segment retrieval. We therefore demonstrate two examples, correct and incorrect predictions, in <ref type="figure">Fig. 5</ref>. As can be seen, the query segments (on the leftmost) of rider, horse, and horse outlines can retrieve corresponding semantically relevant segments in the training set. For the incorrect example, it can be inferred from the retrieved segments that the number tag on the front of bikes was confused by the other number tags on motorbikes, resulting in false predictions.</p><p>Cityscapes: We summarize the quantitative results of fully supervised semantic segmentation on Cityscapes <ref type="bibr" target="#b13">[14]</ref> in Table 2, evaluated on the validation set. Due to limited space, visual results are included in the supplementary.</p><p>The network trained with SegSort outperforms Softmax consistently. Large objects, e.g., 'bus' and 'truck', are improved thanks to more consistent region predictions while small objects, e.g., 'pole' and 'tlight', are better captured. <ref type="figure">Figure 6</ref>. Training data for unsupervised semantic segmentation. We produce fine segmentations (right), HED-owt-ucm, from the contours (middle) detected by HED <ref type="bibr" target="#b72">[73]</ref>, followed by the procedure in gPb-owt-ucm <ref type="bibr" target="#b48">[49]</ref>.  <ref type="table">Table 3</ref>. Quantitative results related to unsupervised semantic segmentation on Pascal VOC 2012 validation set. Our unsupervised trained network (2 nd row) outperforms the baseline (1 st row) of directly clustering pretrained features using HED-owt-ucm <ref type="bibr" target="#b72">[73]</ref> and achieves 76% performance of its supervised counterpart (5 th row). Also, the network fine-tuned from unsupervised pre-trained embeddings (4 th row) outperforms the one without (3 rd row) in both mIoU and boundary f-measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Unsupervised Semantic Segmentation</head><p>We train the model using our framework without any ground truth labels at any level, pixel-wise or image-wise.</p><p>To adapt our approach for unsupervised semantic segmentation, what we need is a good criterion for segmenting an image along visual boundaries, which serves as a pseudo ground truth mask. There is an array of methods that meet the requirement, e.g., SLIC [1] for super-pixels or gPb-owtucm <ref type="bibr" target="#b1">[2]</ref> for hierarchical segmentation. We choose the HED contour detector <ref type="bibr" target="#b72">[73]</ref> pretrained on BSDS500 dataset <ref type="bibr" target="#b1">[2]</ref>, and follow the procedure in gPb-owt-ucm <ref type="bibr" target="#b1">[2]</ref> to produce the hierarchical segmentation, or HED-owt-ucm ( <ref type="figure">Fig. 6)</ref>.</p><p>We train the PSPNet / ResNet-101 network on the same augmented training set on VOC 2012 as in the supervised setting with the same initial learning rate yet for only 10k iterations. The hyper-parameters remain unchanged.</p><p>Note that the contour detector only provides visual boundaries without any concept of semantic segments, yet through our feature learning with segment sorting, our method discovers segments of common features -semantic segmentation without names.</p><p>For the sake of performance evaluation, we assume there is a separate annotated image set available during inference. For each segment under query, we assign a label by the ma- <ref type="figure">Figure 7</ref>. Segment retrieval results for unsupervised semantic segmentation on VOC 2012 vadlidation set. Query segments (leftmost) are framed by the same color in clustering. As is observed, the embeddings learned by unsupervised SegSort attend to more visual than semantic similarities compared to the supervised setting. Hairs, faces, blue shirts, and wheels are retrieved successfully. The last query segment fails because the texture around knee is more similar to animal skins. jority vote of its nearest neighbors from that annotated set. <ref type="table">Table 3</ref> shows that our unsupervised trained network outperforms the baseline of directly clustering pretrained features using HED-owt-ucm <ref type="bibr" target="#b72">[73]</ref> segmentation and further achieves 76% performance of its supervised counterpart. Together, We also showcase one possible way to make use of the unsupervised learned embedding. The network fine-tuned from unsupervised pre-trained embeddings outperforms the one without. <ref type="figure">Fig. 7</ref> shows the embeddings learned by unsupervised SegSort attend to more visual than semantic similarities compared to the supervised setting because the fine segmentation formed by contour detectors partitions the image into visually consistent segments. Hairs, faces, blue shirts, and wheels are retrieved successfully. The last query segment fails because the texture around the knee is more similar to animal skins.</p><p>Automatic Discovery of Visual Groups. We noticed in the retrieval results that CNNs trained this way can discover visual groups. We wonder if such visual structures actually form different clusters (or fine categories).</p><p>We extract all foreground segments in the training set and perform a nearest neighbor based hierarchical agglomerative clustering algorithm FINCH <ref type="bibr" target="#b58">[59]</ref>. FINCH merges two points if one is the nearest neighbor of the other (with undirectional link). This procedure can be performed recur- <ref type="figure">Figure 8</ref>. We perform a nearest neighbor based hierarchical agglomerative clustering, FINCH <ref type="bibr" target="#b58">[59]</ref> on foreground segment prototypes to discover visual groups. Top two rows show random samples from two clusters at the finest level. Bottom table displays clusters at a coarser level of 16 clusters. We show four representative segments per cluster. sively. We start with 1501 segment prototypes and performs FINCH to produce 252, 57, 16, 3, and 1 clusters after each iteration. We visualize some segment groups at the finest level and a coarser level of 16 clusters in <ref type="figure">Fig. 8</ref>.</p><p>A bigger picture of how the segments relate to each other from t-SNE <ref type="bibr" target="#b47">[48]</ref> can be found in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed an end-to-end pixel-wise metric learning approach that is motivated by perceptual organization. We integrated the two essential components, pixel-level and segment-level sorting, in a unified framework, derived from von Mises-Fisher clustering. We demonstrated the proposed approach consistently improves over the conventional pixel-wise prediction approaches for supervised semantic segmentation. We also presented the first attempt for unsupervised semantic segmentation. Intriguingly, the predictions produced by our approach, correct or not, can be inherently explained by the retrieved nearest segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Supplementary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Algorithm</head><p>We summarize the overall algorithm, trained with supervision, in Algorithm 1. The unsupervised algorithm uses the oversegmentation from object detector as pixel sorting and Equation 8 for loss calculation, which encourages pixel embeddings in each segment to form an isolated cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">t-SNE Embedding Visualization</head><p>We visualize the prototype embeddings in the training set using t-SNE <ref type="bibr" target="#b47">[48]</ref>. We display the results from supervised and unsupervised SegSort in <ref type="figure" target="#fig_2">Figure 9</ref> and 10, respectively. This is done by random sampling 5, 000 prototypes in the training set, reducing the dimension from 32 to 2 using t-SNE <ref type="bibr" target="#b47">[48]</ref>, and placing the corresponding patches on the 2D canvas wherever possible.</p><p>For visualization of supervised SegSort, we observe that most background patches form a large cluster in the center with some small visual clusters. Each stretching arm represents one foreground class, with gradual appearance changes from boundaries to object centers. For examples, For visualization of unsupervised SegSort, we observe that clusters are formed more by visual similarities. The cues for clustering are usually color and texture. For examples, wheels are clustered on the rightmost island while animals on the top. Grass and sky are placed on the bottom.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Study on Inference Latency and Memory</head><p>We analyze, compared to Softmax baseline, SegSort's inference latency and memory as they are of practical concern. We conclude the runtime overhead (7%-22%) is manageable and memory overhead (∼ 1.5%) is negligible.</p><p>We conduct experiments with various k-means iterations and numbers of nearest neighbors to learn how they influence the inference performance, summarized in <ref type="table">Table 4</ref>. All experiments (PSPNet inference at single scale) are done using the same GTX 1080 Ti GPU. The overall GPU memory usage overhead is only ∼1.5% as no extra parameters are introduced. The runtime overhead is 7%-22%. We also notice the most runtime overhead is due to k-means instead of kNN (with 36K prototypes), both of which are computed in GPU. With 4 k-means iterations and 11-NN, our method (with 11% runtime overhead) already improves more than 1.5% mIoU. We believe this latency/accuracy trade-off is reasonable, particularly with the benefits such as interpretability.  <ref type="table">Table 4</ref>. Ablation study on runtime (ms) and GPU memory (MiB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Boundary Evaluation</head><p>We explain how we conduct boundary evaluation on semantic segmentation following <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34]</ref>. We first compute semantic boundaries per category for the semantic predictions and ground truth. We then match boundary pixels between predictions and ground truth with maximum distance of 0.01 of image diagonal length. The per-category results are summarized by precision, recall, and f-measure in <ref type="table" target="#tab_5">Table 5</ref> and <ref type="table">Table 6</ref> on VOC and Cityscapes datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Ablation Study</head><p>We conduct experiments for the ablation study to understand how different components affect the performance of supervised SegSort. We decide the hyper-parameters of our main experiments using the experiences learned from the ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of clusters:</head><p>We study how the number of clusters affects the semantic segmentation performance ( <ref type="figure" target="#fig_1">Figure  11)</ref>. We train and test the DeepLabv3+ / MNV2 network     <ref type="table">Table 6</ref>. Per-class boundary evaluation on Cityscapes validation set with PSPNet architecture. From top to bottom: precision, recall, and f-measure, separated by double lines. <ref type="figure" target="#fig_1">Figure 11</ref>. We show how the number of clusters affects the segmentation performance. The highest performance is at 25 clusters, which are slightly more than the number of categories in the dataset. with <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b63">64</ref>, and 81 clusters in the vMF clustering. The highest performance is at 25 clusters, which are slightly more than the number of categories in the dataset.</p><p>Dimension of embeddings: We study how the dimension of embeddings affects the semantic segmentation performance ( <ref type="figure" target="#fig_1">Figure 12)</ref>. We train and test the DeepLabv3+ / MNV2 network with 8, 16, 32, 64, and 128 embedding dimension. We conclude that as long as the embedding dimension is sufficient, i.e., larger than 8, the performance does not change drastically.</p><p>Number of nearest neighbors: We study how the number of nearest neighbors during inference affects the segmentation performance <ref type="figure" target="#fig_1">(Figure 13</ref>). We train the PSPNet / ResNet-101 network as described in the main paper and test it using 1 to 31 (odd numbered) nearest neighbors. We conclude that the segmentation performance is robust to the number of nearest neighbors as the mIoU spans only 0.4%. <ref type="figure" target="#fig_1">Figure 12</ref>. We study how the dimension of embeddings affects the segmentation performance. We conclude that as long as the embedding dimension is sufficient, i.e., larger than 8, the performance does not change drastically. <ref type="figure" target="#fig_1">Figure 13</ref>. We study how the number of nearest neighbors during inference affects the segmentation performance. We conclude that the segmentation performance is robust to the number of nearest neighbors as the mIoU spans only 0.4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">Visualization and Test Results on Cityscapes</head><p>We present the visual comparison in <ref type="figure" target="#fig_0">Figure 14</ref>. We observe large objects, such as 'bus' and 'truck', are improved thanks to more consistent region predictions while small objects, such as 'pole' and 'tlight', are also better captured.</p><p>We also include the per-category segmentation performance on Cityscapes test set in <ref type="table">Table 7</ref>. We observe similar performance trends as on the validation set. We conclude that network trained with SegSort outperforms Softmax consistently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Per-category results on VOC</head><p>We present the per-category results in <ref type="table" target="#tab_6">Table 8</ref> for interested readers. We notice that SegSort with DeepLabv3+ / MNV2 captures better fine structures, such as in bike and mbike while with PSPNet / ResNet-101 enhances more towards detecting small objects, such as in boat and plant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">DeepLabv3+ / ResNet-101 Results on VOC</head><p>We also train our supervised SegSort using DeepLabv3+ with ResNet-101 backbone with exactly same hyperparameters as MNV2 backbone. We include the percategory segmentation performance in <ref type="table">Table 9</ref>. Even though the hyper-parameters might not be optimal, we observe consistent performance improvements over the baseline Softmax method.  <ref type="table">Table 9</ref>. Per-class results on Pascal VOC 2012 validation set, using Deeplabv3 + with Resnet-101 backbone. <ref type="figure" target="#fig_0">Figure 14</ref>. Visual comparison on Cityscapes validation set. We observe large objects, such as 'bus' and 'truck', are improved thanks to more consistent region predictions while small objects, such as 'pole' and 'tlight', are also better captured.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>Visual comparison on PASCAL VOC 2012 validation set. We show the visual examples with DeepLabv3+ (upper 2 rows) and PSPNet (lower 2 rows). We observe prominent improvements on thin structures, such as human leg and chair legs. Also, more consistent region predictions can be observed when context is critical, such as wheels in motorcycles and big trunk of buses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Supervised SegSort algorithm. for number of training iterations do Sample a minibatch with m images {x x x (1) , . . . , x x x (m) } and segmentation masks {y y y (1) , . . . , y y y (m) }. Extract the deep feature embedding {v v v (1) , . . . , v v v (m) } for each image. Compute v v v by concatenating v v v with coordinate features. Initialize R R R (segment IDs) by uniformly partitioning each image for k regions. / * K-Means clustering. * / for number of K-Means iterations do / * The M step. * / for each region j do µ µ µ j ← k∈R R Rj v v v k /|| k∈R R Rj v v v k || end / * The E step. * / R R R ← argmax µ µ µ v v v end Partition a segment if it contains multiple labels. / * Compute prototypes. * / for each region j do µ µ µ j ← k∈R R Rj v v v k /|| k∈R R Rj v v v k || end / * Calculate vMF-N loss. * / Calculate the loss using Equation 10 and back-propagate the errors. end cars and trucks are on the rightmost islands while horses, cows, and sheeps on the leftmost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 9 .</head><label>9</label><figDesc>t-SNE visualization of prototype embeddings from supervised SegSort, framed with category color. Best viewed with zoom-in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 10 .</head><label>10</label><figDesc>t-SNE visualization of prototype embeddings from unsupervised SegSort, framed with category color. Best viewed with zoom-in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell>Base / Backbone / Method</cell><cell cols="2">mIoU f-measure</cell></row><row><cell>DeepLabv3+ / MNV2 / Softmax</cell><cell>72.51</cell><cell>50.90</cell></row><row><cell>DeepLabv3+ / MNV2 / SegSort</cell><cell>74.94</cell><cell>58.83</cell></row><row><cell>PSPNet / ResNet-101 / Softmax</cell><cell>80.12</cell><cell>59.64</cell></row><row><cell cols="2">PSPNet / ResNet-101 / ASM [31] 81.43</cell><cell>62.35</cell></row><row><cell>PSPNet / ResNet-101 / SegSort</cell><cell>81.77</cell><cell>63.71</cell></row><row><cell>DeepLabv3+ / MNV2 / Softmax</cell><cell>73.25</cell><cell>-</cell></row><row><cell>DeepLabv3+ / MNV2 / SegSort</cell><cell>74.88</cell><cell>-</cell></row><row><cell>PSPNet / ResNet-101 / Softmax</cell><cell>80.63</cell><cell>-</cell></row><row><cell>PSPNet / ResNet-101 / SegSort</cell><cell>82.41</cell><cell>-</cell></row></table><note>Quantitative results on Pascal VOC 2012. The first 4 rows with gray colored background are on validation set while the last</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Softmax 97.96 83.89 92.22 57.24 59.31 58.89 68.39 77.07 92.18 63.71 94.42 81.80 63.11 94.85 73.54 84.82 67.42 69.34 77.42 76.72 SegSort 98.18 84.86 92.75 55.63 61.57 63.72 71.66 80.01 92.62 64.64 94.65 82.32 62.75 95.08 77.27 87.07 78.89 63.63 77.51 78.15</figDesc><table><row><cell>Method</cell><cell>road swalk build. wall</cell><cell>fence</cell><cell>pole</cell><cell>tlight tsign</cell><cell>veg. terrain</cell><cell>sky</cell><cell>person rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train mbike bike</cell><cell>mIoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">. We observe</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>unsup. on train aug sup. on train aug sup. on train mIoU f-measure</figDesc><table><row><cell>49.50</cell><cell>40.86</cell></row><row><cell>55.86</cell><cell>44.78</cell></row><row><cell>71.86</cell><cell>55.70</cell></row><row><cell>72.47</cell><cell>56.52</cell></row><row><cell>73.35</cell><cell>55.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Per-class boundary evaluation on Pascal VOC 2012 validation set. From top to bottom: precision, recall, and f-measure, separated by double lines. 77.34 85.60 55.33 49.44 90.34 82.71 77.77 92.95 66.23 95.59 87.50 89.12 92.73 54.55 76.99 72.29 67.87 85.35 78.18 SegSort 89.14 78.81 88.79 55.18 55.45 92.53 87.70 82.32 94.25 70.51 96.07 90.73 87.89 95.61 55.19 77.90 64.79 62.47 86.00 79.54 Softmax 45.81 77.13 58.38 47.94 53.65 58.65 64.86 68.67 65.04 58.90 58.66 73.31 63.08 81.21 57.69 73.35 56.95 54.96 69.71 62.52 SegSort 46.08 79.05 60.75 49.84 57.68 63.37 74.57 75.18 66.28 61.17 60.36 76.53 71.78 83.21 64.58 78.67 65.28 61.61 73.48 66.82 Softmax 59.70 77.24 69.41 51.37 51.46 71.13 72.71 72.94 76.53 62.35 72.71 79.78 73.87 86.59 56.08 75.13 63.71 60.73 76.74 68.96 SegSort 60.76 78.93 72.14 52.37 56.54 75.22 80.60 78.59 77.83 65.51 74.14 83.03 79.02 88.98 59.52 78.29 65.04 62.04 79.25 71.99</figDesc><table><row><cell>Method</cell><cell>road swalk build. wall</cell><cell>fence</cell><cell>pole</cell><cell>tlight tsign</cell><cell>veg. terrain</cell><cell>sky</cell><cell>person rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train mbike bike</cell><cell>mean</cell></row><row><cell cols="2">Softmax 85.68</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Softmax 98.33 84.21 92.14 49.67 55.81 57.62 69.01 74.17 92.70 70.86 95.08 84.21 66.58 95.28 73.52 80.59 70.54 65.54 73.73 76.30 SegSort 98.56 85.84 92.85 52.18 58.44 63.62 73.29 77.93 93.41 72.37 95.10 84.97 67.98 95.41 67.43 80.80 60.69 68.68 74.34 77.05 Table 7. Per-class results on Cityscapes test set. We conclude that network trained with SegSort outperforms Softmax consistently. Softmax 85.02 55.18 80.92 65.87 70.60 89.55 83.39 88.27 35.04 80.30 48.24 79.20 82.13 81.16 81.21 52.59 75.24 47.20 80.20 67.92 72.51 DeepLabv3+ / MNV2 / SegSort 84.80 58.54 81.08 68.92 79.15 89.75 85.24 89.64 34.88 74.60 58.62 84.34 79.07 84.94 85.92 54.65 76.76 50.74 82.95 74.57 74.94 PSPNet / ResNet-101 / Softmax 92.56 66.70 91.10 76.52 80.88 94.43 88.49 93.14 38.87 89.33 62.77 86.44 89.72 88.36 87.48 56.95 91.77 46.23 88.59 77.14 80.12 PSPNet / ResNet-101 / SegSort 92.23 52.68 91.29 80.33 83.92 95.13 90.33 95.44 44.68 90.84 67.37 91.29 91.09 89.66 88.98 67.54 88.06 53.04 87.79 79.97 81.77 DeepLabv3+ / MNV2 / Softmax 85.89 59.20 79.09 61.24 66.47 87.87 85.17 88.80 28.27 78.98 60.67 80.35 83.72 83.90 83.52 59.87 83.43 50.22 74.07 63.91 73.25 DeepLabv3+ / MNV2 / SegSort 79.49 66.32 75.38 66.17 70.71 91.51 84.82 85.54 38.69 74.91 68.99 78.17 80.49 85.08 85.63 60.92 86.47 57.96 73.26 67.39 74.88 PSPNet / ResNet-101 / Softmax 94.01 68.08 88.80 64.87 75.87 95.60 89.59 93.15 37.96 88.20 72.58 89.96 93.30 87.52 86.65 61.90 87.05 60.81 87.13 74.65 80.63 PSPNet / ResNet-101 / SegSort 96.00 67.17 93.37 74.52 77.77 95.07 89.39 93.91 41.31 87.85 73.66 90.15 91.06 85.63 87.86 71.81 90.28 65.99 86.75 75.53 82.41 Per-class results on Pascal VOC 2012. The first 4 rows with gray colored background are on validation set while the last 4 rows are on testing set. Networks trained with SegSort consistently outperform their parametric counterpart (Softmax) by 1.63 to 2.43%. / ResNet-101 / Softmax 90.93 56.70 89.46 73.35 82.13 95.03 87.30 91.88 37.79 83.56 56.32 88.31 83.32 86.11 86.61 58.17 87.65 52.87 88.43 74.19 78.24 Deeplabv3 + / ResNet-101 / SegSort 88.78 51.17 88.12 70.45 83.89 95.12 88.74 94.34 43.12 86.24 59.07 88.86 88.11 86.92 87.58 56.91 85.46 55.32 89.01 73.77 78.85</figDesc><table><row><cell>Method</cell><cell cols="3">road swalk build. wall</cell><cell>fence</cell><cell>pole</cell><cell cols="3">tlight tsign</cell><cell cols="2">veg. terrain</cell><cell>sky</cell><cell cols="3">person rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train mbike bike</cell><cell>mIoU</cell></row><row><cell cols="2">Base / Backbone / Method</cell><cell>aero</cell><cell>bike</cell><cell>bird</cell><cell cols="2">boat bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell><cell>table</cell><cell>dog</cell><cell cols="3">horse mbike person plant sheep sofa</cell><cell>train</cell><cell>tv</cell><cell>mIoU</cell></row><row><cell cols="2">DeepLabv3+ / MNV2 / Base / Backbone / Method</cell><cell>aero</cell><cell>bike</cell><cell>bird</cell><cell cols="2">boat bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell><cell>table</cell><cell>dog</cell><cell cols="3">horse mbike person plant sheep sofa</cell><cell>train</cell><cell>tv</cell><cell>mIoU</cell></row><row><cell>Deeplabv3 +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">rows are on testing set. Networks trained with SegSort consistently outperform their parametric counterpart (Softmax) by 1.63 to 2.43% in mIoU and by 4.07 to 7.97% in boundary f-measure.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Appu</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Süsstrunk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Clustering on the unit hypersphere using von mises-fisher distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joydeep</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Color-and texture-based image segmentation using em and its application to content-based image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chad</forename><surname>Carson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayit</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recognition-by-components: a theory of human image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irving</forename><surname>Biederman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">115</biblScope>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Signature verification using a&quot; siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Spectral graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><forename type="middle">Chung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>American Mathematical Soc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorin</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Meer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huttenlocher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Stacked deconvolutional network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04943</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Superpixel convolutional networks using bilateral inceptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghudeep</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kappler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast image scanning with deep max-pooling convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neighbourhood components analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decomposing a scene into geometric and semantically consistent regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Fulton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The pyramid match kernel: Discriminative classification with sets of image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Algorithm as 136: A k-means clustering algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Manchek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series C (Applied Statistics)</title>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiscale conditional random fields for image labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carreira-Perpindn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Similarity-Based Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A real-time algorithm for signal analysis with the help of the wavelet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Holschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Kronland-Martinet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Morlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ph</forename><surname>Tchamitchian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wavelets: Time-Frequency Methods and Phase Space</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adversarial structure matching for structured prediction tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Wei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Pixel-wise deep learning for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyng-Luh</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01989</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scene collaging: Analysis and synthesis of natural images with semantic layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adaptive affinity fields for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Tsung-Wei Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust higher order potentials for enforcing label consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="324" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recurrent pixel embedding for instance grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Associative hierarchical crfs for object class image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubor</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks with identity mappings for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Anton van den Hengel, and Ian Reid</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Nonparametric scene parsing via label transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Contour and texture analysis for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Recognition by association via learning per-exemplar distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Feedforward semantic segmentation with zoom-out features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payman</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A view of the em algorithm that justifies incremental, sparse, and other variants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Learning in Graphical Models</title>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Modeling local and global deformations in deep learning: Epitomic convolution, multiple instance learning, and sliding window detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Andre</forename><surname>Savalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Large kernel matters-improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Segmenting scenes by matching image composites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyosha</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Efficient parameter-free clustering using first neighbor relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Saquib Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Michaël Mathieu, Rob Fergus, and Yann LeCun</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Multiclass spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">On the role of structure in vision. Human and machine vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Witkin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="page" from="481" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Superparsing: scalable nonparametric image parsing with superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Normface: l 2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Loddon</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krähenbühl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Improving generalization via scalable neighborhood component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance-level discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Unsupervised segmentation of natural images via lossy data compression. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Shankar</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sastry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivienne</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05093</idno>
		<title level="m">Deeperlab: Single-shot image parser</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Segmentation given partial grouping constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Exfuse: Enhancing feature fusion for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
