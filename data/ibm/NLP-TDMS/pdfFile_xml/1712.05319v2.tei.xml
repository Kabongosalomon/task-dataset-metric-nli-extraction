<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep CNN ensembles and suggestive annotations for infant brain MRI segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Dolz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ecole de Technologie Superieure (ETS)</orgName>
								<orgName type="institution">University of Quebec</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Desrosiers</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ecole de Technologie Superieure (ETS)</orgName>
								<orgName type="institution">University of Quebec</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology and BRIC</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<postCode>27599</postCode>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Mathematics and Statistics</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology and BRIC</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<postCode>27599</postCode>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Brain and Cognitive Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<postCode>02841</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><forename type="middle">Ben</forename><surname>Ayed</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ecole de Technologie Superieure (ETS)</orgName>
								<orgName type="institution">University of Quebec</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep CNN ensembles and suggestive annotations for infant brain MRI segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep learning</term>
					<term>MRI</term>
					<term>infant brain segmentation</term>
					<term>3D CNN</term>
					<term>ensemble learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Precise 3D segmentation of infant brain tissues is an essential step towards comprehensive volumetric studies and quantitative analysis of early brain developement. However, computing such segmentations is very challenging, especially for 6-month infant brain, due to the poor image quality, among other difficulties inherent to infant brain MRI, e.g., the isointense contrast between white and gray matter and the severe partial volume effect due to small brain sizes.</p><p>This study investigates the problem with an ensemble of semi-dense fully convolutional neural networks (CNNs), which employs T1-weighted and T2-weighted MR images as input. We demonstrate that the ensemble agreement is highly correlated with the segmentation errors. Therefore, our method provides measures that can guide local user corrections. To the best of our knowledge, this work is the first ensemble of 3D CNNs for suggesting annotations within images. Furthermore, inspired by the very recent success of dense networks [1], we propose a novel architecture, SemiDenseNet, which connects all convolutional layers directly to the end of the network. Our architecture allows the efficient propagation of gradients during training, while limiting the number of parameters, requiring one order of magnitude less parameters than popular medical image segmentation networks such as 3D U-Net <ref type="bibr" target="#b1">[2]</ref>. Another contribution of our work is the study of the impact that early or late fusions of multiple image modalities might have on the performances of deep architectures. We report evaluations of our method on the public data of the MICCAI iSEG-2017 Challenge on 6-month infant brain MRI segmentation, and show very competitive results among 21 teams, ranking first or second in most metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Precise segmentation of infant brain MRI into white matter (WM), gray matter (GM) and cerebrospinal fluid (CSF) is essential to study early brain development. Throughout this period, the postnatal human brain shows its most dynamic phase of development, with a rapid growth of tissues and the formation of key cognitive and motor functions <ref type="bibr" target="#b2">[3]</ref>. Infant brain segmentation is also important to detect brain abnormalities occurring shortly after birth, such as hypoxic ischemic encephalopathy, hydrocephalus or congenital malformations, enabling the prediction of neuro-developmental outcomes.</p><p>Magnetic resonance imaging (MRI) is commonly used for infant brains because it provides a safe and non-invasive way of examining cross-sectional views of the brain in multiple contrasts. Brain MRI in the first two years can be divided in three distinct phases: infantile (ă 6 months), isointense (6-12 months) and early adult-like phase (ą12 months). Images in the isointense phase show patterns of isointense contrast between white and gray matter (e.g., see <ref type="figure">Fig. 1</ref>), which may vary across brain regions due to nonlinear brain development <ref type="bibr" target="#b2">[3]</ref>. These patterns, along with various factors, for instance, limited acquisition time, increased noise, motion artifacts, severe partial volume effect due to smaller brain sizes and ongoing white matter myelination in infant brain images, make automatic segmentation of isointense infant brain MRI a challenging task. <ref type="figure">Figure 1</ref>: Example of data from a training subject.6-month infant brain images from a mid-axial T1w slice (left), the corresponding T2w slice (middle), and the ground-truth labels (right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related work</head><p>A popular approach for automatic segmentation uses atlases to model the anatomical variability of target structures <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. In such approach, an atlas (or multiple atlases) is first registered to a target image and then used to propagate manual labels to this image. When several atlases are considered, labels from individual atlases can be combined into a final segmentation via a label fusion strategy <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14]</ref> such as the STAPLE (Simultaneous Truth and Performance Level Estimation) algorithm <ref type="bibr" target="#b14">[15]</ref>. Atlas-based methods have been widely used in a breadth of segmentation problems, e.g., parcellation of brain MRI into subcortical structures <ref type="bibr" target="#b15">[16]</ref>. Although these methods provide stateof-the-art performance in many applications, they are usually sensitive to the registration process, and may fail if the image has a low contrast or the target structure has a large variability. This is particularly problematic in the case of infant brain segmentation, due to isointense contrasts and the high spatial variability of the infant population.</p><p>To overcome the limitations of atlas-based methods, parametric <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> or deformable <ref type="bibr" target="#b19">[20]</ref> models can be used in a refinement step. Parametric models typically state segmentation as the optimization of an energy function, which integrates pixel/voxel probabilities from the atlas with priors restricting the shape or pairwise interactions of brain tissues <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. Such models often need a large number of annotated images, which are rarely available in practice. Deformable models refine atlas-produced contours in an iterative manner so as to align better with image edges. However, these models are typically structure-specific and difficult to extend to multi-tissue segmentation.</p><p>Recently, deep learning methods based on convolutional neural networks (CNNs) have demonstrated outstanding performances in a wide range of computer vision and image analysis applications. In particular, CNNs have achieved state-of-the-art results for various problems <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>, including the segmentation of infant brain MRI <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>. For instance, Moeskops et al. <ref type="bibr" target="#b24">[25]</ref> presented a multi-scale 2D CNN architecture, which yielded accurate segmentations and spatial consistency using a single image modality (i.e., T2w MRI). To acquire multi-scale information, they considered patches and convolution kernels of multiple sizes. Independent paths were used for patches of different sizes, and the features of these paths were combined at the end of the network. Several recent studies investigated architectures based on multiple modalities as input, in order to overcome the extremely low contrast between WM and GM tissues. For example, Zhang et al. <ref type="bibr" target="#b25">[26]</ref> proposed a deep CNN combining MR-T1, T2 and fractional anisotropy (FA) images. Similarly, a fully convolutional neural network (FCNN) was proposed in <ref type="bibr" target="#b26">[27]</ref> for segmenting isointense phase brain MR images. As further explained in Section 2, a FCNN is a special type of CNN that generates dense pixel predictions. Instead of simply stacking the three modalities at the network input, the network in <ref type="bibr" target="#b26">[27]</ref> processes each modality within an independent path. The final segmentation is obtained by fusing the ensuing paths. These approaches have some important drawbacks.</p><p>First, the architectures in <ref type="bibr" target="#b24">[25]</ref> and <ref type="bibr" target="#b25">[26]</ref> use sliding windows, each defining a region that is processed independently of the other windows. Such a strategy is not efficient because of the many redundant convolution and pooling operations. Furthermore, processing these regions independently yields nonstructured predictions, which affects segmentation accuracy. Second, these networks use 2D patches as input. This does not account for the anatomic context in directions orthogonal to the 2D plane. As first shown in <ref type="bibr" target="#b21">[22]</ref>, and later in <ref type="bibr" target="#b22">[23]</ref>, in different contexts of brain structure segmentation, considering 3D data directly, instead of slice-by-slice, can improve segmentation accuracy. <ref type="table" target="#tab_0">Table 1</ref> provides a brief summary of the existing methods for infant brain tissue segmentation. For a detailed review of the methods proposed to address this task, we refer the reader to the recent work of Makropoulos et al. <ref type="bibr" target="#b27">[28]</ref>  While fully-automatic, learning-based medical image segmentation methods have improved substantially over the last years, the performances in many applications are still insufficient for practical use, more so when the amount of training data is very limited, as is typically the case in medical applications. For instance, manual annotations of brain MRI (i.e., assigning a label to each voxel) is a highly complex and time-consuming process, which requires extensive expertise. This is particularly the case of infant brain MRIs. Therefore, both active and semi/weakly supervised learning frameworks are currently attracting substantial interest in medical image analysis <ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref>. For instance, in the recent study in <ref type="bibr" target="#b36">[37]</ref>, Yang et al. proposed an active learning framework, and showed its potential in the context of segmenting glands in histology images and lymph nodes in ultrasound. The purpose of <ref type="bibr" target="#b36">[37]</ref> was to design algorithms that suggest a small set of images to annotate, which lead to the highest possible performance improvement when adding these new annotations to the training set.</p><p>The framework in <ref type="bibr" target="#b36">[37]</ref> uses an ensemble of deep CNNs to compute an agreement score for candidate instances, and suggests representative instances with the highest uncertainty. However, since suggestions are made at the image level, manual annotations of full images are still required. Using ensemble of CNNs, each trained with a different set of images, can further improve robustness by reducing test error due to variance <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Contributions and outline</head><p>The contributions of this study can be itemized as follows:</p><p>• This work presents the first ensemble of 3D CNNs for suggesting annotations within images. An important benefit of an ensemble of predictors is the ability to measure their level of agreement. This is particularly useful for evaluating the reliability of the segmentations at a voxel level and suggesting local corrections in regions where the ensemble is not confident about the prediction. An important finding in our experiments is that prediction uncertainty, measured as the inverse of predictor agreement within the ensemble, is highly correlated with segmentation errors.</p><p>• Inspired by the recent success of dense networks <ref type="bibr" target="#b0">[1]</ref>, we propose a novel architecture called Semi-DenseNet, which connects all convolutional layers directly to the end of the network. This semidense architecture allows the efficient propagation of gradients during training, while limiting the number of trainable parameters. Our network requires one order of magnitude less parameters than popular medical image segmentation networks such as 3D U-Net <ref type="bibr" target="#b1">[2]</ref>. Furthermore, by combining the feature maps of intermediate convolutional layers into the first fully-connected layer, our architecture injects multiscale information into the final segmentation.</p><p>• As in recent approaches for infant brain MRI segmentation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>, the proposed network addresses the problem of low contrast by using multiple image modalities as inputs. So far, to the best of our knowledge, there is no clear guideline in the literature as to how to combine multimodal images in the network. While stacking available modalities into a single-input network works well in some cases <ref type="bibr" target="#b25">[26]</ref>, other works have shown the advantage of having independent paths for modalities and combining these paths further in the network <ref type="bibr" target="#b26">[27]</ref>. Another contribution of our work is an investigation of the impact that early or late fusions of several modalities might have on the performances.</p><p>• We report evaluations of our method on the publicly available data of the MICCAI iSEG-2017 Grand Challenge on 6-month infant brain MRI Segmentation 1 . We obtained very competitive results among 21 teams, ranking first and second in most metrics.</p><p>The remainder of this paper is as follows. In Section 2, we present the proposed semi-dense architecture, and detail how an ensemble of networks can be used to suggest annotations. We also describe the evaluation protocol used in our study. Section 3 demonstrates the performance of our method on data from the MICCAI iSEG-2017 Challenge. Finally, in Section 4, we discuss the main contributions and results of this work, and propose some potential extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head><p>Convolutional neural networks (CNNs) are a special type of artificial neural networks that learn a hierarchy of increasingly complex features by successive convolution, pooling and non-linear activation operations <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>. Originally designed for image recognition and classification, CNNs are now commonly used in semantic image segmentation. A naive approach follows a sliding-window strategy, where regions defined by the window are processed independently. As explained before, this technique presents two main drawbacks: reduction of segmentation accuracy and low efficiency. An alternative approach, known as fully CNNs (FCNNs) <ref type="bibr" target="#b20">[21]</ref>, mitigates these limitations by considering the network as a single non-linear convolution, which is trained in an end-to-end fashion. An important advantage of FCNNs, compared to standard CNNs, is that they can be applied to images of arbitrary size. Moreover, because the spatial map of class scores is obtained in a single dense inference step, FCNNs can avoid redundant convolution and pooling operations, making them computationally more efficient.</p><p>The proposed architectures ( <ref type="figure" target="#fig_1">Fig. 2 and 3</ref>) are built on top of DeepMedic <ref type="bibr" target="#b21">[22]</ref> and extend our network presented in <ref type="bibr" target="#b22">[23]</ref>, which showed state-of-the-art performance on the task of segmenting subcortical brain structures in MRI. Unlike this network, the proposed architectures use multiple image modalities as input. Moreover, while the previous network has skip-forward connections for only a few convolutional layers, these architectures follow a denser connection strategy, where feature maps from all convolutional layers are aggregated before the first fully-connected layer. Another notable difference is the proposed ensemble learning strategy, where multiple 3D CNNs are combined to improve robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semi-dense 3D fully CNN</head><p>Recent hardware developments, in particular those related to graphic processing units (GPUs), have increased the amount of memory available during inference. This has led to a rise in CNN architectures based on 3D convolutions <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref>, which have a much larger number of parameters than their 2D counterpart. To fit volumetric data into memory, 3D CNNs typically perform pooling operations that down-sample feature maps across the network. However, this down-sampling strategy can lead to a loss of resolution in the segmentation. In <ref type="bibr" target="#b20">[21]</ref>, this issue is addressed by connecting features maps of corresponding levels in the down-sampling and up-sampling streams. The resolution of the input image is recovered by adding deconvolution (or transpose convolution) layers at the end of the network. Unfortunately, this technique may still give coarse-looking segmentations. For instance, thin structures can disappear after pooling, and may not be recovered in the up-sampling path. To avoid this effect, the proposed FCNN architecture preserves resolution by avoiding down-sampling operations entirely.</p><p>The proposed method extends our recent 3D FCNN achitecture <ref type="bibr" target="#b22">[23]</ref>, which is composed of many convolutional layers, each containing several 3D convolution filters (or kernels). Filters in a layer are applied to the output of the previous layer, or to the input volume in the case of the first layer. The result of this operation is known as a feature map. Let m l denotes the number of convolution kernels in layer l of the network, and x n l´1 the 3D array corresponding to the n-th input of layer l. The k-th output feature map of layer l is then given by</p><formula xml:id="formula_0">y k l " f´m l´1 ÿ n"1 w k,n i b x n l´1`b k l¯,<label>(1)</label></formula><p>where w k,n i is a filter convolved with each of the previous layers, b k l is the bias, f is a non-linear activation function and b denotes the convolution operator. Note that feature maps produced by convolutions are slightly smaller than their input volumes: The size difference along each dimension is equal to the filter size in this dimension minus one voxel. Hence, applying a 3ˆ3ˆ3 convolution filter will reduce the input volume by 2 voxels along each dimension. A stride may also be defined for each convolutional layer, representing the displacement of the filter along the three dimensions after each application.</p><p>For the activation function, we used the Parametric Rectified Linear Unit (PReLU) <ref type="bibr" target="#b45">[46]</ref> instead of the popular Rectified Linear Unit (ReLU). This function can be formulated as</p><formula xml:id="formula_1">f px i q " maxp0, x i q`a i¨m inp0, x i q,<label>(2)</label></formula><p>where x i defines the input signal and f px i q represents the output. Here, a i is a scaling coefficient that stops the local gradient from becoming zero when x i is negative. In other words, PReLUs prevent saturation as we approach negative values of x i . While ReLU employs predefined values for a i (typically equal to 0), PReLU requires learning this coefficient. Thus, this activation function can adapt the rectifiers to their inputs, improving the network's accuracy at a negligible extra computational cost. As in standard CNNs, fully-connected layers are added at the end of the network to encode semantic information. However, to ensure that the network contains only convolutional layers, we use the strategy described in <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b21">[22]</ref>, in which fully-connected layers are converted to a large set of 1ˆ1ˆ1 convolutions. Doing this allows the network to retain spatial information and learn the parameters of these layers as in other convolutional layers. Lastly, neurons in the last layer (i.e., the classification layer) are grouped into m " C feature maps, where C denotes the number of classes. The output of the classification layer L is then converted into normalized probability values via a softmax function. The probability score of class c P t1, . . . , Cu is computed as follows:</p><formula xml:id="formula_2">p c " exp`y c Lř C c 1 "1 exp`y c 1 L˘( 3)</formula><p>In addition, we model both local and global context by embedding intermediate-layer outputs in the final prediction. Specifically, we concatenate the output of all convolutional layers into a dense feature map that is fed to the first fully-connected layer. This semi-dense connectivity ( <ref type="figure" target="#fig_1">Fig. 2 and 3</ref>), encourages consistency between features extracted at different scales and facilitates the propagation of gradients during training. <ref type="figure">Figure 2</ref>: Proposed semi-dense architecture using early fusion strategy. T1w and T2w MRI inputs are combined before the first convolutional layer and feature maps from every convolutional layer are connected to the first fully-connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-modal input: early versus late fusion</head><p>The studies in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> suggested that multiple input sources provide complementary information, which can help dealing with low contrasts in infant brain MRI. They showed that combining several image sequences in the CNN, in particular T1w, T2w and FA, yielded more accurate segmentations than using these modalities individually. Following this observation, we extend our architecture in <ref type="bibr" target="#b22">[23]</ref> to accommodate multi-sequence images as input to the network. A common strategy for this task is to merge available modalities at the input of the CNN. This early fusion strategy processes input modalities alongside one another, encouraging their use in all the features of a given layer <ref type="bibr" target="#b25">[26]</ref>. Another approach, referred to as late fusion, employs independent streams for each modality, with features updated separately during training and merged at the end of the network. Considering these strategies, we propose two architectures for our network based on early (  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Ensemble learning to suggest local corrections</head><p>Ensemble learning uses multiple classifiers/regressors based on different models or trained with different sets of instances, so as to reduce errors due to variance. The outputs of individual models in the ensemble are combined into a single prediction, for instance, by averaging or majority voting. The justification for this technique lies in the fact that, when multiple and independent decisions are combined, random errors are reduced. Thus, ensemble learning promotes better generalization and provides higher prediction accuracy than individual models. Recent studies have demonstrated that averaging the predictions of similar CNNs can lead to an increase in performance. For example, Krizhevsky et al. <ref type="bibr" target="#b41">[42]</ref> found that, on the ImageNet 2012 classification benchmark, an ensemble of 5 CNNs achieved a top-5 validation error rate of 16.4%, compared to 18.2% for a single CNN model. By adding another CNN to this ensemble, and making small changes to the network architecture, Zeiler and Fergus <ref type="bibr" target="#b46">[47]</ref> were able to decrease this error further to 14.7%.</p><p>In our approach, multiple CNNs are generated and combined to obtain an aggregated prediction. Given a training set Ω " tpX t , y t qu, t " 1, . . . , N , where X t " px T1 t , x T2 t q are the T1w and T2w images of training subject t and y t is the corresponding annotated ground truth, we build a set of predictors ϕpx; θ k q, k " 1, . . . , K, each trained with a randomly selected subset of Ω. This enforces diversity in the predictors, thereby increasing the ensemble's ability to generalize. In this work, we used an ensemble of K " 10 CNNs and combined their predictions with majority voting.</p><p>An important benefit of an ensemble of predictors is the ability to measure their level of agreement. In <ref type="bibr" target="#b36">[37]</ref>, this measure is used to identify unlabeled images for which the predicted segmentation is uncertain, and to recommend these images to an expert for annotation. In this work, we evaluate the reliability of the segmentation at a voxel level, not at the image level as in <ref type="bibr" target="#b36">[37]</ref>), and suggest local corrections in regions where the ensemble is not confident about the prediction. An important finding in our experiments is that the prediction uncertainty (i.e., the inverse of predictor agreement) is highly correlated with segmentation error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Network parameters and implementation details</head><p>Our CNN is composed of 13 layers in total: 9 convolutional layers in each path, 3 fully-connected layers, and the classification layer. The number of kernels in each convolutional layer, from shallow to deeper, is as follows: 25, 25, 25, 50, 50, 50, 75, 75 and 75. To achieve this depth in a 3D CNN, we employ small kernels of size 3ˆ3ˆ3. Moreover, a unit stride is used for all convolutions to preserve spatial resolution. As in <ref type="bibr" target="#b47">[48]</ref> the activation functions, i.e. PReLU and Batch normalization are employed as 'pre-activation' steps. Thus, each convolutional block is composed by a batch normalization step, followed by a Parametric Rectified Linear Unit (PReLU) activation function and lastly the convolutional filters.</p><p>The three fully-connected layers are composed of 400, 200 and 150 hidden units, respectively. Features maps from each convolutional layer are fed into the first fully-connected layer, thereby incorporating multi-scale information in the segmentation. Since the size of feature maps differs from one layer to the next, they are cropped to fit the size of feature maps in the last convolutional layer. Thus, the input to the first fully-connected layer has a size of num. feature mapsˆ9ˆ9ˆ9, where the number of feature maps is set to 450 in the early fusion and to 900 in late fusion architectures ( <ref type="figure" target="#fig_1">Figs. 2 and 3</ref>, respectively).</p><p>Instead of using the whole 3D image as input, we sample S image segments (i.e., sub-volumes) from the image, x s , s " 1, . . . , S, and feed these segments to the network <ref type="bibr" target="#b21">[22]</ref>. This strategy offers two considerable benefits. First, it reduces the memory requirements of our network, thereby removing the need for spatial pooling. More importantly, it substantially increases the number of training examples without having to perform data augmentation. Network parameters are optimized via the RMSprop optimizer, using cross-entropy as cost function. Let θ denotes the network parameters (i.e., convolution weights, biases and a i from the rectifier units), and y v s the label of voxel v in the s-th image segment. Following the training scheme proposed in <ref type="bibr" target="#b21">[22]</ref>, we define the following cost function:</p><formula xml:id="formula_3">Jpθq "´1 S¨V S ÿ s"1 V ÿ v"1 C ÿ c"1 δpy v s " cq¨log p v c px s q,<label>(4)</label></formula><p>where p v c px s q is the output of the network for voxel v and class c, when the input segment is x s . In <ref type="bibr" target="#b21">[22]</ref>, Kamnitsas et al. found that increasing the size of input segments in training leads to a higher performance, but this performance increase stops beyond segment sizes of 25ˆ25ˆ25. In their network, using this segment size for training, score maps at the classification stage were of size 9ˆ9ˆ9. Since our architecture is one layer deeper, and to keep the same score map sizes, we set the segment size in our network to 27ˆ27ˆ27. This method was used in <ref type="bibr" target="#b22">[23]</ref> with very satisfactory results.</p><p>The initialization of weights in deep CNNs is usually performed by assigning random normaldistributed values to kernel and bias weights. However, using fixed standard deviations to initialize weights might lead to poor convergence <ref type="bibr" target="#b48">[49]</ref>. To overcome this problem, we adopted the strategy proposed in <ref type="bibr" target="#b45">[46]</ref>, and used in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> for segmentation, which allows very deep architectures to converge rapidly. We use a zero-mean Gaussian distribution of standard deviation a 2{n l to initialize the weights in layer l, where n l denotes the number of connections to units within layer l.</p><p>We set momentum to 0.6 and the initial learning rate to 0.001, with the latter reduced by a factor of 2 after every 5 epochs (starting from epoch 10). Instead of employing an adaptive strategy for the learning rate, we used step decay and monitored the evolution of the cost function during training. We observed that it followed an exponentially decreasing curve with small increasing/decreasing slopes and, therefore, kept this simple yet effective strategy. Our 3D FCNNs were trained for 30 epochs, each consisting of 20 subepochs. At each subepoch, a total of 1000 samples were randomly selected from the training images, and processed in batches of size 20.</p><p>The code of the proposed 3D FCNN architecture is implemented in Theano <ref type="bibr" target="#b49">[50]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Dataset</head><p>The images were acquired at the UNC-Chapel Hill on a Siemens head-only 3T scanner with a circular polarized head coil, and were randomly chosen from the pilot study of the Baby Connectome Project (BCP) <ref type="bibr" target="#b2">3</ref> . During scan, infants were asleep, unsedated and fitted with ear protection, with the head secured in a vacuum-fixation device.</p><p>Acquisition parameters. T1-weighted images were acquired with 144 sagittal slices using the following parameters: TR/TE = 1900/4.38 ms, flip angle = 7˝, resolution = 1ˆ1ˆ1 mm 3 . Likewise, T2-weighted images were obtained with 64 axial slices by employing: TR/TE = 7380/119 ms, flip angle = 150˝, resolution =1.25ˆ1.25ˆ1.95 mm 3 .</p><p>Pre-Processing. The preprocessing was performed by the iSEG-2017 organizers. Specifically, T2w images were linearly aligned onto their corresponding T1w images. All images were resampled into an isotropic 1ˆ1ˆ1 mm 3 resolution. Standard image pre-processing steps were then applied using in-house tools, including skull stripping, intensity inhomogeneity correction, and removal of the cerebellum and brain stem. This pre-processing was performed to eliminate the effects that different image registration and bias correction algorithms might have on infant brain segmentation.</p><p>Ground truth generation. The manual labels were prepared by the iSEG-2017 organizers. Instead of starting from scratch, an initial automatic segmentation for 6-month subjects <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref> was generated with the guidance from follow-up 24-month scans with high tissue contrast, using the publicly-available iBEAT tool 4 . Based on this initial automatic segmentation, manual editing was then performed by an experienced neuro-radiologist, to correct segmentation errors in both T1-and T2-weighted MR images. Geometric defects were also removed with the help of surface rendering, using ITK-SNAP. For example, if a hole/handle was found on the surface, the neuro-radiologist first localized the related slices and then checked the segmentation maps of both T1w and T2w images, in order to determine whether to fill the hole or cut the handle. Using this approach, correcting segmentation of a single subject took about one week.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1.">Evaluation</head><p>The MICCAI iSEG-2017 organizers used three metrics to evaluate the accuracy of the competing segmentation methods: Dice Similarity Coefficient (DSC) <ref type="bibr" target="#b52">[53]</ref>, Modified Hausdorff distance (MHD), where the 95-th percentile of all Euclidean distances is employed, and Average Surface Distance (ASD). The first measures the degree of overlap between the segmentation region and ground truth, whereas the other two evaluate boundary distances.</p><p>Dice similarity coefficient (DSC). Let V ref and V auto be, respectively, the reference and automatic segmentations of a given tissue class and for a given subject. The DSC can be defined as:</p><formula xml:id="formula_4">DSC`V ref , V auto˘" 2 | V ref X V auto | | V ref |`| V auto |<label>(5)</label></formula><p>DSC values are within a r0, 1s range, 1 indicating perfect overlap and 0 corresponding to a total mismatch.</p><p>Modified Hausdorff distance (MHD). Let P ref and P auto denote the sets of voxels within the reference and automatic segmentation boundary, respectively. MHD is given by:</p><formula xml:id="formula_5">MHD`P ref , P auto˘" max ! max qPP ref dpq, P auto q, max qPPauto dpq, P ref q ) ,<label>(6)</label></formula><p>where dpq, P q is the point-to-set distance defined by: dpq, P q " min pPP }q´p}, with }.} denoting the Euclidean distance.</p><p>Average surface distance (ASD). Using the same notation as the Hausdorff distance above, the ASD corresponds to:</p><formula xml:id="formula_6">ASD`P ref , P auto˘" 1 |P ref | ÿ p P P ref dpp, P auto q,<label>(7)</label></formula><p>where |.| denotes the cardinality of a set. In distance-based metrics, smaller values indicate higher proximity between two point sets and, thus, a better segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>Three different methods are evaluated in our experiments. The first, referred to as EarlyFusion Single, is a semi-dense network with early fusion of multi-modal images <ref type="figure">(Fig. 2)</ref>. The second, denoted EarlyFusion Ensemble, consists of an ensemble of 10 early-fusion CNNs, trained with different subjects. Finally, the third method, referred to as LateFusion Ensemble, is an ensemble of 10 semi-dense CNNs, each performing a late fusion of modalities in different paths ( <ref type="figure" target="#fig_1">Fig. 3)</ref> and trained with different subjects. For the EarlyFusion Single approach, we used 9 subjects for training and one for validation. In the ensemble methods, for each CNN, 8 different subjects were used for training and 2 subjects for validation. <ref type="table">Table 2</ref> reports the results obtained by the three proposed methods and the top 5 among the 21 teams that participated in the iSEG MICCAI Grand Challenge 5 . In this table, mean DSC, MHD and ASD values are given separately for CSF, WM and GM tissues. We first observe that adopting an ensemble learning strategy (i.e., EarlyFusion Ensemble) results in a small yet noticeable improvement in global performance, with respect to our baseline using a single CNN (i.e., EarlyFusion Single). The results also indicate that fusing features from image modalities in a late stage does not help the segmentation compared to an early fusion strategy. In fact, improvements in the LateFusion Ensemble approach only occurred for 2 out of 9 combinations of metric and tissue (ASD for CSF and MHD for GM). Comparing against other competing approaches, the proposed EarlyFusion Ensemble method obtained the best score in 5 out of 9 cases. In particular, our network yielded the best DSC values for the three brain tissues, and the best MHD and ASD values for CSF and white matter, respectively. Furthermore, all tested methods obtained the highest accuracy for CSF, and slightly better results for GM than WM. As can be seen in <ref type="figure">Fig. 1</ref>, the edges between GM and WM tissues are weak, resulting in a harder classification for voxels lying on these edges.</p><p>Considering results for individual test subjects <ref type="table" target="#tab_3">(Table 3)</ref>, the proposed Early Ensemble approach yields an accurate segmentation in most cases, thus showing its robustness. A lower performance was, however, obtained for a few cases, for example, segmenting the CSF of subject 022, which yielded an ASD of 0.449 mm. Ignoring this result brings the mean ASD for CSF down to 0.11, which is lower than the best ASD value of 0.12 for this tissue. In a paired t-test, our approach performs at the same level as the best competing method (i.e., MSL SKKU), with no significant difference (p ą 0.01) observed in any of the test cases. Note that this competing method is also based on deep CNNs.</p><p>To illustrate the impact on reliability of using an ensemble of CNNs, <ref type="figure" target="#fig_2">Fig. 4</ref> compares the segmentation confidence of the Early Single and Early Ensemble methods, for a given slice of two different <ref type="table">Table 2</ref>: Segmentation results from the iSEG-2017 Segmentation challenge for the top-5 ranked methods. The first set of the results correspond to the three proposed approaches, and the second set to approaches presented by the other competing teams in the top-5 (in alphabetical order). Bold fonts indicate the best performances for each metric and structure. For additional details, we refer the reader to the challenge's website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CSF</head><p>GM WM DSC MHD ASD DSC MHD ASD DSC MHD ASD  subjects. Specifically, the first and third columns depict the probability maps predicted by the single CNN, while the second and fourth columns show the agreement score of the ensemble (i.e., the percentage of CNNs in the ensemble that predicted a given label). We see that the ensemble agreement values are sharper (i.e., closer to 0 -black or 1 -blue) than the predictions of the single CNN. With respect to tissue classes, the differences in confidence are smallest for the CSF, and more significant for WM and GM. This is in line with previous results in <ref type="table">Table 2</ref>, indicating CSF to be an easier tissue to segment. These differences between the methods are illustrated in <ref type="figure" target="#fig_3">Fig. 5</ref>, showing the ensemble's ability to improve the predictions of a single CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Suggestion of local corrections</head><p>We validated the usefulness of the ensemble as to suggesting local corrections by comparing confidence values at individual voxels with segmentation errors. Since the ground truth is not available for test instances of the iSEG segmentation challenge, we used a cross-validation strategy for this task. Given the 10 subjects with reference segmentations, we selected a single subject for testing and used the remaining 9 to train the k " 10 CNNs of the ensemble. For each CNN, 7 subjects were randomly selected for training and the other 2 kept for validation. This process was repeated 5 times, each involving a different test subject. To evaluate the relationship between ensemble confidence and error, we separated confidence values in two groups using an agreement threshold of 60%: low confidence, where agreement among the ensemble's CNNs equal to or less than 60% (no more than 6 CNNs agree), and high confidence, represented by agreement values greater than 60% (at least 7 CNNs agree). <ref type="table" target="#tab_4">Table  4</ref> gives the correlation between the Early Ensemble method's prediction and ground truth value, for low confidence and high confidence voxels. We observe a very high correlation (ą 90%) between highly confident predictions and the ground truth, for all three tissues. When prediction is low, correlation drops to values around 50% for gray matter, and around 10% for the other two tissues. Furthermore, <ref type="figure" target="#fig_4">Fig. 6</ref> shows the distributions of correctly and incorrectly classified voxels according to their confidence. For all three tissues, most correctly classified voxels have a 100% agreement, whereas confidence values of incorrectly classified voxels are more evenly distributed. We note that this distribution differs across tissue types. For CSF, voxels classified incorrectly mostly have a low confidence, while more balanced distributions are observed for GM and WM. Again, this indicates the higher difficulty of segmenting these two tissues compared to CSF. Overall, these results validate our hypothesis that the spatial map of ensemble CNN agreement values can be used to suggest local corrections.</p><p>As qualitative validation, <ref type="figure" target="#fig_5">Fig. 7</ref> shows the examples of confidence values obtained for WM and GM (i.e., percentage of CNNs that predicted these tissues), along with the predicted GT labels. Dark blue  corresponds to a total agreement (i.e., 100% of CNNs voted for the tissue), while dark red indicates the lowest possible agreement (i.e., a single CNN voted for the tissue). Voxels with low confidence (light blue and yellow colors) thus indicate regions with potential segmentation errors, which should be verified by the expert. Visual inspection of these confidence regions, highlighted by the green and pink squares in the figure, corroborates this hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>We presented an ensemble-learning approach, which combines multiple deep CNNs to segment isointense infant brain MRI and suggest local corrections in regions of low confidence. In the proposed CNN architecture, multi-modal MR images are employed to deal with the problem of low contrast, using an early or late fusion strategy to combine these different modalities. Furthermore, global and local features are considered in the classification by connecting feature maps from all convolutional layers to the first fully-connected layer. This semi-dense architecture facilitates the propagation of gradients during training, while limiting the number of network parameters.</p><p>To improve the generalization of our segmentation approach, we adopted an ensemble-learning strategy that combines the output of 10 CNN models by majority voting. Having several independent predictions also allowed us to build a spatial map of ensemble confidence, which measures segmentation reliability at a voxel level. We showed that confidence values can identify potential errors in the automated segmentation, which might require corrections. The proposed approach was evaluated on the iSEG MICCAI Grand Challenge, and compared to 21 other competing teams. Our approach achieved a state-of-the-art performance, obtaining the highest score in most cases.</p><p>The proposed method extends our previous work in <ref type="bibr" target="#b22">[23]</ref> by considering multi-modal images in the network. An important design choice is the strategy as to merging multiple sources of information. Different modalities can be combined in a tensor and fed to the network. Another option is to create independent paths for the modalities, and fuse the features of these paths at the end of the network, as in <ref type="figure">Fig. 2</ref>. Since satisfactory results have been reported in the literature for both strategies, we investigated in this work the effect of an early or late fusion. From <ref type="table">Table 2</ref>, we can see that fusing images in an early stage typically improved the segmentation for most metrics and for all three brain tissues. These findings are not consistent with the results in <ref type="bibr" target="#b26">[27]</ref>, where combining extracted features in a late stage improved segmentation performance. However, unlike this study, these results were obtained using three image modalities (i.e., T1w,T2w, and FA) instead of two, and used a set of subjects different from the one available for this challenge.</p><p>As reported in <ref type="table">Table 2</ref>, combining predictions from several models in an ensemble yields improvements over a single CNN. This confirms the results obtained in computer vision studies, in the context of color image classification <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b46">47]</ref>. These studies showed that ensemble learning can boost the performances of deep neural networks. In the context of medical image segmentation, a very valuable benefit of ensemble learning is the ability to measure segmentation confidence at a voxel-level, which can be used to identify regions that need further inspections by medical experts. We validated this assumption by computing the correlation between automatic predictions and manual annotations obtained for 5 subjects, in regions presenting low or high confidence. While high-confidence regions were highly correlated (ą 90%) to ground-truth values across all three brain structures, low-confidence regions were poorly correlated, in particular for the CSF and WM (« 10%).</p><p>A wide range of techniques were proposed to segment infant brain tissues in MRI (see <ref type="table" target="#tab_0">Table 1</ref>), many of them based on atlases. Although atlas-based approaches were successful in segmenting adult brain images, their application to infant brains is more prone to errors due to the poor tissue contrast and the high spatial variability of the infant population. Such approaches also depend on the image registration step, which is time-consuming and a source of errors. In recent studies, deep CNNs were shown to outperform atlas-based methods for this task. For example, Nie et al. <ref type="bibr" target="#b26">[27]</ref> obtained mean DSC values of 0.852, 0.873 and 0.887 for the CSF, GM and WM, respectively, over 10 subjects. Yet, a limitation of this work was the use of 2D convolutions, which omits important 3D context. Via 3D convolutions, our approach captures spatial information in volumetric data, which is confirmed by a performance improvement over 2D CNN models. Comparing the proposed Early Ensemble approach to the top-5 ranked methods of the iSEG Grand Challenge, our approach achieved very competitive results, ranking first or second in most cases. A paired sample t-test showed that the difference between our approach and the other best ranked method in the challenge (i.e., MSL SKKU) is not statistically significant.</p><p>Although various CNN-based networks have been used successfully for medical image segmentation (e.g., 3D U-Net <ref type="bibr" target="#b1">[2]</ref> or DenseVoxNet <ref type="bibr" target="#b53">[54]</ref>), an important benefit of our architecture is the reduced number of trainable parameters. While U-Net and DenseVoxNet require nearly 19M and 4M of parameters, respectively, our semidense network has less than 1M of parameters (nearly 900,000), which results in a 75%-90% lighter model. This parameter efficiency translates into lower training and segmentation times. For example 3D-UNet requires 3 days for training, whereas our network can be trained in approximately 17 hours using the same hardware.</p><p>A limitation of this study comes from the fact that the ground-truth labels of test instances were not available to the participating teams, which makes interpretation of the results difficult. Based on our cross-validation analysis, we can however conjecture that most errors come from misclassified voxels at the boundaries of GM and WM regions, which have low contrast. Confidence maps show that regions with lowest confidence typically correspond to the borders between these two tissues. Moreover, as mentioned on the iSEG website, manually correcting the data of a single subject took approximately one week. Taking into account that nearly 500 typically developing children will be scanned for the Baby Connectome Project, the adoption of an automatic segmentation tool is highly needed. As reported in our results, the proposed network can segment the data of a subject in 1-2 minutes on a single GPU, or in a few seconds on several GPUs. In addition to its efficiency, the proposed approach can identify potential errors in the automatic segmentation, which could be corrected with limited interactions from the user. A potential extension of this study would be to evaluate our approach in a real clinical setting, in which segmentation time and accuracy is measured for multiple manual raters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented a novel method based on an ensemble of deep CNNs to segment isointense infant brains in multi-modal MRI images. Our fully-convolutional (FCNN) network considers the 3D spatial context of volumetric data and models both local and global information in the segmentation. We investigated a semi-dense architecture, where the features of each convolutional layer are aggregated as input to the first fully-connected layer. Moreover, two different strategies were investigated to combine multiple input image modalities, using either an early or a late fusion of these modalities. An ensemble learning technique, in which the prediction of 10 CNNs are combined using majority voting, was employed to improve the generalization performance of our method. This ensemble also enables to measure segmentation confidence, using the number of CNNs voting for a particular label. While ensembles were used in the past to suggest images for annotation, to our knowledge, this is the first work that investigated the problem at a voxel level.</p><p>The performance of the proposed method was evaluated in the MICCAI iSEG-2017 Grand Challenge on 6-month infant brain MRI segmentation. The results show that our method is very competitive, ranking first or second among 21 competing teams for most of the metrics. Our experiments also demonstrate the benefit of combining the prediction of multiple CNNs, with performance improvements over using a single CNN, and show the link between ensemble agreement and segmentation error. This suggests that our method has the potential to achieve expert-level performance with limited user interactions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 2) or late fusion(Fig. 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Proposed semi-dense architecture using late fusion strategy. Features are extracted from T1w and T2w MRI images through independent paths, and are fused before the first fully-connected layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Probability maps obtained for each tissue of two test subjects. The left column of each example depicts the probability maps obtained from the baseline CNN (i.e. single CNN method). The right column shows voxel-wise segmentation confidence of the ensemble of CNNs. Dark blue indicates highest confidence and dark red lowest confidence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Probability maps of gray matter on a 2D slice obtained by a single CNN (left) and the ensemble (right). Important differences between both methods are highlighted in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Distribution of confidence values for correctly (blue bars) and incorrectly (yellow bars) classified voxels, for the three tissues of a given subject.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Visual inspection of segmentation confidence values. From left to right: white matter, gray matter, predicted segmentation and reference labels. Green boxes highlight regions with large differences between the predicted contours and the reference standard, where as pink boxes are used to indicate small differences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>A brief summary of existing methods for infant brain tissue segmentation.</figDesc><table><row><cell>Method</cell><cell>Technique</cell><cell>Modality</cell><cell>Stage**</cell><cell>Time</cell></row><row><cell>Prastawa et al., 2005 [4]</cell><cell>Parametric (Graph clustering)</cell><cell>T1, T2</cell><cell>IF</cell><cell>-</cell></row><row><cell>Weisenfeld et al., 2006 [5]</cell><cell>Atlas + Bayesian Classifier</cell><cell>T1, T2</cell><cell>IF</cell><cell>-</cell></row><row><cell>Nishida et al., 2006 [29]</cell><cell>-</cell><cell>T1</cell><cell>IF</cell><cell>-</cell></row><row><cell>Xue et al., 2007 [17]</cell><cell>Parametric (EM-MRF)</cell><cell>T2</cell><cell>IS</cell><cell>-</cell></row><row><cell>Song et al., 2007 [6]</cell><cell>-</cell><cell>T2</cell><cell>IF</cell><cell>-</cell></row><row><cell>Anbeek et al., 2008 [30]</cell><cell>K-Nearest Neighbours</cell><cell>T2,IR</cell><cell>IF</cell><cell>-</cell></row><row><cell>Weisenfeld and Warfield, 2009 [7]</cell><cell>Multi-atlas</cell><cell>T1,T2</cell><cell>IF</cell><cell>-</cell></row><row><cell>Shi et al., 2010 [9]</cell><cell>Multi-atlas</cell><cell>T2</cell><cell>IF</cell><cell>-</cell></row><row><cell>Wang et al., 2011 [20]</cell><cell>Level-sets</cell><cell>T2</cell><cell>IF</cell><cell>-</cell></row><row><cell>Gui et al., 2012 [31]</cell><cell>-</cell><cell>T1,T2</cell><cell>IS</cell><cell>60-75 min</cell></row><row><cell>Ledig et al., 2012 [18]</cell><cell>Parametric (MRF)</cell><cell>T2</cell><cell>IS</cell><cell>-</cell></row><row><cell>Makropoulos et al.,2012 [32]</cell><cell>Multi-atlas</cell><cell>T2</cell><cell>IS</cell><cell>95 min</cell></row><row><cell>Melbourne et al., 2012 [11]</cell><cell>Parametric (EM-MRF)</cell><cell>T2</cell><cell>IS</cell><cell>15 min</cell></row><row><cell>Wang et al., 2012 [14]</cell><cell>Multi-atlas</cell><cell>T1, T2</cell><cell>IS</cell><cell>7 min</cell></row><row><cell>Wu et Avants, 2012 [19]</cell><cell>Parametric (MAP-EM)</cell><cell>T1, T2</cell><cell>IS</cell><cell>80-100 min</cell></row><row><cell>Cardoso et al., 2013 [12]</cell><cell>Parametric (EM-MRF)</cell><cell>T1</cell><cell>IS</cell><cell>-</cell></row><row><cell>He and Parikh, 2013 [33]</cell><cell>-</cell><cell>T2, PD</cell><cell>IF</cell><cell>-</cell></row><row><cell>Wang et al., 2014 [34]</cell><cell>Multi-atlas</cell><cell>T1,T2,FA</cell><cell>IF/IS/EA</cell><cell>-</cell></row><row><cell>Wang et al., 2014 [13]</cell><cell>Multi-atlas</cell><cell>T2</cell><cell>IF</cell><cell>-</cell></row><row><cell>Li et al., 2015 [35]</cell><cell>Random Forests</cell><cell>T1,T2,FA</cell><cell>IS</cell><cell>5 min</cell></row><row><cell>Moeskops et al., 2015 [36]</cell><cell>SVM</cell><cell>T2</cell><cell>IS</cell><cell>-</cell></row><row><cell>Zhang et al., 2015 [26]</cell><cell>2D CNN</cell><cell>T1,T2,FA</cell><cell>IS</cell><cell>1 min/slice</cell></row><row><cell>Moeskops et al., 2016 [25]</cell><cell>2D CNN</cell><cell>T2</cell><cell>IS</cell><cell>-</cell></row><row><cell>Nie et al., 2016 [27]</cell><cell>2D CNN</cell><cell>T1,T2,FA</cell><cell>IS</cell><cell>-</cell></row></table><note>**Infantile (IF) / Isointense (IS) / Early-adult like (EA)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and is publicly available 2 . Training and testing were performed on a server equipped with an NVIDIA Tesla P100 GPU with 16 GB of RAM memory. Training a single network takes around 30 min per epoch, and around 17 hours in total. The segmentation of a 3D MR scan requires 10 seconds per CNN model, on average. On a single GPU, segmenting a new subject with the ensemble of CNNs takes around 100 seconds.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Segmentation results obtained by the proposed EarlyFusion Ensemble approach on the 13 test subjects of the iSEG Segmentation challenge.</figDesc><table><row><cell></cell><cell></cell><cell>CSF</cell><cell></cell><cell></cell><cell>GM</cell><cell></cell><cell></cell><cell>WM</cell><cell></cell></row><row><cell>Subject ID</cell><cell>DSC</cell><cell>MHD</cell><cell>ASD</cell><cell>DSC</cell><cell>MHD</cell><cell>ASD</cell><cell>DSC</cell><cell>MHD</cell><cell>ASD</cell></row><row><cell>#11</cell><cell>0.9637</cell><cell>7.5498</cell><cell cols="7">0.1024 0.9300 4.3589 0.2838 0.9065 8.4853 0.3423</cell></row><row><cell>#12</cell><cell>0.9526</cell><cell>6.7823</cell><cell cols="7">0.1287 0.9037 6.4807 0.3780 0.8702 6.6333 0.4558</cell></row><row><cell>#13</cell><cell cols="9">0.9619 10.1980 0.1158 0.9276 6.4031 0.3172 0.9104 9.4868 0.3685</cell></row><row><cell>#14</cell><cell>0.9423</cell><cell>8.9443</cell><cell cols="7">0.1565 0.9091 6.3246 0.3767 0.8912 5.7446 0.4203</cell></row><row><cell>#15</cell><cell>0.9607</cell><cell>9.2736</cell><cell cols="7">0.1064 0.9281 4.5826 0.3159 0.9054 7.0000 0.3728</cell></row><row><cell>#16</cell><cell cols="9">0.9582 10.8167 0.1179 0.9208 8.1240 0.3237 0.9111 6.5574 0.3733</cell></row><row><cell>#17</cell><cell>0.9609</cell><cell>8.1240</cell><cell cols="7">0.0405 0.9270 7.8740 0.2828 0.9119 5.9161 0.3277</cell></row><row><cell>#18</cell><cell>0.9646</cell><cell>9.4340</cell><cell cols="7">0.1040 0.9201 6.4031 0.3151 0.9033 7.1414 0.3735</cell></row><row><cell>#19</cell><cell>0.9598</cell><cell>9.0000</cell><cell cols="7">0.1111 0.9202 5.6569 0.3198 0.9088 8.1854 0.3830</cell></row><row><cell>#20</cell><cell cols="9">0.9524 10.0499 0.1352 0.9039 6.7082 0.4207 0.8690 5.9161 0.5073</cell></row><row><cell>#21</cell><cell>0.9587</cell><cell>9.4340</cell><cell cols="7">0.1132 0.9156 5.8310 0.3343 0.8908 5.8310 0.4275</cell></row><row><cell>#22</cell><cell>0.9454</cell><cell>8.7750</cell><cell cols="7">0.4491 0.9158 8.4853 0.3951 0.8896 7.0711 0.1248</cell></row><row><cell>#23</cell><cell>0.9598</cell><cell>9.0000</cell><cell cols="7">0.1087 0.9196 6.1644 0.3339 0.8934 6.7082 0.4080</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Correlation between the prediction of the proposed method and the ground truth in regions of high and low confidence (values in parenthesis correspond to the percentage of voxels belonging to each of these regions.)</figDesc><table><row><cell>CSF</cell><cell></cell><cell>GM</cell><cell></cell><cell>WM</cell><cell></cell></row><row><cell>High Conf.</cell><cell>Low Conf.</cell><cell>High Conf.</cell><cell>Low Conf.</cell><cell>High Conf.</cell><cell>Low Conf.</cell></row><row><cell cols="6">Subject 01 0.92 (99.20%) 0.11 (0.80%) 0.93 (96.81%) 0.48 (3.19%) 0.92 (97.32%) 0.10 (2.68%)</cell></row><row><cell cols="6">Subject 02 0.93 (99.02%) 0.13 (0.98%) 0.94 (96.60%) 0.51 (3.40%) 0.94 (97.44%) 0.10 (2.57%)</cell></row><row><cell cols="6">Subject 03 0.94 (99.12%) 0.10 (0.88%) 0.94 (96.68%) 0.49 (3.32%) 0.94 (97.43%) 0.09 (2.57%)</cell></row><row><cell cols="6">Subject 04 0.91 (99.18%) 0.07 (0.82%) 0.92 (96.79%) 0.49 (3.21%) 0.92 (97.52%) 0.09 (2.48%)</cell></row><row><cell cols="6">Subject 05 0.93 (99.05%) 0.08 (0.95%) 0.94 (96.76%) 0.48 (3.24%) 0.94 (97.59%) 0.11 (2.41%)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://iseg2017.web.unc.edu/rules/results/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.github.com/josedolz/SemiDenseNet 3 http://babyconnectomeproject.org 4 http://www.nitrc.org/projects/ibeat/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://iseg2017.web.unc.edu/rules/results/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the National Science and Engineering Research Council of Canada (NSERC), discovery grant program, and by the ETS Research Chair on Artificial Intelligence in Medical Imaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ö</forename><surname>Içek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
	<note>3d u-</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Maturation of white matter in the human brain: a review of magnetic resonance studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zijdenbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain research bulletin</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="255" to="266" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic segmentation of MR images of the developing newborn brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prastawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="457" to="466" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">I</forename><surname>Weisenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mewes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Warfield</surname></persName>
		</author>
		<title level="m">Biomedical Imaging: Nano to Macro, 2006. 3rd IEEE International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="766" to="769" />
		</imprint>
	</monogr>
	<note>Segmentation of newborn brain MRI</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Clinical neonatal brain MRI segmentation using adaptive nonparametric data models and intensity-based markov priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Awate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Licht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="883" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic segmentation of newborn brain MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">I</forename><surname>Weisenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Warfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="564" to="572" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neonatal brain image segmentation in longitudinal MRI studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="391" to="400" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Construction of multi-region-multireference atlases for neonatal brain MRI segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="684" to="693" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Infant brain atlases from neonates to 1-and 2-year-olds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">18746</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Neobrains12 challenge: adaptive neonatal MRI brain segmentation with myelinated white matter class and automated extraction of ventricles i-iv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Melbourne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Marlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Neonatal Brain Segmentation</publisher>
			<biblScope unit="page" from="16" to="21" />
		</imprint>
		<respStmt>
			<orgName>MICCAI Grand Challenge</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">AdaPT: an adaptive preterm segmentation algorithm for neonatal brain MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Melbourne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Modat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Marlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="97" to="108" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Segmentation of neonatal brain MR images using patch-driven level sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="141" to="158" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An atlas-based method for neonatal MR brain tissue segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kuklisova-Murgasova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the MICCAI Grand Challenge: Neonatal Brain Segmentation</title>
		<meeting>the MICCAI Grand Challenge: Neonatal Brain Segmentation</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="28" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simultaneous truth and performance level estimation (STAPLE): an algorithm for the validation of image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Warfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="903" to="921" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Massoptier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vermandel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Segmentation algorithms of subcortical brain structures on MRI for radiotherapy and radiosurgery: a survey</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="200" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic segmentation and reconstruction of the cortex from neonatal MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="461" to="477" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neonatal brain segmentation using second order neighborhood information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Serag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Aljabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Perinatal and Paediatric Imaging: PaPI, MICCAI</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Automatic registration-based segmentation for neonatal brains using ANTs and Atropos, MICCAI Grand Challenge: Neonatal Brain Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Avants</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="36" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic segmentation of neonatal images using convex optimization and coupled level sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="805" to="817" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">F</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="61" to="78" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ayed, 3D fully convolutional networks for subcortical segmentation in MRI: A large-scale study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Esophagus segmentation in CT via 3D fully convolutional neural network and random walk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fechter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adebahr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Baltas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic segmentation of MR brain images with a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moeskops</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Mendrik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Benders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Išgum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1252" to="1261" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for multi-modality isointense infant brain image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="214" to="224" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for multi-modality isointense infant brain image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 13th International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1342" to="1345" />
		</imprint>
	</monogr>
	<note>Biomedical Imaging (ISBI)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A review on automatic fetal and neonatal brain MRI segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Counsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<imprint>
			<pubPlace>NeuroImage</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Detailed semiautomated MRI based morphometry of the neonatal brain: preliminary results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Makris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vangel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fischl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Caviness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Grant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1041" to="1049" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Probabilistic brain tissue segmentation in neonatal magnetic resonance imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Vincken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Groenendaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Van Osch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Der Grond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pediatric research</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="158" to="163" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Morphology-driven automatic segmentation of MR images of the neonatal brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lisowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Faundez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Hüppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lazeyras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1565" to="1579" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Counsell, D. Rueckert, Automatic tissue and structural segmentation of neonatal brain MRI using Expectation-Maximization, MICCAI Grand Chall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Aljabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Serag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Hajnal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neonatal Brain Segmentation</title>
		<imprint>
			<biblScope unit="page" from="9" to="15" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automated detection of white matter signal abnormality using T2 relaxometry: application to brain segmentation on term MRI in very preterm infants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="328" to="340" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Integration of sparse multimodality representation and anatomical constraint for isointense infant brain MR image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="152" to="164" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Links: Learning-based multisource integration framework for segmentation of infant brain images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="160" to="172" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic segmentation of MR brain images of preterm infants using supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moeskops</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Benders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Chiţ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Kersbergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Groenendaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Išgum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page" from="628" to="641" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Suggestive annotation: A deep active learning framework for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="399" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deepcut: Object segmentation from bounding box annotations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rajchl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Passerat-Palmbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Damodaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Hajnal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="674" to="683" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for network-based cardiac MR image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sinclair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rajchl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tarroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="253" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sinclair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pawlowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rajchl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Daniel Rueckert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01468</idno>
		<title level="m">Ensembles of multiple models and architectures for robust brain tumour segmentation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">3D deeply supervised network for automatic liver segmentation from CT volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="149" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV), 2016 Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Automatic 3D liver location and segmentation via convolutional neural network and graph cut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="171" to="182" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Theano: A CPU and GPU math compiler in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Python in Science Conf</title>
		<meeting>9th Python in Science Conf</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Longitudinally guided level sets for consistent tissue segmentation of neonates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human brain mapping</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="956" to="972" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">4D multi-modality tissue segmentation of serial infant images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">44596</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Measures of the amount of ecologic association between species</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Dice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="302" />
			<date type="published" when="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Automatic 3D cardiovascular MR segmentation with densely-connected volumetric convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="287" to="295" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
