<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MixConv: Mixed Depthwise Convolutional Kernels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
							<email>tanmingxing@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Brain Mountain View</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Brain Mountain View</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MixConv: Mixed Depthwise Convolutional Kernels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>TAN, LE: MIXCONV: MIXED DEPTHWISE CONVOLUTIONAL KERNELS 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Depthwise convolution is becoming increasingly popular in modern efficient Con-vNets, but its kernel size is often overlooked. In this paper, we systematically study the impact of different kernel sizes, and observe that combining the benefits of multiple kernel sizes can lead to better accuracy and efficiency. Based on this observation, we propose a new mixed depthwise convolution (MixConv), which naturally mixes up multiple kernel sizes in a single convolution. As a simple drop-in replacement of vanilla depthwise convolution, our MixConv improves the accuracy and efficiency for existing MobileNets on both ImageNet classification and COCO object detection. To demonstrate the effectiveness of MixConv, we integrate it into AutoML search space and develop a new family of models, named as MixNets, which outperform previous mobile models including MobileNetV2 [23] (ImageNet top-1 accuracy +4.2%), ShuffleNetV2 [18] (+3.5%), MnasNet [29] (+1.3%), ProxylessNAS [2] (+2.2%), and FBNet [30] (+2.0%). In particular, our MixNet-L achieves a new state-of-the-art 78.9% ImageNet top-1 accuracy under typical mobile settings (&lt;600M FLOPS). Code is at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional neural networks (ConvNets) have been widely used in image classification, detection, segmentation, and many other applications. A recent trend in ConvNets design is to improve both accuracy and efficiency. Following this trend, depthwise convolutions are becoming increasingly more popular in modern ConvNets, such as MobileNets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23]</ref>, ShuffleNets <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33]</ref>, NASNet <ref type="bibr" target="#b34">[35]</ref>, AmoebaNet <ref type="bibr" target="#b20">[21]</ref>, MnasNet <ref type="bibr" target="#b28">[29]</ref>, and EfficientNet <ref type="bibr" target="#b27">[28]</ref>. Unlike regular convolution, depthwise convolutional kernels are applied to each individual channel separately, thus reducing the computational cost by a factor of C, where C is the number of channels. While designing ConvNets with depthwise convolutional kernels, an important but often overlooked factor is kernel size. Although conventional practice is to simply use 3x3 kernels <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref>, recent research results have shown larger kernel sizes such as 5x5 kernels <ref type="bibr" target="#b28">[29]</ref> and 7x7 kernels <ref type="bibr" target="#b1">[2]</ref> can potentially improve model accuracy and efficiency.</p><p>In this paper, we revisit the fundamental question: do larger kernels always achieve higher accuracy? Since first observed in AlexNet <ref type="bibr" target="#b13">[14]</ref>, it has been well-known that each convolutional kernel is responsible to capture a local image pattern, which could be edges in early stages and objects in later stages. Large kernels tend to capture high-resolution patterns c 2019. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. (a) MobileNetV1 (b) MobileNetV2 <ref type="figure">Figure 1</ref>: Accuracy vs kernel sizes -Each point represents a model variant of MobileNet V1 <ref type="bibr" target="#b6">[7]</ref> and V2 <ref type="bibr" target="#b22">[23]</ref>, where model size is represented by point size. Larger kernels lead to more parameters, but the accuracy actually drops down when kernel size is larger than 9x9.  with more details at the cost of more parameters and computations, but do they always improve accuracy? To answer this question, we systematically study the impact of kernel sizes based on MobileNets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23]</ref>. <ref type="figure">Figure 1</ref> shows the results. As expected, larger kernel sizes significantly increase the model size with more parameters; however, model accuracy first goes up from 3x3 to 7x7, but then drops down quickly when the kernel size is larger than 9x9, suggesting very large kernel sizes can potentially hurt both accuracy and efficiency. In fact, this observation aligns to the very first intuition of ConvNets: in the extreme case that kernel size is equal to the input resolution, a ConvNet simply becomes a fully-connected network, which is known to be inferior <ref type="bibr" target="#b6">[7]</ref>. This study suggests the limitations of single kernel size: we need both large kernels to capture high-resolution patterns and small kernels to capture low-resolution patterns for better model accuracy and efficiency. Based on this observation, we propose a mixed depthwise convolution (MixConv), which mixes up different kernel sizes in a single convolution op, such that it can easily capture different patterns with various resolutions. <ref type="figure" target="#fig_1">Figure 2</ref> shows the structure of MixConv, which partitions channels into multiple groups and apply different kernel sizes to each group of channels. We show that our MixConv is a simple drop-in replacement of vanilla depthwise convolution, but it can significantly improve MobileNets accuracy and efficiency on both ImageNet classification and COCO object detection.</p><p>To further demonstrate the effectiveness of our MixConv, we leverage neural architecture search <ref type="bibr" target="#b28">[29]</ref> to develop a new family of models named as MixNets. Experimental results show our MixNet models significantly outperform all previous mobile ConvNets, such as ShuffleNets <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33]</ref>, MnasNet <ref type="bibr" target="#b28">[29]</ref>, FBNet <ref type="bibr" target="#b29">[30]</ref>, and ProxylessNAS <ref type="bibr" target="#b1">[2]</ref>. In particular, our large-size MixNet-L achieves a new state-of-the-art 78.9% ImageNet top-1 accuracy under typical model size and FLOPS settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Efficient ConvNets: In recent years, significant efforts have been spent on improving Con-vNet efficiency, from more efficient convolutional operations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11]</ref>, bottleneck layers <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref>, to more efficient architectures <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. In particular, depthwise convolution has been increasingly popular in all mobile-size ConvNets, such as MobileNets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23]</ref>, ShuffleNets <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33]</ref>, MnasNet <ref type="bibr" target="#b28">[29]</ref>, and beyond <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35]</ref>. Recently, EfficientNet <ref type="bibr" target="#b27">[28]</ref> even achieves both state-of-the-art ImageNet accuracy and ten-fold better efficiency by extensively using depthwise and pointwise convolutions. Unlike regular convolution, depthwise convolution performs convolutional kernels for each channel separately, thus reducing parameter size and computational cost. Our proposed MixConv generalizes the concept of depthwise convolution, and can be considered as a drop-in replacement of vanilla depthwise convolution.</p><p>Multi-Scale Networks and Features: Our idea shares a lot of similarities to prior multibranch ConvNets, such as Inceptions <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref>, Inception-ResNet <ref type="bibr" target="#b25">[26]</ref>, ResNeXt <ref type="bibr" target="#b30">[31]</ref>, and NASNet <ref type="bibr" target="#b34">[35]</ref>. By using multiple branches in each layer, these ConvNets are able to utilize different operations (such as convolution and pooling) in a single layer. Similarly, there are also many prior work on combining multi-scale feature maps from different layers, such as DenseNet <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> and feature pyramid network <ref type="bibr" target="#b14">[15]</ref>. However, unlike these prior works that mostly focus on changing the macro-architecture of neural networks in order to utilize different convolutional ops, our work aims to design a drop-in replacement of a single depthwise convolution, with the goal of easily utilizing different kernel sizes without changing the network structure.</p><p>Neural Architecture Search: Recently, neural architecture search <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref> has achieved better performance than hand-crafted models by automating the design process and learning better design choices. Since our MixConv is a flexible operation with many possible design choices, we employ existing architecture search methods similar to <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref> to develop a new family of MixNets by adding our MixConv into the search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MixConv</head><p>The main idea of MixConv is to mix up multiple kernels with different sizes in a single depthwise convolution op, such that it can easily capture different types of patterns from input images. In this section, we will discuss the feature map and design choices for MixConv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MixConv Feature Map</head><p>We start from the vanilla depthwise convolution. Let X (h,w,c) denotes the input tensor with shape (h, w, c), where c is the spatial height, w is the spatial width, and c is the channel size.</p><p>Let W (k,k,c,m) denotes a depthwise convolutional kernel, where k × k is the kernel size, c is the input channel size, and m is the channel multiplier. For simplicity, here we assume kernel width and height are the same k, but it is straightforward to generalize to cases where kernel width and height are different.s The output tensor Y (h,w,c·m) would have the same spatial shape (h, w) and multiplied output channel size m · c, with each output feature map value calculated as:</p><formula xml:id="formula_0">Y x,y,z = ∑ − k 2 ≤i≤ k 2 ,− k 2 ≤ j≤ k 2 X x+i,y+ j,z/m ·W i, j,z , ∀z = 1, ..., m · c<label>(1)</label></formula><p>Unlike vanilla depthwise convolution, MixConv partitions channels into groups and applies different kernel sizes to each group, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. More concretely, the input tensor is partitioned into g groups of virtual tensors &lt;X (h,w,c 1 ) , ...,X (h,w,c g ) &gt;, where all virtual tensorsX have the same spatial height h and width w, and their total channel size is equal to the original input tensor:</p><formula xml:id="formula_1">c 1 + c 2 + ... + c g = c.</formula><p>Similarly, we also partition the convolutional kernel into g groups of virtual kernels &lt;Ŵ (k 1 ,k 1 ,c 1 ,m) , ...,Ŵ (k g ,k g ,c g ,m) &gt;. For t−th group of virtual input tensor and kernel, the corresponding virtual output is calculated as:</p><formula xml:id="formula_2">Y t x,y,z = ∑ − k t 2 ≤i≤ k t 2 ,− k t 2 ≤ j≤ k t 2X t x+i,y+ j,z/m ·Ŵ t i, j,z , ∀z = 1, ..., m · c t<label>(2)</label></formula><p>The final output tensor is a concatenation of all virtual output tensors &lt;Ŷ 1 x,y,z 1 , ...,Ŷ g x,y,z g &gt;:</p><formula xml:id="formula_3">Y x,y,z o = Concat Ŷ 1 x,y,z 1 , ...,Ŷ g x,y,z g<label>(3)</label></formula><p>where   </p><formula xml:id="formula_4">z o = z 1 + ... + z g = m · c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MixConv Design Choices</head><p>MixConv is a flexible convolutional op with several design choices:</p><p>Group Size g: It determines how many different types of kernels to use for a single input tensor. In the extreme case of g = 1, a MixConv becomes equivalent to a vanilla depthwise convolution. In our experiments, we find g = 4 is generally a safe choice for MobileNets, but with the help of neural architecture search, we find it can further benefit the model efficiency and accuracy with a variety of group sizes from 1 to 5.  Kernel Size Per Group: In theory, each group can have arbitrary kernel size. However, if two groups have the same kernel size, then it is equivalent to merge these two groups into a single group, so we restrict each group has different kernel size. Furthermore, since small kernel sizes generally have less parameters and FLOPS, we restrict kernel size always starts from 3x3, and monotonically increases by 2 per group. In other words, group i always has kernel size 2i + 1. For example, a 4-group MixConv always uses kernel sizes {3x3, 5x5, 7x7, 9x9}. With this restriction, the kernel size for each group is predefined for any group size g, thus simplifying our design process.</p><p>Channel Size Per Group: In this paper, we mainly consider two channel partition methods: (1) Equal partition: each group will have the same number of filters; (2) Exponential partition: the i-th group will have about 2 −i portion of total channels. For example, given a 4-group MixConv with total filter size 32, the equal partition will divide the channels into <ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b7">8)</ref>, while the exponential partition will divide the channels into <ref type="bibr" target="#b15">(16,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b3">4)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dilated Convolution:</head><p>Since large kernels need more parameters and computations, an alternative is to use dilated convolution <ref type="bibr" target="#b31">[32]</ref>, which can increase receptive field without extra parameters and computations. However, as shown in our ablation study in Section 3.4, dilated convolutions usually have inferior accuracy than large kernel sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MixConv Performance on MobileNets</head><p>Since MixConv is a simple drop-in replacement of vanilla depthwise convolution, we evaluate its performance on classification and detection tasks with existing MobileNets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>ImageNet Classification Performance: <ref type="figure" target="#fig_4">Figure 4</ref> shows the performance of MixConv on ImageNet classification <ref type="bibr" target="#b21">[22]</ref>. Based on MobileNet V1 and V2, we replace all original 3x3 depthwise convolutional kernels with larger kernels or MixConv kernels. Notably, MixConv always starts with 3x3 kernel size and then monotonically increases by 2 per group, so the rightmost point for MixConv in the figure has six groups of filters with kernel size {3x3, 5x5, 7x7, 9x9, 11x11, 13x13}. In this figure, we observe: (1) MixConv generally uses much less parameters and FLOPS, but its accuracy is similar or better than vanilla depthwise convolution, suggesting mixing different kernels can improve both efficiency and accuracy;   <ref type="figure">Figure 5</ref>: Per-layer impact of kernel size -s2 denotes stride 2, while others have stride 1.</p><p>(2) In contrast to vanilla depthwise convolution that suffers from accuracy degradation with larger kernels, as shown in <ref type="figure">Figure 1</ref>, MixConv is much less sensitive to very large kernels, suggesting mixing different kernels can achieve more stable accuracy for large kernel sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COCO Detection Performance:</head><p>We have also evaluated our MixConv on COCO object detection based on MobileNets. <ref type="table" target="#tab_4">Table 1</ref> shows the performance comparison, where our MixConv consistently achieves better efficiency and accuracy than vanilla depthwise convolution. In particular, compared to the vanilla depthwise7x7, our MixConv357 (with 3 groups of kernels {3x3, 5x5, 7x7}) achieves 0.6% higher mAP on MobileNetV1 and 1.1% higher mAP on MobileNetV2 using fewer parameters and FLOPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>To better understand MixConv, we provide a few ablation studies:</p><p>MixConv for Single Layer: In addition of applying MixConv to the whole network, <ref type="figure">Figure  5</ref> shows the per-layer performance on MobileNetV2. We replace one of the 15 layers with either (1) vanilla DepthwiseConv9x9 with kernel size 9x9; or (2) MixConv3579 with 4 groups of kernels: {3x3, 5x5, 7x7, 9x9}. As shown in the figure, large kernel size has different impact on different layers: for most of layers, the accuracy doesn't change much, but for certain layers with stride 2, a larger kernel can significantly improve the accuracy. Notably, although MixConv3579 uses only half parameters and FLOPS than the vanilla DepthwiseConv9x9, our MixConv achieves similar or slightly better performance for most of the layers.  Channel Partition Methods: <ref type="figure" target="#fig_5">Figure 6</ref> compares the two channel partition methods: equal partition (MixConv) and exponential partition (MixConv+exp). As expected, exponential partition requires less parameters and FLOPS for the same kernel size, by assigning more channels to smaller kernels. Our empirical study shows exponential channel partition only performs slightly better than equal partition on MobileNetV1, but there is no clear winner if considering both MobileNet V1 and V2. A possible limitation of exponential partition is that large kernels won't have enough channels to capture high-resolution patterns.</p><p>Dilated Convolution: <ref type="figure" target="#fig_5">Figure 6</ref> also compares the performance of dilated convolution (denoted as MixConv+dilated). For kernel size KxK, it uses a 3x3 kernel with dilation rate (K − 1)/2: for example, a 9x9 kernel will be replaced by a 3x3 kernel with dilation rate 4. Notably, since Tensorflow dilated convolution is not compatible with stride 2, we only use dilated convolutions for a layer if its stride is 1. As shown in the figure, dilated convolution has reasonable performance for small kernels, but the accuracy drops quickly for large kernels. Our hypothesis is that when dilation rate is big for large kernels, a dilated convolution will skip a lot of local information, which would hurt the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MixNet</head><p>To further demonstrate the effectiveness of MixConv, we leverage recent progress in neural architecture search to develop a new family of MixConv-based models, named as MixNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Architecture Search</head><p>Our neural architecture search settings are similar to recent MnasNet <ref type="bibr" target="#b28">[29]</ref> and FBNet <ref type="bibr" target="#b29">[30]</ref>, which use MobileNetV2 <ref type="bibr" target="#b22">[23]</ref> as the baseline network structure, and search for the best kernel size, expansion ratio, channel size, and other design choices. Our search space also includes swish activation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref>, squeeze-and-excitation module <ref type="bibr" target="#b7">[8]</ref> (similar to <ref type="bibr" target="#b28">[29]</ref>), and grouped convolutions with group size 1 or 2 for 1x1 convolutions (similar to <ref type="bibr" target="#b29">[30]</ref>). However, unlike these prior works that use vanilla depthwise convolution as the basic convolutional op, we adopt our proposed MixConv as the search options. Specifically, we have five MixConv candidates with group size g = 1, ..., 5:</p><p>• 3x3: MixConv with one group of filters (g = 1) with kernel size 3x3.  • ...</p><p>• 3x3, 5x5, 7x7, 9x9, 11x11: MixConv with five groups of filters (g = 5) with kernel size {3x3, 5x5, 7x7, 9x9, 11x11}. Each group has roughly the same number of channels.</p><p>In order to simplify the search process, we don't include exponential channel partition or dilated convolutions in our search space, but it is trivial to integrate them in future work. Similar to recent neural architecture search approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>, we directly search on ImageNet train set, and then pick a few top-performing models from search to verify their accuracy on ImageNet validation set and transfer learning datasets. <ref type="table" target="#tab_7">Table 2</ref> shows the ImageNet performance of MixNets. Here we obtain MixNet-S and M from neural architecture search, and scale up MixNet-M with depth multiplier 1.3 to obtain MixNet-L. All models are trained with the same settings as MnasNet <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MixNet Performance on ImageNet</head><p>In general, our MixNets outperform all latest mobile ConvNets: Compared to the handcrafted models, our MixNets improve top-1 accuracy by 4.2% than MobileNetV2 [23] and 3.5% than ShuffleNetV2 <ref type="bibr" target="#b17">[18]</ref>, under the same FLOPS constraint; Compared to the latest automated models, our MixNets achieve better accuracy than MnasNet (+1.3%), FBNets (+2.0%), ProxylessNAS (+2.2%) under similar FLOPS constraint. Our models also achieve similar performance as the latest MobileNetV3 <ref type="bibr" target="#b5">[6]</ref>, which is developed concurrently with our work with several manual optimizations in addition of architecture search. In particular, our MixNet-L achieves a new state-of-the-art 78.9% top-1 accuracy under typical mobile FLOPS (&lt;600M) constraint. <ref type="figure" target="#fig_6">Figure 7</ref> visualizes the ImageNet performance comparison. We observe that recent progresses on neural architecture search have significantly improved model performance    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">MixNet Architectures</head><p>To understand why our MixNets achieve better accuracy and efficiency, <ref type="figure" target="#fig_7">Figure 8</ref> illustrates the network architecture for MixNet-S and MixNet-M from <ref type="table" target="#tab_7">Table 2</ref>. In general, they both use a variety of MixConv with different kernel sizes throughout the network: small kernels are more common in early stage for saving computational cost, while large kernels are more common in later stage for better accuracy. We also observe that the bigger MixNet-M tends to use more large kernels and more layers to pursing higher accuracy, with the cost of more parameters and FLOPS. Unlike vanilla depthwise convolutions that suffer from serious accuracy degradation for large kernel sizes <ref type="figure">(Figure 1</ref>), our MixNets are capable of utilizing very large kernels such as 9x9 and 11x11 to capture high-resolution patterns from input images, without hurting model accuracy and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>TrainSize TestSize Classes CIFAR-10 <ref type="bibr" target="#b12">[13]</ref> 50,000 10,000 10 CIFAR-100 <ref type="bibr" target="#b12">[13]</ref> 50,000 10,000 100 Oxford-IIIT Pets <ref type="bibr" target="#b18">[19]</ref> 3,680 3,369 37 Food-101 <ref type="bibr" target="#b0">[1]</ref> 75,750 25,250 101  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Transfer Learning Performance</head><p>We have also evaluated our MixNets on four widely used transfer learning datasets, including CIFAR-10/100 <ref type="bibr" target="#b12">[13]</ref>, Oxford-IIIT Pets <ref type="bibr" target="#b18">[19]</ref> , and Food-101 <ref type="bibr" target="#b0">[1]</ref>. <ref type="table" target="#tab_10">Table 3</ref> shows their statistics of train set size, test set size, and number of classes. <ref type="figure" target="#fig_8">Figure 9</ref> compares our MixNet-S/M with a list of previous models on transfer learning accuracy and FLOPS. For each model, we first train it from scratch on ImageNet and than finetune all the weights on the target dataset using similar settings as <ref type="bibr" target="#b11">[12]</ref>. The accuracy and FLOPS data for MobileNets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23]</ref>, Inception <ref type="bibr" target="#b23">[24]</ref>, ResNet <ref type="bibr" target="#b4">[5]</ref>, DenseNet <ref type="bibr" target="#b9">[10]</ref> are from <ref type="bibr" target="#b11">[12]</ref>. In general, our MixNets significantly outperform previous models on all these datasets, especially on the most widely used CIFAR-10 and CIFAR-100, suggesting our MixNets also generalize well to transfer learning. In particular, our MixNet-M achieves 97.92% accuracy with 3.49M parameters and 352M FLOPS, which is 11.4x more efficient with 1% higher accuracy than ResNet-50 <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we revisit the impact of kernel size for depthwise convolution, and identify that traditional depthwise convolution suffers from the limitations of single kernel size. To address this issue, we proposes MixConv, which mixes multiple kernels in a single op to take advantage of different kernel sizes. We show that our MixConv is a simple drop-in replacement of vanilla depthwise convolution, and improves the accuracy and efficiency for MobileNets, on both image classification and object detection tasks. Based on our proposed MixConv, we further develop a new family of MixNets using neural architecture search techniques. Experimental results show that our MixNets achieve significantly better accuracy and efficiency than all latest mobile ConvNets on both ImageNet classification and four widely used transfer learning datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Mixed depthwise convolution (MixConv) -Unlike vanilla depthwise convolution that applies a single kernel to all channels, MixConv partitions channels into groups and apply different kernel size to each group.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>A demo of TensorFlow MixConv.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3</head><label>3</label><figDesc>shows a simple demo of TensorFlow python implementation for MixConv. On certain platforms, MixConv could be implemented as a single op and optimized with group convolution. Nevertheless, as shown in the figure, MixConv can be considered as a simple drop-in replacement of vanilla depthwise convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>MixConv performance on ImageNet -Each point denotes a model with kernel size from 3x3 to 13x13, same asFigure 1. MixConv is smaller, faster, and achieves higher accuracy than vanilla depthwise convolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Performance of exponential partition (+exp) and dilated kernels (+dilated).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>ImageNet performance comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>MixNet architectures -MixNet-S and MixNet-M are from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Transfer learning performance -MixNet-S/M are fromTable 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1907.09595v3 [cs.CV] 1 Dec 2019</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>74.2</cell><cell></cell><cell></cell></row><row><cell>ImageNet Top-1 Accuracy (%)</cell><cell>71.0 71.2 71.4 71.6 71.8 72.0</cell><cell></cell><cell></cell><cell>ImageNet Top-1 Accuracy (%)</cell><cell>73.0 73.2 73.4 73.6 73.8 74.0</cell><cell></cell><cell></cell></row><row><cell></cell><cell>70.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>k3x3</cell><cell>k5x5</cell><cell>k7x7</cell><cell>k9x9 k11x11 k13x13</cell><cell>k3x3</cell><cell>k5x5</cell><cell>k7x7</cell><cell>k9x9 k11x11 k13x13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>is the final output channel size.</figDesc><table><row><cell>def mixconv(x, filters,  *  * args):</cell></row><row><cell># x: input features with shape [N,H,W,C]</cell></row><row><cell># filters: a list of filters with shape [K_i, K_i,</cell></row><row><cell>C_i, M_i] for i−th group.</cell></row><row><cell>G = len(filters) # number of groups.</cell></row><row><cell>y = []</cell></row><row><cell>for xi, fi in zip(tf.split(x, G, axis=−1), filters):</cell></row><row><cell>y.append(tf.nn.depthwise_conv2d(xi, fi,  *  * args))</cell></row><row><cell>return tf.concat(y, axis=−1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison on COCO object detection.</figDesc><table><row><cell>ImageNet Top-1 Accuracy (%)</cell><cell>72.25 72.50 72.75 73.00 73.25 73.50</cell><cell cols="3">DepthwiseConv9x9</cell><cell></cell><cell></cell><cell cols="3">MixConv3579</cell><cell></cell></row><row><cell></cell><cell>72.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>1 2 (</cell><cell>3</cell><cell>4 5 (</cell><cell>6</cell><cell>7</cell><cell>8 9 (</cell><cell>1 0</cell><cell>1 1 1 2</cell><cell>1 3</cell><cell>1 4</cell></row><row><cell></cell><cell></cell><cell cols="2">s 2 )</cell><cell cols="2">s 2 )</cell><cell></cell><cell cols="2">s 2 )</cell><cell cols="2">( s 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">MobileNetV2 Layer ID</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc>MixNet performance results on ImageNet 2012<ref type="bibr" target="#b21">[22]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 2 .</head><label>2</label><figDesc>We mainly highlight MixConv kernel size (e.g. {3x3, 5x5}) and input/output tensor shape.<ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref> than previous hand-crafted mobile ConvNets<ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref>. However, by introducing a new type of efficient MixConv, we can further improve model accuracy and efficiency based on the same neural architecture search techniques.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 3 :</head><label>3</label><figDesc>Transfer learning datasets.</figDesc><table><row><cell></cell><cell>98.0 1.0</cell><cell></cell><cell>CIFAR10</cell><cell></cell><cell></cell><cell>CIFAR100</cell><cell>93</cell><cell></cell><cell cols="2">Oxford-IIIT Pets</cell><cell>Food-101</cell></row><row><cell>Accuracy(%)</cell><cell>0.4 0.6 0.8 96.5 97.0 97.5</cell><cell></cell><cell></cell><cell>84 86</cell><cell></cell><cell></cell><cell>91 92</cell><cell></cell><cell></cell><cell>86.5 87.0 87.5</cell></row><row><cell></cell><cell>0.0 0.0 0.2 0 96.0</cell><cell cols="2">0.2 1000 2000 3000 4000</cell><cell>82</cell><cell>0</cell><cell>0.4 1000 2000 3000 4000</cell><cell>90</cell><cell>0</cell><cell cols="2">0.6 1000 2000 3000 4000 85.5 86.0</cell><cell>0</cell><cell>0.8</cell><cell>1.0 1000 2000 3000 4000</cell></row><row><cell></cell><cell></cell><cell cols="2">FLOPS (Millions)</cell><cell></cell><cell></cell><cell>FLOPS (Millions)</cell><cell></cell><cell></cell><cell cols="2">FLOPS (Millions)</cell><cell>FLOPS (Millions)</cell></row><row><cell></cell><cell cols="2">MobileNetV1</cell><cell>MobileNetV2</cell><cell></cell><cell></cell><cell>MobileNetV2(1.4)</cell><cell cols="3">Inception-v1</cell><cell>ResNet-50</cell><cell>DenseNet-121</cell><cell>MixNet</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page" from="446" to="461" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware. ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="1610" to="02357" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sigmoid-weighted linear units for neural network function approximation in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Elfwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiji</forename><surname>Uchibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Doya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="770" to="778" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02244</idno>
		<title level="m">Searching for mobilenetv3</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Laurens van der Maaten, and Kilian Q Weinberger. Multi-scale dense networks for resource efficient image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danlu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &lt;0.5 mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Do better imagenet models transfer better? CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progressive neural architecture search. ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">DARTS: Differentiable architecture search. ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m">Searching for activation functions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Going deeper with convolutions. CVPR</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="2818" to="2826" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<title level="m">Rethinking model scaling for convolutional neural networks. ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="5987" to="5995" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
