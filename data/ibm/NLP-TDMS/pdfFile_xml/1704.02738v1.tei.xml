<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detail-revealing Deep Video Super-resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong † University of Toronto ‡ Megvii Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong † University of Toronto ‡ Megvii Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong † University of Toronto ‡ Megvii Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong † University of Toronto ‡ Megvii Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong † University of Toronto ‡ Megvii Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Detail-revealing Deep Video Super-resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous CNN-based video super-resolution approaches need to align multiple frames to the reference. In this paper, we show that proper frame alignment and motion compensation is crucial for achieving high quality results. We accordingly propose a "sub-pixel motion compensation" (SPMC) layer in a CNN framework. Analysis and experiments show the suitability of this layer in video SR. The final end-to-end, scalable CNN framework effectively incorporates the SPMC layer and fuses multiple frames to reveal image details. Our implementation can generate visually and quantitatively high-quality results, superior to current state-of-the-arts, without the need of parameter tuning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As one of the fundamental problems in image processing and computer vision, video or multi-frame super-resolution (SR) aims at recovering high-resolution (HR) images from a sequence of low-resolution (LR) ones. In contrast to singleimage SR where details have to be generated based on only external examples, an ideal video SR system should be able to correctly extract and fuse image details in multiple frames. To achieve this goal, two important sub-problems are to be answered: <ref type="bibr" target="#b0">(1)</ref> how to align multiple frames to construct accurate correspondence; and (2) how to effectively fuse image details for high-quality outputs.</p><p>Motion Compensation While large motion between consecutive frames increases the difficulty to locate corresponding image regions, subtle sub-pixel motion contrarily benefits restoration of details. Most previous methods compensate inter-frame motion by estimating optical flow <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref> or applying block-matching <ref type="bibr" target="#b27">[28]</ref>. After motion is estimated, traditional methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref> reconstruct the HR output based on various imaging models and image priors, typically under an iterative estimation framework. Most of these methods involve rather intensive caseby-case parameter-tuning and costly computation.</p><p>Code will be available upon acceptance: link Recent deep-learning-based video SR methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref> compensate inter-frame motion by aligning all other frames to the reference one, using backward warping. We show that such a seemingly reasonable technical choice is actually not optimal for video SR, and improving motion compensation can directly lead to higher quality SR results. In this paper, we achieve this by proposing a sub-pixel motion compensation (SPMC) strategy, which is validated by both theoretical analysis and extensive experiments.</p><p>Detail Fusion Besides motion compensation, proper image detail fusion from multiple frames is the key to the success of video SR. We propose a new CNN framework that incorporates the SPMC layer, and effectively fuses image information from aligned frames. Although previous CNNbased video SR systems can produce sharp-edge images, it is not entirely clear whether the image details are those inherent in input frames, or learned from external data. In many practical applications such as face or text recognition, only true HR details are useful. In this paper we provide insightful ablation study to verify this point.</p><p>Scalability A traditionally-overlooked but practicallymeaningful property of SR systems is the scalability. In many previous learning-based SR systems, the network structure is closely coupled with SR parameters, making them less flexible when new SR parameters need to be applied. For example, ESPCN <ref type="bibr" target="#b25">[26]</ref> output channel number is determined by the scale factor. VSRnet <ref type="bibr" target="#b13">[14]</ref> and VESPCN <ref type="bibr" target="#b1">[2]</ref> can only take a fixed number of temporal frames as input, once trained.</p><p>In contrast, our system is fully scalable. First, it can take arbitrary-size input images. Second, the new SPMC layer does not contain trainable parameters and can be applied for arbitrary scaling factors during testing. Finally, the ConvLSTM-based <ref type="bibr" target="#b28">[29]</ref> network structure makes it possible to accept an arbitrary number of frames for SR in testing phase.</p><p>neural networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26]</ref>. Most of them resize input frames before sending them to the network <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>, and use very deep <ref type="bibr" target="#b14">[15]</ref>, recursive <ref type="bibr" target="#b15">[16]</ref> or other networks to predict HR results. Shi et al. <ref type="bibr" target="#b25">[26]</ref> proposed a subpixel network, which directly takes lowresolution images as input, and produces a high-res one with subpixel location. Ledig et al. <ref type="bibr" target="#b17">[18]</ref> used a trainable deconvolution layer instead.</p><p>For deep video SR, Liao et al. <ref type="bibr" target="#b18">[19]</ref> adopted a separate step to construct high-resolution SR-drafts, which are obtained under different flow parameters. Kappeler et al. <ref type="bibr" target="#b13">[14]</ref> estimated optical flow and selected corresponding patches across frames to train a CNN. In both methods, motion estimation is separated from training. Recently, Caballero et al. <ref type="bibr" target="#b1">[2]</ref> proposed the first end-to-end video SR framework, which incorporates motion compensation as a submodule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motion Estimation</head><p>Deep neural networks were also used to solve motion estimation problems. Zbontar and LeCun <ref type="bibr" target="#b30">[31]</ref> and Luo et al. <ref type="bibr" target="#b21">[22]</ref> used CNNs to learn a patch distance measure for stereo matching. Fischer et al. <ref type="bibr" target="#b7">[8]</ref> and Mayer et al. <ref type="bibr" target="#b24">[25]</ref> proposed end-to-end networks to predict optical flow and stereo disparity.</p><p>Progress was made in spatial transformer networks <ref type="bibr" target="#b9">[10]</ref> where a differentiable layer warps images according to predicted affine transformation parameters. Based on it, Warp-Net <ref type="bibr" target="#b12">[13]</ref> used a similar scheme to extract sparse correspondence. Yu et al. <ref type="bibr" target="#b29">[30]</ref> warped output based on predicted optical flow as a photometric loss for unsupervised optical flow learning. Different from these strategies, we introduce a Sub-pixel Motion Compensation (SPMC) layer, which is suitable for the video SR task.  We first introduce our notations for video SR. It takes a sequence of N F = (2T + 1) LR images as input (T is the size of temporal span in terms of number of frames), where Ω L = {I L −T , · · · , I L 0 , · · · , I L T }. The output HR image I H 0 corresponds to center reference frame I L 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Sub-pixel Motion Compensation (SPMC)</head><p>LR Imaging Model The classical imaging model for LR images <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref> is expressed as</p><formula xml:id="formula_0">I L i = SKW 0→i I H 0 + n i ,<label>(1)</label></formula><p>where W 0→i is the warping operator to warp from the 0th to ith frame. K and S are downsampling blur and decimation operators, respectively. n i is the additive noise to frame i. For simplicity's sake, we neglect operator K in the following analysis, since it can be absorbed by S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flow Direction and Transposed Operators</head><p>Operator W 0→i indicates the warping process. To compute it, one needs to first calculate the motion field F i→0 (from the ith to 0th frame), and then perform backward warping to produce the warped image. However, current deep video SR methods usually align other frames back to I L 0 , which actually makes use of flow F 0→i .</p><p>More specifically, directly minimizing the L 2 -norm re-</p><formula xml:id="formula_1">construction error i SW 0→i I H 0 − I L i 2 results in I H 0 = ( i W T 0→i S T SW 0→i ) −1 ( i W T 0→i S T I L i ). (2)</formula><p>With certain assumptions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>, W T 0→i S T SW 0→i becomes a diagonal matrix. The solution to Eq. (2) reduces to a feedforward generation process of</p><formula xml:id="formula_2">I H 0 = i W T 0→i S T I L i i W T 0→i S T 1 ,<label>(3)</label></formula><p>where 1 is an all-one vector with the same size as I L i . The operators that are actually applied to I L i are S T and W T 0→i . S T is the transposed decimation corresponding to zero-upsampling. W T 0→i is the transposed forward warping using flow F i→0 . A 1D signal example for these operators is shown in <ref type="figure" target="#fig_1">Fig. 1</ref>. We will further analyze the difference of forward and backward warping after explaining our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Method</head><p>Our method takes a sequence of N F LR images as input and produces one HR image I H 0 . It is an end-to-end fully trainable framework that comprises of three modules: motion estimation, motion compensation and detail fusion. They are respectively responsible for motion field estimation between frames; aligning frames by compensating motion; and finally increasing image scale and adding image details. We elaborate on each module in the following.  <ref type="figure">Figure 2</ref>. Our framework. Network configuration for the ith time step. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPMC</head><formula xml:id="formula_3">F i Grid Generator Y 0 (a) (b) ... ... L I i Sampler L I -T L I T L I 0 F T F 0 F -T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motion Estimation</head><p>The motion estimation module takes two LR frames as input and produces a LR motion field as</p><formula xml:id="formula_4">F i→j = Net M E (I L i , I L j ; θ M E ),<label>(4)</label></formula><p>where F i→j = (u i→j , v i→j ) is the motion field from frame I L i to I L j . θ M E is the set of module parameters. Using neural networks for motion estimation is not a new idea, and existing work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30]</ref> already achieves good results. We have tested FlowNet-S <ref type="bibr" target="#b7">[8]</ref> and the motion compensation transformer (MCT) module from VESPCN <ref type="bibr" target="#b1">[2]</ref> for our task. We choose MCT because it has less parameters and accordingly less computation cost. It can process 500+ single-channel image pairs (100 × 100 in pixels) per second. The result quality is also acceptable in our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">SPMC Layer</head><p>According to the analysis in Sec. 2, we propose a novel layer to utilize sub-pixel information from motion and simultaneously achieve sub-pixel motion compensation (SPMC) and resolution enhancement. It is defined as</p><formula xml:id="formula_5">J H = Layer SP M C (J L , F ; α),<label>(5)</label></formula><p>where J L and J H are input LR and output HR images, F is optical flow used for transposed warping and α is the scaling factor. The layer contains two submodules.</p><p>Sampling Grid Generator In this step, transformed coordinates are first calculated according to estimated flow</p><formula xml:id="formula_6">F = (u, v) as x s p y s p = W F ;α x p y p = α x p + u p y p + v p ,<label>(6)</label></formula><p>where p indexes pixels in LR image space. x p and y p are the two coordinates of p. u p and v p are the flow vectors estimated from previous stage. We denote transform of coordinates as operator W F ;α , which depends on flow field F and scale factor α. x s p and y s p are the transformed coordinates in an enlarged image space, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Differentiable Image Sampler</head><p>Output image is constructed in the enlarged image space according to x s p and y s p . The resulting image J H q is</p><formula xml:id="formula_7">J H q = p=1 J L p M (x s p − x q )M (y s p − y q ),<label>(7)</label></formula><p>where q indexes HR image pixels. x q and y q are the two coordinates for pixel q in the HR grid. M (·) is the sampling kernel, which defines the image interpolation methods (e.g. bicubic, bilinear, and nearest-neighbor).</p><p>We further investigate differentiability of this layer. As indicated in Eq. (5), the SPMC layer takes one LR image J L and one flow field F = (u, v) as input, without other trainable parameters. For each output pixel, partial derivative with respect to each input pixel is</p><formula xml:id="formula_8">∂J H q ∂J L p = p=1 M (x s p − x q )M (y s p − y q ).<label>(8)</label></formula><p>It is similar to calculating partial derivatives with respect to flow field (u p , v p ) using the chain rule as</p><formula xml:id="formula_9">∂J H q ∂u p = ∂J H q ∂x s p · ∂x s p ∂u p = α p=1 J L p M (x s p − x q )M (y s p − y q ),<label>(9)</label></formula><p>where M (·) is the gradient of sampling kernel M (·). Similar derivatives can be derived for ∂Jq ∂vp . We choose M (x) = max(0, 1 − |x|), which corresponds to the bilinear interpolation kernel, because of its simplicity and convenience to calculate gradients. Our final layer is fully differentiable, allowing back-propagating loss to flow fields smoothly. The advantages of having this type of layers is threefold.</p><p>• This layer can simultaneously achieve motion compensation and resolution enhancement. Note in most previous work, they are separate steps (e.g. backward warping + bicubic interpolation).</p><p>• This layer is parameter free and fully differentiable, which can be effectively incorporated into neural networks with almost no additional cost.</p><p>• The rationale behind this layer roots from accurate LR imaging model, which ensures good performance in theory. It also demonstrates good results in practice, as we will present later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Detail Fusion Net</head><p>The SPMC layer produces a series of motion compensated frames {J H i } expressed as</p><formula xml:id="formula_10">J H i = Layer SP M C (I L i , F i→0 ; α).<label>(10)</label></formula><p>Design of the following network is non-trivial due to the following considerations. First, {J H i } are already HR-size images that produce large feature maps, thus computational cost becomes an important factor.</p><p>Second, due to the property of forward warping and zeroupsampling, {J H i } is sparse and majority of the pixels are zero-valued (e.g. about 15/16 are zeros for scale factor 4×). This requires the network to have large receptive fields to capture image patterns in J H i . Using simple interpolation to fill these holes is not a good solution because interpolated values would dominate during training.</p><p>Finally, special attention needs to be paid to the use of the reference frame. On the one hand, we rely on the reference frame as the guidance for SR so that the output HR image is consistent with the reference frame in terms of image structures. On the other hand, over-emphasizing the reference frame could impose an adverse effect of neglecting information in other frames. The extreme case is that the system behaves like a single-image SR one.</p><p>Network Architecture We design an encoder-decoder <ref type="bibr" target="#b23">[24]</ref> style structure with skip-connections (see <ref type="figure">Fig. 2</ref>) to tackle above issues. This type of structure has been proven to be effective in many image regression tasks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref>. The encoder sub-network reduces the size of input HR image to 1/4 of it in our case, leading to reduced computation cost. It also makes the feature maps less sparse so that information can be effectively aggregated without the need of employing very deep networks. Skip-connections are used for all stages to accelerate training.</p><p>A ConvLSTM module <ref type="bibr" target="#b28">[29]</ref> is inserted in the middle stage as a natural choice for sequential input. The network structure includes</p><formula xml:id="formula_11">f i = Net E (J H i ; θ E ) g i , s i = ConvLSTM(f i , s i−1 ; θ LST M )<label>(11)</label></formula><formula xml:id="formula_12">I (i) 0 = Net D (g i , S E i ; θ D ) + I L↑ 0</formula><p>where Net E and Net D are encoder and decoder CNNs with parameters θ E and θ D . f i is the output of encoder net. g i is the input of decoder net. s i is the hidden state for LSTM at the ith step. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Strategy</head><p>Our framework consists of three major components, each has a unique functionality. Training the whole system in an end-to-end fashion with random initialization would result in zero flow in motion estimation, making the final results similar to those of single-image SR. We therefore separate training into three phases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phase 1</head><p>We only consider Net M E in the beginning of training. Since we do not have ground truth flow, unsupervised warping loss is used as <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30]</ref> </p><formula xml:id="formula_13">L M E = T i=−T I L i −Ĩ L 0→i 1 + λ 1 ∇F i→0 1 ,<label>(12)</label></formula><p>whereĨ L 0→i is the backward warped I L 0 according to estimated flow F i→0 , using a differentiable layer similar to spatial transformer <ref type="bibr" target="#b9">[10]</ref>. Note that this image is in low resolution, aligned with I L i . ∇F i→0 1 is the total variation term on each (u, v)-component of flow F i→0 . λ 1 is the regularization weight. We set λ 1 = 0.01 in all experiments.  Phase 2 We then fix the learned weights θ M E and only train Net DF . This time we use Euclidean loss between our estimated HR reference frame and the ground truth as</p><formula xml:id="formula_14">L SR = T i=−T κ i I H 0 − I (i) 0 2 2 ,<label>(13)</label></formula><p>where I (i) 0 is our network output in the ith time step, corresponding to reference frame I L 0 . {κ i } are the weights for each time step. We empirically set κ −T = 0.5 and κ T = 1.0, and linearly interpolate intermediate values.</p><p>Phase 3 In the last stage, we jointly tune the whole system using the total loss as</p><formula xml:id="formula_15">L = L SR + λ 2 L M E ,<label>(14)</label></formula><p>where λ 2 is the weight balancing two losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct our experiments on a PC with an Intel Xeon E5 CPU and an NVIDIA Titan X GPU. We implement our framework on the TensorFlow platform <ref type="bibr" target="#b5">[6]</ref>, which enables us to easily develop our special layers and experiment with different network configurations.</p><p>Data Preparation For the super-resolution task, training data needs to be of high-quality without noise while containing rich fine details. To our knowledge, there is no such publicly available video dataset that is large enough to train our deep networks. We thus collect 975 sequences from high-quality 1080p HD video clips. Most of them are commercial videos shot with high-end cameras and contain both natural-world and urban scenes that have rich details. Each sequence contains 31 frames following the configuration of <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref>. We downsample the original frames to 540 × 960 pixels as HR ground truth using bicubic interpolation. LR input is obtained by further downsampling HR frames to 270 × 480, 180 × 320 and 135 × 240 sizes. We randomly choose 945 of them as training data, and the rest 30 sequences are for validation and testing.</p><p>Model Training For model training, we use Adam solver <ref type="bibr" target="#b16">[17]</ref> with learning rate of 0.0001, β 1 = 0.9 and β 2 = 0.999. We apply gradient clip only to weights of ConvLSTM module (clipped by global norm 3) to stabilize the training process. At each iteration, we randomly sample N F consecutive frames (e.g. N F = 3, 5, 7) from one sequence, and randomly crop a 100 × 100 image region as training input. The corresponding ground truth is accordingly cropped from the reference frame with size 100α × 100α where α is the scaling factor. Above parameters are fixed for all experiments. Batch size varies according to different settings, which is determined as the maximal value allowed by GPU memory.</p><p>We first train the motion estimation module using only loss L M E in Eq. (12) with λ 1 = 0.01. After about 70,000 iterations, we fix the parameters θ M E and train the system using only loss L SR in Eq. (13) for 20,000 iterations. Finally, all parameters are trained using total loss L in Eq. (14), λ 2 is empirically chosen as 0.01. All trainable variables are initialized using Xavier methods <ref type="bibr" target="#b8">[9]</ref>.</p><p>In the following analysis and experiments, we train sev- eral models under different settings. For simplicity, we use ×(·) to denote scaling factors (e.g. ×2, ×3, and ×4). And F(·) is used as the number of input frames (e.g. F3, F5, and F7). Moreover, our ConvLSTM based DF net produces multiple outputs (one for each time step), we use {#1, #2, · · · } to index output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Effectiveness of SPMC Layer</head><p>We first evaluate the effectiveness of the proposed SPMC layer. For comparison, a baseline model BW (F3-×4) is used. It is achieved by fixing our system in <ref type="figure">Fig. 2</ref>, except replacing the SPMC layer with backward warping, followed by bicubic interpolation, which is a standard alignment procedure. An example is shown in <ref type="figure" target="#fig_5">Fig. 4</ref>. In <ref type="figure" target="#fig_5">Fig. 4(a)</ref>, bicubic ×4 for reference frame contains severe aliasing for the tile patterns. Baseline model BW produces 3 outputs corresponding to three time steps in <ref type="figure" target="#fig_5">Fig. 4(b)-(d)</ref>. Although results are sharper when more frames are used, tile patterns are obviously wrong compared to ground truth in <ref type="figure" target="#fig_5">Fig. 4(e)</ref>. This is due to loss of sub-pixel information as analyzed in Section 2. The results are similar to the output of single image SR, where the reference frame dominates.</p><p>As shown in <ref type="figure" target="#fig_5">Fig. 4(f)</ref>, if we only use one input image in our method, the recovered pattern is also similar to <ref type="figure" target="#fig_5">Fig. 4(a)-(d)</ref>. However, with more input frames fed into the system, the restored images dramatically improve, as shown in <ref type="figure" target="#fig_5">Fig. 4</ref>(g)-(h), which are both sharper and closer to the ground truth. Quantitative values on our validation set are listed in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Detail Fusion vs. Synthesis</head><p>We further investigate if our recovered details truly exist in original frames. One example is already shown in <ref type="figure" target="#fig_5">Fig. 4</ref>. Here we conduct a more illustrative experiment by replacing all input frames with the same reference frame. Specifically, <ref type="figure">Fig. 5</ref>(f)-(h) are outputs using 3 consecutive frames (F3-×3). The numbers and logo are recovered nicely. However, if we only use 3 copies of the same reference frame as input and test them on the same pre-trained model, the results are almost the same as using only one frame. This manifests that our final result shown in <ref type="figure">Fig. 5(h)</ref> is truly recovered from the 3 different input frames based on their internal detail information, rather than synthesized from external examples because if the latter holds, the synthesized details should also appear even if we use only one reference frame. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">DF-Net with Various Inputs</head><p>Our proposed detail fusion (DF) net takes only J H i as input. To further evaluate if the reference frame is needed, we design two baseline models. Model DF-bic and DF-0up respectively add bicubic and zero-upsampled I L 0 as another channel of input to DF net. Visual comparison in <ref type="figure">Fig. 6</ref> shows that although all models can recover reasonable details, the emphasis on the reference frame may mislead detail recovery and slightly degrade results quantitatively on the evaluation set (see <ref type="table" target="#tab_1">Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparisons with Video SR Methods</head><p>We compare our method against previous video SR methods on the evaluation dataset. BayesSR <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref> is viewed as the best-performing traditional method that iteratively estimates motion flow, blur kernel, noise and the HR image. DESR <ref type="bibr" target="#b18">[19]</ref> ensembles "draft" based on estimated flow, which makes it an intermediate solution between traditional and CNN-based methods. We also include a recent deep-learning-based method VSRnet <ref type="bibr" target="#b13">[14]</ref> in comparison. We use author-provided implementation for all these methods. VESPCN <ref type="bibr" target="#b1">[2]</ref> did not provide code or pre-trained model, so we only list their reported PSNR/SSIM on the 4video dataset VID4 <ref type="bibr" target="#b19">[20]</ref>. The quantitative results are listed in <ref type="table" target="#tab_2">Table 2</ref>. Visual comparisons are shown in <ref type="figure">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparisons with Single Image SR</head><p>Since our framework is flexible, we can set N F = 1 to turn it into a single image SR solution. We compare this approach with three recent image SR methods: SRCNN <ref type="bibr" target="#b2">[3]</ref>, FSRCNN <ref type="bibr" target="#b3">[4]</ref> and VDSR <ref type="bibr" target="#b14">[15]</ref>, on dataset Set5 <ref type="bibr" target="#b0">[1]</ref> and Set14 <ref type="bibr" target="#b31">[32]</ref>. To further compare the performance of using multiple frames against single, we compare all single image methods with our method under F3 setting on our evaluation dataset SPMCS. The quantitative results are listed in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>For the F1 setting on Set5 and Set14, our method produces comparable or slightly lower PSNR or SSIM results. Under the F3 setting, our method outperforms image SR methods by a large margin, indicating that our multi-frame setting can effectively fuse information in multiple frames. An example is shown in <ref type="figure" target="#fig_8">Fig. 9</ref>, where single image SR can-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Real-World Examples</head><p>The LR images in the above evaluation are produced though downsampling (bicubic interpolation). Although this is a standard approach for evaluation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>, the generated LR images may not fully resemble the real-world cases. To verify the effectiveness of our method on real-world data, we captured four examples as shown in <ref type="figure">Fig. 8</ref>. For each object, we capture a short video using a hand-held cellphone camera, and extract 31 consecutive frames from it. We then crop a 135 × 240 region from the center frame, and use TLD tracking <ref type="bibr" target="#b11">[12]</ref> to track and crop the same region from all other frames as the input data to our system. <ref type="figure">Fig. 8</ref> shows the SR result of the center frame for each sequence. Our method faithfully recovers the textbook characters and fine image details using the F7-×4 model. More examples are included in our supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Model Complexity and Running Time</head><p>Using our un-optimized TensorFlow code, the F7-×4 model takes about 0.26s to process 7 input images with size 180 × 120 for one HR output. In comparison, reported timings for other methods (F31) are 2 hours for Liu et al. <ref type="bibr" target="#b19">[20]</ref>, 10 min. for Ma et al. <ref type="bibr" target="#b22">[23]</ref>, and 8 min. for DESR <ref type="bibr" target="#b18">[19]</ref>. VS-Rnet <ref type="bibr" target="#b13">[14]</ref> requires ≈40s for F5 configuration. Our method can be further accelerated to 0.19s for F5 and 0.14s for F3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Concluding Remarks</head><p>We have proposed a new deep-learning-based approach for video SR. Our method includes a sub-pixel motion compensation layer that can better handle inter-frame motion for this task. Our detail fusion (DF) network that can effectively fuse image details from multiple images after SPMC alignment. We have conducted extensive experiments to validate the effectiveness of each module. Results show that our method can accomplish high-quality results both qualitatively and quantitatively, at the same time being flexible on scaling factors and numbers of input frames.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Visualization of operators in image formation. (a) Decimation operator S (2×) reduces the input 1D signal to its halfsize. The transpose S T corresponds to zero-upsampling. (b) With arrows indicating motion, warping operator W produces the blue signal from the gray one through backward warping. W T produces the green signal through forward warping. (c) Illustration of matrices S, S T , W and W T . Grayed and white blocks indicate values 1 and 0 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Subpixel Motion Compensation layer (×4). (a) Layer diagram. (b) Illustration of the SPMC layer (×4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>S E i for all i are intermediate feature maps of Net E , used for skip-connection. I L↑ 0 is the bicubic upsampled I L 0 . I (i) 0 is the ith time step output. The first layer of Net E and the last layer of Net D have kernel size 5 × 5. All other convolution layers use kernel size 3 × 3, including those inside ConvLSTM. Deconvolution layers are with kernel size 4 × 4 and stride 2. Rectified Linear Units (ReLU) are used for every conv/deconv layer as the activation function. For skip-connection, we use SUM operator between connected layers. Other parameters are labeled in Fig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Ground truth (f) Using SPMC #1 (g) Using SPMC #2 (h) Using SPMC #3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Effectiveness of SPMC Layer (F3-×4). (a) Bicubic ×4. (b)-(d) Output for each time step using BW. (e) Ground truth. (f)-(h) Outputs using SPMC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>SR using multiple frames (F3-×3). (a) Bicubic ×3. (b)-(d) Outputs for each time step using 3 reference frames that are the same. (e) Ground truth. (f)-(h) Outputs using 3 consecutive frames. Detail fusion net with various inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Comparisons with video SR methods.(a) Bicubic ×4 (b) Ours (c) Bicubic ×4 (d) Ours Real-world examples under configuration (F7-×4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Comparisons with single image SR methods. (a) Bicubic ×4. (b)-(d) Output from image SR methods. (e) Our result using 1 frame. (f) Our result using 3 frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Performance of baseline models</figDesc><table><row><cell>Model (F3)</cell><cell>BW</cell><cell>DF-Bic</cell><cell>DF-0up</cell><cell>Ours</cell></row><row><cell>SPMCS (×4)</cell><cell>29.23 / 0.82</cell><cell>29.67 / 0.83</cell><cell>29.65 / 0.83</cell><cell>29.69 / 0.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison with video SR methods (PSNR/SSIM) .94 36.71 / 0.96 SPMCS×3 28.85 / 0.82 29.42 / 0.87 -28.55 / 0.85 31.92 / 0.90 SPMCS×4 27.02 / 0.75 27.87 / 0.80 26.64 / 0.76 24.76 / 0.77 29.69 / 0.84</figDesc><table><row><cell>Method (F3)</cell><cell>Bicubic</cell><cell>BayesSR</cell><cell>DESR</cell><cell>VSRnet</cell><cell>Ours (F3)</cell></row><row><cell cols="5">SPMCS×2 32.48 / 0.92 31.85 / 0.92 33.39 / 0Method (F5) -Bicubic BayesSR DESR VSRNet</cell><cell>Ours (F5)</cell></row><row><cell cols="3">SPMCS×2 32.48 / 0.92 31.82 / 0.92</cell><cell>-</cell><cell cols="2">35.44 / 0.95 36.62 / 0.96</cell></row><row><cell cols="3">SPMCS×3 28.85 / 0.82 29.55 / 0.87</cell><cell>-</cell><cell cols="2">30.73 / 0.88 32.10 / 0.90</cell></row><row><cell cols="6">SPMCS×4 27.02 / 0.75 28.03 / 0.81 26.97 / 0.77 28.35 / 0.79 29.89 / 0.84</cell></row><row><cell cols="2">Method (F3) BayesSR</cell><cell>DESR</cell><cell>VSRNet</cell><cell>VESPCN</cell><cell>Ours (F3)</cell></row><row><cell>Vid4×3</cell><cell>25.64 / 0.80</cell><cell>-</cell><cell cols="3">25.31 / 0.76 27.25 / 0.84 27.49 / 0.84</cell></row><row><cell>Vid4×4</cell><cell cols="5">24.42 / 0.72 23.50 / 0.67 22.81 / 0.65 25.35 / 0.76 25.52 / 0.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison with image SR methods (PSNR/SSIM) ×2) 36.66 / 0.95 37.00 / 0.96 37.53 / 0.96 37.35 / 0.96 -Set 5 (×3) 32.75 / 0.91 33.16 / 0.92 33.66 / 0.92 33.45 / 0.92 -Set 5 (×4) 30.49 / 0.86 30.71 / 0.88 31.35 / 0.88 30.96 / 0.87 -Set 14 (×2) 32.45 / 0.91 32.63 / 0.91 33.03 / 0.91 32.70 / 0.91 -Set 14 (×3) 29.30 / 0.82 29.43 / 0.83 29.77 / 0.83 29.36 / 0.83 -Set 14 (×4) 27.45 / 0.75 27.59 / 0.77 28.01 / 0.77 27.57 / 0.76 -SPMCS (×2) 35.20 / 0.95 35.56 / 0.95 36.14 / 0.96 36.23 / 0.96 36.71 / 0.96 SPMCS (×3) 30.66 / 0.87 30.87 / 0.88 31.26 / 0.89 31.18 / 0.88 31.92 / 0.90 SPMCS (×4) 28.29 / 0.79 28.43 / 0.79 28.80 / 0.81 28.80 / 0.80 29.69 / 0.84 not recover the tiled structure of the building. In contrast, our F3 model can faithfully restore it.</figDesc><table><row><cell>Method</cell><cell>SRCNN</cell><cell>FSRCNN</cell><cell>VDSR</cell><cell>Ours (F1)</cell><cell>Ours (F3)</cell></row><row><cell>Set 5 (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alberi-Morel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Real-time video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accelerating the superresolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast super-resolution reconstruction algorithm for pure translational motion and common space-invariant blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hel-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1187" to="1193" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software available from tensorflow.org</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast and robust multiframe super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1327" to="1344" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazırbaş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">Flownet: Learning optical flow with convolutional networks. ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Aistats</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Trackinglearning-detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1409" to="1422" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Warpnet: Weakly supervised matching for single-view reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video super-resolution with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="122" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04802</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video superresolution via deep draft-ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="531" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A bayesian approach to adaptive video super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient deep learning for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5695" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Handling motion blur in multi-frame super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5224" to="5232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2802" to="2810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08387</idno>
		<title level="m">Deep video deblurring</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Superresolution without explicit subpixel motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1958" to="1975" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05842</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Curves and Surfaces</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
