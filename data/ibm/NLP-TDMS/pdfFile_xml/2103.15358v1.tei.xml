<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♥</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♥</forename><forename type="middle">†</forename><surname>Bin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">♥</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
						</author>
						<title level="a" type="main">Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a new Vision Transformer (ViT) architecture Multi-Scale Vision Longformer, which significantly enhances the ViT of [11] for encoding highresolution images using two techniques. The first is the multi-scale model structure, which provides image encodings at multiple scales with manageable computational cost. The second is the attention mechanism of vision Longformer, which is a variant of Longformer [2], originally developed for natural language processing, and achieves a linear complexity w.r.t. the number of input tokens. A comprehensive empirical study shows that the new ViT significantly outperforms several strong baselines, including the existing ViT models and their ResNet counterparts, and the Pyramid Vision Transformer from a concurrent work [43], on a range of vision tasks, including image classification, object detection, and segmentation. The models and source code used in this study will be released to public soon.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vision Transformer (ViT) <ref type="bibr" target="#b10">[11]</ref> has shown promising results on image classification tasks for its strong capability of long range context modeling. But its quadratic increase of both computational and memory complexity hinders its application on many vision tasks that require highresolution feature maps computed on high-resolution images <ref type="bibr" target="#b0">1</ref> , like object detection <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b22">23]</ref>, segmentation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b4">5]</ref>, and human pose estimation <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b33">34]</ref>. Vision-language tasks, like VQA, image captioning, and image-text retrieval, also benefit from high-resolution feature maps <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b48">49]</ref>, which are extracted with pre-trained CNN models. Developing a vision Transformer that can process high-resolution feature maps is a critical step toward the goal of unifying the model architecture of vision and language modalities and improv-♥ Microsoft Corporation † indicates equal contributions. <ref type="bibr" target="#b0">1</ref> In this paper, encoding a high-resolution image means generating high-resolution feature maps for high-resolution images.</p><p>ing multi-modal representation learning.</p><p>In this paper, we propose a new vision Transformer architecture Multi-Scale Vision Longformer, which significantly enhances the baseline ViT <ref type="bibr" target="#b10">[11]</ref> for encoding highresolution images using two techniques: <ref type="bibr" target="#b0">(1)</ref> the multi-scale model structure, and (2) the attention mechanism of vision Longformer.</p><p>Models with multi-scale (pyramid, hierarchical) structure provide a comprehensive encoding of an image at multiple scales, while keeping the computation and memory complexity manageable. Deep convolutional networks are born with such multi-scale structure, which however is not true for the conventional ViT architecture. To obtain a multi-scale vision Transformer, we stack multiple (e.g., four) vision Transformers (ViT stages) sequentially. The first ViT stage operates on a high-resolution feature map but has a small hidden dimension. As we go to later ViT stages, the feature map resolution reduces while the hidden dimension increases. The resolution reduction is achieved by performing patching embedding at each ViT stage. In our experiments, we find that with the same number of model parameters and the same model FLOPs, the multiscale ViT achieves a significantly better accuracy than the vanilla ViT on image classification task. The results show that the multi-scale structure not only improves the computation and memory efficiency, but also boosts the classification performance. The proposed multi-scale ViT has the same network structure as conventional (multi-scale) CNN models such as ResNet <ref type="bibr" target="#b12">[13]</ref>, and can serve as a replaceand-plug-in choice for almost all ResNet applications. In this paper, we demonstrate this plausible property in image classification, object detection and instance segmentation.</p><p>The multi-scale structure alone is not sufficient to scale up ViT to process high-resolution images and feature maps, due to the quadratic increase of the computation and memory complexity with respect to the number of tokens in the self-attention layers. Compared to natural language tasks where data is 1-D, this problem is more severe in vision tasks where the increase in complexity is quartic (fourth order) with the increase of image resolution. For example, the computational complexity of a 4× higher resolution multihead self attention (MSA) layer (hidden dimension reduced by 4, i.e., 4H × 4W × D 4 ) equals to that of 64 layers in the original size (i.e., H × W × D). To address this challenge, we develop a 2-D version of Longformer <ref type="bibr" target="#b1">[2]</ref>, called Vision Longformer, to achieve a linear complexity w.r.t. the number of tokens (quadratic w.r.t. resolution). Our experiments show that compared to the baseline ViT, Vision Longformer shows no performance drop while significantly reduces the computational and memory cost in encoding images. The result indicates that the "local attention + global memory" structure in Vision Longformer is a desirable inductive bias for vision Transformers. We also compare Vision Longformer with other efficient attention mechanisms. The result again validates its superior performance on both image classification and object detection tasks.</p><p>The main contributions of this paper are two-fold: (1) We propose a new vision Transformer that uses the multiscale model structure and the attention mechanism of 2-D Longformer for efficient high-resolution image encoding. <ref type="bibr" target="#b1">(2)</ref> We perform a comprehensive empirical study to show that the proposed ViT significantly outperforms strong baselines, including previous ViT models, their ResNet counterparts, and a model from a concurrent work, on image classification, object detection and segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The Vision Transformer (ViT) <ref type="bibr" target="#b10">[11]</ref> applies a standard Transformer, originally developed for natural language processing (NLP), for image encoding by treating an image as a word sequence, i.e., splitting an image into patches (words) and using the linear embeddings of these patches as an input sequence. ViT has shown to outperform convolution neural network (CNN) models such as the ResNet <ref type="bibr" target="#b12">[13]</ref>, achieving state-of-the-art performance on multiple image classification benchmarks, where training data is sufficient. DeiT <ref type="bibr" target="#b39">[40]</ref> is another computer vision model that leverages Transformer. It uses a teacher-student strategy specific to Transformers to improve data efficiency in training. Thus, compared to ViT, it requires much less training data and computing resources to produce state-of-the-art image classification results. In addition to image classification, Transformers have also been applied to other compute vision tasks, including object detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b9">10]</ref>, segmentation <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b43">44]</ref>, image enhancement <ref type="bibr">[4,</ref><ref type="bibr" target="#b45">46]</ref>, image generation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b5">6]</ref>, video processing <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b52">53]</ref>, and vision-language tasks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Developing an efficient attention mechanism for highresolution image encoding is the focus of this work. Our model is inspired by the efficient attention mechanisms developed for Transformers, most of which are for NLP tasks. These mechanisms can be grouped into four categories. The first is the sparse attention mechanism, including content-independent sparsity <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b13">14]</ref> and content-dependent sparsity <ref type="bibr">[17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b50">51]</ref>. Axial Transformer <ref type="bibr" target="#b13">[14]</ref> and Image Transformer <ref type="bibr" target="#b27">[28]</ref> are among few sparsity-based efficient attentions that are developed for image generation. The second is the memory-based mechanism, including Compressive Transformers <ref type="bibr" target="#b29">[30]</ref> and Set Transformer <ref type="bibr" target="#b18">[19]</ref>. These models use some extra global tokens as static memory and allow all the other tokens to attend only to those global tokens. The third is the low-rank based mechanism. For example the Linformer <ref type="bibr" target="#b41">[42]</ref> projects the input key-value pairs into a smaller chunk, and performs cross-attention between the queries and the projected key-value pairs. The fourth is the (generalized) kernel-based mechanism, including Performer <ref type="bibr" target="#b8">[9]</ref> and Linear Transformers <ref type="bibr" target="#b15">[16]</ref>. Many models utilize hybrid attention mechanisms. For example, Longformer <ref type="bibr" target="#b1">[2]</ref>, BigBird <ref type="bibr" target="#b46">[47]</ref> and ETC <ref type="bibr" target="#b0">[1]</ref> combine the sparsity and memory mechanisms; Synthesizers <ref type="bibr" target="#b35">[36]</ref> combines the sparsity and low-rank mechanisms. Readers may refer to <ref type="bibr" target="#b38">[39]</ref> and <ref type="bibr" target="#b37">[38]</ref> for a comprehensive survey and benchmarks, respectively.</p><p>In this paper, we developed a 2-D version of Longformer <ref type="bibr" target="#b1">[2]</ref>, called Vision Longformer, which utilizes both the sparsity and memory mechanisms. Its conv-like sparsity mechanism is conceptually similar to the sparsity mechanism used in the Image Transformer <ref type="bibr" target="#b27">[28]</ref>.</p><p>The multi-scale vision Transformer architecture is another technique we use in our proposed high-resolution vision Longformer. The hierarchical Transformers <ref type="bibr" target="#b26">[27]</ref> for NLP contain two stages, with the first stage processing overlapping segments and the second stage using the embeddings of the CLS tokens from all segments as input. In our proposed Vision Longformer, size reduction is performed by the patch embedding at the beginning of each stage, by merging all tokens in a patch from previous stage into a single token at the current stage. We typically use 4 stages for our model since we have empirically verified that using 4 stages is better than using 2 or 3 stages, especially for object detection tasks. Informer <ref type="bibr" target="#b50">[51]</ref> takes a similar stacked multistage approach to encoding long sequences, where the size reduction between stages is achieved by max-pooling.</p><p>Pyramid Vision Transformer (PVT) <ref type="bibr" target="#b42">[43]</ref> is a concurrent work of ours. Both PVT and our model use a multiscale architecture where multiple (slightly modified) ViTs are stacked. The authors of PVT propose the spatialreduction attention (SRA) to alleviate the cost increase in self-attention layers. However, the computation and memory complexity of PVT still increases quartically w.r.t. resolution (with a much smaller constant). Therefore, it becomes prohibitively expensive for PVT to encode images with resolution above 800. We have compared our method to PVT and validated its superior performance . An E-ViT (a × n / p) module is a ViT encoder with an efficient attention mechanism a, n efficient transformer blocks, input patch size p. We add a LayerNorm after the patch embedding and change the original 1-D positional embedding to a (x, y)-type 2-D embedding. We add ng extra global tokens, as a form of global memory, and simply throw them away when going to the next stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-Scale Stacked Vision Transformers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-Scale Model Architecture</head><p>Efficient ViT (E-ViT). As shown in <ref type="figure" target="#fig_0">Figure 1</ref> (Bottom), we improve the encoding efficiency of vision Transformer by making the following modifications to the vanilla ViT. The modified ViT is referred to as Efficient ViT (E-ViT).</p><p>1. We add a Layer Normalization (LayerNorm) after the patch embedding.</p><p>2. We define a number of global tokens, including the CLS token. Correspondingly, the tokens associated with image and feature patches are referred to as local tokens afterwards.</p><p>3. We change the 1-D positional embedding of the local tokens to a 2-D positional embedding by separately encoding x and y coordinates, and concatenating them.</p><p>4. We replace the vanilla full self-attention with an efficient attention mechanism, denoted by a, which will be described in detail in Sections 3.2 and 3.3.</p><p>Except for attention a, E-ViT has the following architecture parameters inherited from the vanilla ViT : input patch size p, number of attention blocks n, hidden dimension d and number of heads h, denoted as E-ViT(a × n/p ; h, d, n g ).</p><p>Using the full attention mechanism (i.e., a = full) and one global token (i.e., the CLS token with n g = 1), the deficient E-ViT(full × 12/16 ; h, d, 1) models still achieve better image classification performance on ImageNet than the baseline ViT for both tiny (h = 3, d = 192) and small (h = 6, d = 384) model sizes, as shown in <ref type="table" target="#tab_1">Table 2</ref>. The performance gain is attributed to the added LayerNorm, as we show in the Supplementary. The 2-D positional embedding is used mainly for saving parameters for generating high-resolution feature maps. Mathematically, an E-ViT(a × n/p ; h, d, n g ) encoding module can be written as:</p><formula xml:id="formula_0">z 0 = [x 1 g ; . . . ; x ng g ; LN (x 1 p E); . . . ; LN (x n l p E)] + E ops ,<label>(1)</label></formula><formula xml:id="formula_1">z k = M SA a (LN (z k−1 )) + z k−1 , k = 1, .., n (2) z k = M LP (LN (z k−1 )) + z k−1 , k = 1, .., n,<label>(3)</label></formula><p>where LN is the added Layer Normalization after the patch embedding E, E ops ∈ R (n l +ng)×d contains the 2-D positional embedding of n l local tokens and the 1-D positional embedding of n g global tokens, M SA a is the multi-head self-attention with attention type a, and M LP is the feedforward block in a standard Transformer.</p><p>Stack multiple E-ViT modules as multi-scale vision Transformers. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> (Top), a multiscale Vision Transformer is built by stacking multiple E-ViT modules (or stages). In what follows, we describe several design choices we have made when building the multiscale ViT.</p><p>What are the patch size and hidden dimension at each stage? As required in object detection and human pose estimation, for models with 4-scale feature maps, the first feature map needs to down-sample the image by 4 and thus stage 1 can be written as E-ViT(a 1 × n 1 /4 ; h 1 , d 1 , n g,1 ).</p><p>We typically use only one attention block, i.e., n 1 = 1.</p><p>The first stage generates the highest-resolution feature map, which consumes lots of memory. We also construct several 3-stage models, whose first stage patch size is 8. For later stages, the patch sizes are set to 2, which downsizes the feature map resolution by 2. Following the practice in ResNet, we increase the hidden dimension twice when downsizing the feature map resolution by 2. We list a few representative model configurations in <ref type="table" target="#tab_0">Table 1</ref>. Different attention types (a) have different choices of number of global tokens n g . But they share the same model configurations. Thus we do not specify a and n g in <ref type="table" target="#tab_0">Table 1</ref>. number of attention blocks n, input patch size p, number of heads h and hidden dimension d. See the meaning of these parameters in <ref type="figure" target="#fig_0">Figure 1</ref> (Bottom).</p><p>How to connect global tokens between consecutive stages? The choice varies at different stages and among different tasks. For the tasks in this paper, e.g., classification, object detection, instance segmentation, we simply discard the global tokens and only reshape the local tokens as the input for next stage. In this choice, global tokens only plays a role of an efficient way to globally communicate between distant local tokens, or can be viewed as a form of global memory. These global tokens are useful in vision-language tasks, in which the text tokens serve as the global tokens and will be shared across stages. Should we use the average-pooled layer-normed features or the LayerNormed CLS token's feature for image classification? The choice makes no difference for flat models. But the average-pooled feature performs better than the CLS feature for multi-scale models, especially for the multiscale models with only one attention block in the last stage as shown in <ref type="table" target="#tab_0">Table 1</ref>. Readers refer to the Supplementary for an ablation study. As reported in <ref type="table" target="#tab_1">Table 2</ref>, the multi-scale models perform better than the flat models even in low-resolution classification problems. This shows the importance of multiscale structure on classification tasks. However, the full self-attention mechanism suffers from the quartic computation/memory complexity w.r.t. the resolution of feature maps, as shown in <ref type="table" target="#tab_1">Table 2</ref>. Thus, it is impossible to train 4-stage multi-scale ViTs with full attention using the same setting (batch size and hardware) used for DeiT training. , and ImageNet accuracy with image size 224. "Ti-Full-2,9,1" stands for a tiny-scale 3-stage multiscale ViT with a = full attention and with 2,9,1 number of attention blocks in each stage, respectively. Similarly, small models start with "S-". Since all our multi-scale models use average-pooled feature from the last stage for classification, we report Top-1 accuracy of E-ViT(full/16) both with the CLS feature (first) and with the average-pooled feature (second). The multi-scale models consistently outperform the flat models, but the memory usage of full attention quickly blows up when only one high-resolution block is introduced. The Vision Longformer ("ViL-") saves FLOPs and memory, without performance drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Vision Longformer: An Efficient Attention Mechanism</head><p>We propose to use the "local attention + global memory" efficient mechanism, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref> (Left), to reduce the computational and memory cost when using E-ViT to generate high-resolution feature maps. The 2-D Vision Longformer is an extension of the 1-D Longformer <ref type="bibr" target="#b1">[2]</ref> originally developed for NLP tasks. We add n g global tokens (including the CLS token) that are allowed to attend to all other tokens, serving as global memory. We also allow local tokens to only attend to global tokens and their local 2-D neighbors within a window size, and thus limit the increase of the attention cost to be linear w.r.t. number of input tokens. In summary, there are four components in this "local Right: the Low-rank based attention mechanism. Without "local→local" attentions in Vision Longformer, we get the Global Former. With a linear layer as the projection, we get Linformer <ref type="bibr" target="#b41">[42]</ref>. With a conv layer with equal kernel size and stride, we get Spatial Reduction Attention (SRA) <ref type="bibr" target="#b42">[43]</ref>. attention + global memory" mechanism, namely global-toglobal, local-to-global, global-to-local, and local-to-local, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref> (Left). In Equation 2, a Multi-head Self-Attention (MSA) block with the vision Longformer attention mechanism is denoted as M SA ViL , i.e., a = ViL in Equation 2. Theoretical complexity. Given the numbers of global and local tokens, denoted by n g and n l respectively, and local attention window size w, the memory complexity of the M SA ViL block is O(n g (n g + n l ) + n l w 2 ). Although <ref type="bibr" target="#b1">[2]</ref> points out that separating the attention parameters for global and local tokens is useful, we do not observe any gain in our experiments and thus simply let them share the same set of attention parameters. We empirically set the window size w to 15 for all E-ViT stages, which makes our model comparable with the global attention window size 14 of ViT/16 acted on 224 × 224 images. With such a window size, only attentions in the first two stages (in 4-stage multi-scale ViTs) are local. The attentions in the later two stages are equivalent 2 to full attention. In our experiments, we find that it is sufficient to use only one global token (n g = 1) for ImageNet classification problems. So, the effective memory complexity of the M SA ViL block is O((15 2 + 1)n l ), which is linear w.r.t. the number of tokens.</p><p>Results in <ref type="table" target="#tab_1">Table 2</ref> show that in comparison with the full attention models, the proposed multi-scale vision Longformer achieves a similar or slightly better performance, while saving significant memory and computation cost. It indicates that the "local attention + global memory" mechanism is a good inductive bias for vision Transformers. Although the savings are negligible for feature maps with resolution 28 × 28 (i.e., the feature maps in the first stage of 3-stage multi-scale), the savings are much more significant for feature maps with resolution 56 × 56 (i.e., the feature maps in the first stage of a 4-stage multi-scale model). It turns out that the higher resolution the feature map is, the closer the practical memory usage is to the theoretical complexity. Three implementations of Vision Longformer. Vision Longformer is conceptually similar to conv-like local attention. We have implemented vision Longformer in three ways: (1) using Pytorch's unfold function (nn.unfold or tensor.unfold), (2) using a customized CUDA kernel and (3) using a sliding chunk approach. The unfold implementation is simple but very slow, i.e., 24 times slower than full attention on 40 × 40 × 768 feature map. The implementation using the customized CUDA kernel is about 20% faster than the full attention in the same setting, while achieving the theoretical memory complexity. The sliding-chunk approach is the fastest, which is 60% faster than the full attention with a cost of consuming 20% more memory than the theoretical complexity. Readers refer to the Supplementary for a detailed comparison of these implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Other Efficient Attention Mechanisms</head><p>We compare Vision Longformer with the following alternative choices of efficient attention methods. We put detailed descriptions of these methods and their experimental setup in the Supplementary. Pure global memory (a = global). In Vision Longformer, see <ref type="figure" target="#fig_1">Figure 2</ref> (Left), if we remove the local-to-local attention, then we obtain the pure global memory attention mechanism (called Global Attention hereafter). Its memory complexity is O(n g (n g + n l )), which is also linear w.r.t. n l . However, for this pure global memory attention, n g has to be much larger than 1. We gradually increase n g (by 2 each time) and its performance gets nearly saturated at 128. Therefore, n g = 128 is the default for this Global attention. Linformer <ref type="bibr" target="#b41">[42]</ref> (a = LIN) projects the n l × d dimensional keys and values to K × d dimensions using additional projection layers, where K n l . Then the n l queries only attend to these projected K key-value pairs. The memory complexity of Linformer is O(Kn l ). We gradually increase K (by 2 each time) and its performance gets nearly saturated at 256. Therefore, K = 256 is the default for this Linformer attention, which turns out to be the same with the recommended value. Notice that Linformer's projection layer (of dimension K ×n l ) is specific to the current n l , and cannot be transferred to higher-resolution tasks that have a different n l . Spatial Reduction Attention (SRA) <ref type="bibr" target="#b42">[43]</ref> (a = SRA) is similar to Linformer, but uses a convolution layer with kernel size R and stride R to project the key-value pairs, hence resulting in n l /R 2 compressed key-value pairs. Therefore, The memory complexity of SRA is O(n 2 l /R 2 ), which is still quadratic w.r.t. n l but with a much smaller constant 1/R 2 . When transferring the ImageNet-pretrained SRA-models to high-resolution tasks, SRA still suffers from the quartic computation/memory blow-up w.r.t. the feature map resolution. Pyramid Vision Transformer <ref type="bibr" target="#b42">[43]</ref> uses this SRA to build multi-scale vision transformer backbones, with different spatial reduction ratios (R 1 = 8, R 2 = 4, R 3 = 2, R 4 = 1) for each stage. With this PVT's setting, the key and value feature maps at all stages are essentially with resolution H/32 × W/32. Performer <ref type="bibr" target="#b8">[9]</ref> (a = performer) uses random kernels to approximate the Softmax computation in MSA, and achieves a linear computation/memory complexity with respect to n l and the number of random features. We use the default 256 orthogonal random features (OR) for Performer, and provide other details in the Supplementary. DeiT-Tiny, has 4 stages and uses patch size 4x4 in the initial pixel space. "1,2,8,1" are the numbers of attention blocks in each stage. "Par-xformer" indicates multi-scale ViTs with multiple attention types: the first two stages utilize the "xformer" sparse attention and the last two stages still use full attention. In the "Trans2Det" columns, indicates that the ImageNet-pre-trained model can be used to initialize detection backbones, means not. − means that it can be transferred, but the corresponding detection models consumes prohibitively large memory due to the need of using high resolution feature maps. SRA/32 downsizes key/value feature maps with the same schedule in PVT <ref type="bibr" target="#b42">[43]</ref>, while SRA/64 downsizes more aggressively to make the memory managerable for downstream high-resolution tasks.</p><p>Compare vision longformer with other attention mechanisms. On ImageNet classification tasks in <ref type="table" target="#tab_2">Table 3</ref>, all efficient attention mechanisms above show a large performance gap from Vision Longformer. Linformer performs very competitively. Global attention and Performer have a similar performance with the DeiT model (72.2 for tiny and 79.8 for small). We use spatial reduction ratios 16, 8, 4, 2 from stage1 to stage4 for the multi-scale SRA model, which is different from the reduction ratios 8, 4, 2, 1 in PVT <ref type="bibr" target="#b42">[43]</ref>.</p><p>This more aggressive spatial reduction makes the classification performance worse in <ref type="table" target="#tab_2">Table 3</ref>, but makes the memory cost manageable when transfer to detection tasks for input image size 8000 × 1333. For a more complete comparison of these models, including model parameters, FLOPs and memory usage, please refer to the Supplementary.</p><p>Why is Longformer better? One possible reason is that the conv-like sparsity is a good inductive bias for vision transformers, compared with other attention mechanisms. This is supported by the visualization of the attention maps from pretrained DeiT models <ref type="bibr" target="#b39">[40]</ref>. Another explanation is that Vision Longformer keeps the key and value feature maps high resolution. However, low resolution-based attention mechanims like Linformer and SRA and pure global attention lose the high-resolution information in the key and value feature maps. We will conduct deeper studies to better understand the effects of different efficient attention mechanisms for vision Transformers.</p><p>Mixed attention mechanisms (Partial -former) for classification tasks. For classification tasks with 224×224 image size as input, the feature map size at Stage3 in multi-scale ViTs is 14 × 14. This is the same as the feature map size in ViT and DeiT, which best suits for full attention. A natural choice is to use efficient attention in the first two stages (with high-resolution feature map but with small number of blocks) and to use full attention in the last two stages. Multiscale ViTs with this mixed attention mechanisms are called "Parital X-former". We also report these Partial X-formers' performance in <ref type="table" target="#tab_2">Table 3</ref>. All these Partial X-formers perform well on ImageNet classification, with very little (even no) gap between Full Attention and Vision Longformer. These Partial X-forms achieve very good accuracy-efficiency performance for low-resolution classification tasks. We do not have "Partial ViL" for classification because ViL's window size is 15, and thus its attention mechanism in the last two stages is equivalent to the full attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Transfer to High-resolution Vision Tasks</head><p>Similar to the transfer-ability of ImageNet-pretrained CNN weights to downstream high-resolution tasks, such as object detection and segmentation, multi-scale vision Longformer pretrained on ImageNet can also be transferred to such high-resolution tasks, as we will show in Section 4.3.</p><p>However, Linformer is not transferable because the weights of the linear projection layer is specific to a resolution. The Partial X-formers and Multi-scale ViT with full attention are not transferable due to its prohibitively large memory usage after transferred to high-resolution tasks. In</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we show the final performance of Multiscale Vision Longformer (short for ViL) on ImageNet classification in Section 4.1 and downstream high-resolution detection tasks in Section 4.3.We mainly follow the DeiT training configuration for ImageNet classification training, and use the standard ×1 training schedule with the "AdamW" optimizer for detection tasks. We refer to the Supplementary for detailed experimental settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ImageNet Classification</head><p>Following DeiT <ref type="bibr" target="#b39">[40]</ref> and PVT <ref type="bibr" target="#b42">[43]</ref>, we build multi-scale ViLs with four different sizes, i.e., tiny, small, medium and base. We experiment two strategies to scale the small model to medium and base sizes: deeper or wider, resulting in Medium/Base-Deep/Wide, respectively.The detailed model configuration is specified in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>We train multi-scale ViLs purely on ImageNet, following the setting in DeiT <ref type="bibr" target="#b39">[40]</ref>. In <ref type="table" target="#tab_3">Table 4</ref>, we report our results and compare with ResNets <ref type="bibr" target="#b12">[13]</ref>, ViT <ref type="bibr" target="#b10">[11]</ref> and DeiT <ref type="bibr" target="#b39">[40]</ref> trained in the same setting. Our models at all scales outperform other models in the same scale by a large margin. Moreover, our small model (size comparable with ResNet50) even outperforms X101-64x4d/ViT-Base/Deit-Base/PVT-Large models. Our medium-size models further improves based on the ViL-Small model by a large margin. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">ImageNet-21K pretraining and ImageNet-1K finetuning</head><p>When trained purely on ImageNet-1K, there is no further performance increase from ViL-Medium to ViL-Base, even a significant decrease for the ViL-Base-W model. This is consistent with the observation in ViT: large pure transformer based models can be trained well when training data is sufficient. For ViL-Base size models, it is insufficient to only trained on ImageNet. In our supplementary material, we report ViL's performance on ImageNet, when pretrained on ImageNet-21k and finetuned on ImageNet. Therefore, we conducted experiments that ViL-Medium/Base models are first pretrained on ImageNet-21k with image size 224 2 and finetuned on ImageNet-1K with image size 384 2 . For ViT models on image size 224 2 , there are in total 24×24 tokens with full attention. For ViLmodels on image size 224 2 , we set the window sizes to be <ref type="bibr" target="#b12">(13,</ref><ref type="bibr">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b24">25)</ref> from Stage1 to Stage4. Therefore, in the last two stages, the ViLmodels' attention is equivalent to full attention.</p><p>In <ref type="table">Table 5</ref>, we can see that the performance gets boosted significantly after ImageNet-21K pretraining for all ViLmedium and base models. Especially for the ViL-Base-W model, its ImageNet Top-1 accuracy is improved from 81.86 to 86.14. We want to point out that the performance of ViL-Medium-D model has surpassed that of ViT-Base/16, ViT-Large/16 and BiT-152x4-M, in the ImageNet-21K pretraining setting. The performance of ViL-Base models are even better. This shows the superior performance and parameter efficiency of ViLmodels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Detection Tasks</head><p>We apply our ViL to two representative object detection pipelines including RetinaNet <ref type="bibr" target="#b22">[23]</ref> and Mask-RCNN <ref type="bibr" target="#b11">[12]</ref>. We follow the conventional setting to use our vision longformer as the backbone to generate feature maps for both detection pipelines. Similar to <ref type="bibr" target="#b42">[43]</ref>, we extract the features from all four scales and then feed them to the detection and/or instance segmentation head. To adapt the learned positional embedding to the higher image resolution in detection, we perform bilinear interpolation on it prior to the training. In our experiments, all models are evaluated on COCO dataset <ref type="bibr">[24]</ref>, with 118k images for training and 5k images for evaluation. We report the results for both 1× and 3×+MS training schedules, and compare them with two backbone architectures: ResNet <ref type="bibr" target="#b12">[13]</ref> and the most recent PVT <ref type="bibr" target="#b11">[12]</ref>.</p><p>In <ref type="table">Table 6</ref>, we can find our proposed ViL achieves significantly better performance than the ResNet baselines. Compared with the concurrent PVT architecture, our model also outperforms it with fairly large margin. Particularly, our ViL-Tiny has 1.9 points improvement over PVT-Tiny while using much less number of parameters (16.6M v.s. 23.0M). Note that in all other settings (Small, Medium and Base), our model also has less or comparable number of parame-ters than the counterparts. The similar trend is also observed with Mask R-CNN pipeline. As shown in <ref type="table">Table 7</ref>, our ViL backbone surpasses ResNet baseline and PVT on both object detection and instance segmentation, with less or comparable number of parameters. Using our largest model ViL-Base with only 76.1M parameters, we can achieve 45.7 AP b and significantly outperforms ResNeXt101-64x4d and PVT-Large that use more parameters. Beside the model size, our ViL backbones also consume less computational cost (GFLOPs) than the ResNet counterparts. These consistent and significant improvements with both RetinaNet and Mask R-CNN demonstrate the promise of our proposed ViL when using it as the image encoder for high-resolution dense object detection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study for Detection Tasks</head><p>Here, we answer the question that whether different efficient attention mechanisms can be equally applied to the dense prediction tasks like object detection and instance segmentation. Similar to Sec 4.4, we study SRA <ref type="bibr" target="#b42">[43]</ref>, Global Transformer and Performer and their corresponding partial version with Mask R-CNN pipeline (trained with the 1× schedule). As we can see in <ref type="table">Table 8</ref>, when all use the efficient attention mechanisms, ViL achieves much better performance than the other three mechanisms. Specifically, our ViL achieves 41.2 AP b while the other three are all around 35.0 AP b . This is reasonable because except for ViL, all other efficient mechanisms have relatively worse pretrained model on ImageNet and thus poor representations for the downstream tasks. When all using partial version of efficient attention mechanisms, we can observe the gaps between different mechanisms shrink to around 1.0 point while our ViL still outperform all others. For Performer particularly, we further convert the its pretrained partial version to fully efficient version to investigate its transferrability. As we can see, it has even worse performance than the pure Performer. These results indicate that not all efficient mechanisms can be equally transferred to highresolution dense prediction tasks, and the proposed ViL is the only choice which retains the good performance across different settings.  <ref type="table">Table 8</ref>. Comparing different efficient attention mechanisms for object detection with Mask R-CNN. All use small model for the experiments. We report the run-time memory cost (G) when training each model in the last column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we have presented a new Vision Transformer (ViT) architecture Multi-Scale Vision Longformer to address the computational and memory efficiency which prevents the vanilla ViT model from applying to vision tasks requiring high-resolution feature maps. We mainly developed two techniques: 1) a multi-scale model structure designed for Transformers to provide image encoding at multiple scales with manageable computational cost, and 2) an efficient 2-D attention mechanism of vision Longformer for achieving a linear complexity w.r.t. the number of input tokens. The architecture design and the efficient attention mechanism are validated with comprehensive ablation studies. Our experimental results show that the new ViT archi-tecture effectively addresses the computational and memory efficiency problem and outperforms several strong baselines on image classification and object detection. <ref type="table">Table 9</ref>. Model architecture for multi-scale stacked ViTs. Architecture parameters for each E-ViT module E-ViT(a × n/p ; h, d): number of attention blocks n, input patch size p, number of heads h and hidden dimension d. See the meaning of these parameters in <ref type="figure" target="#fig_0">Figure 1</ref> (Bottom). <ref type="table" target="#tab_0">Table 10</ref> summarizes our training setups for our different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Experimental settings</head><p>For the ImageNet classification task, our setting mainly follow that in DeiT <ref type="bibr" target="#b39">[40]</ref>. For example, we do not use dropout but use random path. We use all data augmentations in DeiT <ref type="bibr" target="#b39">[40]</ref>, except that we apply Repeated Augmentation only on Medium and Base models. When fine-tuning from a ImageNet-21K pretrained checkpoint, we mainly follow the practice of ViT <ref type="bibr" target="#b10">[11]</ref>, train on image size 384 × 384, use SGD with momentum 0.9, use no weight decay, and use only random cropping for data augmentation.</p><p>For COCO object detection/segmentation tasks, we follow the standard "1×" and "3 × +MS" schedules. We only change the optimizer from SGD to AdamW and search for good initial learning rate and weight decay. For the "1×" schedule, the input image scale is fixed to be (800, 1333) for the min and max sizes, respectively. For the "3 × +MS" schedule, the input image is randomly resized to have min size in {640, 672, 704, 736, 768, 800}. We found that there is obvious over-fitting in Training ViL-Medium and ViL-Base models on COCO, mainly because that these two models are relatively large but they are only pretrained on Im-ageNet. Therefore, we are taking the best checkpoint (one epoch per checkpoint) along the training trajectory to report the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Ablation study on the architecture design of multi-scale vision Longformer</head><p>In this section, we present two ablation studies on the model architecture of multi-scale vision Longformer. Ablation of the effects of LayerNorm and 2-D positional embedding in the patch embedding. In <ref type="table" target="#tab_1">Table 2</ref>, we show that our flat model E-ViT(full × 12/16), which only differs from the standard ViT/DeiT model by an newly-added LayerNorm after the patch embedding and the 2-D positional embedding, has better performance than the standard ViT/DeiT model. In <ref type="table" target="#tab_0">Table 11</ref>, we show that this better performance comes from the newly-added LayerNorm. Feature from the CLS token or from average pooling? As shown in <ref type="table" target="#tab_0">Table 12</ref>, for ViL models that has only one attention block in the last stage (ViL 1-2-8-1), the average pooled feature from all tokens works better than the feature of the CLS token. However, when there are more than 2 attention blocks in the last stage (ViL 1-1-8-2), the difference between these two features disappears. The ViL 1-1-8-2 model has better performance than the ViL 1-2-8-1 model because it has more trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. A comprehensive comparison of different attention mechanisms on ImageNet classification</head><p>We compare different attention mechanisms with different model sizes and architectures in <ref type="table" target="#tab_0">Table 13 and Table 14</ref>.</p><p>In <ref type="table" target="#tab_0">Table 13</ref>, we show their performance on ImageNet-1K classification problem, measured by Top-1 accuracy. In <ref type="table" target="#tab_0">Table 14</ref>, we show their number of parameters and FLOPs. We would like to comment that FLOPs is just a theoretical estimation of computation complexity, and it may not fit well the space/time cost in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementations and Efficiency of Vision Longformer In Practice</head><p>There is a trivial implementation of the conv-like sliding window attention, in which we compute the full quadratic attention and then mask out non-neighbor tokens. This approach suffers from the quadratic complexity w.r.t. number of tokens (quartic w.r.t. feature map size), and is impractical for real use, as shown by the blue curve in <ref type="figure" target="#fig_2">Figure 4</ref>. We  <ref type="table" target="#tab_0">Table 12</ref>. For ViL models that has only one attention block in the last stage (ViL 1-2-8-1), the average pooled feature from all tokens works better than the feature of the CLS token. When there are more than 2 attention blocks in the last stage (ViL 1-1-8-2), the difference between these two features disappears.</p><p>versions: one using nn.functional.unfold (denoted as "unfold/nn.F") and the other using tensor.unfold (denoted as "unfold/tensor"). As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, the "unfold/tensor" version (red solid line) is more efficient both in time and memory than the "unfold/nn.F" version (red dotted line). However, both of them are even slower and use more memory than the full attention! <ref type="figure">Figure 3</ref>. The sliding-chunk implementation of Vision Longformer. This implementation (Right) lets one token attends to more tokens than the exact conv-like local attention (Left). Our sliding-chunk implementation has the choice to be exactly the same with the conv-like local attention (Left), by masking out tokens that should not be attended to. For chunks on the boundaries, our implementation supports both zero padding and cyclic padding.</p><p>2. Using a customized CUDA kernel, denoted as "cuda kernel". We make use of the TVM, like what has done in Longformer <ref type="bibr" target="#b1">[2]</ref>, to write a customized CUDA kernel for vision longformer. As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, the "cuda kernel" (green line) achieves the theoretical optimal memory usage. Its time complexity is also reduced to linear w.r.t. number of tokens (quadratic w.r.t. feature map size). However, since it's not making use of the highly optimized matrix multiplication libraries in CUDA, it's speed is still slow in practice.</p><p>3. Using a sliding chunk approach, illustrated in <ref type="figure">Figure</ref> 3. For this sliding chunk approach, we have two subversions: one using Pytorch's autograd to compute backward step (denoted as "SCw/Autograd") and the other writing a customized torch.autograd.Function with hand-written backward function (denoted as "SCw/Handgrad"). Both sub versions of this sliding chunk approach are fully implemented with Pytorch functions and thus make use of highly optimized matrix multiplication libraries in CUDA. As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, both of them are faster than the "cuda kernel" implementation.</p><p>In the sliding chunk approach, to achieve a conv-like local attention mechanism with window size 2w + 1, we split the feature map into chunks with size w × w. Each chunk only attends to itself and its 8 neighbor chunks. The Pytorch Autograd will save 9 copies of the feature map (9 nodes in the computing graph) for automatic back-propogration, which is not time/memory efficient. The "SCw/Handgrad" version defines a customized torch.autograd.Function with hand-written backward function, which greatly saves the memory usage and also speeds up the algorithm, as shown in <ref type="figure" target="#fig_2">Figure 4</ref>. We would like to point out that the memory usage of the "SCw/Handgrad" version is nearly optimal (very close to that of the "cuda kernel"). Similar speed-memory trade-off with different implementations of local attention mechanism has been observed in the 1-D Longformer <ref type="bibr" target="#b1">[2]</ref>, too; see <ref type="figure" target="#fig_0">Figure 1</ref> in <ref type="bibr" target="#b1">[2]</ref>. We would like to point out that Image Transformer <ref type="bibr" target="#b27">[28]</ref> has an implementation of of 2-D <ref type="figure">Figure 5</ref>. Compare of three modes of our "SCw/Handgrad" implementation of conv-like local attention: exact conv-like sliding window attention, sliding chunk attention without padding for boundary chunks, and sliding chunk attention with cyclic padding for boundary chunks. The are nearly the same in terms of running time (including forward and backward) and memory usage. The window size is 17 and thus chunk size is 8.</p><p>conv-like local attention mechanism, which is similar to our "SCw/Autograd" version. The Image Transformer <ref type="bibr" target="#b27">[28]</ref> applies it to the image generation task.</p><p>This sliding-chunk implementation <ref type="figure">(Figure 3</ref> Right) lets one token attends to more tokens than the exact conv-like local attention <ref type="figure">(Figure 3</ref> Left). Our sliding-chunk imple- <ref type="figure">Figure 6</ref>. Running time (including forward and backward) and memory usage of our "SCw/Handgrad" implementation of convlike local attention (sliding chunk attention without padding mode) with different window sizes. The speed is not sensitive to the window size for small window sizes (≤ 17) and the memory usage monotonically increases. mentation has the choice to be 1. exactly the same with the conv-like local attention (Left), by masking out tokens that should not be attended to, 2. sliding chunk without padding, in which the chunks on the boundary have less chunks to attend to, 3. sliding chunk with cyclic padding, in which the chunks on the boundary still attend to 9 chunks with cyclic padded chunks.</p><p>. Since these three modes only differs by the attention masks to mask out invalid tokens, their speed and memory usage are nearly the same, as shown in <ref type="figure">Figure 5</ref>. For all the results reported in this paper, we use "sliding chunk without padding". In <ref type="figure">Figure 6</ref>, we show the running time (including forward and backward) and memory usage of our "SCw/Handgrad" implementation of conv-like local attention (sliding chunk attention without padding mode) with different window sizes. We can see that the speed is not sensitive to the window size for small window sizes (≤ 17) and the memory usage monotonically increases.</p><p>Finally, both the "unfold/nn.F" and the "cuda kernel" implementations support dilated conv-like attention. The customized CUDA kernel is even more flexible to support different dilations for different heads. The sliding-chunk implementation does not support this dilated conv-like attention. In this paper, we always use the sliding-chunk implementation due to its superior speed and nearly optimal memory complexity.</p><p>In <ref type="figure" target="#fig_2">Figure 4</ref>, 5 and 6, the evaluation is performed on a single multi-head self-attention module (MSA) with the conv-like local attention mechanism, instead of on the full multi-scale vision longformer. With this evaluation, we can clearly see the difference among different implementations of the conv-like local attention mechanism.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>A Multi-scale vision Transformers (bottom) by stacking 4 E-ViT modules (Top)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Left: the Vision Longformer attention mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Compare of running time (including forward and backward) and memory usage of different implementations of the convlike attention in vision longformer. All of these implementations shown in the figures are mathematically equivalent, doing the exact conv-like sliding window attention with window size 17.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="3">Readers refer to the Supple-</cell></row><row><cell cols="5">mentary for the complete list of model configurations used</cell></row><row><cell>in this paper,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Size</cell><cell>Stage1 n,p,h,d</cell><cell>Stage2 n,p,h,d</cell><cell>Stage3 n,p,h,d</cell><cell>Stage4 n,p,h,d</cell></row><row><cell>Tiny</cell><cell cols="3">1,4,1,48 1,2,3,96 9,2,3,192</cell><cell>1,2,6,384</cell></row><row><cell>Small</cell><cell cols="4">1,4,3,96 2,2,3,192 8,2,6,384 1,2,12,768</cell></row><row><cell>Medium-D</cell><cell cols="4">1,4,3,96 4,2,3,192 16,2,6,384 1,2,12,768</cell></row><row><cell cols="5">Medium-W 1,4,3,192 2,2,6,384 8,2,8,512 1,2,12,768</cell></row><row><cell>Base-D</cell><cell cols="4">1,4,3,96 8,2,3,192 24,2,6,384 1,2,12,768</cell></row><row><cell>Base-W</cell><cell cols="4">1,4,3,192 2,2,6,384 8,2,12,768 1,2,16,1024</cell></row><row><cell>Tiny-3stage</cell><cell cols="2">2,8,3,96</cell><cell>9,2,3,192</cell><cell>1,2,6,384</cell></row><row><cell>Small-3stage</cell><cell cols="2">2,8,3,192</cell><cell cols="2">9,2,6,384 1,2,12,768</cell></row></table><note>Model architecture for multi-scale stacked ViTs. Archi- tecture parameters for each E-ViT stageE-ViT(a × n/p ; h, d):</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>Model</cell><cell cols="3">#Params FLOPs Memory (M) (G) (M)</cell><cell>Top-1 (%)</cell></row><row><cell>Ti-DeiT / 16 [40]</cell><cell>5.7</cell><cell>1.3</cell><cell>33.4</cell><cell>72.2</cell></row><row><cell>Ti-E-ViT(full/16)</cell><cell>5.7</cell><cell>1.3</cell><cell>33.4</cell><cell>73.2/73.1</cell></row><row><cell>Ti-Full-1,10,1</cell><cell>7.12</cell><cell>1.35</cell><cell>45.8</cell><cell>75.9</cell></row><row><cell>Ti-Full-2,9,1</cell><cell>6.78</cell><cell>1.45</cell><cell>60.6</cell><cell>75.8</cell></row><row><cell>Ti-Full-1,1,9,1</cell><cell>6.71</cell><cell>2.29</cell><cell>155.0</cell><cell>76.1</cell></row><row><cell>Ti-Full-1,2,8,1</cell><cell>6.37</cell><cell>2.39</cell><cell>170.5</cell><cell>75.6</cell></row><row><cell>Ti-ViL-1,10,1</cell><cell>7.12</cell><cell>1.27</cell><cell>38.3</cell><cell>75.6±0.23</cell></row><row><cell>Ti-ViL-2,9,1</cell><cell>6.78</cell><cell>1.29</cell><cell>45.5</cell><cell>75.9±0.08</cell></row><row><cell>Ti-ViL-1,1,9,1</cell><cell>6.71</cell><cell>1.33</cell><cell>52.7</cell><cell>76.2±0.12</cell></row><row><cell>Ti-ViL-1,2,8,1</cell><cell>6.37</cell><cell>1.35</cell><cell>60.0</cell><cell>76.0±0.10</cell></row><row><cell>S-DeiT / 16 [40]</cell><cell>22.1</cell><cell>4.6</cell><cell>67.1</cell><cell>79.9</cell></row><row><cell>S-E-ViT(full/16)</cell><cell>22.1</cell><cell>4.6</cell><cell>67.1</cell><cell>80.4/80.7</cell></row><row><cell>S-Full-1,10,1</cell><cell>27.58</cell><cell>4.84</cell><cell>78.5</cell><cell>81.7</cell></row><row><cell>S-Full-2,9,1</cell><cell>26.25</cell><cell>5.05</cell><cell>93.8</cell><cell>81.7</cell></row><row><cell>S-Full-1,1,9,1</cell><cell>25.96</cell><cell>6.74</cell><cell>472.9</cell><cell>-</cell></row><row><cell>S-Full-1,2,8,1</cell><cell>24.63</cell><cell>6.95</cell><cell>488.3</cell><cell>-</cell></row><row><cell>S-ViL-1,10,1</cell><cell>27.58</cell><cell>4.67</cell><cell>73.0</cell><cell>81.6</cell></row><row><cell>S-ViL-2,9,1</cell><cell>26.25</cell><cell>4.71</cell><cell>81.4</cell><cell>81.8</cell></row><row><cell>S-ViL-1,1,9,1</cell><cell>25.96</cell><cell>4.82</cell><cell>108.5</cell><cell>81.8</cell></row><row><cell>S-ViL-1,2,8,1</cell><cell>24.63</cell><cell>4.86</cell><cell>116.8</cell><cell>82.0</cell></row></table><note>. Flat vs Multi-scale Models with full self-attention: Num- ber of paramers, FLOPS, training time, memory per image (with Pytorch Automatic Mixed Precision enabled)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>Multi-scale</cell><cell>Tiny-4stage / 4</cell><cell cols="2">Small-4stage / 4</cell><cell>Trans</cell></row><row><cell>Models</cell><cell>1,1,9,1 1,2,8,1</cell><cell cols="2">1,1,9,1 1,2,8,1</cell><cell>2Det</cell></row><row><cell>Full</cell><cell>76.06 75.60</cell><cell cols="2">OOM OOM</cell><cell>-</cell></row><row><cell>ViL</cell><cell>76.18 75.98</cell><cell cols="2">81.79 81.99</cell><cell></cell></row><row><cell>Global</cell><cell>71.52 72.00</cell><cell cols="2">79.17 78.97</cell><cell></cell></row><row><cell cols="2">Linformer [42] 74.71 74.74</cell><cell cols="2">81.19 80.98</cell><cell></cell></row><row><cell>SRA/64[43]</cell><cell>69.08 68.78</cell><cell cols="2">76.35 76.37</cell><cell></cell></row><row><cell>SRA/32[43]</cell><cell>73.22 73.2</cell><cell>79.96</cell><cell>79.9</cell><cell>-</cell></row><row><cell>Performer</cell><cell>71.12 73.09</cell><cell cols="2">78.81 78.72</cell><cell></cell></row><row><cell>Par-Global</cell><cell>75.32 75.4</cell><cell>81.6</cell><cell>81.45</cell><cell>-</cell></row><row><cell>Par-Linformer</cell><cell>75.56 75.33</cell><cell cols="2">81.66 81.79</cell><cell></cell></row><row><cell>Par-SRA/32</cell><cell>75.2 75.26</cell><cell cols="2">81.62 81.61</cell><cell>-</cell></row><row><cell>Par-Performer</cell><cell>75.34 75.93</cell><cell cols="2">81.72 81.72</cell><cell>-</cell></row></table><note>. Overall comparison of different attention mechanisms on ImageNet classification top-1 accuracy (%), with input size 224. Tiny-4stage / 4 means that the model has a comparable size with</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>Model</cell><cell cols="2">#Params (M) GFLOPs</cell><cell>Top-1 (%)</cell></row><row><cell>R18</cell><cell>11.7</cell><cell>1.8</cell><cell>69.8</cell></row><row><cell>DeiT-Tiny/16[40]</cell><cell>5.7</cell><cell>1.3</cell><cell>72.2</cell></row><row><cell>PVT-Tiny[43]</cell><cell>13.2</cell><cell>1.9</cell><cell>75.1</cell></row><row><cell>ViL-Tiny</cell><cell>6.7</cell><cell>1.3</cell><cell>76.3</cell></row><row><cell>R50</cell><cell>25.6</cell><cell>4.1</cell><cell>78.5</cell></row><row><cell>DeiT-Small/16[40]</cell><cell>22.1</cell><cell>4.6</cell><cell>79.9</cell></row><row><cell>PVT-Small[43]</cell><cell>24.5</cell><cell>3.8</cell><cell>79.8</cell></row><row><cell>ViL-Small</cell><cell>24.6</cell><cell>4.9</cell><cell>82.0</cell></row><row><cell>R101</cell><cell>44.7</cell><cell>7.9</cell><cell>79.8</cell></row><row><cell>ViT-Small/16[11]</cell><cell>48.8</cell><cell>9.9</cell><cell>80.8</cell></row><row><cell>PVT-Medium[43]</cell><cell>44.2</cell><cell>6.7</cell><cell>81.2</cell></row><row><cell>ViL-Medium-W</cell><cell>39.8</cell><cell>10.8</cell><cell>82.9</cell></row><row><cell>ViL-Medium-D</cell><cell>39.7</cell><cell>8.7</cell><cell>83.3</cell></row><row><cell>X101-64x4d</cell><cell>83.5</cell><cell>15.6</cell><cell>81.5</cell></row><row><cell>ViT-Base/16[11]</cell><cell>86.6</cell><cell>17.6</cell><cell>77.9</cell></row><row><cell>DeiT-Base/16[40]</cell><cell>86.6</cell><cell>17.6</cell><cell>81.8</cell></row><row><cell>PVT-Large[43]</cell><cell>61.4</cell><cell>9.8</cell><cell>81.7</cell></row><row><cell>ViL-Base-W</cell><cell>79.0</cell><cell>17.5</cell><cell>81.9</cell></row><row><cell>ViL-Base-D</cell><cell>55.7</cell><cell>13.4</cell><cell>83.2</cell></row></table><note>. Number of paramers, FLOPS and ImageNet accuracy. Trained on ImageNet-1K with image size 224. Our ViL models are highlighted with gray background.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>AP AP 50 AP 75 AP S AP M AP L AP AP 50 AP 75 AP S AP M AP L Object detection performance on the COCO val2017 with RetinaNet. The FLOPs (G) are measured at resolution 800 × 1333, and FLOPs for PVT architecture are not available. Our ViL models are pre-trained on ImageNet-1K and highlighted with gray background. Object detection and instance segmentation performance on the COCO val2017 with Mask R-CNN. The FLOPs (G) are measured at resolution 800 × 1333, and FLOPs for PVT architecture are not available. Our ViL models are pre-trained on ImageNet-1K and highlighted with gray background.</figDesc><table><row><cell>Backbone</cell><cell cols="2">#Params FLOPs (M) (G)</cell><cell></cell><cell cols="4">RetinaNet 1x schedule</cell><cell></cell><cell></cell><cell cols="4">RetinaNet 3x + MS schedule</cell><cell></cell></row><row><cell>ResNet18</cell><cell>21.3</cell><cell cols="3">190.33 31.8 49.6</cell><cell>33.6</cell><cell>16.3</cell><cell>34.3</cell><cell cols="3">43.2 35.4 53.9</cell><cell>37.6</cell><cell>19.5</cell><cell>38.2</cell><cell>46.8</cell></row><row><cell>PVT-Tiny[43]</cell><cell>23.0</cell><cell>n/a</cell><cell cols="2">36.7 56.9</cell><cell>38.9</cell><cell>22.6</cell><cell>38.8</cell><cell cols="3">50.0 39.4 59.8</cell><cell>42.0</cell><cell>25.5</cell><cell>42.0</cell><cell>52.1</cell></row><row><cell>ViL-Tiny</cell><cell>16.64</cell><cell cols="3">171.91 38.8 59.0</cell><cell>41.3</cell><cell>23.8</cell><cell>41.5</cell><cell cols="3">51.8 40.7 61.2</cell><cell>43.2</cell><cell>26.4</cell><cell>43.9</cell><cell>53.6</cell></row><row><cell>ResNet50</cell><cell>37.7</cell><cell cols="3">239.32 36.3 55.3</cell><cell>38.6</cell><cell>19.3</cell><cell>40.0</cell><cell cols="3">48.8 39.0 58.4</cell><cell>41.8</cell><cell>22.4</cell><cell>42.8</cell><cell>51.6</cell></row><row><cell>PVT-Small[43]</cell><cell>34.2</cell><cell>n/a</cell><cell cols="2">40.4 61.3</cell><cell>43.0</cell><cell>25.0</cell><cell>42.9</cell><cell cols="3">55.7 42.2 62.7</cell><cell>45.0</cell><cell>26.2</cell><cell>45.2</cell><cell>57.2</cell></row><row><cell>ViL-Small</cell><cell>35.68</cell><cell cols="3">252.21 41.6 62.5</cell><cell>44.1</cell><cell>24.9</cell><cell>44.6</cell><cell cols="3">56.2 42.9 63.8</cell><cell>45.6</cell><cell>27.8</cell><cell>46.4</cell><cell>56.3</cell></row><row><cell>ResNet101</cell><cell>56.7</cell><cell cols="3">319.07 38.5 57.8</cell><cell>41.2</cell><cell>21.4</cell><cell>42.6</cell><cell cols="3">51.1 40.9 60.1</cell><cell>44.0</cell><cell>23.7</cell><cell>45.0</cell><cell>53.8</cell></row><row><cell>ResNeXt101-32x4d</cell><cell>56.4</cell><cell cols="3">319.07 39.9 59.6</cell><cell>42.7</cell><cell>22.3</cell><cell>44.2</cell><cell cols="3">52.5 41.4 61.0</cell><cell>44.3</cell><cell>23.9</cell><cell>45.5</cell><cell>53.7</cell></row><row><cell>PVT-Medium[43]</cell><cell>53.9</cell><cell>n/a</cell><cell cols="2">41.9 63.1</cell><cell>44.3</cell><cell>25.0</cell><cell>44.9</cell><cell cols="3">57.6 43.2 63.8</cell><cell>46.1</cell><cell>27.3</cell><cell>46.3</cell><cell>58.9</cell></row><row><cell>ViL-Medium</cell><cell>50.77</cell><cell cols="3">338.93 42.9 64.0</cell><cell>45.4</cell><cell>27.0</cell><cell>46.1</cell><cell cols="3">57.2 43.7 64.6</cell><cell>46.4</cell><cell>27.9</cell><cell>47.1</cell><cell>56.9</cell></row><row><cell>ResNeXt101-64x4d</cell><cell>95.5</cell><cell cols="3">483.59 41.0 60.9</cell><cell>44.0</cell><cell>23.9</cell><cell>45.2</cell><cell cols="3">54.0 41.8 61.5</cell><cell>44.4</cell><cell>25.2</cell><cell>45.4</cell><cell>54.6</cell></row><row><cell>PVT-Large[43]</cell><cell>71.1</cell><cell>n/a</cell><cell cols="2">42.6 63.7</cell><cell>45.4</cell><cell>25.8</cell><cell>46.0</cell><cell cols="3">58.4 43.4 63.6</cell><cell>46.1</cell><cell>26.1</cell><cell>46.0</cell><cell>59.5</cell></row><row><cell>ViL-Base</cell><cell>66.74</cell><cell>443.0</cell><cell cols="2">44.3 65.5</cell><cell>47.1</cell><cell>28.9</cell><cell>47.9</cell><cell cols="3">58.3 44.7 65.5</cell><cell>47.6</cell><cell>29.9</cell><cell>48.0</cell><cell>58.1</cell></row><row><cell>Backbone</cell><cell cols="2">#Params FLOPs (M) (G)</cell><cell cols="5">Mask R-CNN 1x schedule AP b AP b 50 AP b 75 AP m AP m 50</cell><cell>AP m 75</cell><cell cols="5">Mask R-CNN 3x + MS schedule AP b AP b 50 AP b 75 AP m AP m 50</cell><cell>AP m 75</cell></row><row><cell>ResNet18</cell><cell>31.2</cell><cell cols="2">131.03 34.0</cell><cell>54.0</cell><cell>36.7</cell><cell>31.2</cell><cell>51.0</cell><cell>32.7</cell><cell>36.9</cell><cell>57.1</cell><cell>40.0</cell><cell>33.6</cell><cell>53.9</cell><cell>35.7</cell></row><row><cell>PVT-Tiny[43]</cell><cell>32.9</cell><cell>n/a</cell><cell>36.7</cell><cell>59.2</cell><cell>39.3</cell><cell>35.1</cell><cell>56.7</cell><cell>37.3</cell><cell>39.8</cell><cell>62.2</cell><cell>43.0</cell><cell>37.4</cell><cell>59.3</cell><cell>39.9</cell></row><row><cell>ViL-Tiny</cell><cell>26.9</cell><cell>94.3</cell><cell>38.7</cell><cell>61.1</cell><cell>41.3</cell><cell>36.2</cell><cell>58.3</cell><cell>38.2</cell><cell>41.2</cell><cell>63.0</cell><cell>44.7</cell><cell>37.9</cell><cell>59.8</cell><cell>40.6</cell></row><row><cell>ResNet50</cell><cell>44.2</cell><cell>180.0</cell><cell>38.0</cell><cell>58.6</cell><cell>41.4</cell><cell>34.4</cell><cell>55.1</cell><cell>36.7</cell><cell>41.0</cell><cell>61.7</cell><cell>44.9</cell><cell>37.1</cell><cell>58.4</cell><cell>40.1</cell></row><row><cell>PVT-Small[43]</cell><cell>44.1</cell><cell>n/a</cell><cell>40.4</cell><cell>62.9</cell><cell>43.8</cell><cell>37.8</cell><cell>60.1</cell><cell>40.3</cell><cell>43.0</cell><cell>65.3</cell><cell>46.9</cell><cell>39.9</cell><cell>62.5</cell><cell>42.8</cell></row><row><cell>ViL-Small</cell><cell>45.0</cell><cell>174.3</cell><cell>41.8</cell><cell>64.1</cell><cell>45.1</cell><cell>38.5</cell><cell>61.1</cell><cell>41.4</cell><cell>43.4</cell><cell>64.9</cell><cell>47.0</cell><cell>39.6</cell><cell>62.1</cell><cell>42.4</cell></row><row><cell>ResNet101</cell><cell>63.2</cell><cell cols="2">259.77 40.4</cell><cell>61.1</cell><cell>44.2</cell><cell>36.4</cell><cell>57.7</cell><cell>38.8</cell><cell>42.8</cell><cell>63.2</cell><cell>47.1</cell><cell>38.5</cell><cell>60.1</cell><cell>41.3</cell></row><row><cell>ResNeXt101-32x4d</cell><cell>62.8</cell><cell cols="2">259.77 41.9</cell><cell>62.5</cell><cell>45.9</cell><cell>37.5</cell><cell>59.4</cell><cell>40.2</cell><cell>44.0</cell><cell>64.4</cell><cell>48.0</cell><cell>39.2</cell><cell>61.4</cell><cell>41.9</cell></row><row><cell>PVT-Medium[43]</cell><cell>63.9</cell><cell>n/a</cell><cell>42.0</cell><cell>64.4</cell><cell>45.6</cell><cell>39.0</cell><cell>61.6</cell><cell>42.1</cell><cell>44.2</cell><cell>66.0</cell><cell>48.2</cell><cell>40.5</cell><cell>63.1</cell><cell>43.5</cell></row><row><cell>ViL-Medium</cell><cell>60.1</cell><cell>261.1</cell><cell>43.4</cell><cell>65.9</cell><cell>47.0</cell><cell>39.7</cell><cell>62.8</cell><cell>42.1</cell><cell>44.6</cell><cell>66.3</cell><cell>48.5</cell><cell>40.7</cell><cell>63.8</cell><cell>43.7</cell></row><row><cell>ResNeXt101-64x4d</cell><cell>101.9</cell><cell cols="2">424.29 42.8</cell><cell>63.8</cell><cell>47.3</cell><cell>38.4</cell><cell>60.6</cell><cell>41.3</cell><cell>44.4</cell><cell>64.9</cell><cell>48.8</cell><cell>39.7</cell><cell>61.9</cell><cell>42.6</cell></row><row><cell>PVT-Large[43]</cell><cell>81.0</cell><cell>n/a</cell><cell>42.9</cell><cell>65.0</cell><cell>46.6</cell><cell>39.5</cell><cell>61.9</cell><cell>42.5</cell><cell>44.5</cell><cell>66.0</cell><cell>48.3</cell><cell>40.7</cell><cell>63.4</cell><cell>43.7</cell></row><row><cell>ViL-Base</cell><cell>76.1</cell><cell>365.1</cell><cell>45.1</cell><cell>67.2</cell><cell>49.3</cell><cell>41.0</cell><cell>64.3</cell><cell>44.2</cell><cell>45.7</cell><cell>67.2</cell><cell>49.9</cell><cell>41.3</cell><cell>64.4</cell><cell>44.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10 .Table 11 .</head><label>1011</label><figDesc>Hyperparameters for training. We use MsViT to represent the multi-scale vision transformers with different kinds of attention mechanisms, including our Vision Longformer (ViL). For the experiments trained on COCO, MsViT is combined with the Retinanet or Mask R-CNN. The training configs for Retinanet or Mask R-CNN are the same, and we still use MsViT for their unified short name. We do not apply gradient clipping for all ImageNet classification training and apply gradient clipping at global norm 1 for COCO object detection/segmentation. We use AdamW for all our experiments, except that we use SGD with momentum 0.9 for the ImageNet-384 fine-tuning experiments. Ablation of the effects of LayerNorm and 2-D positional embedding in the patch embedding of the E-ViT module, with ImageNet Top-1 accuracy. The improvement over DeiT<ref type="bibr" target="#b39">[40]</ref> comes from the added LayerNorm. The 2-D positional embedding is mainly for saving parameters for high-resolution feature maps. The column names of "CLS" and "Ave Pool" indicate how the image feature is obtained for the linear classification head.</figDesc><table><row><cell></cell><cell>Model</cell><cell></cell><cell>Dataset</cell><cell>Epoch</cell><cell>Base Lr</cell><cell cols="3">LR decay Weight decay Batch size</cell></row><row><cell cols="2">MsViT-Tiny</cell><cell></cell><cell>ImageNet</cell><cell>300</cell><cell>1e-3</cell><cell>cosine</cell><cell>0.1</cell><cell>1024</cell></row><row><cell cols="2">MsViT-Small</cell><cell></cell><cell>ImageNet</cell><cell>300</cell><cell>1e-3</cell><cell>cosine</cell><cell>0.1</cell><cell>1024</cell></row><row><cell cols="2">MsViT-Meidum-W</cell><cell></cell><cell>ImageNet</cell><cell>300</cell><cell>8e-4</cell><cell>cosine</cell><cell>0.1</cell><cell>1024</cell></row><row><cell cols="2">MsViT-Meidum-D</cell><cell></cell><cell>ImageNet</cell><cell>300</cell><cell>8e-4</cell><cell>cosine</cell><cell>0.1</cell><cell>1024</cell></row><row><cell cols="2">MsViT-Base-W</cell><cell></cell><cell>ImageNet</cell><cell>150</cell><cell>4e-4</cell><cell>cosine</cell><cell>0.1</cell><cell>1024</cell></row><row><cell cols="2">MsViT-Base-D</cell><cell></cell><cell>ImageNet</cell><cell>150</cell><cell>8e-4</cell><cell>cosine</cell><cell>0.1</cell><cell>1024</cell></row><row><cell cols="2">MsViT-Meidum-W</cell><cell cols="2">ImageNet-21k</cell><cell>90</cell><cell>5e-4</cell><cell>cosine</cell><cell>0.1</cell><cell>1024</cell></row><row><cell cols="2">MsViT-Meidum-D</cell><cell cols="2">ImageNet-21k</cell><cell>90</cell><cell>5e-4</cell><cell>cosine</cell><cell>0.1</cell><cell>1024</cell></row><row><cell cols="2">MsViT-Base-W</cell><cell cols="2">ImageNet-21k</cell><cell>90</cell><cell>3e-4</cell><cell>cosine</cell><cell>0.1</cell><cell>1024</cell></row><row><cell cols="2">MsViT-Base-D</cell><cell cols="2">ImageNet-21k</cell><cell>90</cell><cell>5e-4</cell><cell>cosine</cell><cell>0.1</cell><cell>1024</cell></row><row><cell cols="2">MsViT-Meidum-W</cell><cell cols="2">ImageNet-384</cell><cell>10</cell><cell>[2, 4]*e-2</cell><cell>cosine</cell><cell>0.</cell><cell>512</cell></row><row><cell cols="2">MsViT-Meidum-D</cell><cell cols="2">ImageNet-384</cell><cell>10</cell><cell>[2, 4]*e-2</cell><cell>cosine</cell><cell>0.</cell><cell>512</cell></row><row><cell cols="2">MsViT-Base-W</cell><cell cols="2">ImageNet-384</cell><cell>10</cell><cell>[2, 4]*e-2</cell><cell>cosine</cell><cell>0.</cell><cell>512</cell></row><row><cell cols="2">MsViT-Base-D</cell><cell cols="2">ImageNet-384</cell><cell>10</cell><cell>[2, 4]*e-2</cell><cell>cosine</cell><cell>0.</cell><cell>512</cell></row><row><cell></cell><cell>Model</cell><cell></cell><cell>Dataset</cell><cell>iterations</cell><cell>Base Lr</cell><cell cols="3">LR decay Weight decay Batch size</cell></row><row><cell cols="2">MsViT-Tiny-1x</cell><cell></cell><cell>COCO</cell><cell>60k-80k-90k</cell><cell>1e-4</cell><cell>multi-step</cell><cell>0.1</cell><cell>16</cell></row><row><cell cols="2">MsViT-Small-1x</cell><cell></cell><cell>COCO</cell><cell>60k-80k-90k</cell><cell>1e-4</cell><cell>multi-step</cell><cell>0.1</cell><cell>16</cell></row><row><cell cols="2">MsViT-Meidum-D-1x</cell><cell></cell><cell>COCO</cell><cell>60k-80k-90k</cell><cell>1e-4</cell><cell>multi-step</cell><cell>0.1</cell><cell>16</cell></row><row><cell cols="2">MsViT-Base-D-1x</cell><cell></cell><cell>COCO</cell><cell>60k-80k-90k</cell><cell>8e-5</cell><cell>multi-step</cell><cell>0.1</cell><cell>16</cell></row><row><cell cols="2">MsViT-Tiny-3x+ms</cell><cell></cell><cell>COCO</cell><cell>180k-240k-270k</cell><cell>1e-4</cell><cell>multi-step</cell><cell>0.1</cell><cell>16</cell></row><row><cell cols="2">MsViT-Small-3x+ms</cell><cell></cell><cell>COCO</cell><cell>180k-240k-270k</cell><cell>1e-4</cell><cell>multi-step</cell><cell>0.1</cell><cell>16</cell></row><row><cell cols="3">MsViT-Meidum-D-3x+ms</cell><cell>COCO</cell><cell>180k-240k-270k</cell><cell>1e-4</cell><cell>multi-step</cell><cell>0.1</cell><cell>16</cell></row><row><cell cols="2">MsViT-Base-D-3x+ms</cell><cell></cell><cell>COCO</cell><cell>180k-240k-270k</cell><cell>8e-5</cell><cell>multi-step</cell><cell>0.1</cell><cell>16</cell></row><row><cell>Model</cell><cell cols="4">Tiny CLS Ave Pool CLS Ave Pool Small</cell><cell>Model</cell><cell cols="3">Tiny CLS Ave Pool CLS Ave Pool Small</cell></row><row><cell cols="2">DeiT/16[40] 72.2 +Layernorm 73.04</cell><cell>-73.16</cell><cell>79.8 80.54</cell><cell>-80.32</cell><cell cols="2">ViL 1-2-8-1 75.72 ViL 1-1-8-2 76.18</cell><cell>75.98 76.25</cell><cell>81.65 82.12</cell><cell>81.99 82.08</cell></row><row><cell>+2D Pos</cell><cell>73.21</cell><cell>73.09</cell><cell>80.44</cell><cell>80.75</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">only use it to verify the correctness of our other implemen-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>tations.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">We have implemented Vision Longformer in three ways:</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">1. Using Pytorch's unfold function. We have two sub-</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Equivalent in our sliding chunks implementation, which is our default choice.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Section 4.4, we also show the superior performance of Vision Longformer over other attention mechanisms, on the object detection and segmentation tasks.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Settings</head><p>A. <ref type="bibr" target="#b0">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Model configurations</head><p>We listed the model configuration of all models used in this paper in <ref type="table">Table 9</ref>. We do not specify the attention mechanism here, because the model configuration is the same for all attention mechanisms and the attention-specific parameters are specified in <ref type="table">Table 15</ref>. lized in this work</p><p>In this paper, we compare Vision Longformer with the following alternative choices of efficient attention methods. Pure global memory (a = global). In Vision Longformer, see <ref type="figure">Figure 2</ref> (Left), if we remove the local-to-local attention, then we obtain the pure global memory attention mechanism (called Global Attention hereafter). Its memory complexity is O(n g (n g + n l )), which is also linear w.r.t. n l . However, for this pure global memory attention, n g has to be much larger than 1. In practice, we set different numbers of global tokens for different stages, as shown in <ref type="table">Table 15</ref>, with more global tokens in the first 2 stages and less in the last 2 stages. This setting makes the memory/computation complexity comparable with other attention mechanisms under the same model size. Linformer <ref type="bibr" target="#b41">[42]</ref> (a = LIN) projects the n l × d dimensional keys and values to K × d dimensions using additional projection layers, where K n l . Then the n l queries only attend to these projected K key-value pairs. The memory complexity of Linformer is O(Kn l ). We gradually increase K (by 2 each time) and its performance gets nearly satu-rated at 256. Therefore, K = 256 is our choice for this Linformer attention, which turns out to be the same with the recommended value. Notice that Linformer's projection layer (of dimension K ×n l ) is specific to the current n l , and cannot be transferred to higher-resolution tasks that have a different n l . It is possible to transfer Linformer's weight by resizing feature maps of a different size to the original feature map size that Linformer is trained with and then applying the Linformer's projection. We do not explore this choice in this work. Spatial Reduction Attention (SRA) <ref type="bibr" target="#b42">[43]</ref> (a = SRA) is similar to Linformer, but uses a convolution layer with kernel size R and stride R to project the key-value pairs, hence resulting in n l /R 2 compressed key-value pairs. Therefore, The memory complexity of SRA is O(n 2 l /R 2 ), which is still quadratic w.r.t. n l but with a much smaller constant 1/R 2 . When transferring the ImageNet-pretrained SRAmodels to high-resolution tasks, SRA still suffers from the quartic computation/memory blow-up w.r.t. the feature map resolution. Pyramid Vision Transformer <ref type="bibr" target="#b42">[43]</ref> uses this SRA to build multi-scale vision transformer backbones, with different spatial reduction ratios (R 1 = 8, R 2 = 4, R 3 = 2, R 4 = 1) for each stage. With this PVT's setting, the key and value feature maps at all stages are essentially with resolution H 32 × W 32 . This choice is able to scale up to image resolution 600×1000, but the memory usage is much larger than ResNet counterparts for 800 × 1333.</p><p>In this paper, we benchmarked the performance of SRA/32 with SR ratios R 1 = 8, R 2 = 4, R 3 = 2, R 4 = 1 (same as PVT <ref type="bibr" target="#b42">[43]</ref>) and SRA/64 with SR ratios R 1 = 16, R 2 = 8, R 3 = 4, R 4 = 2 (two times more downsizing from that in PVT <ref type="bibr" target="#b42">[43]</ref>), as shown in <ref type="table">Table 15</ref>. The SRA/64 setting makes the memory usage comparable with other efficient attention mechanisms under the same model size, but introduces more parameters due to doubling the kernel size of the convolutional projection layer. Performer <ref type="bibr" target="#b8">[9]</ref> (a = performer) uses random kernel approximations to approximate the Softmax computation in MSA, and achieves a linear computation/memory complexity with respect to n l and the number of random features K. We use the default K = 256 orthogonal random features (OR) for Performer. The memory/space complexity of performer is O(Kd + n l d + Kn l ) while its computation/time complexity is O(Kn l d). For the time complexity, we ignore the complexity of generating the orthogonal random features, which in practice cannot be ignored during training. We refer to Section B.3 in <ref type="bibr" target="#b8">[9]</ref> for a detailed discussion of theoretical computation/memory complexity of Performer.</p><p>One important technique in training Performer is to redraw the random features during training. In our Ima-geNet classification training, we adopt a heuristic adaptive redrawing schedule: redraw every 1 + 5T iterations in Epoch T (T = 0, 1, ..., 299). In our COCO object detection/segmentation training, the Performer is initialized from ImageNet pretrained checkpoint and thus there is no need to redraw very frequently in the initial training stage.Therefore, we redraw the random features every 1000 iterations in COCO object detection/segmentation training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Stage1 <ref type="formula">Stage2</ref>   <ref type="table">Table 15</ref>. Attention mechanism specific hyper-parameters. See Appendix D for the details of these attention mechanisms.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Etc: Encoding long and structured data in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08483</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00364</idno>
		<title level="m">Pre-trained image processing transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR, 2020. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11740</idno>
		<title level="m">Uniter: Learning universal image-text representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Up-detr: Unsupervised pre-training for object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugeng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junying</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09094</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Axial attention in multidimensional transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12180</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">In defense of grid features for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10267" to="10276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
	<note>Nikolaos Pappas, and François Fleuret</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11370</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutation-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungtaek</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3744" to="3753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unicoder-VL: A universal encoder for vision and language by cross-modal pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06066</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Visualbert: A simple and performant baseline for vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Oscar: Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Vil-BERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical transformers for long document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pappagari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zelasko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Carmiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="838" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Blockwise selfattention for long document understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02972</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05507</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taghi</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vl-Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Pre-training of generic visual-linguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">LXMERT: Learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Synthesizer: Rethinking self-attention in transformer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00743</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sparse sinkhorn attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<idno>PMLR, 2020. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="9438" to="9447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Long range arena: A benchmark for efficient transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04006</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06732</idno>
		<title level="m">Efficient transformers: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Max-deeplab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00759</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">End-toend video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14503</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning texture transformer network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5791" to="5800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinava</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14062</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning joint spatial-temporal transformations for video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="528" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Vinvl: Revisiting visual representations in vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">End-to-end object detection with adaptive clustering transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09315</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Informer: Beyond efficient transformer for long sequence time-series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieqi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wancai</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07436</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Unified vision-language pretraining for image captioning and VQA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with masked transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiny</forename></persName>
		</author>
		<title level="m">Model Stage1 Stage2 Stage3 Stage4 n</title>
		<imprint>
			<date type="published" when="0969" />
			<biblScope unit="page">384</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Small</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">768</biblScope>
			<date type="published" when="0962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-D</forename><surname>Medium</surname></persName>
		</author>
		<imprint>
			<date type="published" when="0964" />
			<biblScope unit="page">768</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-W</forename><surname>Medium</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1922" />
			<biblScope unit="page">768</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-D</forename><surname>Base</surname></persName>
		</author>
		<imprint>
			<date type="published" when="0968" />
			<biblScope unit="page">768</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-W</forename><surname>Base</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1922" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">1024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Small</title>
		<imprint>
			<biblScope unit="page">768</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Small</title>
		<imprint>
			<biblScope unit="page">768</biblScope>
			<date type="published" when="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Small</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">768</biblScope>
			<date type="published" when="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Small</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">768</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Small</title>
		<imprint>
			<biblScope unit="page">768</biblScope>
			<date type="published" when="0961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Small</title>
		<imprint>
			<biblScope unit="page">768</biblScope>
			<date type="published" when="0962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Tiny-4stage / 4 means that the model has comparable size with DeiT-Tiny, has 4 stages and uses patch size 4x4 in the initial pixel space. 1-2-8-1 means that the model contains 4 stages, each stage has 1/2/8/1 MSA-FFN blocks, respectively. *Partial* means that the last two stages, which contain most of the attention blocks, still use full attention. Vision Longformer does not have *Partial* version because its window size is set as 15 (comparable with the ViT(DeiT)/16 feature map size 14), and its attention mechanism in the last two stages is equivalent to full attention. * indicates that the training batch size is 256 (with learning rate linearly scaled down)</title>
	</analytic>
	<monogr>
		<title level="m">Table 13. Overall comparison in ImageNet top-1 accuracy, with input size 224</title>
		<imprint/>
	</monogr>
	<note>different from all other experiments with batch size 1024 in this table</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Tiny-4stage / 4 means that the model has comparable size with DeiT-Tiny, has 4 stages and uses patch size 4x4 in the initial pixel space. 1-2-8-1 means that the model contains 4 stages, each stage has 1/2/8/1 MSA-FFN blocks, respectively. *Partial* means that the last two stages, which contain most of the attention blocks, still use full attention. Vision Longformer does not have *Partial* version because its window size is set as 15 (comparable with the ViT(DeiT)/16 feature map size 14</title>
	</analytic>
	<monogr>
		<title level="m">Overall comparison in number of parameters (M) and GFLOPs, with input size 224</title>
		<imprint/>
	</monogr>
	<note>Table 14. and its attention mechanism in the last two stages is equivalent to full attention</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
