<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Contextual Representations for Semantic Parsing with Generation-Augmented Pre-Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Shi</surname></persName>
							<email>peng.shi@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ng</surname></persName>
							<email>patricng@amazon.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">AWS AI Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
							<email>zhiguow@amazon.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">AWS AI Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Zhu</surname></persName>
							<email>henghui@amazon.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">AWS AI Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">Hanbo</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">AWS AI Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
							<email>juwanga@amazon.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">AWS AI Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santos</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">AWS AI Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
							<email>bxiang@amazon.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">AWS AI Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Contextual Representations for Semantic Parsing with Generation-Augmented Pre-Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most recently, there has been significant interest in learning contextual representations for various NLP tasks, by leveraging large scale text corpora to train large neural language models with self-supervised learning objectives, such as Masked Language Model (MLM). However, based on a pilot study, we observe three issues of existing general-purpose language models when they are applied to text-to-SQL semantic parsers: fail to detect column mentions in the utterances, fail to infer column mentions from cell values, and fail to compose complex SQL queries. To mitigate these issues, we present a model pre-training framework, Generation-Augmented Pre-training (GAP), that jointly learns representations of natural language utterances and table schemas by leveraging generation models to generate pre-train data. GAP MODEL 1 is trained on 2M utterance-schema pairs and 30K utterance-schema-SQL triples, whose utterances are produced by generative models. Based on experimental results, neural semantic parsers that leverage GAP MODEL as a representation encoder obtain new state-of-the-art results on both SPIDER and CRITERIA-TO-SQL benchmarks. This refers to the language models that are pre-trained with GAP framework. Pain Point 1: Fail to match and detect the column mentions. Utterance: Which professionals live in a city containing the substring 'West'? List his or her role, street, city and state. Prediction: SELECT role code, street, state FROM Professionals WHERE city LIKE '%West%' Error: Missing column city in SELECT clause. Pain Point 2: Fail to infer columns based on cell values. Utterance: Give the average life expectancy for countries in Africa which are republics? Prediction: SELECT Avg(LifeExpectancy) FROM country WHERE Continent = 'Africa' Error: Missing GovernmentForm = 'Republic'. Pain Point 3: Fail to compose complex target SQL. Utterance: Which semesters do not have any student enrolled? List the semester name. Prediction: SELECT semester name FROM Semesters WHERE semester id NOT IN (SELECT semester name FROM Student Enrolment) Error: Should use semester id in nested SQL to align with the column in WHERE clause.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Recently, deep contextual language models <ref type="bibr" target="#b6">(Devlin et al. 2018;</ref><ref type="bibr" target="#b22">Liu et al. 2019b;</ref><ref type="bibr" target="#b17">Lewis et al. 2019;</ref><ref type="bibr" target="#b8">Dong et al. 2019;</ref><ref type="bibr" target="#b29">Raffel et al. 2019</ref>) have shown their effective modeling ability for text, achieving state-of-the-art results in series of NLP tasks. These models capture the syntactic and semantic information of the input text, generating fine-grained contextual embeddings, which can be easily applied to downstream models. Despite the success of large scale pre-trained language models on various tasks, it is less clear how to extend them to semantic parsing tasks such as text-to-SQL (Warren and Pereira 1982; <ref type="bibr" target="#b27">Popescu, Etzioni, and Kautz 2003;</ref><ref type="bibr" target="#b26">Popescu et al. 2004;</ref><ref type="bibr" target="#b18">Li, Yang, and Jagadish 2006)</ref>, which requires joint reasoning of the natural language utterance and structured database schema information. Recent work <ref type="bibr" target="#b12">(Guo et al. 2019;</ref><ref type="bibr" target="#b33">Wang et al. 2019;</ref><ref type="bibr">Bogin, Gardner, and Berant 2019b,a)</ref> shows that with more powerful pre-trained language models, the highly domain-specific semantic parsers <ref type="table">Table 1</ref>: Error examples collected from the SPIDER development set based on the RAT-SQL + BERT ).</p><p>can be further improved, even though these language models are trained for pure text encoding.</p><p>However, based on error analysis on the output of neural language model-based text-to-SQL systems, we observe that these models can be further enhanced if we could mitigate the following three pain points, which are also illustrated in <ref type="table">Table 1.</ref> (1) The model is ineffective to match and detect column names in utterances. The model should learn to detect column names mentioned in utterances by matching utterance tokens with the schema, and use the matched columns in the generated SQL. The error analysis indicates that, in some cases, models miss some columns when synthesizing the target SQL, while the column is mentioned explicitly in the utterance.</p><p>(2) The model fails to infer the columns implicitly from cell values. This problem is trickier than the first one, because the model is expected to infer the column name based on some cell values mentioned in the utterance, instead of just matching the utterance tokens with the schema. This requires the model to have more domain knowledge. For example, as presented in the second section of <ref type="table">Table 1</ref>, the model should know republics is a GovernmentForm.</p><p>(3) The model should learn to compose complex queries. Besides the column selection, to generate a correct SQL, the model should learn to attach the selected columns to the correct clauses. This is a non-trivial task, especially when the target SQL is complex, e.g., when the query is nested. As shown in the last section of Table 1, the model should learn to use corresponding column semester id in the nested SQL, instead of using column semester name.</p><p>Recent work has demonstrated that jointly pre-training on utterances and table contents (e.g., column names and cell values) can benefit downstream tasks such as table parsing and semantic parsing <ref type="bibr" target="#b39">(Yin et al. 2020;</ref><ref type="bibr" target="#b13">Herzig et al. 2020</ref>). These models are pre-trained using the Masked Language Modeling (MLM) task by either masking tokens from the utterance input or tokens from the schema input. However, this learning objective can only model the alignment between the utterance and schema implicitly. We hypothesize that, in order to cope with the three pain points previously listed, it is necessary to use pre-training objectives that enforce the learning of contextual representations that better capture the alignment between utterances and schema/table contents.</p><p>In this work, we present a language model pre-training framework, Generation-Augmented Pre-training (GAP), that exploits multiple learning objectives <ref type="bibr">(pre-training tasks)</ref> and synthetic data generation to jointly learn contextual representations of natural language utterances and table schema. We propose the following three new learning objectives that not only enforce joint learning but also improve the ability of the model to grasp more domain knowledge, which is helpful in cross-domain scenarios: (1) column prediction task, which is a pre-training task that consists in giving a label for each column in the input schema to decide whether it is used in the input utterance or not. This task is intent to improve the column detection ability of the model. (2) column recovery task, which consists in randomly replacing some of the column names with one of their cell values and asking the model to recover the original column name either based on the cell value itself or based on the contextual information of the utterance when the column is explicitly mentioned in the utterance. This learning objective is meant to enhance the column inferring ability of the model. (3) SQL generation, which consists in generating SQL queries given utterances and schema. This task can boost the ability of the model to compose complex queries by leveraging large scale SQL datasets from the Web.</p><p>A key challenge to use the proposed pre-training tasks is training data. Although it is easy to obtain large scale datasets of crawled tables and SQL queries, it is difficult to obtain high-quality utterances interrelated with the tables or logically consistent with crawled SQL queries. Recent work used the surrounding text of tables as a proxy of natural language utterances <ref type="bibr" target="#b39">(Yin et al. 2020;</ref><ref type="bibr" target="#b13">Herzig et al. 2020)</ref>. However, this option is far from optimal because those texts are dissimilar to user utterances in terms of text length, composition and content. The surrounding text of a table is usually a paragraph, while natural language utterances in the downstream task are short sentences. Furthermore, the content of surrounding text of tables can be quite noisy because the text may be irrelevant to the table. In GAP , we overcome the pre-training data challenge through the use of synthetic data. We propose two sequence-to-sequence (seq2seq) generative models, SQL-to-text and table-to-text, that can produce large scale datasets with enough quality for pre-training. We train our generative models by finetuning <ref type="bibr">BART (Lewis et al. 2019</ref>), a state-of-the-art pre-trained language model. Concurrently, <ref type="bibr" target="#b41">Yu et al. (2020b)</ref> and <ref type="bibr" target="#b5">Deng et al. (2020)</ref> utilized synthetic data generated from synchronized contextfree grammar and existing data-to-text datasets <ref type="bibr" target="#b24">(Parikh et al. 2020)</ref> for pre-training, respectively, which requires extra crowd and expert annotation efforts.</p><p>The outcome of GAP is a pre-trained model that can be plugged into neural semantic parsers to compute contextual representations of utterances and schema. We apply GAP to text-to-SQL semantic parsing datasets, and experimental results show that systems augmented with GAP outperform state-of-the-art semantic parsers on SPIDER and CRITERIA-TO-SQL datasets. In summary, our work presents the following main contributions: • Based on an error analysis, we spot three main issues in pre-trained LM-based text-to-SQL semantic parsers. • We propose a new framework for pre-training semantic parsers that exploits multiple pre-training tasks and synthetic data. • We present three novel learning objectives that alleviate the three main issues spotted with pre-trained LMs for semantic parsing. • We propose a novel strategy to overcome pre-training data challenges by leveraging SQL-to-Text and <ref type="table">Table-</ref>to-Text generative models to generate synthetic data for learning joint representations of textual data and table schema. • To the best of our knowledge, this is the first work to effectively use both crawled SQL and crawled tables to enhance the text-to-SQL semantic parsing task. Our code is public for future work.</p><p>2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>We first present the architecture of the semantic parsers, and then introduce the pre-training model in the GAP framework. Lastly, we describe how to obtain the synthetic pretraining data with generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text-to-SQL Semantic Parser</head><p>The Text-to-SQL semantic parser translates natural language utterances to SQL queries. The semantic parsers in our experiments are based on the encoder-decoder architecture. Given an utterance U = {x 1 , x 2 , ..., x n } and an schema S consisting of tables T = {t 1 , t 2 , ..., t |T | } and columns C = {c 1 , c 2 , ..., c |C| }, we leverage the contextual encoder to obtain the representations of utterance tokens and schema. The decoder is required to compute a distribution P (Y |X, S) over SQL programs. Based on different model designs, the decoder learning target Y can be raw SQL tokens (Zhang What is the earliest year in which a film has result "nominated" at the Oscars?"</p><p>Find the employees whose salary is higher than the salary of the employee with first name 'Blake'.  <ref type="bibr" target="#b12">(Guo et al. 2019)</ref> or AST tree <ref type="bibr" target="#b3">(Bogin, Gardner, and Berant 2019b;</ref><ref type="bibr" target="#b39">Yin et al. 2020</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training Model</head><p>The left part of <ref type="figure">Figure 1</ref> presents an overview of GAP in the pre-training stage. Given an utterance U and schema S, GAP MODEL takes as input the concatenation of U and the column names c in S in the following format X = {&lt;s&gt; U &lt;col&gt; c 1 &lt;col&gt; c 2 ... &lt;col&gt; c |C| &lt;/s&gt;}, where c i denotes the i-th column in schema S.</p><p>With the 12-layer transformers, each token in the input can be encoded as contextual representations, denoted as h. For different learning objectives, the representations are utilized by different decoders. To jointly learn contextual representations for utterances and schemas and mitigate the three pain points discussed in the intro, we leverage four learning objectives in the pre-training: Besides the Masked Language Model (MLM), we propose learning objectives including Column Prediction (CPred), Column Recovery (CRec), and SQL Generation (GenSQL). Multi-task learning is leveraged for these learning objectives, Column Prediction (CPred): The Column Prediction learning objective encourages the model to capture the alignment signals between the utterance and schema, by predicting whether a column is used in the utterance or not. An illustration is shown in the pink component of <ref type="figure">Figure 1</ref>. Specifically, based on the representations obtained from the transformer encoder, a two-layer MLP is applied on each column representation g col , which is obtained from the output of an average pooling layer that aggregates all sub-tokens of the corresponding column. Afterward, a sigmoid activation function is applied to obtain the probability that the corresponding column is mentioned in the utterance. The GAP MODEL maximizes P θ enc (Y c |X) where Y c is a 0/1 label for a column and X is in its unmasked version. Column Recovery (CRec): The Column Recovery learning objective strengthens the model's ability to discover the connections between the cell values and the column names, by recovering the column name based on a sampled cell value. For example, as shown in left yellow part of <ref type="figure">Figure 1</ref>, the model recovers the column name job from cell value manager. Generally, the transformer decoder recovers column names based on two information sources: one is the actual cell value, and the other one is the column name mentions in the utterance. We design the following rules for the value replacement:</p><p>• If a column is not mentioned in the utterance, we will replace the column name with its cell value with probability of 0.5. In this case, the column name will be recovered from cell value without other contextual information.</p><p>• If a column is mentioned, we will directly replace the column name with its cell value. In this case, the model can leverage the contextual information from the utterance and the cell value to recover the column name.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SQL Generation (GenSQL):</head><p>This learning objective is directly related to the downstream task. Based on the representation from the transformer encoder, the GAP MODEL decoder maximizes p dec (y sql |h). This learning target encourages the model to learn to compose complex SQL that requires logical reasoning, considering that there are a large number of sophisticated SQLs in crawled data. For example, the GAP MODEL decoder needs to generate the column in appropriate position such as in the ORDER BY clause or WHERE clause, instead of just predicting the column is used or not. Specifically, the GAP MODEL decoder emits the target SQL token by token with a close vocabulary set, which is composed of the SQL keywords vocabulary and column names. The embeddings of the SQL keywords are randomly initialized and trained during the pre-training phase. The column representations are obtained in the same way as the one used in Column Prediction learning objective, by averaging the column's sub-tokens representations. At each decoding step, the decoder generates a hidden vector and then a dotproduct operation is applied on it and the target vocabulary representations, yielding a probability distribution over the vocabulary set. Masked Language Model(MLM): We use the standard MLM objective, with a masking rate of 35% sub-tokens in the whole input sequence, including the utterance and schema. Based on the representation from transformer encoder, GAP MODEL employs a transformer decoder to maximize p θ (x|x m ) on large scale utterance-schema pairs, where x m is the masked version of x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training Data Generation</head><p>As discussed, previous pre-training approaches such as TaBERT <ref type="bibr" target="#b39">(Yin et al. 2020</ref>) and TAPAS <ref type="bibr" target="#b13">(Herzig et al. 2020)</ref> use the surrounding texts of the tables as a proxy of natural language utterance. However, those texts are noisy and sometimes are not directly related to the table contents.</p><p>In the downstream task, the input texts are usually utterances/user queries, which are short and highly dependent on the schema and contents of the structured data. In order to minimize the gap between pre-training and downstream tasks, we adopt a state-of-the-art pre-trained sequence-tosequence model, such as BART, to generate high-quality utterances based on crawled SQLs or structured tables. As shown in the right part of <ref type="figure">Figure 1</ref>, we design two different models, namely SQL-to-Text generation model and <ref type="table">Table-</ref>to-Text generation model, for handling the two different inputs. Specifically, the SQL-to-text generation model takes the SQL as input and generates the utterance that explains the query intent. The other model, the <ref type="table">Table-</ref>to-Text generation model, generates utterances based on a set of sampled column names and cell values from tables. In this way, we can generate utterances interrelated with tables without composing queries that might be suspicious. SQL-to-Text Generation: We crawl 30K SQLs from GitHub 3 . To generate utterances for these SQL queries, we train a SQL-to-Text model on the SPIDER dataset. The input is the original SQL and it is directly tokenized by the BART tokenizer without additional pre-processing. After finetuning BART, the model can generate high-quality utterances logically consistent with the input SQL, achieving a 0.1934 BLEU score on the development set. Then we use the model to generate utterances for crawled SQLs. We further extract columns and tables in each SQL as positive schema candidates, denoted as schema pos . We also sample columns and tables from the pool which are extracted from other SQLs as negative candidates, denoted as schema neg . The final schema is composed of these two parts. The utterance-schema-SQL triples are then collected for the GenSQL learning objective in the pre-training phase. <ref type="table">Table-</ref>to-Text Generation: Generating utterances from tables is different because query intents are not given. Instead of synthesizing noisy SQLs and then translating into natural language utterances, we propose a We then linearize the sampled candidates into {column name | associated cell value list} and concatenate them into a sequence, separated by &lt;sep&gt; token. Furthermore, to control the complexity and diversity of the generated text, we integrate three types of control codes into the model input:</p><p>• Aggregator-based control code: Including COUNT, MAX, MIN, AVG, and SUM. For the first two sampled columns, we randomly sample an aggregator for each with the probability γ 1 (we use γ 1 as 0.5) if the column type matches with the selected aggregator, e.g., aggregator SUM should be applied on numerical type column. If the control codes are sampled, they will be appended to the associated cell value list of the corresponding column.</p><p>• Structure control code: Including IN, NOT IN, INTERSECT, UNION, and EXCEPT. For each example, with probability of γ 2 (we use γ 2 as 0.35), we randomly sample one of them with uniform distribution. Otherwise, NONE is used. This control code is used as the first item of the input sequence.</p><p>• Order-based control code: We add {LIMIT : number} as a part of the control code, which is usually used in an ORDER BY based query. With this control code, the generated utterances usually contain phrases that constrain the number of query results should be returned, e.g., Show the name of aircrafts with top three lowest speed..</p><p>We fine-tune a BART model on SPIDER dataset to create the generator. To align with our designed input, we convert the SQL into the format we expected. We extract all the columns and their associated aggregators and values from the SQL. We also obtain any special control codes that appears in the SQL. After fine-tuning, the model achieves 0.1821 BLEU score on the development set. Afterwards, we apply the finetuned model to the crawled tables and generate high-quality utterances. The utterance-schema pairs are collected for the learning objectives including MLM, CPred, and CRec in pre-training phase.</p><p>For the pre-training step, we need to decide whether a column is mentioned in the utterance or not. To create the label for this, we directly regard all the sampled columns to have positive label. This is based on the assumption that the generation model uses all the columns to synthesize the utterance, and does not have the hallucination issue that models generate some columns names or cell values that are not presented in the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Configurations</head><p>In the pre-training, we train our GAP MODEL with the underlying transformers initialized with BART (Lewis et al. 2019) model. During the fine-tuning phase, we only leverage the encoder component of the GAP MODEL with 12layer transformers as the contextual encoder for the semantic parsers. For more details, please refer to appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tasks, Datasets and Baseline Systems</head><p>For the downstream tasks, we conduct experiments on two datasets to show the effectiveness of our framework. SPIDER: SPIDER dataset <ref type="bibr" target="#b42">(Yu et al. 2018</ref>) is a text-to-SQL dataset with 10,181 annotated parallel utterance-database-SQL triples. Different from WikiSQL, the examples in the SPIDER dataset is more complex, involving nested query, set operation, and multiple tables joining. The exact set match accuracy is the evaluation metrics. The test set is not publicly available. For baseline parser, we use RAT-SQL ) model as our baseline system to report the endto-end performance. RAT-SQL model is the state-of-the-art parser in the SPIDER dataset, which leverages the 8-layer relation-aware transformer to model the connections among tables and utterance. To show that the GAP MODEL can be plugged into different neural semantic parsers, we further use IRNet <ref type="bibr" target="#b12">(Guo et al. 2019</ref>) model for ablation study. IR-Net semantic parser is based on SemQL grammar, which is an effective intermediate representation for SQL. IRNet is efficient in terms of training time, which requires 1 day for training, while RAT-SQL model requires approximately 5 days for training. We augment the encoder part of our GAP MODEL to these base parsers, by replacing their original contextual encoders. CRITERIA-TO-SQL: CRITERIA-TO-SQL is a dataset to facilitate retrieving eligible patients for a trial from the electronic health record database. The task is to translate the eligibility criteria to executable SQL queries. For example, a criteria statement any infection requiring parenteral antibiotic therapy or causing fever (i.e., temperature &gt; 100.5f ) ≤ 7 days prior to registration is required to be interpreted into SQL SELECT id FROM records WHERE active infection = 1 AND (parenteral antibiotic therapy = 1 OR causing fever = 1 OR temperature &gt; 100.5). The dataset contains 2003 annotated examples, and the evaluation metrics are the SQL accuracy and execution accuracy. Our baseline system for CRITERIA-TO-SQL dataset is adopted from <ref type="bibr" target="#b43">(Yu et al. 2020c</ref>), a slot-filling based model that takes advantages of the prior grammar knowledge to design the sketch. We denote this system as YXJ model. The system uses the BERT-base as the contextual encoder. <ref type="table" target="#tab_2">Table 2</ref> shows the end-to-end results on SPIDER dataset. Based on the codebase 4 provided by <ref type="bibr" target="#b33">Wang et al. (2019)</ref>, we replicate the RAT-SQL + BERT large model, achieving 0.665 exact set match accuracy on the development set. This matches the RAT-SQL V2 + BERT but still worse than its V3. By replacing the BERT-large with the encoder of BART 5 , we obtain accuracy of 0.676 on the development set and 0.651 on test set. The BART Encoder based model achieves comparable results with RAT-SQL V3 + 4 https://github.com/microsoft/rat-sql 5 The encoder of BART has 12-layer transformers while BERTlarge has 24-layer transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spider Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Dev Test</p><p>Global-GNN <ref type="bibr" target="#b2">(Bogin et al., 2019a)</ref> 0.527 0.474 EditSQL + BERT  0.576 0.534 IRNet + BERT <ref type="bibr" target="#b12">(Guo et al. 2019)</ref> 0.619 0.547 RyanSQL V2 + BERT <ref type="bibr" target="#b4">(Choi et al. 2020)</ref> 0.706 0.606 RAT-SQL V2 + BERT      <ref type="table" target="#tab_3">Table 3</ref>. The BERT results are adopted from <ref type="bibr" target="#b33">Wang et al. (2019)</ref>, which is the stateof-the-art system on SPIDER dataset. Comparing the RAT-   For comparison, we sample 40 examples from SPI-DER development set which the baseline system RAT-SQL+BART Encoder fails in. Because we focus more on the following three error types as we discussed in the introduction part: column selection error, column inferring error and SQL composing error, we ignore other error types during the sampling. We analyze the predictions of both the RAT-SQL+BART Encoder and RAT-SQL+GAP MODEL Encoder. The statistics are shown in <ref type="table" target="#tab_5">Table 4</ref>. The numbers in the <ref type="table">Table represent</ref> the error count of each error type. Based on the analysis results, we can find that the GAP MODEL Encoder can alleviate all the three error types, especially the column selection and SQL composing error. <ref type="table" target="#tab_6">Table 5</ref> shows the test results of the CRITERIA-TO-SQL dataset. The YXJ model <ref type="bibr" target="#b43">(Yu et al. 2020c</ref>) is built upon BERT-base encoder and sketch-based decoder, achieving the state-of-the-art performance of 0.142 SQL accuracy and 0.158 execution accuracy. We use this system as our baseline. Instead of using the BERT encoder, we augment the model with more powerful pre-trained language models such as RoBERTa and BART. These two pre-trained language models yield significant improvement over the BERT baseline, achieving 0.294 and 0.307 on the SQL accuracy, respectively. After executing the generated SQL queries against the database, these two models obtain 0.538 and 0.558 execution accuracy, respectively. By replacing the BART encoder with GAP MODEL, the parser obtains 2.0% improvement on the SQL accuracy and 3.6% improvement on the execution accuracy, which registers new state-of-theart performance. This also confirms our assumption that the  <ref type="table">Table 6</ref>: Ablation study on different learning objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Criteria-to-SQL Results</head><p>parsers can benefit from better quality of contextual encoders that jointly reason over utterances and schemas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Learning Objectives</head><p>We investigate four different learning objectives in this work, namely Masked Language Model (MLM), Column Prediction (CPred), Column Recovery (CRec) and SQL Generation (GenSQL). We conduct the ablation study on SPIDER development set to compare the first three learning objectives under two different conditions: One is with GenSQL learning objective and the other one is without. We use the IRNet based model in the ablation study because it is more efficient in training than RAT-SQL based model, and it can achieve comparable performance. We also want to show that our GAP MODEL is plugin-able and can augment different semantic parsers. <ref type="table">Table 6</ref> shows the ablation results. The first section of the <ref type="table">Table 6</ref> shows the results of three baseline systems that are based on IRNet model: IRNet + BERT, IRNet + TaBERT and IRNet + RoBERTa. These results confirm that improving the encoder quality of the semantic parser is a promising direction to pursue.</p><p>In the second section of the <ref type="table">Table 6</ref>, we present detailed ablation study results. Without the GenSQL learning objective, compared with baseline (IRNet + BART Encoder), the three learning objectives (MLM, CPred, CRec) can improve the performance of the parser, with a 1.7%, 1.9% and 2.5% increase, respectively. This indicates that these learning objectives improve the encoding quality of the transformer encoder. Based on the standard unsupervised learning objective MLM, we observe that the CPred and CRec learning objectives are helpful, which lead the model to the accuracy of 0.704 and 0.711, respectively. When we further combine the three learning objectives, the semantic parser's effectiveness is furthered boosted, achieving accuracy of 0.715, a 3.5% increase over its baseline.</p><p>With the GenSQL learning objective, the comparison of these three learning objectives is based on a higher baseline with accuracy of 0.699. This indicates that the Gen-SQL learning objective is valuable. Under this experimental condition, we observe that the MLM learning objective brings consistent improvement over the baseline with 1.8% increase on the accuracy. For the CPred and CRec, the ac-  <ref type="table">Table 7</ref>: The ablation study on different inputs for the pretraining based on the IRNet based model. curacy is boosted by 1.1% and 2.0%, respectively. When we combine the MLM with the CPred, we only observe comparable results with the MLM, without further significant improvement. However, the CRec learning objective brings the MLM a step forward, achieving the 0.728 on the accuracy. The combination of the three learning objectives under w/ GenSQL condition improve 2.4% on accuracy over the baseline. These results show that GenSQLand CRecare two salient learning objectives, leading the model to obtain accuracy more than 0.720, registering a new state-of-the-art performance on public development set on SPIDER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of Pre-Training Inputs</head><p>Whether to use utterance in pre-training: To prove that the utterance is beneficial in the pre-training, we conduct an ablation study by comparing the pre-trained models which are trained with and without utterance. Our experiments are based on the MLM and CRec learning objectives because the other two (CPred and GenSQL) require the utterance as the input based on their task definitions. Similarly, we use IRNet as our base parser.</p><p>The experimental results on SPIDER development set are shown in <ref type="table">Table 7</ref>. As we can see, if the GAP MODEL is trained with MLM learning objective without utterances as part of the input, the semantic parser performance drops to 0.678 from 0.697, which is lower than the baseline (0.680) by 0.2%. For the CRec learning objective, the accuracy drops from 0.705 to 0.688, a 1.7% decrease, if the GAP is trained without utterance. Even though, CRec learning objective trained without utterances is still helpful, which improves the baseline model by 0.8%. This aligns with our analysis of the CRec learning objective: model can leverage two information sources to recover the column name. If there are no utterances, the model can only use the signals the cell values provide to recover the column name. Furthermore, when the model can access more contextual information, which is provided by the utterance, the model can learn better encoding ability by learning to align the cell values and the column names in the utterances. Whether to use schema in pre-training: Another input choice is to only keep the utterances in the pre-training. This experimental setting is to justify that the model's improvement is not solely from better utterance representation. This input strategy is only applicable to the MLM learning ob-jective as the schema is a necessary component for other learning objectives. As shown in the MLM w/o schema entry in <ref type="table">Table 7</ref>, the model performance drops to 0.679, indicating that learning joint utterance and schema representation is necessary for this task. Whether to use the generated text or the surrounding text of the table: The value of the generated text is already justified by the learning objectives such as CPred or Gen-SQL, because the definitions of these learning objectives require the generated utterances that cannot be obtained from the surrounding text of the table (denoted as surrounding text). Here, we further rationalize our generation-augmented framework on MLM and CRec learning objectives by replacing the generated text with the surrounding text.</p><p>The results are presented in the entries of MLM (surrounding text) and CRec (surrounding text) of <ref type="table">Table 7</ref>. Overall, we can observe that the generation technique is superior to using the surrounding text as a proxy in the MLM and CRec learning objectives, considering the models drop 1.6% and 0.8% on accuracy, respectively. We also find that the CReclearning objective is more robust for pre-training, given that the fine-tuned model performance gets less influence compared with the one with MLM learning objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of Pre-trained Model</head><p>As the GAP MODEL provides gains on the text-to-SQL benchmarks, understanding what they learn is important. Following previous work <ref type="bibr" target="#b20">(Liu et al. 2019a;</ref><ref type="bibr" target="#b14">Hewitt and Liang 2019;</ref><ref type="bibr" target="#b15">Hewitt and Manning 2019)</ref>, we design a probing task, Column-Value Matching (CVM), to examine the extent to which the model can align the cell values in the utterances and the columns, i.e., the probes need to predict which column the cell value belongs to. Specifically, given the column spans and cell value spans (part of utterances), we can obtain their representations with contextual encoders such as BART or GAP MODEL Encoder, and an average pooling layer. We further compress the representations into another space with linear transformation, denoted as {c j } and v i , respectively. The probability of selecting column c j given cell value v i is determined by p(c j |v i ) ∝ exp(v i c j ). During training, parameters of language model encoders are fixed. Here, we conduct the probing task training on the SPIDER dataset.</p><p>Note that the unavailability of span annotations of cell values in SPIDER dataset leads to further data pre-processing. Since human annotation is costly, we try to annotate the spans by automatic aligning the cell values in SQL to the utterance tokens. For a cell value used in SQL, assuming it has n tokens, we obtain all n-grams from the utterance, and select the best candidate based on the fuzzy matching score 6 (determined by Levenshtein Distance) when the score is higher than a threshold (we use 60 in our experiment). For integers in the SQL, we also leverage a dictionary to map it to English words when searching for their matches. If n-gram candidates are founded, the cell value will be used in the probing experiment. During the train-  ing, the encoder (e.g. BART Encoder or GAP MODEL Encoder) is fixed and only the parameters of probes are tune. The probes are optimized with Adam optimizer with crossentropy loss function. The learning rate is 1e − 5 and the model is trained for 100 epochs on SPIDER dataset with batch size of 96. The evaluation metric is instance-level accuracy, i.e., the prediction is correct if every cell value used in the utterance is matched with the correct column.</p><p>The results are shown in <ref type="table" target="#tab_10">Table 8</ref>. We report the accuracy of the BART Encoder model as our probing baseline, which achieves accuracy of 23.17%. With GAP MODEL (MLM) Encoder, the accuracy raises to 32.72%, indicating that the model learns to align the cell values and column names implicitly. By providing stronger supervisions, the MLM+CRec based model and MLM+CPred based models obtain higher accuracy (36.78% and 44.51%), showing that the models capture more alignment signals, contributing to better semantic parser performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Semantic Parsing: The semantic parsing task is framed as mapping the natural language utterances to meaning representations. The meaning representations can be executed in a variety of environments such as data analysis by translating the natural language queries into database queries. Based on different meaning representations, the semantic parsing task can be classified into three regimes <ref type="bibr" target="#b16">(Kamath and Das 2018)</ref>: logic based formalism such as λ-DCS <ref type="bibr" target="#b19">(Liang 2013)</ref>, graph based formalism such as AMR <ref type="bibr" target="#b1">(Banarescu et al. 2013)</ref> and UCCA <ref type="bibr" target="#b0">(Abend and Rappoport 2013)</ref>, and programming languages such as Python and SQL. Recently, more interests are concentrated on the SQL-based semantic parsing, and most of the work try to solve the problem with general encoder-decoder architecture. Overall, they enhance the models based on following aspects: (1) Improving the decoding mechanism <ref type="bibr" target="#b37">(Yin and Neubig 2017;</ref><ref type="bibr" target="#b7">Dong and Lapata 2018;</ref><ref type="bibr" target="#b31">Rubin and Berant 2020)</ref>; (2) Improving the decoding target <ref type="bibr" target="#b12">(Guo et al. 2019);</ref><ref type="bibr"></ref> (3) Improving the model encoding ability <ref type="bibr" target="#b2">Bogin, Gardner, and Berant 2019a;</ref><ref type="bibr" target="#b39">Yin et al. 2020;</ref><ref type="bibr" target="#b32">Scholak et al. 2020;</ref><ref type="bibr" target="#b23">Ma et al. 2020;</ref><ref type="bibr" target="#b5">Deng et al. 2020;</ref><ref type="bibr" target="#b41">Yu et al. 2020b</ref>); (4) Reranking over the generated candidates to improve parses quality <ref type="bibr" target="#b17">(Kelkar et al. 2020;</ref><ref type="bibr" target="#b38">Yin and Neubig 2019)</ref>. GAP advances the line of (3) by leveraging generation models and three novel learning objectives to enhance the utterance-schema representations. Question Generation and <ref type="table">Table-</ref>to-Text Generation: The question generation task is to generate grammatically and semantically correct questions. The generated questions are usually used for enhancing the question answering mod-els <ref type="bibr" target="#b9">(Duan et al. 2017;</ref><ref type="bibr" target="#b11">Guo et al. 2018;</ref><ref type="bibr" target="#b40">Yu et al. 2020a;</ref><ref type="bibr" target="#b45">Zhong et al. 2020)</ref>. The table-to-text generation task is to generate declarative sentences that describe the information provided by the table <ref type="bibr" target="#b21">(Liu et al. 2017;</ref><ref type="bibr" target="#b10">Gong et al. 2019;</ref><ref type="bibr" target="#b24">Parikh et al. 2020;</ref><ref type="bibr" target="#b28">Radev et al. 2020)</ref>. Our <ref type="table">Table-</ref>to-Text model is a combination of these two directions, focusing on generating questions from table, i.e., composing questions based on the sampled columns and cell values, without providing the detailed information about "what to ask". Pre-training Models: Recent pre-training techniques exploit external knowledge (e.g. entity-level information, commonsense knowledge, knowledge graph) into large-scale pretrained language models <ref type="bibr" target="#b25">Peters et al. 2019;</ref><ref type="bibr" target="#b30">Rosset et al. 2020</ref>). More recently, <ref type="bibr" target="#b39">Yin et al. (2020)</ref>, <ref type="bibr" target="#b13">Herzig et al. (2020)</ref>, leverage the semistructured table data to enhance the representation ability of language models. Concurrently, <ref type="bibr" target="#b41">Yu et al. (2020b)</ref> and <ref type="bibr" target="#b5">Deng et al. (2020)</ref> leveraged synchronous context-free grammar to generate synthetic data and utilized existing high-quality data-to-text dataset for pre-training, respectively. Different from these work, we explore the direction of utilizing the generators to enhance the joint utterances and structured schema encoding ability of the pre-trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we spot three pain points in the Text-to-SQL semantic parsing task, and propose a generation-augmented pre-training framework to alleviate them, with four different learning objectives. Experimental results on SPIDER dataset and CRITERIA-TO-SQL dataset show the effectiveness of this framework, which achieves state-of-the-art performance on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training Data</head><p>Utterance- <ref type="table">Table Pairs:</ref> We extract the tables from English Wikipedia. We further apply the following heuristic strategies to pre-process the extracted tables: (1) Removing tables with less than 4 columns; (2) Removing tables with less than 3 rows; (3) Removing columns whose names have more than 10 tokens; (4) Removing columns whose cell values have more than 50% empty string; (5) Filtering cell values with more than 5 tokens or contains any pre-defined non-ASCII characters. After the pre-processing, we obtain 500K tables.</p><p>For each table, we then randomly sample the column names, cell values and control codes as the <ref type="table">Table-</ref>to-Text generation model input to produce the utterances. We apply the following strategies to sample inputs: (1) We randomly generate a integer from 2 to 6, denoting the number of columns we will sample; (2) We sample the wildcard * with probability of 0.2; (3) We sample one of the structure control codes with probability of 0.35; (4) We sample the order-based control code with probability of 0.25; (5) For the first two sampled columns, we randomly sample one of the aggregators with probability of 0.5; (6) For each column without any associated aggregator-based control code, we sample one value from that column with probability of 0.4. We then generate 4 instances per table and we finally obtain 2M training instances. Utterance-Schema-SQL Triples: We crawl the SQL from GitHub repositories if the SQL can be parsed by one of the SQL parsers: moz-sql-parser 7 and sqlparse 8 . We apply the trained SQL-to-Text generation model to the SQL and obtain 30K utterance-SQL pairs. To obtain the schema, for each SQL, we extract the table names and column names from the SQL as positive candidates and randomly sample table names and column names from other SQL as negative candidates. The combination of these two components are regarded as the associated schema. We then obtain utteranceschema-SQL triples for GenSQL learning objective training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Details</head><p>As discussed in the previous section, each epoch contains 2M utterance-table pairs and 30K utterance-schema-SQL triples. We train the GAP MODEL with multi-task training strategies: 30K utterance-schema-SQL triples are for Gen-SQL learning objective and 2M utterance-table pairs are evenly split for the other three learning objectives, including MLM, CPred and CRec. We train the model for 6 epochs with batch size of 64 on 4 Tesla V100 GPUs. The model is optimized with Adam optimizer (Kingma and Ba 2014) with a learning rate of 1e − 5 and linearly decayed learning rate schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis for SPIDER Dataset</head><p>We further select examples from the SPIDER development set, presented in <ref type="table" target="#tab_11">Table 9</ref>, to show the improved 7 https://github.com/mozilla/moz-sql-parser 8 https://github.com/andialbrecht/sqlparse prediction of our model. The baseline system refers to RAT-SQL+BART Encoder model and our model refers to the RAT-SQL+GAP MODEL Encoder. Overall, our model achieve better column selection performance, either explicit matching between the schema and utterance (e.g. in Example 1, how much does it weigh should match weight instead of pet age), or implicit matching (e.g. in Example 4, arriving in ASY Airport should match DestAirport instead of Airline). Furthermore, our model can handle complex question better (e.g. in Example 5, our model can generate HAVING avg(LifeExpectancy) &lt; 72 condition).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 1</head><p>Utterance: What type of pet is the youngest animal, and how much does it weigh? Baseline: SELECT PetType, pet age FROM Pets ORDER BY pet age ASC LIMIT 1 Our Model: SELECT PetType , weight FROM pets ORDER BY pet age ASC LIMIT 1 Example 2 Utterance: What is the average and maximum age for each pet type? Baseline: SELECT T2.PetType, Avg(T3.Age), Max(T2.pet age) FROM Has Pet AS T1 JOIN Pets AS T2 ON T1.pet id = T2.pet id JOIN Student AS T3 ON T1.student id = T3.student id GROUP BY T2.PetType Our Model: SELECT avg(pet age) , max(pet age) , pettype FROM pets GROUP BY pettype  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>What are the names of the singers who performed in a concert in 2014? Baseline: SELECT T2.Name FROM singer in concert AS T1 JOIN singer AS T2 ON T1.singer id = T2.singer id JOIN concert AS T3 ON T1.convert id = T3.concert id WHERE T1.concert ID = 2014 Our Model: SELECT T2.name FROM singer in concert AS T1 JOIN singer AS T2 ON T1.singer id = T2.singer id JOIN concert AS T3 ON T1.concert id = T3.concert id WHERE T3.year = 2014 Example 4 Utterance: Count the number of United Airlines flights arriving in ASY Airport. Baseline: SELECT Count( * ) FROM airlines AS T1 JOIN flights AS T2 ON T2.Airline = T1.uid WHERE T1.Airline = 'United Airlines' AND T2.Airline = 'ASY' Our Model: SELECT count( * ) FROM AIRLINES AS T1 JOIN FLIGHTS AS T2 ON T2.Airline = T1.uid WHERE T1.Airline = 'United Airlines' AND T2.DestAirport = 'ASY' Example 5 Utterance: What are the different continents and the total popuation and average life expectancy corresponding to each, for continents that have an average life expectancy less than 72? Baseline: SELECT Count( * ), Avg(LifeExpectancy), Avg(LifeExpectancy) FROM country WHERE LifeExpectancy &lt; 72 GROUP BY country.Continent Our Model: SELECT sum(Population) , avg(LifeExpectancy) , Continent FROM country GROUP BY Continent HAVING avg(LifeExpectancy) &lt; 72 Example 6 Utterance: Give the ids of documents that have between one and two paragraphs. Baseline: SELECT T2.Document ID FROM Paragraphs AS T1 JOIN Documents AS T2 ON T1.Document ID = T2.Document ID GROUP BY T1.Document ID HAVING Count( * ) &lt; 2 Our Model: SELECT Document ID FROM Paragraphs GROUP BY Document ID HAVING count( * ) BETWEEN 1 AND 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table-to-Text generation model that can directly transform a set of column names and cell values into user queries without query intent constrains. Specifically, we sample column names and 3 https://github.com cell values (both are referred as candidates) from tables. For example, based on the table in the right part of Figure 1, we can sample columns Year, Film and Result, and a cell value Nominated.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="5">: Exact set match accuracy on the public develop-</cell></row><row><cell cols="5">ment set and hidden test set of SPIDER.  † denotes that the</cell></row><row><cell cols="5">algorithms are concurrent work and leaderboard results are</cell></row><row><cell cols="3">public after our paper submission.</cell><cell></cell></row><row><cell>RAT-SQL</cell><cell>Easy</cell><cell cols="2">Medium Hard</cell><cell>Extra All</cell></row><row><cell>+BERT</cell><cell>0.830</cell><cell>0.713</cell><cell cols="2">0.583 0.384 0.656</cell></row><row><cell cols="2">+BART Encoder 0.826</cell><cell>0.711</cell><cell cols="2">0.581 0.370 0.651</cell></row><row><cell>+GAP MODEL</cell><cell>0.872</cell><cell>0.751</cell><cell cols="2">0.637 0.412 0.697</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Breakdown results on SPIDER hidden test set.</figDesc><table><row><cell>BERT large model on the hidden test set with less en-</cell></row><row><cell>coder layer (BART encoder has 12-layer transformers while</cell></row><row><cell>BERT large model has 24-layer transformers). With our</cell></row><row><cell>GAP MODEL, the RAT-SQL can be further augmented,</cell></row><row><cell>benefiting from enhanced contextual encoding ability. The</cell></row><row><cell>model achieves accuracy of 0.718 on the development set</cell></row><row><cell>and 0.697 on the hidden test set. This confirms the effective-</cell></row><row><cell>ness of the Generation-augmented pre-training. This perfor-</cell></row><row><cell>mance achieves the state-of-the-art performance on the hid-</cell></row><row><cell>den test set with less model parameters on SPIDER dataset</cell></row><row><cell>at the time of writing. Comparing scores of the develop-</cell></row><row><cell>ment set and the test set, we observe BART based mod-</cell></row><row><cell>els (+BARR Encoder or GAP MODEL) have better gener-</cell></row><row><cell>alization ability on the hidden test, considering that the gap</cell></row><row><cell>between the development set and test set is smaller than the</cell></row><row><cell>model such as RAT-SQL V3 + BERT. Concurrently, Yu et al.</cell></row><row><cell>(2020b) used synchronized context-free grammar to gener-</cell></row><row><cell>ate synthetic data for pre-training; Deng et al. (2020) lever-</cell></row><row><cell>aged existing large-scale data-to-text dataset for enhancing</cell></row><row><cell>the structured data representations. Both of them achieve</cell></row><row><cell>comparable performance as ours, but require more model</cell></row><row><cell>parameters (24-layer transformers in the pre-trained model)</cell></row><row><cell>and extra crowd and expert annotation efforts.</cell></row><row><cell>Based on the complexity of the SQL, the examples in</cell></row><row><cell>SPIDER are classified into four types: Easy, Medium, Hard,</cell></row><row><cell>Extra Hard. Here, we provide a breakdown analysis on the</cell></row><row><cell>SPIDER test set, as shown in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Error counts of different types for RAT-SQL+BART Encoder and RAT-SQL+GAP MODEL Encoder.</figDesc><table><row><cell>Model</cell><cell cols="2">SQL Acc. Exec. Acc.</cell></row><row><cell>SQLNet (Xu et al. 2017)</cell><cell>0.132</cell><cell>0.139</cell></row><row><cell>YXJ (Yu et al. 2020c)</cell><cell>0.142</cell><cell>0.158</cell></row><row><cell>YXJ + Roberta (ours)</cell><cell>0.294</cell><cell>0.538</cell></row><row><cell>YXJ + BART Encoder (ours)</cell><cell>0.307</cell><cell>0.558</cell></row><row><cell>YXJ + GAP MODEL (ours)</cell><cell>0.327</cell><cell>0.594</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Test results of Criteria-to-SQL. The SQL accuracy and the execution accuracy are reported.</figDesc><table /><note>SQL+BERT model and RAT-SQL+BART Encoder model, we can find that the performance of RAT-SQL+BART is comparable with the state of the art, but with fewer model parameters (12-layer transformers in BART encoder v.s. 24- layer transformers in BERT-large encoder). We also find that the RAT-SQL+GAP MODEL Encoder can have significant improvement over its baseline RAT-SQL+BART Encoder on each hardness level.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Results of Value-Column Matching Probing Task.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Selected Examples.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/awslabs/gap-text2sql</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/seatgeek/fuzzywuzzy</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Universal conceptual cognitive annotation (ucca)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Abend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="228" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th linguistic annotation workshop and interoperability with discourse</title>
		<meeting>the 7th linguistic annotation workshop and interoperability with discourse</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Global Reasoning over Database Structures for Text-to-SQL Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.11214</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Representing schema structure with graph neural networks for text-to-sql parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06241</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">RYANSQL: Recursively Applying Sketch-based Slot Fillings for Complex Text-to-SQL in Cross-Domain Databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Shin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ArXiv abs/2004.03125</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Awadallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12773</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Structure-Grounded Pretraining for Text-to-SQL. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Coarse-to-fine decoding for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04793</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13063" to="13075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Question generation for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="866" to="874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Tableto-text generation with effective hierarchical encoder on three dimensions (row, column and time)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02304</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06304</idno>
		<title level="m">Question generation from sql queries improves neural semantic parsing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Towards complex text-to-sql in crossdomain database with intermediate representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08205</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Eisenschlos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02349</idno>
		<title level="m">TAPAS: Weakly Supervised Table Parsing via Pre-training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Designing and interpreting probes with control tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03368</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A structural probe for finding syntax in word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4129" to="4138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00978</idno>
		<title level="m">A survey on semantic parsing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Denoising sequence-to-sequence pre-training for natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kelkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Relan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vaichal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Relan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00557</idno>
		<idno>arXiv:1910.13461</idno>
	</analytic>
	<monogr>
		<title level="m">Bertrand-DR: Improving Text-to-SQL using a Discriminative Re-ranker</title>
		<meeting><address><addrLine>Bart</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Adam: A method for stochastic optimization. translation, and comprehension</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Constructing a generic natural language interface for an XML database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jagadish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4408</idno>
		<title level="m">Lambda dependency-based compositional semantics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.08855</idno>
		<title level="m">Linguistic knowledge and transferability of contextual representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09724</idno>
		<title level="m">Table-to-text generation by structure-aware seq2seq learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mention Extraction and Linking for SQL Query Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6936" to="6942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14373</idno>
		<title level="m">ToTTo: A Controlled Table-To-Text Generation Dataset</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1909.04164</idno>
		<title level="m">Knowledge enhanced contextual word representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Modern natural language interfaces to databases: Composing statistical parsing with semantic tractability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Armanasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards a theory of natural language interfaces to databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th international conference on Intelligent user interfaces</title>
		<meeting>the 8th international conference on Intelligent user interfaces</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sivaprasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">F</forename><surname>Rajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krishna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02871</idno>
		<title level="m">DART: Open-Domain Structured Data Record to Text Generation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00655</idno>
		<title level="m">Knowledge-Aware Language Model Pretraining</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12412</idno>
		<title level="m">SmBoP: Semiautoregressive Bottom-up Semantic Parsing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scholak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11119</idno>
		<title level="m">DuoRAT: Towards Simpler Text-to-SQL Models</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04942</idno>
		<title level="m">RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.01808</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>et al. 2020. K-adapter: Infusing knowledge into pre-trained models with adapters</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An efficient easily adaptable system for interpreting natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Warren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="110" to="122" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.09637</idno>
		<title level="m">Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A syntactic neural model for general-purpose code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01696</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reranking for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4553" to="4559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08314</idno>
		<title level="m">TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generating Multi-hop Reasoning Questions to Improve Machine Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
		<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="281" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">V</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">2020b</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13845</idno>
		<title level="m">Grammar-Augmented Pre-Training for Table Semantic Parsing</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Spider: A large-scale human-labeled dataset for complex and crossdomain semantic parsing and text-to-sql task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08887</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dataset and Enhanced Model for Eligibility Criteria-to-SQL Semantic Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th Language Resources and Evaluation Conference</title>
		<meeting>The 12th Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5829" to="5837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Editing-Based SQL Query Generation for Cross-Domain Context-Dependent Questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">V</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Grounded adaptation for zero-shot executable semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07396</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
