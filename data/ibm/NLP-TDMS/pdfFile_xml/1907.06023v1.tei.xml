<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structure-Aware Residual Pyramid Network for Monocular Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotian</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering Laboratory for Brain-inspired Intelligence Technology and Application</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejin</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering Laboratory for Brain-inspired Intelligence Technology and Application</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering Laboratory for Brain-inspired Intelligence Technology and Application</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Structure-Aware Residual Pyramid Network for Monocular Depth Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Monocular depth estimation is an essential task for scene understanding. The underlying structure of objects and stuff in a complex scene is critical to recovering accurate and visually-pleasing depth maps. Global structure conveys scene layouts, while local structure reflects shape details. Recently developed approaches based on convolutional neural networks (CNNs) significantly improve the performance of depth estimation. However, few of them take into account multi-scale structures in complex scenes. In this paper, we propose a Structure-Aware Residual Pyramid Network (SARPN) to exploit multi-scale structures for accurate depth prediction. We propose a Residual Pyramid Decoder (RPD) which expresses global scene structure in upper levels to represent layouts, and local structure in lower levels to present shape details. At each level, we propose Residual Refinement Modules (RRM) that predict residual maps to progressively add finer structures on the coarser structure predicted at the upper level. In order to fully exploit multi-scale image features, an Adaptive Dense Feature Fusion (ADFF) module, which adaptively fuses effective features from all scales for inferring structures of each scale, is introduced. Experiment results on the challenging NYU-Depth v2 dataset demonstrate that our proposed approach achieves state-of-the-art performance in both qualitative and quantitative evaluation. The code is available at https:// github.com/ Xt-Chen/ SARPN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Monocular depth estimation, which aims to predict the depth value of each pixel from a given RGB image, is crucial for understanding scene geometry, and can be applied to facilitate other vision tasks, such as semantic segmentation <ref type="bibr" target="#b2">[Park et al., 2017]</ref> and hand tracking <ref type="bibr" target="#b2">[Qian et al., 2014]</ref>. It is an ill-posed problem because of the inherent ambiguity due to perspective projection. Recently, CNN-based approaches have achieved significant success in monocular depth estimation [Laina et * Corresponding author   <ref type="bibr" target="#b2">., 2016;</ref><ref type="bibr" target="#b1">Fu et al., 2018;</ref><ref type="bibr" target="#b3">Xu et al., 2018b;</ref><ref type="bibr" target="#b2">Hu et al., 2019]</ref>. To resolve the ambiguity, they typically employ an encoder-decoder architecture to implicitly fuse features that represents object appearance, geometry, semantics, spatial relations, etc. The encoder gradually extracts multiscale features, and the decoder employs multi-stage upsampling as well as shortcut connections to restore object details in high-resolution predictions.</p><p>Though a great improvement on average pixel-wise metrics has been made, the underlying structure of objects and stuff is not well preserved by current CNN-based methods. The problem becomes especially challenging when the size of objects and stuff varies widely in complex scenes. As <ref type="figure" target="#fig_0">Figure 1</ref> shows, it is challenging for existing approaches to accurately recover the large-scale geometry (walls) and local details (boundaries and small parts) at the same time. This inaccurate inference at regions of diverse scales motivates us to fully exploit the hierarchical scene structure in depth prediction. Scene structure, depicting the organization and arrangement of multiple interrelated elements in a complex scene, varies widely according to the element type. The global structure represents the spatial arrangement of large-size elements such as walls, floors, and furniture objects. Local structure describes geometric details of objects and their parts. The natural hierarchy of scene structure provides essential con-straints between the depth values of pixels in multiple scales. Although previous CNN-based techniques extract multi-scale image features and gradually fuse them to predict a depth map, the underlying hierarchical structure of the scene has not been taken into account.</p><p>In this paper, we introduce a Structure-Aware Residual Pyramid Network (SARPN) to fully exploit scene structures in multiple scales for depth prediction. A Residual Pyramid Decoder (RPD) is proposed to predict multi-scale depth maps in a coarse-to-fine manner. Depth maps in upper levels in the pyramid represent the global scene structure, while depth maps in lower levels capture more local structures of objects or parts. To convey the global structure and constrain the generation of finer details, we proposed a residual refinement module to predict residual depth maps, which progressively add details on the scene structure on a larger scale. In order to fuse multi-scale features extracted from the input image for residual prediction, we propose an Adaptive Dense Feature Fusion (ADFF) module to adaptively select more effective features for each scale. Integrating the residual pyramid decoder and adaptive dense feature fusion module, our method simultaneously preserves the hierarchical scene structures and produces accurate depth estimation for both large-size shapes and fine details of small object parts, as Figure 1 shows. Our contributions are summarized as follows:</p><p>• We propose a Structure-Aware Residual Pyramid Network (SARPN), which takes the underlying scene structure in multiple scales into account for accurate depth prediction.</p><p>• Our Adaptive Dense Feature Fusion (ADFF) module adaptively selects features from all scales to predict residual depths at different structure scales.</p><p>• The proposed method achieves state-of-the-art performance on the challenging NYUD v2 dataset. More importantly, the visual quality of recovered depth maps is significantly improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In recent years, CNNs have become the most successful techniques for various visual tasks, and were firstly used for monocular depth estimation <ref type="bibr" target="#b1">[Eigen et al., 2014]</ref> in a multiple scale scheme. Later on, fully convolutional network (FCN) was proposed for semantic segmentation <ref type="bibr" target="#b2">[Long et al., 2015]</ref> and has been widely used in many dense prediction tasks, including depth estimation. When FCN-based architecture was first adopted for depth estimation, the resolution and accuracy ware largely improved by using ResNet to extract features and up-projection blocks <ref type="bibr" target="#b2">[Laina et al., 2016]</ref>. In order to improve the quality of depth estimation for local details, many strategies have been introduced. Applying conditional random field as postprocessing <ref type="bibr" target="#b2">[Li et al., 2015]</ref> or integrating it in CNNs <ref type="bibr" target="#b3">[Xu et al., 2017]</ref> largely improves the prediction quality for small objects. Later, an attention model is integrated to improve the estimation performance <ref type="bibr" target="#b3">[Xu et al., 2018b]</ref>. Multi-scale architecture becomes a common solution to avoid the loss of local details caused by spatial pooling and convolutions <ref type="bibr" target="#b1">[Fu et al., 2018]</ref>. Instead of multi-scale network structure, dilated convolution is used to extract multi-scale features for depth estimation . <ref type="bibr" target="#b2">Hu et al. [2019]</ref> proposed an effective multi-scale feature fusion module to produce clear object boundaries. Although these methods have achieved remarkable results by fusing multi-scale features, they still face the problem of inaccurate prediction for complex scenes of which the structure varies largely in scales, from large room layout to fine object details.</p><p>In order to better restore structure details, a few methods design new loss functions to explicitly constrain scene geometry. Zheng et al. <ref type="bibr">[2018]</ref> proposed an order-sensitive softmax loss to constrain global layouts. Similarly, Fu et al. <ref type="bibr">[2018]</ref> used an ordinary regression loss. With respect to clear boundaries and details, a loss function is designed by combining depth, surface normal and gradient in a local neighborhood of depth maps <ref type="bibr" target="#b2">[Hu et al., 2019]</ref>.</p><p>Due to the strong correlation between many visual tasks, such as depth estimation, semantic segmentation, and normal estimation, many approaches employ a joint task learning framework. A multi-scale CNN was designed to simultaneously perform semantic segmentation, depth estimation, and normal estimation <ref type="bibr">[Eigen and Fergus, 2015]</ref>. A set of intermediate auxiliary tasks are utilized to guide the final depth estimation and semantic segmentation <ref type="bibr" target="#b3">[Xu et al., 2018a]</ref>. Zhang et al. proposed a novel joint task-recursive learning method to recursively refine the results of depth estimation and semantic segmentation . A synergy network is proposed to automatically learn information sharing strategy between depth estimation and semantic segmentation <ref type="bibr" target="#b2">[Jiao et al., 2018]</ref>. Moreover, based on the observed long-tail distribution of depth values, an attention-driven loss is also designed to improve the accuracy <ref type="bibr" target="#b2">[Jiao et al., 2018]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Our network consists of three main parts: an encoder for multi-scale feature extraction, an adaptive dense feature fusion module, and a residual pyramid decoder, as <ref type="figure" target="#fig_2">Figure 2</ref> shows. We first introduce the network architecture in Sec. 3.1. The residual pyramid decoder and adaptive dense feature fusion module are explained in Sec. 3.2 and 3.3, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Structure-Aware Residual Pyramid Network</head><p>Our approach begins with an encoder which extracts multi-</p><formula xml:id="formula_0">scale features {F i ex } L i=1 from the input image, where F i ex</formula><p>indicates the feature maps extracted at the i-th level. L is the number of layers in our network. Following the stateof-the-art approach <ref type="bibr" target="#b2">[Hu et al., 2019]</ref>, we use SENet <ref type="bibr" target="#b2">[Hu et al., 2018]</ref> as the backbone of our encoder. It extracts more effective features by re-weighting features of different channels. Given an input image with size W × H, the size of these feature maps are respectively [ W 2 i , H 2 i ], and they carry both high-level semantic information and low-level detail information. Then, these multi-scale feature maps are simultaneously fed to our dense feature fusion module to produce a Fused Feature Pyramid (FFP). These feature maps in FFP are represented by</p><formula xml:id="formula_1">{F i f s } L i=1 ,</formula><p>where F i f s indicates the fused feature maps at the i-th level of the pyramid of fused features. In the decoder part, different from the previous methods that directly predict a depth map by sequentially upsampling feature maps <ref type="bibr" target="#b2">[Laina et al., 2016;</ref><ref type="bibr" target="#b2">Hu et al., 2019]</ref>, our residual pyramid progressively predicts multiple depth maps in a coarse-to-fine manner. The depth map at the top level with size W 32 × H 32 is predicted first as the initial scene layout. We utilize a 1 × 1 convolution operation to reduce the channel number of the feature maps F L ex to the same as the channel number of feature maps F L f s of fused feature pyramid and concatenate them together. A residual block is used to predict a depth map D L in size of [ W 2 L , H 2 L ] from the concatenated feature maps. Then we gradually refine the depth prediction by our proposed residual pyramid decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Residual Pyramid Decoder</head><p>Our residual pyramid decoder predicts depth maps of multiple scales in order to restore the hierarchical scene structures in a coarse-to-fine manner. As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, the depth maps in lower resolutions depicts more global scene layout, while the depth maps in higher resolutions contain more structure details. In each level of the pyramid decoder, we predict a residual map instead of a dense depth map from fused image features in FFP. The residual map and the depth map predicted at the upper level are integrated together to produce a refined depth map in the current scale using our Residual Refinement Module (RRM). The components of each RRM are shown in <ref type="figure" target="#fig_3">Figure 3</ref>. The depth map D i+1 predicted at the upper scale is upsampled to the current scale by bilinear interpolation. A residual depth map D i res is generated by utilizing the fused features F i f s . After adding the residual map and the upsampled depth map, a residual block, which contains three convolutional layers, is employed to re- fine the prediction and outputs a depth map D i at the i-th scale. This residual architecture induces our network to effectively represent the structure details at each scale and hierarchically refine scene structures. Meanwhile, the global scene layout is well preserved by our residual pyramid decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adaptive Dense Feature Fusion</head><p>In general, due to pooling operations and convolution operations with strides in CNNs, a large amount of low-level visual features are lost. As a result, it is difficult for the decoder to recover the lost low-level structure details. However, both low-level features and high-level features are critical for predicting residual maps in all layers, because the residual maps convey additional details on a global scene structure, as the residual pyramid illustrates in <ref type="figure" target="#fig_2">Figure 2</ref>. In order to provide sufficient information for the prediction of a residual map in each level, we propose an Adaptive Dense Feature Fusion (ADFF) module. This dense fusion module consists of L Multi-scale Feature Fusion (MFF) modules to predict L fused feature maps, which compose a fused feature pyramid for residual prediction.</p><p>In each layer, the MFF adaptively selects eligible features from all feature scales when predicting the depth map for Method REL RMS log 10 δ &lt; 1.25 δ &lt; 1.25 2 δ &lt; 1.  <ref type="table">Table 1</ref>: Comparisons with state-of-the-art depth estimation approaches on NYUD v2 Dataset. Note that joint task learning is employed in the methods marked by *. The best results on each metric among the single-task approaches are marked in bold type. The results better than ours are marked in italics.</p><p>each individual scale. We follow the detailed implementation of MFF proposed in <ref type="bibr" target="#b2">[Hu et al., 2019]</ref>. The L feature maps {F i ex } i=1,...,L are first resized to the resolution of current scale using bilinear interpolation and refined with a residual refine block. The refined feature maps are concatenated and fed into a conv-layer to reduce the number of channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss Function</head><p>In order to train our residual pyramid network for predicting accurate depth maps while preserving scene structures in various scales, we compute the difference between the predicted depth map D i and the ground-truth G i at each scale and combine the losses of all scales together. For each scale, we follow the definition of the loss function proposed in <ref type="bibr" target="#b2">[Hu et al., 2019]</ref>. It consists of three terms, l depth considering the pixel-wise difference between the predicted depth D l and the ground truth G l , l grad which penalizes errors round edges, and l normal to further improve fine details. Combing all the L scales, our loss function for the entire network is formulated as</p><formula xml:id="formula_2">L = L i=1 l i depth + l i grad + l i normal .<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To demonstrate the effectiveness of the proposed approach, we evaluate our approach on the challenging NYUD v2 dataset <ref type="bibr">[Silberman et al., 2012]</ref>. We compare our approach with a couple of state-of-the-art approaches and show the superiority of the proposed method on both quantitative and qualitative evaluations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>The NYU-Depth v2 dataset [Silberman et al., 2012] contains 464 video sequences of indoor scenes captured with Microsoft Kinect. 654 aligned RGB-Depth pairs are provided for testing depth estimation methods for indoor scenes. All images have a resolution of 640 × 480. To training our network, we use the training dataset which contains 50K RGBD images, select and then augment in the same way as <ref type="bibr" target="#b2">[Hu et al., 2019]</ref>. Each image is downsampled to 320 × 240 using bilinear interpolation, and then center-cropped to 304 × 228. The predicted depth maps are in a resolution of 152 × 114. For testing, the predicted depth maps are upsampled to match the size of the corresponding ground truth using bilinear interpolation.</p><p>We implement the proposed model using PyTorch <ref type="bibr" target="#b2">[Paszke et al., 2017]</ref>. The encoder, SENet, is initialized by a model pretrained on ImageNet <ref type="bibr" target="#b0">[Deng et al., 2009]</ref>. The other lay- ers in our network are randomly initialized. We use a step learning rate decay policy with Adam optimizer, and starting from an initial learning rate of l init = 10 −4 . It is reduced to 10% every 5 epochs. We use β 1 = 0.9, β 2 = 0.999, and weight decay as 10 −4 . The proposed network was trained for 20 epochs with a batch size of 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Evaluation</head><p>Following previous studies, we adopt four metrics including average relative error (REL), root mean squared error (RMS), mean log 10 error (log 10), and accuracy with three thresholds, to quantitatively evaluate our depth estimation performance. <ref type="table">Table 1</ref> shows the results of our SARPN and recent approaches. Among the approaches of single task learning, our approach performs the best on REL, log 10 error, and accuracy with three thresholds. We are in the third position with respect to RMS. We speculate that the methods <ref type="bibr" target="#b1">[Fu et al., 2018;</ref><ref type="bibr" target="#b2">Jiao et al., 2018]</ref> pay more attention to the absolute pixel-wise accuracy when designing their networks and loss functions, ignoring fine structures of target scenes. As a result, these methods achieve higher performance in RMS, but performs worse on the REL metric and other metrics.</p><p>We also compare our method with four approaches that employ joint task learning <ref type="bibr">[Eigen and Fergus, 2015;</ref><ref type="bibr" target="#b3">Xu et al., 2018a;</ref><ref type="bibr" target="#b2">Jiao et al., 2018]</ref>. The results demonstrated that our method outperforms three methods and achieves comparative performance with <ref type="bibr" target="#b2">[Jiao et al., 2018]</ref>, even they use a large number of extra labels for semantic segmentation during the training process. Moreover, the depth maps produced by <ref type="bibr" target="#b2">[Jiao et al., 2018]</ref> present very blurry object boundaries and miss geometric details. We compare the predicted depth maps in <ref type="figure" target="#fig_4">Figure 4</ref> to demonstrate the capa-  <ref type="bibr" target="#b1">Fu et al., 2018]</ref> 0.320 0.583 0.402 <ref type="bibr" target="#b2">[Hu et al., 2019]</ref> 0.644 0.508 0.562 Ours 0.645 0.520 0.570 <ref type="bibr">et al., 2016]</ref> 0.536 0.422 0.463 <ref type="bibr" target="#b3">[Xu et al., 2018a]</ref> 0.600 0.366 0.439 <ref type="bibr" target="#b1">[Fu et al., 2018]</ref> 0.316 0.473 0.412 <ref type="bibr" target="#b2">[Hu et al., 2019]</ref> 0.668 0.505 0.568 Ours 0.663 0.523 0.578 <ref type="bibr">et al., 2016]</ref> 0.670 0.479 0.548 <ref type="bibr" target="#b3">[Xu et al., 2018a]</ref> 0.794 0.407 0.525 <ref type="bibr" target="#b1">[Fu et al., 2018]</ref> 0.483 0.512 0.485 <ref type="bibr" target="#b2">[Hu et al., 2019]</ref> 0.759 0.540 0.623 Ours 0.749 0.554 0.630 bility of our method on restoring clear object boundaries and finer details. We also analyze the contribution of each component in our proposed network. We use a simple UNet-like architecture as our baseline, where SENet <ref type="bibr" target="#b2">[Hu et al., 2018]</ref> is employed as the backbone of our encoder. The decoder in our baseline employs a multi-stage upsampling scheme to recover a depth map. A variant (baseline+RPD) is implemented by adding the proposed RPD on the baseline model. As shown in Table 1, the performance is gradually improved by incorporating RPD and ADFF. More specifically, after adding the proposed RPD, performance among all the metrics are improved by a large margin from the baseline, while REL decreases <ref type="figure">Figure 6</ref>: 3D projection from predicted depth maps. Our method better preserves the scene structure of various scales, especially the flat shape of large planar regions. by 6.5%, RMS decreases by 3.5%, log 10 error decreases by 3.8%. After adding the ADFF module, the performance is further improved, while REL decreases by 3.5%, RMS decreases by 2.7% and log 10 error decreases by 4%.</p><formula xml:id="formula_3">0.5 [Laina</formula><formula xml:id="formula_4">1.0 [Laina</formula><p>In order to prove the effectiveness of our method on preserving object details, we also compute edge accuracy to measure the quality of recovered edge details, same as <ref type="bibr" target="#b2">[Hu et al., 2019]</ref>. Precision, Recall, and F1 score are computed according to edge pixels in the ground truth map. From <ref type="table" target="#tab_2">Table 2</ref>, we can see that our F1 score surpasses all other methods under three different thresholds. This indicates that our method restores the most structure details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Evaluation</head><p>We compare a series of depth maps predicted by our method and other state-of-the-art methods <ref type="bibr" target="#b2">[Laina et al., 2016;</ref><ref type="bibr" target="#b3">Xu et al., 2017;</ref><ref type="bibr" target="#b1">Fu et al., 2018;</ref><ref type="bibr" target="#b2">Hu et al., 2019]</ref> in <ref type="figure" target="#fig_5">Figure 5</ref>. It can be seen that the depth maps predicted by our method are visually better than other methods. Scene structures are well preserved in different scales, especially for large planar regions and object details. For example, our method predicts accurate geometric details for the bookshelf in the first row, the chair in the third row, and the sofa in the fifth row. For large planar regions (the upper-left wall in the second row, and the floor of the third), our method also generates better results.</p><p>To better illustrate the capability of our method on preserving scene structure of large planar regions, we project the predicted depth maps as 3D point clouds and render them in novel views. As <ref type="figure">Figure 6</ref> shows, our reprojected results are the closest to ground truth. In particular, the large wall regions recovered by our method are much more flat, while other methods suffer from severe distortions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Generalization</head><p>In addition to the NYUD v2 dataset, we further explore the generalization ability of our proposed network on other datasets. We test our network, which is trained on the NYUD v2 dataset only, on ScanNet dataset  and <ref type="bibr">SUN-RGBD dataset [Song et al., 2015]</ref>, which contain more diverse RGBD data. As shown in <ref type="figure" target="#fig_6">Figure 7</ref>, even the data distribution of these two datasets and NYU Depth v2 is greatly different, our method could recover structures in various scales, including smooth large planar regions and object details. Moreover, our method also fills holes in the ground truth depth map automatically while maintains the scene structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a Structure-Aware Residual Pyramid Network for accurate monocular depth estimation. A residual pyramid decoder is introduced to predict multi-scale depth maps, which takes the underlying hierarchical scene structures into account. The residual pyramid induces our network to progressively add finer structures at a specific scale while preserving the coarser layout predicted at the upper level. Meanwhile, by using the proposed adaptive dense feature fusion module, the image features from all scales are adaptively fused when predicting the residual depth map for each scale. Experiment results demonstrate that our method achieves state-of-the-art performance in both quantitative and qualitative evaluation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Problems in depth prediction: (a)(b) inaccurate depth values on large planar regions, such as walls. (c)(d) blurry boundaries and missing details (chair legs). Our approach simultaneously recovers large planar structures and object details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>al</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The network architecture. Our Structure-Aware Residual Pyramid Network consists of an encoder which extracts multi-scale visual features, a Residual Pyramid Decoder (RPD) which progressively infers depth maps in a coarse-to-fine manner, and an Adaptive Dense Feature Fusion (ADFF) module for dense feature fusion. The residual pyramid effectively adds structure details in each level based on the scene layout predicted at a coarser level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>A Residual Refinement Module (RRM) for the i-th level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Comparison with [Jiao et al., 2018]. The depth maps predicted by our method preserve much more accurate depth around object boundaries and keep finer structures, as highlighted in the boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results on the NYUD2 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>More results by applying our model on SUN-RGBD dataset (a)(b) and ScanNet dataset (c)(d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Accuracy of recovered edge pixels in depth maps under different thresholds.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<editor>Fergus, 2015] David Eigen and Rob Fergus</editor>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Detail preserving depth estimation from a single image using attention guided networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eigen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="304" to="313" />
		</imprint>
	</monogr>
	<note>3DV</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<editor>Xiaojuan Qi, Renjie Liao, Zhengzhe Liu, Raquel Urtasun, and Jiaya Jia</editor>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>Shuran Song</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
	<note>IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pad-net: Multi-tasks guided predictionand-distillation network for simultaneous depth estimation and scene parsing</title>
	</analytic>
	<monogr>
		<title level="m">Kecheng Zheng, Zheng-Jun Zha, Yang Cao, Xuejin Chen, and Feng Wu</title>
		<editor>Zhang, Zhen Cui, Chunyan Xu, Zequn Jie, Xiang Li, and Jian Yang</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1381" to="1388" />
		</imprint>
	</monogr>
	<note>ACM Multimedia Conference on Multimedia Conference</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
