<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Local Relation Networks for Image Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
							<email>hanhu@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Local Relation Networks for Image Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The convolution layer has been the dominant feature extractor in computer vision for years. However, the spatial aggregation in convolution is basically a pattern matching process that applies fixed filters which are inefficient at modeling visual elements with varying spatial distributions. This paper presents a new image feature extractor, called the local relation layer, that adaptively determines aggregation weights based on the compositional relationship of local pixel pairs. With this relational approach, it can composite visual elements into higher-level entities in a more efficient manner that benefits semantic inference. A network built with local relation layers, called the Local Relation Network (LR-Net), is found to provide greater modeling capacity than its counterpart built with regular convolution on large-scale recognition tasks such as ImageNet classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Humans have a remarkable ability to "see the infinite world with finite means" <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b1">2]</ref>. From perceiving a limited set of low-level visual primitives, they can productively compose unlimited higher-level visual concepts, from which an understanding of a viewed scene can be formed.</p><p>In computer vision, this compositional behavior may be approximated by the building of hierarchical representations in a convolutional neural network, where different layers represent different levels of visual elements. At lower layers, basic elements such as edges are extracted. These are combined at middle layers to form object parts, and then finally at higher layers, whole objects are represented <ref type="bibr" target="#b30">[31]</ref>.</p><p>Although a series of convolutional layers can construct a hierarchical representation, its mechanism for composing lower-level elements into higher-level entities can be viewed as highly inefficient in regards to conceptual inference. Rather than recognizing how elements can be meaningfully joined together, convolutional layers act as templates, where input features are spatially aggregated according to convolutional filter weights. For an effective compo- sition of features, suitable filters would need to be learned and applied. This requirement is problematic when trying to infer visual concepts that have significant spatial variability, such as from geometric deformation as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, since filter learning could potentially face a combinatorial explosion of different valid compositional possibilities <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b21">22]</ref>. In this paper, we present a new computational network layer, called the local relation layer, in which meaningful compositional structure can be adaptively inferred among visual elements in a local area. In contrast to convolution layers which employ fixed aggregation weights over spatially neighboring input features, our new layer adapts the aggregation weights based on the composability of local pixel pairs. Inspired by recent works on relation modeling <ref type="bibr" target="#b0">[1]</ref>, composability is determined by the similarity of two pixels' feature projections into a learned embedding space. This embedding may additionally account for geometric priors, which have proven to be useful in visual recognition tasks <ref type="bibr" target="#b0">1</ref> . By learning how to adaptively compose pixels in a local area, a more effective and efficient compositional hierarchy can be built.</p><p>Local relation layers can be used as a direct replacement of convolutional layers 2 in deep networks, with little added overhead. Using these layers, we have developed a network architecture called Local Relation Network (LR-Net) that follows the practice in ResNet <ref type="bibr" target="#b9">[10]</ref> of stacking residual blocks to enable optimization of very deep networks. Given the same computation budget, LR-Net with 26 layers and bottleneck residual blocks surpasses the regular 26layer ResNet by an absolute 3% in top-1 accuracy on the ImageNet image classification task <ref type="bibr" target="#b6">[7]</ref>. Improved accuracy is also achieved with basic residual blocks and on deeper networks (50 and 101 layers).</p><p>Besides strong image classification performance, we demonstrate several favorable properties of local relation networks. One of them is their greater effectiveness in utilizing large kernel neighborhoods compared to regular convolution networks. While regular ConvNets mainly employ 3 × 3 kernels due to saturation at larger sizes, LR-Net is found to benefit from kernels of 7 × 7 or even larger. We additionally show that the network is more robust to adversarial attacks, likely due to its compositional power in the spatial domain.</p><p>We note that while deep neural networks all form a bottom-up hierarchy of image features, they generally determine feature aggregation weights in a top-down manner. By contrast, our compositional approach computes the weights bottom-up. There exist a few recent methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27]</ref> that also do so, but they are either not applicable to largescale recognition tasks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b10">11]</ref> or act in only a complementary role to regular convolution, rather than as a replacement <ref type="bibr" target="#b26">[27]</ref>. Moreover, these methods do spatial aggregation over the whole input feature map and do not consider geometric relationships between pixels, while our network demonstrates the importance of locality and geometric priors. With this work, it is shown that a bottom-up approach to determining feature aggregation weights can be both practical and effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Convolution Layers and Extensions The convolution layer has existed for several decades <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>. Its recent popularity started with the impressive performance of AlexNet <ref type="bibr" target="#b16">[17]</ref> in classifying objects on ImageNet <ref type="bibr" target="#b6">[7]</ref>. Since then, the convolution layer has been almost exclusively used in extracting basic visual features.</p><p>Extensions to the regular convolution layer have been proposed. In one direction, a better accuracy-efficiency tradeoff is obtained by limiting the scope of aggregated input channels. Representative works include group convo-lution <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28]</ref> and depthwise convolution <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12]</ref>. Another direction is to modify the spatial scope for aggregation. This has been done to enlarge the receptive field, such as through atrous/dilated convolution <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29]</ref>, and to enhance the ability to model geometric deformation, via active <ref type="bibr" target="#b13">[14]</ref> and deformable convolution <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>Some works relax the requirement of sharing aggregation weights/scopes across positions. A straightforward approach is taken with the locally connected layer <ref type="bibr" target="#b23">[24]</ref>, which learns independent aggregation weights for different positions. Its application is limited due to the loss of important properties from regular convolution, including translation invariance and knowledge transfer from one position to others. In other works along this direction, convolution layers are proposed which generate position-adaptive aggregation weights <ref type="bibr" target="#b14">[15]</ref> or an adaptive aggregation scope <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>We note that regular convolution and the above extensions all operate in a top-down manner, determining their convolution behavior based on image appearance or spatial positions within a receptive field. In contrast, the proposed layer determines aggregation weights in a bottom-up fashion based on composability of local pixel pairs, which we believe provides a more efficient encoding of spatial composition in the visual world. At the same time, the proposed layer follows and adapts several favorable design principles from these convolution variants, such as locality, use of geometric priors, and weight/meta-weight sharing across positions, which have been found to be crucial in effectively extracting visual features.</p><p>Capsule Networks To address some shortcomings of convolution layers, there have been recent works that determine the aggregation weights in a bottom-up manner based on the composability of pixel pairs. A representative work is Capsule Networks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b10">11]</ref>, in which composability is computed by an iterative routing process. In each routing step, the aggregation weights are enlarged if the vectors before and after aggregation are close to each other, and they are reduced otherwise. This self-strengthening process in capsule networks is similar to the process of a filtering bubble, a popular phenomenon in social networks where the connection between agents with the same interests becomes stronger, while the connections become weaker when interests are dissimilar.</p><p>Although the routing method is inspiring, the computation is not well aligned with current learning infrastructure such as back-propagation and multi-layer networks. In contrast, the composability of pixel pairs in the local relation layer is computed by the similarity of pixel pairs in an embedding space with learnt embedding parameters, which is more friendly to current learning infrastructure. The local relation layer is also differentiated from capsule networks by its aggregation computation process, including its spatial scope (local vs. global) and geometric priors (with vs. without). With these differences, local relation networks are significantly more practical than existing methods based on bottom-up aggregation.</p><p>Self-Attention / Graph Networks The proposed local relation layer is also related to self-attention models <ref type="bibr" target="#b24">[25]</ref> used in natural language processing, and to graph networks applied on non-grid data <ref type="bibr" target="#b2">[3]</ref>. These works share a basic structure similar to general relation modeling <ref type="bibr" target="#b0">[1]</ref>, which naturally introduces compositionality in the networks.</p><p>Due mainly to their powerful composition modeling ability, these methods have become the dominant approaches in their respective fields. However, in computer vision, there are few works involving such compositionality in their network architecture <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref>. In <ref type="bibr" target="#b12">[13]</ref>, relationships between object proposals are modeled, which leads to improved accuracy as well as the first fully end-to-end object detector. The relation modeling in that work is applied to non-grid data. In <ref type="bibr" target="#b26">[27]</ref>, relationships are modeled between pixels, as in our work. However, the goal is different. While <ref type="bibr" target="#b26">[27]</ref> extracts long-range context as complementary to the convolution layer, we pursue a basic image feature extractor with more representation power for spatial composition than the convolution layer.</p><p>In this sense, our work bridges the general philosophy of introducing compositionality into representation, which has proven effective in processing sequential and non-grid data, and applicability as a basic feature extractor for computer vision. Such a goal is non-trivial and requires adaptations from both sides.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A General Formulation</head><p>In this section, we describe a general formulation for basic image feature extractors, based on which the proposed local relation layer will be presented. Denote the input and output of a layer by x ∈ R C×H×W and y ∈ R C ×H ×W , with C, C being the channels of input/output features and H, W, H , W the intput/output spatial resolution. Existing basic image extraction layers generally produce the output feature by a weighted aggregation of input features,</p><formula xml:id="formula_0">y(c , p ) = c∈Ω c ,p∈Ω p ω(c , c, p , p) · x(c, p),<label>(1)</label></formula><p>where c, c and p = (h, w), p = (h , w ) index the input and output channels and feature map positions, respectively; Ω c and Ω p denote the scope for channel and spatial aggregation of input features in producing the output feature value at channel c and position p , respectively; ω(c , c, p , p) denotes the aggregation weight from c, p to c , p . Existing basic image feature extraction layers are differentiated mainly by three aspects: parameterization method, aggregation scope, and aggregation weights.</p><p>Parameterization method defines the model weights to be learnt. The most common parameterization method is to directly learn the aggregation weights ω <ref type="bibr" target="#b17">[18]</ref>. There are also some methods that learn a meta network {θ} on input features to generate adaptive aggregation weights <ref type="bibr" target="#b14">[15]</ref> or an adaptive aggregation scope across spatial positions <ref type="bibr" target="#b5">[6]</ref>, or learn a fixed prior about spatial aggregation scope (Ω) <ref type="bibr" target="#b13">[14]</ref>.</p><p>In general, the parameterization is shared across spatial position to enable translation invariance.</p><p>Aggregation scope defines the range of channels and spatial positions involved in aggregation computation. For channel scope, regular convolution includes all input channels in computing each channel output. For greater efficiency, some methods consider only one or a group of input channels in producing one channel of the output feature <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b4">5]</ref>. Recently, there have been methods where multiple or all output channels share the same aggregation weights <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b22">23]</ref>. For spatial scope, most methods constrain the aggregation computation in a local area. Restricting aggregation to a local area can not only significantly reduce computation, but also help introduce an information bottleneck that facilitates learning of visual patterns. Nevertheless, recent non-convolution methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b22">23]</ref> mostly adopt a full spatial scope for aggregation computation.</p><p>Aggregation weights are typically learned as network parameters or are computed from them. Almost all variants of convolution obtain their aggregation weights in a top-down manner, where they are either fixed across positions or determined by a meta network on the input features at the position. There are also some non-convolution methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b22">23]</ref> that compute the aggregation weights in a bottom-up fashion, with the weights determined by the composability of a pixel pair. In contrast to convolution variants whose aggregation weights depend heavily on geometric priors, such priors are seldom used in recent non-convolution methods. <ref type="table">Table 1</ref> presents a summary of existing basic image feature extractors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Local Relation Layer</head><p>In this section, we introduce the local relation layer. Expressed within the general formulation of Eqn. (1), its aggregation weights are defined as 3</p><formula xml:id="formula_1">ω(p , p) = softmax(Φ(f θq (x p ), f θ k (x p )) + f θg (p − p )), (2) where the term Φ(f θq (x p ), f θ k (x p )</formula><p>) is a measure of composability between the target pixel p and a pixel p within its position scope, based on their appearance after transformations f θq and f θ k , following recent works on relation <ref type="table">Table 1</ref>. A summary of basic image feature extractors. The "parameterization" column indicates the model weights to be learnt. The symbols ω, {θ}, Ω denote aggregation weights, weights of meta networks, and spatial sampling points, respectively. "share" indicates whether the parameterized weights are shared across position. The aggregation scope is given over both the channel and spatial domains. The "aggregation weight" column covers three aspects: how aggregation weights are computed from parameterized weights ("computation" sub-column); inclusion of geometric priors ("geo." sub-column); type of computation ("type" sub-column regular ω all/one/no local ω top-down group <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28]</ref> ω group/one/no local ω top-down depthwise <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12]</ref> ω one/one/no local ω top-down dilated <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29]</ref> ω all/one/no atrous ω top-down active <ref type="bibr" target="#b13">[14]</ref> ω, Ω all/one/no Ω ω top-down local connected <ref type="bibr" target="#b23">[24]</ref> ω all/one/no local ω top-down dynamic filters <ref type="bibr" target="#b14">[15]</ref> θ all/one/no local f θ (x p ) top-down deformable <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32]</ref> ω, θ all/one/no</p><formula xml:id="formula_2">Ω(θ) ω top-down non-local [27] θ k , θ q one/one/all full Φ(f θq (x p ), f θ k (x p )) bottom-up capsule [23, 11] θ one/one/group full route(y p , f θ (x p )) bottom-up local relation (our) θ k , θ q , θ g one/one/group local softmax Ω (Φ(f θq (x p ), f θ k (x p )) + f θg (p − p ))</formula><p>bottom-up modeling <ref type="bibr" target="#b0">[1]</ref>. The term f θg (p−p ) defines the composability of a pixel pair (p, p ) based on a geometric prior. The geometric term adopts the relative position as input and is translationally invariant. This new layer belongs to class of bottom-up methods, as indicated in <ref type="table">Table 1</ref>, as it determines composability based on the properties of the two visual elements. In the following, we present its design and discuss its differences from existing bottom-up methods. These differences lead to significant higher accuracy on image recognition benchmarks. Its performance also is comparable to or surpasses state-ofthe-art top-down convolution methods.</p><p>Locality The bottom-up methods typically aggregate input features from over the full image. In contrast, the local relation layer limits the aggregation computation to a local area, e.g., a 7 × 7 neighborhood. We find that constraining the aggregation scope to a local neighborhood is crucial for feature learning in visual recognition (see <ref type="table">Table 3</ref>).</p><p>Compared with the convolution variants which also constrain the aggregation computation to a spatial neighborhood, the local relation layer proves more effective in utilizing larger kernels. While convolution variants usually exhibit performance saturation with neighborhoods larger than 3 × 3, the local relation layer yields steady improvements in accuracy when increasing the neighborhood size from 3 × 3 to 7 × 7 (see <ref type="table">Table 3</ref>). This difference may be due to the representation power of convolution layer being bottlenecked by the number of fixed filters, hence there is no benefit from a larger kernel size. In contrast, the local relation layer composes local pixel pairs in a flexible bottom-up manner that allows it to effectively model visual patterns of increasing size and complexity. We use a 7 × 7 kernel size by default.</p><p>Appearance composability We follow a general approach for relation modeling <ref type="bibr" target="#b0">[1]</ref> to compute appearance composability Φ(f θq (x p ), f θ k (x p )), where x p and x p are projected to a query (by a channel transformation layer f θq ) and key (by a channel transformation layer f θ k ) embedding space, respectively. While in previous works the query and key are vectors, in the local relation layer, we use scalars to represent them so that the computation and representation are lightweight. We find that scalars work also well and have better speed-accuracy trade-off compared to vectors (see <ref type="table">Table 4</ref>).</p><p>We consider the following instantiations of function Φ, which we later show to work similarly well (see <ref type="table">Table 6</ref>): a) squared difference:</p><formula xml:id="formula_3">Φ(q p , k p ) = −(q p − k p ) 2 .<label>(3)</label></formula><p>b) absolute difference:</p><formula xml:id="formula_4">Φ(q p , k p ) = −|q p − k p |,<label>(4)</label></formula><p>c) multiplication:</p><formula xml:id="formula_5">Φ(q p , k p ) = q p · k p ,<label>(5)</label></formula><p>We use Eqn.  The geometric prior is encoded by a small network on the relative position of p to p . The small network consists of two channel transformation layers, with a ReLU activation in between. We find that using a small network to compute the geometric prior values is better than directly learning the values, especially when the neighborhood size is large (see <ref type="table">Table 3</ref>). This is possibly because a small network on relative position treats relative positions as vectors in metric space, while the direct method treats different relative positions as independent identities.</p><p>Note that the inference process with using a small network is the same as that of directly learning the geometric priors. In fact, during inference, the fixed learnt weights θ g will induce fixed geometric prior values f θg (∆p) for a relative position ∆p. We use these fixed geometric prior values instead of the original model weights θ g for more convenient inference.</p><p>Weight normalization We use SoftMax normalization over the spatial scope Ω to compute the final aggregation weights. Such normalization is found to be crucial in balancing the contributions of the appearance composability and geometric prior terms (see <ref type="table">Table 6</ref>).</p><p>Channel sharing Following <ref type="bibr" target="#b22">[23]</ref>, the local relation layer uses channel sharing in aggregation computation, where multiple channels share the same aggregation weights. Channel sharing can generally reduce model size and facilitate GPU memory scheduling for efficient implementation. We observe no accuracy drop with up to 8 channels (default) sharing the same aggregation (see <ref type="table">Table 5</ref>), while achieving more than 3× actual speed-up than that of 1 channel per aggregation in our CUDA kernel implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complexity and implementation</head><p>The local relation layer is summarized in <ref type="figure" target="#fig_2">Figure 2</ref>. Given an H × W input feature map, k × k spatial neighborhood, C channels, and m channels per aggregation computation, the total computational complexity (in FLOPs) of a local relation layer with stride s is</p><formula xml:id="formula_6">C = O ( 1 + s 2 m + 1)C(C + k 2 ) HW s 2 .<label>(6)</label></formula><p>In our experiments, a naive implementation by a CUDA kernel is used, which is several times slower than regular convolution with the same FLOPs 4 . Note that convolution has a highly optimized implementations with careful memory scheduling. Optimization of memory scheduling for the local relation layer will be a focus of our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Local Relation Networks</head><p>Local relation layers can be used to replace spatial convolution layers in deep neural networks. In this section, we describe layer replacement in the ResNet architecture <ref type="bibr" target="#b9">[10]</ref>, where residual blocks with the same topology are stacked. <ref type="figure" target="#fig_1">Figure 3</ref> illustrates the replacement of the 3 × 3 convolution layer in the bottleneck/basic residual blocks and the first 7 × 7 convolution layer in ResNet. For residual blocks, we keep the FLOPs the same by adapting the expansion ratio (α) of the layer to be replaced. For the first 7 × 7 convolution layer, we transform the 3 × H × W input to a feature map of 64 × H × W by a channel transformation layer, and follow this with a 7×7 local relation layer. The replacement of the 7 × 7 convolution layer consumes similar FLOPs and has comparable accuracy on ImageNet recognition. In the experiments, we will mainly ablate the effects of replacing 3 × 3 convolution layers in residual blocks.</p><p>After replacing all convolution layers in ResNet, we obtain a network which we call the Local Relation Network (LR-Net). <ref type="table">Table 2</ref> shows a comparison of ResNet-50 and LR-Net-50 (with default hyper-parameters of 7 × 7 kernel size and m = 8 channels per aggregation). LR-Net-50 uses similar FLOPs but has a slightly smaller model size because of its channel sharing in aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We perform an ablation study on the ImageNet-1K image classification task. To facilitate the study given limited GPU resources, we conduct the study using LR-Net-26, which is a 26 layer local relation network adapted from ResNet-26. The networks have 8 bottleneck residual blocks, with {2, 2, 2, 2} blocks for res2, res3, res4, res5, respectively. We also report results on networks stacked by basic residual blocks (LR-Net-18) and with larger depth of   layers (LR-Net-50, LR-Net-101). The robustness of LR-Nets to adversarial attacks is examined as well.</p><p>Our experimental settings and hyper-parameters mostly follow <ref type="bibr" target="#b27">[28]</ref>. Please see the appendix for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Ablation Study</head><p>Impact of spatial scope <ref type="table">Table 3</ref> presents the impact of varying aggregation spatial scope for the proposed local re-lation networks, as well as the regular ResNet-26 network and its variant, ResNet-DW-26 <ref type="bibr" target="#b19">[20]</ref>, where the regular convolution layer is replaced by depthwise convolution. We have the following observations. a) Importance of locality Existing bottom-up methods typically compute spatial aggregation over the entire input feature map <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b22">23]</ref>. We first compare the proposed local relation networks, which enforces a locality constraint on spatial aggregation scope, to the equivalent method without this constraint (the "full image" column in <ref type="table">Table 3</ref>) <ref type="bibr" target="#b4">5</ref> .</p><p>Without encoding any geometric priors (noted as "NG" in the table), we observe a huge improvement by changing the aggregation computation from using the whole input feature map to just a 7×7 neighborhood (from 50.7 to 71.9). Surprisingly, while the effectiveness of convolution networks is ascribed to the explicit modeling of geometric priors, we obtain competitive accuracy on ImageNet classification purely by applying a locality constraint to a geometric-free aggregation method (71.9 vs. 72.8), demonstrating the effectiveness of the locality constraint.</p><p>For the LR-Net-26 models which encode the geometric prior term described in Section 4, we also observe significant accuracy improvement, from 68.4 to 75.7. Noting that geometric priors can also act as a method to limit the aggregation scope (positions with smaller geometric prior values will contribute little to the final aggregation computation), the locality constraint further constrains the aggregation scope.</p><p>The locality constraint may also provide an information bottleneck to the network, which aids representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b) LR-Net Benefits from large kernel</head><p>The regular ResNet-26 model has similar accuracy with 3×3 and 5×5 kernels and loses accuracy when kernel size is larger than 5×5. For ResNet-DW-26 models, the accuracy is almost unchanged when moving from 3×3 to 9×9.</p><p>In contrast, both LR-Net-26 variants (with/without geometric prior terms) obtain steadily improved accuracy when the kernel size grows from 3×3 to 7×7: 70.8→ 71.5→71.9 for LR-Net-26 (NG) which has no geometric prior term, and 73.6 → 74.9 → 75.7 for LR-Net-26 which includes both the appearance composability and geometric prior terms. The results demonstrate the superiority of the proposed LR-Net in harnessing large kernels.</p><p>Effect of geometric prior In the last three rows of Table 3, encoding of geometric priors is ablated. Both geometric prior embedding methods perform better than that <ref type="table">Table 3</ref>. Recognition performance of different architectures with varying spatial aggregation scope and different geometric prior terms on ImageNet classification. Top-1 and top-5 accuracy (%) is reported. "NG" denotes local relation networks without the geometric prior term. "G*" represents the method that directly learns the geometric prior values as described in Section 4. For fair comparison, we set all the architectures to have similar FLOPs with the regular ResNet-26 model, by adapting their bottleneck ratio α. For ResNet-(DW)-26 networks, we omit the "full image" column due to implementation difficulty.  without geometric priors for all spatial scopes, demonstrating their usefulness in visual feature learning. Comparing the two geometric prior encoding methods, applying a small network on relative positions (the last row) performs better than directly learning independent geometric prior values. The gap between them is larger when the kernel size is larger (0.4 at 3×3 and 3.1 at 9×9), showing that it is crucial to additionally account for relative positions, especially when the neighborhood is large. <ref type="figure" target="#fig_5">Figure 4</ref> shows the learnt 7×7 geometric prior values after softmax at four stages of LR-Net-26. In general, for lower layers, the priors are sharper, indicating preference for stronger constraints in the learning of appearance composability. For higher layers, the priors are smoother, indicating preference for greater freedom.</p><p>Other designs We also ablate various design elements. a) Effect of query/key dim <ref type="table">Table 4</ref> ablates the accuracy of the proposed LR-Net-26 model with varying key/query dimensions. We follow <ref type="bibr" target="#b24">[25]</ref> to compute the appearance composablity between key and query vectors. We find decreased accuracy with increasing key/query dimension, indicating the superiority of scalars over typically-used vectors, as well as a better speed-accuracy tradeoff. b) Effect of channel sharing <ref type="table">Table 5</ref> ablates the LR-Net-26 model with varying numbers of shared channels per aggregation (m). The accuracy of LR-Net-26 is maintained when m is as large as 8, while being 3× faster than not sharing (m = 1). c) Composability term <ref type="table">Table 6</ref> ablates over different appearance composabilty terms: Eqn. (3), Eqn. (4) and Eqn. <ref type="bibr" target="#b4">(5)</ref>. They are found to work comparably well. <ref type="figure" target="#fig_6">Figure 5</ref> exhibits representative examples of key and query maps learnt using the default term of Eqn. <ref type="formula" target="#formula_3">(3)</ref>, which indicate that composability between semantic visual elements are learnt (girl and dog, tennis ball and racket). d) Softmax normalization <ref type="table">Table 6</ref> shows that including the softmax normalization in Eqn. (2) improves accuracy by 0.9, indicating the importance of normalization in balancing the two terms.</p><p>Comparison with other bottom-up methods <ref type="table">Table 7</ref> compares LR-Net with other bottom-up methods, i.e. nonlocal neural networks <ref type="bibr" target="#b26">[27]</ref>. By directly replacing the 3×3 convolution layer in the ResNet-26 model by non-local modules, the model (NL-26) achieves an accuracy of 47.7, far lower than its regular counterpart. By applying the nonlocal modules after every residual block, top-1 accuracy of 73.4 is obtained, which is 0.6 higher than its regular counterpart, with about 2× more computation.</p><p>The local relation layer is designed to replace convolution layers for better representation power. It achieves a 2.9 gain over the regular ResNet counterpart with a similar computation load. We note that the non-local module is complementary to local relation networks, bringing a 0.3 gain when applied after every local relation block (see the last row).</p><p>On different/deeper networks In <ref type="table">Table 8</ref>, we evaluate LR-Net with different/deeper network architectures, including ResNet-18 which consists of 8 basic residual blocks and ResNet-50/101 which use the same type of bottleneck residual blocks but have more layers (50 and 101 layers). The proposed networks are also effective on these architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Robustness to adversarial attacks</head><p>We test the ability of LR-Net to withstand adversarial attacks using the white-box multi-step PGD attack method <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b15">16]</ref>, under both targeted and untargeted attacks. Targeted attacks randomly choose one wrong class as the target, while untargeted attacks succeed as long as the model produces wrong predictions. We utilize the hyperparameters from <ref type="bibr" target="#b15">[16]</ref> of the attacking methods, and employ the targeted multi-step PGD adversarial method for training with the same hyper-parameters except for the number of attack steps, set to 16 due to limited GPU resources. <ref type="table">Table 9</ref> compares the robustness of LR-Net-26 and the regular ResNet-26/ResNet-50 models against white-box adversarial attacks on ImageNet. The LR-Net-26 model performs significantly better than ResNet-26 model against both the targeted (+6.3) and untargeted attacks (+12.4). The LR-Net-26 model also performs better than the ResNet-50 model (+0.8 for targeted attacks and +4.3 for untargeted attacks), which uses about 2× more FLOPs and has better top-1 accuracy in regular training (see the last column of <ref type="table">Table 9</ref>). These results indicate that the superior performance of LR-Net in adversarial robustness is not purely due to larger capacity but also because of the architecture itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and Future Works</head><p>This paper presents the local relation layer, a basic image feature extractor following the general philosophy of introducing compositionality into representation. A deep network composed by this new layer demonstrates strong results on ImageNet classification, significantly expanding the practicality of bottom-up methods, which are long believed to be more fundamental in representation than topdown methods such as convolution.</p><p>We note that the study of this new layer is still at an early stage. Future directions include: 1) better GPU memory scheduling for faster implementation; 2) better designs to outperform advanced convolution methods such as deformable convolution <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32]</ref>; 3) exploring other properties and the applicability on other vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1. Implementation Details</head><p>All architectures take a 3×224×224 image as input. The architectures use a skip connection for the shortcut branch of all residual blocks except for across stages where a channel transformation layer followed by batch normalization is used. In res3, res4 and res5, downsampling is applied on the 3 × 3 convolution layer or the local relation layer in the first residual blocks. For fair comparison in ablation experiments, we adapt the bottleneck ratio α to ensure the same FLOPs for different architectures.</p><p>In training, the randomly cropped images and employ scale and aspect ratio augmentation. We perform SGD optimization with a mini-batch of 1024 on 16 GPUs for all experiments except for the experiments of adversarial training in which 32 GPUs are used. The initial learning rate is 0.4, with linear warm-up in the first 5 epochs, and decays by 10× at the 30th, 60th and 90th epochs, respectively, following <ref type="bibr" target="#b8">[9]</ref>. The total learning period is 110 epochs, with a weight decay of 0.0001 and momentum of 0.9. In inference, we use a single 224×224 center crop from the resized images with a shorter size of 256. Top-1 and top-5 accuracy are reported.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of the 3×3 convolution layer and the 3×3 local relation layer. While 3 channels are required by convolution to represent the spatial variability between bird eye and beak, the local relation layer requires only 1 channel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 3 )</head><label>3</label><figDesc>by default. Geometric priors Another important aspect differentiating the local relation layer from other bottom-up methods is the inclusion of geometric priors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The local relation layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of replacing the first 7 × 7 convolution layer (a) and the bottleneck/basic residual blocks (b)(c) in the ResNet architecture. "CT" denotes the channel transformation layer and "LR" denotes the local relation layer. "7×7 (8), 64" represents kernel size of 7×7, channel sharing of m = 8 and output channel of 64. "s = 2" represents a stride of 2. All layers are followed by a batch normalization layer and a ReLU activation layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of learnt geometric prior values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Illustration of learnt key and query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table 2. (Left) ResNet-50. (Right) LR-Net-50 with 7 × 7 kernel size and m = 8 channels per aggregation computation. Inside the brackets are the shape of a residual block, and outside the brackets is the number of stacked blocks in a stage. LR-Net-50 requires similar FLOPs as ResNet-50 and a slightly smaller number of parameters.</figDesc><table><row><cell cols="2">stage output</cell><cell></cell><cell>ResNet-50</cell><cell></cell><cell cols="3">LR-Net-50 (7×7, m=8)</cell></row><row><cell cols="5">res1 112×112 7×7 conv, 64, stride 2</cell><cell cols="3">1×1, 64 7×7 LR, 64, stride 2</cell></row><row><cell></cell><cell></cell><cell cols="6">3×3 max pool, stride 2 3×3 max pool, stride 2</cell></row><row><cell cols="2">res2 56×56</cell><cell>  </cell><cell>1×1, 64 3×3 conv, 64</cell><cell>  ×3</cell><cell>  </cell><cell>1×1, 100 7×7 LR, 100</cell><cell>  ×3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1×1, 256</cell><cell></cell><cell></cell><cell>1×1, 256</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>1×1, 128</cell><cell></cell><cell></cell><cell>1×1, 200</cell><cell></cell></row><row><cell cols="2">res3 28×28</cell><cell> </cell><cell>3×3 conv, 128</cell><cell> ×4</cell><cell> </cell><cell>7×7 LR, 200</cell><cell> ×4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1×1, 512</cell><cell></cell><cell></cell><cell>1×1, 512</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>1×1, 256</cell><cell></cell><cell></cell><cell>1×1, 400</cell><cell></cell></row><row><cell cols="2">res4 14×14</cell><cell> </cell><cell>3×3 conv, 256</cell><cell> ×6</cell><cell> </cell><cell>7×7 LR, 400</cell><cell> ×6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1×1, 1024</cell><cell></cell><cell></cell><cell>1×1, 1024</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>1×1, 512</cell><cell></cell><cell></cell><cell>1×1, 800</cell><cell></cell></row><row><cell>res5</cell><cell>7×7</cell><cell> </cell><cell>3×3 conv, 512</cell><cell> ×3</cell><cell> </cell><cell>7×7 LR, 800</cell><cell> ×3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1×1, 2048</cell><cell></cell><cell></cell><cell>1×1, 2048</cell><cell></cell></row><row><cell></cell><cell>1×1</cell><cell cols="3">global average pool 1000-d fc, softmax</cell><cell cols="3">global average pool 1000-d fc, softmax</cell></row><row><cell cols="2"># params</cell><cell></cell><cell>25.5×10 6</cell><cell></cell><cell></cell><cell>23.3×10 6</cell><cell></cell></row><row><cell cols="2">FLOPs</cell><cell></cell><cell>4.3×10 9</cell><cell></cell><cell></cell><cell>4.3×10 9</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For example, geometric priors are intrinsically encoded in the convolution layer, as its aggregation weights are parameterized on relative positions. This is an important property leading to its success in visual recognition.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Since 1×1 convolutions do not involve filtering over neighboring pixels, we do not treat them as convolutions in this paper and refer to them as channel transformations<ref type="bibr" target="#b18">[19]</ref>. Nevertheless, in some figures/tables, we use 1 × 1 to denote a channel transformation layer for notation convenience.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Since one output channel strictly uses one input channel in aggregation computation, we omit the c, c for notational convenience.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The LR-Net-26 network introduced in Section 5 is about 3× slower than that of a regular ResNet-26 model on a Titan Xp GPU.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We follow<ref type="bibr" target="#b26">[27]</ref> to reduce the computation complexity of the "full image" method, by adopting downsampled key feature maps at high resolution stages: 4× for res2, 2× for res3 and 2× for res4. Without this, the accuracy of "full image" methods would be even lower.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recognition-by-components: a theory of human image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">115</biblScope>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Matrix capsules with em routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Active convolution: Learning the shape of convolution for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4201" to="4209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06373</idno>
		<title level="m">Adversarial logit pairing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep learning: A critical appraisal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marcus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00631</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3856" to="3866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Humboldt</surname></persName>
		</author>
		<title level="m">On Language: On the Diversity of Human Language Construction and Its Influence on the Mental Development of the Human Species. Cambridge Texts in the History of Philosophy</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04025</idno>
		<title level="m">Deep nets: What have they ever done for vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11168</idno>
		<title level="m">Deformable convnets v2: More deformable, better results</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
