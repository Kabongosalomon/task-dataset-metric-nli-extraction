<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
							<email>lindsey.li@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
							<email>yen-chun.chen@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
							<email>yu.cheng@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
							<email>zhe.gan@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
							<email>licheng.yu@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present HERO, a novel framework for large-scale video+language omnirepresentation learning.</p><p>HERO encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a Cross-modal Transformer via multimodal fusion, and global video context is captured by a Temporal Transformer. In addition to standard Masked Language Modeling (MLM) and Masked Frame Modeling (MFM) objectives, we design two new pre-training tasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global and local temporal alignment; and (ii) Frame Order Modeling (FOM), where the model predicts the right order of shuffled video frames.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Inspired by <ref type="bibr">BERT (Devlin et al., 2019)</ref>, largescale multimodal pre-training has prevailed in the realm of vision-and-language research <ref type="bibr">(Lu et al., 2019;</ref><ref type="bibr">Tan and Bansal, 2019;</ref><ref type="bibr">Chen et al., 2020b)</ref>. There are many early players in the area, including <ref type="bibr">ViLBERT (Lu et al., 2019)</ref>, <ref type="bibr">LXMERT (Tan and Bansal, 2019)</ref>, <ref type="bibr">UNITER (Chen et al., 2020b)</ref>, <ref type="bibr">VL-BERT (Su et al., 2020)</ref> and <ref type="bibr">Unicoder-VL (Li et al., 2020a)</ref>. However, most large-scale pre-trained models are tailored for static images, not dynamic videos. <ref type="bibr">VideoBERT (Sun et al., 2019b)</ref> is the first to apply BERT to learn joint embedding for videotext pairs. But since only discrete tokens are used to represent video frames, rich video frame features are not fully utilized. To remedy this, <ref type="bibr">CBT (Sun et al., 2019a)</ref> proposes to use a contrastive loss, but mainly for video representation learning alone, with text input only considered as side information. UniViLM <ref type="bibr">(Luo et al., 2020</ref>) takes a step further and considers both understanding and generation tasks.</p><p>Several constraints inherently limit the success of existing models. (i) Most model designs are direct adaptation of BERT, taking simple concatenation of subtitle sentences and visual frames as input, while losing the temporal alignment between video and text modalities. (ii) Pre-training tasks are directly borrowed from image+text pre-training methods, without exploiting the sequential nature of videos. (iii) Compared to diverse image domains investigated in existing work, video datasets used in current models are restricted to cooking or narrated instructional videos <ref type="bibr">(Miech et al., 2019)</ref>, excluding video sources that contain dynamic scenes and complex social interactions.</p><p>To tackle these challenges, we present a new video-and-language large-scale pre-training framework -HERO (Hierarchical EncodeR for Omnirepresentation learning). As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, HERO takes as input a sequence of video clip frames and their accompanying subtitle sentences. 2 Instead of adopting a flat BERT-like encoder, HERO encodes multimodal inputs in a hierarchical fashion, with (i) a Cross-modal Transformer to fuse a subtitle sentence and its accompanying local video frames, followed by (ii) a Temporal Transformer to obtain a sequentially contextualized embedding for each video frame, using all the surrounding frames as global context. The proposed hierarchical model first absorbs visual and textual local context on frame level, which is then transferred to a global video-level temporal context. Experiments show that this novel model design achieves better performance than a flat BERT-like architecture.</p><p>Four pre-training tasks are designed for HERO: (i) Masked Language Modeling (MLM); (ii) Masked Frame Modeling (MFM); (iii) Video-Subtitle Matching (VSM); and (iv) Frame Order Modeling <ref type="bibr">(FOM)</ref>. Compared to prior work, the key novelty is VSM and FOM, which encourage explicit temporal alignment between multimodalities as well as full-scale exploitation of the sequential nature of video input. In VSM, the model considers not only global alignment (predicting whether a subtitle matches the input video clip), but also local temporal alignment (retrieving the moment where the subtitle should be localized in the video clip). In FOM, we randomly select and shuffle a subset of video frames, and the model is trained to restore their original order. Extensive ablation studies demonstrate that both VSM and FOM play a critical role in video+language pre-training.</p><p>To empower the model with richer knowledge beyond instructional videos used in prior work, we jointly train HERO with both HowTo100M (narrated instructional videos) <ref type="bibr">(Miech et al., 2019</ref>) and a large-scale TV dataset (containing TV episodes spanning across different genres) <ref type="bibr">(Lei et al., 2018</ref><ref type="bibr">(Lei et al., , 2020a</ref><ref type="bibr">Liu et al., 2020)</ref>. Compared to factual descriptions in HowTo100M, the TV dataset contains more complex plots that require comprehensive interpretation of human emotions, social dynamics and causal relations of events, making it a valuable supplement to HowTo100M and a closer approximation to real-life scenarios.</p><p>Existing pre-trained models are evaluated on YouCook2 <ref type="bibr">(Zhou et al., 2018a)</ref> and <ref type="bibr">MSR-VTT (Xu et al., 2016a)</ref> datasets. YouCook2 focuses on cooking videos only, and the captions in MSR-VTT are very simple. To evaluate our model on more challenging benchmarks, we collect two new datasets on video-moment retrieval and question answering, How2R and How2QA. In addition, we evaluate HERO on popular retrieval and QA tasks such as TVR <ref type="bibr">(Lei et al., 2020b)</ref> and <ref type="bibr">TVQA (Lei et al., 2018)</ref>, where HERO outperforms existing models by a large margin. We further demonstrate the generalizability of our model by adapting it to (i) diverse downstream tasks: video-and-language inference and video captioning tasks, achieving new state of the art on <ref type="bibr">VIOLIN (Liu et al., 2020)</ref> and <ref type="bibr">TVC (Lei et al., 2020b)</ref> benchmarks; (ii) different video types: single-channel videos (video-only) and multi-channel videos (video + subtitle), reporting superior performance over existing state of the art on DiDeMo (Anne Hendricks et al., 2017a) and MSR-VTT.</p><p>Our main contributions are summarized as follows. (i) We present HERO, a hierarchical Transformer-based model for video+language representation learning. (ii) We propose new pretraining tasks VSM and FOM, which complement MLM and MRM objectives by better capturing temporal alignment between multimodalities in both global and local contexts. (iii) Different from previous work that mainly relies on HowTo100M, we include additional video datasets for pre-training, encouraging the model to learn from richer and more divserse visual content. (iv) We collect two new datasets based on HowTo100M for video-moment retrieval/QA, and will release the new benchmarks to foster future study. HERO achieves new state of the art across all the evaluated tasks.</p><p>Since the birth of <ref type="bibr">BERT (Devlin et al., 2019)</ref>, there has been continuing advancement in language model pre-training, such as XLNet <ref type="bibr">(Yang et al., 2019)</ref>, <ref type="bibr">RoBERTa (Liu et al., 2019)</ref>, <ref type="bibr">ALBERT (Lan et al., 2020</ref><ref type="bibr">), UniLM (Dong et al., 2019</ref><ref type="bibr">), and T5 (Raffel et al., 2019</ref>, which epitomizes the superb power of large-scale pre-training. Satellited around BERT, there is parallel growing interest in model compression <ref type="bibr">(Sun et al., 2019c;</ref><ref type="bibr">Shen et al., 2020)</ref> and extension to generation tasks <ref type="bibr">(Chen et al., 2020a;</ref><ref type="bibr">Wang and Cho, 2019)</ref>.</p><p>Branching out from language processing to multimodal, subsequent studies also emerge in vi-sion+language space. Prominent work includes <ref type="bibr">ViLBERT (Lu et al., 2019)</ref>, <ref type="bibr">LXMERT (Tan and Bansal, 2019)</ref>, <ref type="bibr">VL-BERT (Su et al., 2020</ref><ref type="bibr">), Unicoder-VL (Li et al., 2020a</ref><ref type="bibr">), B2T2 (Alberti et al., 2019</ref>, <ref type="bibr">UNITER (Chen et al., 2020b)</ref> and <ref type="bibr">VILLA (Gan et al., 2020)</ref>. A detailed review can be found in Appendix A.7.</p><p>Contrast to the boom in image+text area, pretraining for video+language is still in its infancy.</p><p>So far, <ref type="bibr">VideoBERT (Sun et al., 2019b)</ref>, <ref type="bibr">CBT (Sun et al., 2019a)</ref>, <ref type="bibr">MIL-NCE (Miech et al., 2020)</ref>, <ref type="bibr">Act-BERT (Zhu and Yang, 2020)</ref> and <ref type="bibr">UniViLM (Luo et al., 2020)</ref> are the only existing work exploring this space, covering downstream tasks from textbased video retrieval <ref type="bibr">(Zhou et al., 2018a;</ref><ref type="bibr">Xu et al., 2016b)</ref> and video question answering <ref type="bibr">(Maharaj et al., 2017;</ref><ref type="bibr">Lei et al., 2020a)</ref> to video captioning <ref type="bibr">(Zhou et al., 2018b)</ref>.</p><p>In this paper, we aim to propel video+language omni-representation learning in four dimensions: (i) better model architecture design; (ii) better pretraining task design; (iii) diversification of training corpora; and (iv) new high-quality benchmarks for downstream evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hierarchical Video+Language Encoder</head><p>In this section, we explain the proposed HERO architecture and the four pre-training tasks in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Architecture</head><p>Model architecture of HERO is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, which takes the frames of a video clip and the textual tokens of subtitle sentences as inputs. They are fed into a Video Embedder and a Text Embedder to extract initial representations. HERO computes contextualized video embeddings in a hierarchical procedure. First, local textual context of each visual frame is captured by a Crossmodal Transformer, computing the contextualized multi-modal embeddings between a subtitle sentence and its associated visual frames. The encoded frame embeddings of the whole video clip are then fed into Temporal Transformer to learn the global video context and obtain the final contextualized video embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Embedder</head><p>We denote visual frames of a video clip as v = {v i } Nv i=1 and its subtitle as s = {s i } Ns i=1 (N v is the number of visual frames in a video clip and N s is the number of sentences in each subtitle). For Text Embedder, we follow <ref type="bibr">Liu et al. (2019)</ref> and tokenize a subtitle sentence s i into a sequence of WordPieces <ref type="bibr">(Wu et al., 2016)</ref>, i.e., w s i = {w j s i } L j=1 (L is the number of tokens in s i ). The final representation for each sub-word token is obtained via summing up its token embedding and position embedding, followed by a layer normalization (LN) layer. For Video Embedder, we first use <ref type="bibr">ResNet (He et al., 2016)</ref> pretrained on ImageNet <ref type="bibr">(Deng et al., 2009) and</ref><ref type="bibr">Slow-Fast (Feichtenhofer et al., 2019)</ref> pre-trained on <ref type="bibr">Ki-netics (Kay et al., 2017)</ref> to extract 2D and 3D visual features for each video frame. These features are concatenated as visual features and fed through a fully-connected (FC) layer to be projected into the same lower-dimensional space as token embeddings. Since video frames are sequential, their position embeddings can be calculated in the same way as in the Text Embedder. The final embedding of a frame is obtained by summing up FC outputs and position embeddings and then passing through an LN layer. After Input Embedder, token and frame embeddings for w s i and v s i 3 are denoted as W emb</p><formula xml:id="formula_0">s i ∈ R L×d and V emb s i ∈ R K×d (d is the hidden size).</formula><p>Cross-modal Transformer To utilize the inherent alignment between subtitles and video frames, for each subtitle sentence s i , we first learn contextualized embeddings between the corresponding tokens w s i and its associated visual frames v s i through cross-modal attention. Inspired by the recent success <ref type="bibr">(Chen et al., 2020b;</ref><ref type="bibr">Lu et al., 2019)</ref> of using Transformer <ref type="bibr">(Vaswani et al., 2017)</ref> for multimodal fusion, we also use a multi-layer Transformer here. The outputs from Cross-modal Transformer is a sequence of contextualized embeddings for each subtitle token and each video frame:</p><formula xml:id="formula_1">V cross s i , W cross s i = f cross (V emb s i , W emb s i ) ,<label>(1)</label></formula><p>where f cross (·, ·) denotes the Cross-modal Transformer, V cross</p><formula xml:id="formula_2">s i ∈ R K×d and W cross s i ∈ R L×d .</formula><p>Temporal Transformer After collecting all the visual frame embeddings V cross = {V cross s i } Ns i=1 ∈ R Nv×d from the output of Cross-modal Transformer, we use another Transformer as temporal attention to learn contextualized video embeddings from the global context of a video clip. To avoid losing positional information, we use residual connection <ref type="bibr">(He et al., 2016)</ref> to add back V emb ∈ R Nv×d . The final contextualized video embeddings are calculated as:</p><formula xml:id="formula_3">V temp = f temp (V emb + V cross ) ,<label>(2)</label></formula><p>where f temp (·) denotes the Temporal Transformer, and V temp ∈ R Nv×d . Compared to flat BERT-like encoder, which directly concatenates all textual tokens and visual frames as inputs, the proposed 3 vs i = {v j s i } K j=1 denotes the set of visual frames paired with subtitle sentence si, based on their timestamps. Refer to Appendix A.4 for details.  model effectively utilizes the temporal alignment between subtitle sentences and video frames for multimodal fusion in a more fine-grained manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Modal Transformer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Modal Transformer</head><p>In the experiments, we show that our model design far outperforms a flat BERT-like baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pre-training Tasks</head><p>We introduce four tasks for pre-training. During training, we sample one task per mini-batch to prevent different tasks from corrupting each others' input. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, MFM and MLM are in analogy to <ref type="bibr">BERT (Devlin et al., 2019)</ref>. Word masking is realized by replacing a word with special token [MASK], and frame masking by replacing a frame feature vector with zeros. Following Chen et al. (2020b), we only mask one modality each time while keeping the other modality intact. VSM is designed to learn both local alignment (between visual frames and a subtitle sentence) and global alignment (between a video clip and a sequence of subtitle sentences). FOM is designed to model sequential characteristics of video, by learning the original order of randomly reordered frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Masked Language Modeling</head><p>The inputs for MLM include: (i) sub-word tokens from the i-th subtitle sentence w s i ; (ii) visual frames v s i aligned with w s i ; and (iii) mask indices</p><formula xml:id="formula_4">m ∈ N M . 4</formula><p>In MLM, we randomly mask out input words with a probability of 15%, and replace the masked tokens w m s i with special tokens <ref type="bibr">[MASK]</ref>. <ref type="bibr">5</ref> The goal is to predict these masked words based on the observation of their surrounding words w \m s i and the visual frames aligned with the sentence v s i , by minimizing the negative log-likelihood:</p><formula xml:id="formula_5">L MLM (θ) = −E D log P θ (w m s i |w \m s i , v s i ) ,<label>(3)</label></formula><p>where θ denotes trainable parameters. Each pair (w s i , v s i ) is sampled from the training set D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Masked Frame Modeling</head><p>Similar to MLM, we also sample frames and mask their visual features with a probability of 15%. However, the difference is that MLM is performed on a local context (i.e., the output of Cross-modal Transformer), while MFM is performed on a global context (i.e., the output of Temporal Transformer). The model is trained to reconstruct masked frames v m , given the remaining frames v \m and all the subtitle sentences s. The visual features of masked frames are replaced by zeros. Unlike textual tokens that are represented as discrete labels, visual features are high-dimensional and continuous, thus cannot be supervised via class likelihood. Instead, we propose two variants for MFM, which share the same objective base:</p><formula xml:id="formula_6">L MFM (θ) = E D f θ (v m |v \m , s) .<label>(4)</label></formula><p>Masked Frame Feature Regression (MFFR) MFFR learns to regress the output on each masked frame v (i) m to its visual features. Specifically, we apply an FC layer to convert the output frame rep-</p><formula xml:id="formula_7">resentations into a vector h θ (v (i) m ) of the same dimension as the input visual feature r(v (i) m ).</formula><p>Then we apply L2 regression between the two:</p><formula xml:id="formula_8">f θ (v m |v \m , s) = M i=1 h θ (v (i) m ) − r(v (i) m ) 2 2 .</formula><p>Masked Frame Modeling with Noise Contrastive Estimation (MNCE) Instead of directly regressing the real values of masked visual features, we use the softmax version of Noise Contrastive Estimation (NCE) loss <ref type="bibr">(Jozefowicz et al., 2016)</ref>, which is widely adopted in self-supervised representation learning <ref type="bibr">(Sun et al., 2019a;</ref><ref type="bibr">Hjelm et al., 2019;</ref><ref type="bibr">Oord et al., 2018)</ref>. NCE loss encourages the model to identify the correct frame (given the context) compared to a set of negative distractors. Similar to MFFR, we feed the output of the masked frames v (i) m into an FC layer to project them into a vector g θ (v (i) m ). Moreover, we randomly sample frames from the output of unmasked frames as negative distractors v neg = {v</p><formula xml:id="formula_9">(j) neg |v (j) neg ∈ v \m }, which are also transformed through the same FC layer as g θ (v (j) neg ). The final objec- tive minimizes the NCE loss: f θ (v m |v \m , s) = M i=1 log NCE(g θ (v (i) m )|g θ (v neg )).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Video-Subtitle Matching</head><p>The inputs to VSM are: (i) a sampled query s q from all subtitle sentences; (ii) the whole video clip v; and (iii) the remaining subtitle sentences s \q for the video clip. We expect the model to learn: (i) local alignment -the start and end index y st , y ed ∈ {1, ..., N v }, indicating the span of visual frames aligned with the query; 6 and (ii) global alignment -to which video the sampled query is matched.</p><p>In VSM, we follow XML <ref type="bibr">(Lei et al., 2020b)</ref> to compute the matching scores between the query and visual frames at both local and global levels. Specifically, we extract the output of Temporal Transformer as the final visual frame representation V temp ∈ R Nv×d . The query is fed into Crossmodal Transformer to compute its textual representations W cross sq = f cross (0, W embed sq ). Based on this, we use a query encoder <ref type="bibr">(Lei et al., 2020b)</ref>, consisting of a self-attention layer, two linear layers and an LN layer, to obtain the final query vector</p><formula xml:id="formula_10">q ∈ R d from W cross sq .</formula><p>Local Alignment The local query-video matching score is computed using dot product:</p><formula xml:id="formula_11">S local (s q , v) = V temp q ∈ R Nv .<label>(5)</label></formula><p>Two trainable 1D convolution filters are applied to the scores, followed by a softmax layer, to generate two probability vectors p st , p ed ∈ R Nv , representing the probabilities of every position being the start and end of the ground-truth span. During training, we sample 15% subtitle sentences as queries for each video, and use the cross-entropy loss to predict the start and end index for local alignment:</p><formula xml:id="formula_12">L local = −E D log(p st [y st ]) + log(p ed [y ed ]) ,</formula><p>where p[y] denotes indexing the y-th element of the vector p. Note that, XML computes the query-video matching score for each modality separately, and the final matching score is the sum of the two scores. In our HERO model, multimodal fusion is performed in a much earlier stage.</p><p>Global Alignment The global matching score is computed by max-pooling the cosine similarities between each frame and the query:</p><formula xml:id="formula_13">S global (s q , v) = max V temp ||V temp || q ||q|| .<label>(6)</label></formula><p>We use a combined hinge loss L h (Yu et al., 2018a) over positive and negative query-video pairs. For each positive pair (s q , v), we replace v or s q with one other sample from in the same mini-batch to construct two sets of negative examples: (s q ,v) and (ŝ q , v). The training loss is specified as:</p><formula xml:id="formula_14">L h (S pos , S neg ) = max(0, δ + S neg − S pos ) , L global = −E D [L h (S global (s q , v), S global (ŝ q , v)) + L h (S global (s q , v), S global (s q ,v))] ,<label>(7)</label></formula><p>where δ is the margin hyper-parameter. The final loss L VSM = λ 1 L local + λ 2 L global , where λ 1 and λ 2 are hyper-parameters balancing the two terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Frame Order Modeling</head><p>The inputs for FOM are: (i) all subtitle sentences s;</p><p>(ii) visual frames v; and (iii) the reorder indices <ref type="bibr">7</ref> We randomly select 15% of the frames to be shuffled, and the goal is to reconstruct their original timestamps, denoted as</p><formula xml:id="formula_15">r = {r i } R i=1 ∈ N R .</formula><formula xml:id="formula_16">t = {t i } R i=1 , where t i ∈ {1, ..., N v }.</formula><p>We formulate FOM as a classification problem, where t is the ground-truth labels of the reordered frames.</p><p>Specifically, reordering happens after the multimodal fusion of subtitle and visual frames. The reordered features are fed into Temporal Transformer to produce reordered visual frame embeddings V temp r . These embeddings are transformed through an FC layer, followed by a softmax layer to produce a probability matrix P ∈ R Nv×Nv , where each column p i ∈ R Nv represents the scores of N v timestamp classes that the i-th timestamp belongs to. The final objective is to minimize the the negative log-likelihood (cross-entropy loss):</p><formula xml:id="formula_17">L FOM = −E D R i=1 log P[r i , t i ] .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we describe comprehensive experiments on downstream tasks and provide ablation studies for in-depth analysis of different pretraining settings.</p><p>To validate the effectiveness of HERO, we evaluate on a wide variety of downstream tasks, including Text-based Video/ Video-moment Retrieval, Video Question Answering, Video-and-language Inference, and Video Captioning. We consider 6 existing benchmarks: TVR <ref type="figure">(</ref> Detailed descriptions and evaluation metrics on each task can be found in Appendix A.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pre-training Datasets</head><p>Our pre-training dataset is composed of 7.6M video clips with their accompanying subtitles from TV and HowTo100M datasets. We exclude all the videos that appear in the downstream tasks to avoid contamination in evaluation. 7 R is the number of reordered frames, and r is the set of reorder indices. Each video is associated with a narration as subtitles that are either written manually or from an Automatic Speech Recognition (ASR) system. The average duration of videos in HowTo100M is 6.5 minutes. We cut the videos into 60-second clips to make them consistent with the TV dataset, and exclude videos in non-English languages. These pre-processing steps result in a subset of 7.56M video clips, accompanied with English subtitles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">New Benchmarks</head><p>Existing benchmarks are mostly built on videos from either a single domain or a single modality. In order to evaluate on diverse video content that reflects multimodality challenges, we introduce two new datasets as additional benchmarks: How2R for text-based video-moment retrieval, and How2QA for video question answering.</p><p>How2R Amazon Mechanical Turk (AMT) is used to collect annotations on HowTo100M videos. <ref type="figure">Figure 6a</ref> in Appendix shows the interface for annotation. We randomly sample 30k 60-second clips from 9,421 videos and present each clip to the turkers, who are asked to select a video segment containing a single, self-contained scene. After this segment selection step, another group of workers are asked to write descriptions for each displayed segment. Narrations are not provided to the workers to ensure that their written queries are based on visual content only. These final video segments are 10-20 seconds long on average, and the length of queries ranges from 8 to 20 words.</p><p>From this process, we have collected 51,390 queries for 24k 60-second clips from 9,371 videos in HowTo100M, on average 2-3 queries per clip. We split the video clips and its associated queries into 80% train, 10% val and 10% test. How2QA To collect another dataset for video QA task, we present the same set of selected video clips to another group of AMT workers for multichoice QA annotation. Each worker is assigned with one video segment and asked to write one question with four answer candidates (one correct and three distractors). Similarly, narrations are hidden from the workers to ensure the collected QA pairs are not biased by subtitles.</p><p>We observe that human-written negative answers suffer from serious bias (i.e., models can learn to predict correctly without absorbing any information from the video or subtitles). To mitigate this, we use adversarial matching <ref type="bibr">(Zellers et al., 2019)</ref> to replace one of the three written negative answers by a correct answer from another question that is most relevant to the current one. Similar to TVQA, we also provide the start and end points for the relevant moment for each question. After filtering low-quality annotations, the final dataset contains 44,007 QA pairs for 22k 60-second clips selected from 9035 videos. We split the data into 80% train, 10% val and 10% test sets. More details about data collection can be found in Appendix A.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We analyze the effectiveness of model design, especially different combinations of pre-training tasks and datasets, through extensive ablation studies.</p><p>Optimal Setting of Pre-training Tasks To search for the optimal setting of pre-training tasks, we conduct a series of extensive ablation studies to test each setting, using video-moment retrieval and QA downstream tasks as evaluation. We observe significant performance lift by adding VSM (L4), and the local and global alignments between subtitle and visual frames learned through VSM are especially effective on TVR and How2R. Adding additional MFFR (L5) reaches slightly worse results. Our observation is that MFFR is competing with (instead of complimentary to) MNCE during pre-training, which renders the effect of MFFR negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Pre-training Datasets</head><p>We study the effect of pre-training datasets by comparing TV dataset with HowTo100M. In this study, we first pre-train our model on HowTo100M dataset (L6). We observe a performance drop on TVR, while a performance boost on TVQA, How2R and How2QA, compared to the model trained on TV dataset (L4). Our hypothesis is that text-based video-moment retrieval is more sensitive to video domains. Although HowTo100M dataset contains much more videos, the model still benefits more from being exposed to similar TV videos during pre-training.</p><p>Hierarchical Design vs. Flat Architecture To validate the effectiveness of our model design, we compare HERO with two baselines (with and without pre-training): (i) Hierarchical Transformer (H-TRM) baseline, constructed by simply replacing the Cross-modal Transformer with a RoBERTa model  and encoding subtitles only; 8 (ii) Flat BERT-like encoder (F-TRM). 9 For this ablation experiment, we use TVR and TVQA as evaluation tasks. Results are summarized in <ref type="table">Table 2</ref>: (i) Without pre-training, F-TRM is much worse than HERO on both tasks. This is due to H-TRM and HERO's explicit exploitation of the temporal alignment between two modalities of videos. (ii) Pre-training lifts HERO performance by a large margin, but not much for F-TRM or H-TRM. This indicates that cross-modal interactions and temporal alignments learned by HERO through pre-training can provide better representations for downstream tasks.</p><p>HERO vs. SOTA with and w/o Pre-training We compare HERO with task-specifc state of the art (SOTA) models, including XML (Lei et al., 2020b) for TVR and STAGE (Lei et al., 2020a) for TVQA. As shown in <ref type="table">Table 2</ref>, our model consistently outperforms SOTA models on both tasks, with or without pre-training. Note that for TVQA, STAGE is trained with additional supervision on spatial grounding with region-level features for each frame. Without additional supervisions, HERO is able to achieve better performance. <ref type="bibr">8</ref> The inputs to Temporal Transformer in H-TRM are the summation of initial frame embedding and max-pooled subtitle embeddings from RoBERTa. 9 F-TRM takes as input a single sequence by concatenating the embeddings of visual frames and all subtitle sentences, and encodes them through one multi-layer Transformer. <ref type="bibr">10</ref>   <ref type="table">Table 2</ref>: Ablation study on model design, comparing HERO to a flat BERT-like encoder (F-TRM) baseline, a Hierarchical Transformer (H-TRM) baseline, and taskspecific SOTA models on TVR and TVQA val set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Key Conclusions</head><p>The main observations from these extensive ablation studies are summarized as follows:</p><p>• The optimal pre-training setting is MLM + MNCE + FOM + VSM, when trained on HowTo100M dataset and TV dataset.</p><p>• FOM effectively helps downstream tasks that rely on temporal reasoning (e.g., video QA tasks).</p><p>• VSM encourages frame-subtitle alignment, which is especially effective for videomoment retrieval tasks.</p><p>• The hierarchical design in HERO explicitly aligns subtitles and frames, while a flat model architecture can only learn this alignment through implicit attention.</p><p>• HERO consistently outperforms SOTA with and without pre-training, which further demonstrates the effectiveness of HERO model design. Results on Single-channel <ref type="table" target="#tab_5">Tasks Table 3b</ref> presents results on DiDeMo for text-based video-moment retrieval task and MSR-VTT for text-based video retrieval task. On DiDeMo, HERO surpasses XML by +0.55/+4.72/+10.65 on R@1/10/100, without leveraging Temporal Endpoint Feature used in XML. On MSRVTT, HERO outperforms existing video pre-training model (HowTo100M) by +1.9/+3.2/+4.9 on R@1/5/10. To evaluate in multi-channel setting, we also finetuned HERO on MSR-VTT and DiDeMo using both video channel and extracted subtitle channel (with ASR tools). When augmenting DiDeMo/MSR-VTT with ASR inputs, HERO performance is further improved. Although our model design focuses on truly multimodal videos (video+subtitle input), these results demonstrate HEROs superior general-12 To be consistent with TVR leaderboard, results are reported on tIoU&gt;0.7 without nms. izability to different video types (multi-and singlechannel). More results and analysis are provided in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on Downstream Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present a hierarchical encoder for video+language omni-representation pre-training. Our HERO model presents a hierarchical architecture, consisting of Cross-modal Transformer and Temporal Transformer for multi-modal fusion. Novel pre-training tasks are proposed to capture temporal alignment both locally and globally. Pretrained on two large-scale video datasets, HERO exceeds state of the art by a significant margin when transferred to multiple video-and-language tasks. Two new datasets on text-based video-moment retrieval and video QA are introduced to serve as additional benchmarks for downstream evaluation. We consider extension of our model to other videoand-language tasks as future work, as well as developing more well-designed pre-training tasks.</p><p>Pre-training greatly lifts HERO performance on VIOLIN by approximately +2.9%. However, HERO, without pre-training, presents worse performance than the SOTA baseline. Unlike Multistream, which leverages fine-grained region-level features, our results are reported on global framelevel features. Therefore, it may be difficult for HERO to capture the inconsistency between hypothesis and video content. For example, changes of hypotheses about region-level attributes (color, shape, and etc.) may result in different conclusions. Extending HERO for region-level video representations could be an interesting future direction.</p><p>HERO is also extensible to generation task: multi-modal video captioning. Our results on TVC show that HERO with pre-training surpasses MMT by a large margin. Although pre-training is only applied to the encoder, it significantly improves HERO performance on TVC across all metrics. When no pre-training is applied, HERO is slightly inferior to the SOTA baseline. Our hypothesis is that TVC has short video context (with video length of 9-second on average) but our model is designed for long video representation learning (TVR/TVQA with video length of 76-second on average). How to design pre-training tasks for MMT on TVC or including decoder pre-training for HERO are left for future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Qualitative Analysis</head><p>Visualization of VSM One way to understand how HERO aligns subtitles with video frames is to visualize the Video-Subtitle Matching pre-training task. We provide some examples of the top-1 moment predictions for VSM on both TV and HowTo100M corpora. As shown in <ref type="figure" target="#fig_3">Figure 2</ref>, the predicted moments (red) largely overlap with the ground truth moments (green) with minor differences. In <ref type="figure" target="#fig_3">Figure 2a</ref>, we human could probably identify the moment by the speaker information and the visual clue of character's emotion. For <ref type="figure" target="#fig_3">Figure 2b</ref>, objects (rubber bands) might be the key matching clue. The success of HERO to correctly match the moments might be a positive signal that its pre-training captures those human-identified patterns, hence leads to its strong video understanding capability. However, more thorough analysis, both quantitative and qualitative, is needed to interpret what video-language pre-trained models have learned, which we leave to future works.    <ref type="figure" target="#fig_6">Figure 3</ref> provides visualization examples of the attention maps learned by the Cross-modal Transformer. For completeness, we briefly discuss each pattern here:</p><p>• Vertical: Attention to a specific frame.</p><p>• Diagonal: Locally-focused attention to the token/frame itself or preceding/following tokens/frames.</p><p>• Vertical + Diagonal: Mixture of Vertical and Diagonal.</p><p>• Block: Intra-modality attention, i.e., textual self-attention or visual self-attention.</p><p>• Heterogeneous: Diverse attentions that cannot be categorized and highly dependent on actual input.</p><p>• Reversed Block: Cross-modality attention, i.e., text-to-frame and frame-to-text attention.</p><p>Note that we observe patterns slightly different from <ref type="bibr">Chen et al. (2020b)</ref>: Vertical patterns <ref type="figure" target="#fig_6">(Figure 3a)</ref> are usually over a specific frame instead of special tokens <ref type="bibr">([CLS]</ref> or [SEP]). We leave more sophisticated attention analysis/probing to future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Downstream Adaptation</head><p>The pre-trained model can be readily adapted to downstream video+language tasks through end-toend finetuning. Below, we describe the detailed adaptation approach to four downstream tasks: (i) text-based video moment retrieval, (ii) video question answering, (iii) video-and-language inference and (iv) multimodal video captioning.</p><p>Text-based Video-moment Retrieval The input video clip with accompanying subtitles is encoded by HERO as illustrated in <ref type="figure" target="#fig_7">Figure 4</ref>. The input query is encoded by the query encoder from the VSM pre-training task. We follow the same procedure as in VSM to compute query-video matching scores both locally (frame-level, for moment retrieval) and globally (clip-level, for video retrieval). The model is finetuned end-to-end using loss L VSM . Similarly, we let the margin δ = 0.1 and set λ 1 = 0.01 and λ 2 = 8 in the loss term L VSM .</p><p>Video Question Answering For Video QA, we consider the multiple-choice setting. As illustrated in <ref type="figure" target="#fig_8">Figure 5</ref>, for each answer candidate, the corresponding QA pair is appended to each of the subtitle sentences and fed into the Cross-modal Transformer to perform early fusion with local textual context. In addition, these QA pairs are also appended to the input of Temporal Transformer to be fused with global video context. We use a simple attention layer to compute the weighted-sumacross-time of the QA-aware frame representations  from the Temporal Transformer output.</p><p>These final QA-aware global representations are then fed through an MLP and softmax layer to obtain the probability score p </p><formula xml:id="formula_18">L ans = − 1 N N i=1 log p (i) ans [y i ] ,<label>(9)</label></formula><p>where y i is the index of the ground-truth answer for question i. When supervision is available, 14 we 14 Some existing Video QA tasks require localizing 'frames also include the span prediction loss:   ground-truth start and end positions for question i. The final loss L QA = L ans + λL span , where λ is the hyper-parameter that balance the above two terms. Empirically, we found that λ = 0.5 yields the best model performance.</p><formula xml:id="formula_19">L span = − 1 2N N i=1 (log p (i) st [y st i ] + log p (i) ed [y ed i ]) ,<label>(10)</label></formula><p>Video-and-Language Inference Similar to Video QA, each natural language hypothesis (or query) is appended to each of the subtitle sentences and also to the input of Temporal Transformer. A simple attention pooling layer is added to HERO to obtain the final query-aware global representations.</p><p>Video-and-language inference task can be regarded as a binary classification problem. We supervise the training using cross-entropy loss.</p><p>Multimodal Video Captioning With a simple addition of a Transformer decoder <ref type="bibr">(Vaswani et al., 2017)</ref>, we can extend HERO for multimodal video captioning. We feed the whole subtitle-aligned video clip into HERO and obtain the subtitle-fused video representation for each frame. Next, frame representations are grouped by the "moment of interest" using the time interval provided in the cap- tion annotation. The decoder-to-encoder attention is applied on the representations of the corresponding video moment and the decoder is trained with conventional left-to-right language modeling crossentropy loss together with the HERO encoder endto-end. To make the comparison to <ref type="bibr">MMT (Lei et al., 2020b)</ref> as fair as possible, we use shallow Transformer decoder (2-layer) with 768 hidden size. We do not use self-critical RL or its variants to optimize test metrics. Following MMT, greedy decoding is used at inference.</p><p>Single-channel Tasks Although HERO is designed for multi-channel videos (video+subtitle), we can easily extend it to single-channel video (video-only) tasks by adding an empty-string subtitle input and pair it with the whole frame sequence. For DiDeMo, we follow the same procedure as in VSM to compute both frame-level (for moment retrieval) and clip-level (for video retrieval) queryvideo matching scores. For MSR-VTT, a text-based video retrieval task, only clip-level scores are computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Frames/Subtitles Pre-processing</head><p>Given a pair of video clip and its associated subtitle, we first extract a sequence of visual frames v = {v i } Nv i=1 at a fixed frame rate (N v is the number of visual frames in a video clip). The subtitle is parsed into sentences s = {s i } Ns i=1 (N s is the number of sentences in each subtitle). Note that N v = N s in most cases, since a subtitle sentence may last for several visual frames. We then align the subtitle sentences temporally with the visual frames. Specifically, for each subtitle sentence s i , we pair it with a sequence of visual frames whose timestamps overlap with the subtitle timestamp, and denote these visual frames as</p><formula xml:id="formula_20">v s i = {v j s i } K j=1</formula><p>(K is the number of overlapping frames with s i ).</p><p>In the case that multiple sentences overlap with the same visual frame, we always pair the frame with the one with maximal temporal Intersection over Union (tIoU) to avoid duplication. It is possible that a subtitle sentence is not paired with any visual frame, and in this case, we concatenate it to the neighboring sentences to avoid information loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Implementation Details</head><p>We extract 2304-dimensional Slowfast <ref type="bibr">(Feichtenhofer et al., 2019)</ref> features at a fixed frame rate (TV: 2/3 frame per second, HowTo100M: 1/2 frame per second). and 2048-dimensional <ref type="bibr">ResNet-101 (He et al., 2016)</ref> features at doubled frame rate and maxpooled to get a clip-level feature. The final frame features is concatenation of the two features with dimension 4352. The model dimensions are set to (L=6, H=768, A=12) for Cross-Modal Transformer and (L=3, H=768, A=12) for Temporal Transformer, where L is the number of stacked Transformer blocks; H stands for hidden activation dimension and A is the number of attention heads. For pre-training task VSM, we let the margin δ = 0.1 and set λ 1 = 0.01 and λ 2 = 8 in the loss term L VSM . Our models are implemented based on Py-Torch <ref type="bibr">(Paszke et al., 2017)</ref>. <ref type="bibr">15</ref> To speed up training, we use Nvidia Apex 16 for mixed precision training. Gradient accumulation <ref type="bibr">(Ott et al., 2018)</ref> is applied to reduce multi-GPU communication overheads. All pre-training experiments are run on Nvidia V100 GPUs (32GB VRAM; NVLink connection). We use AdamW optimizer <ref type="bibr">(Loshchilov and Hutter, 2019)</ref> with a learning rate of 3e − 5 and weight decay of 0.01 to pre-train our model. The best pre-trained model is trained on 16 V100 GPUs for about 3 weeks. Finetuning experiments are implemented on the same hardware or Titan RTX GPUs (24GB VRAM) with AdamW optimizer but different learning rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Downstream Tasks</head><p>TVR <ref type="bibr">(Lei et al., 2020b)</ref> is the first to introduce text-based video-moment Retrieval task for multichannel videos (video+subtitle): given a natural language query, a model is required to not only retrieve the most relevant video clip from the video corpus, but also localize the relevant moment in the retrieved video clip. TVR is built upon the TV dataset, split into 80% train, 10% val, 5% testpublic and 5% test-private. On average, 5 queries were collected for each video clip. Among them, 74.2% of queries are related to video only, 9.1% to text only, and 16.6% to both video and text. signed for text-based video-moment retrieval on single-channel videos (video-only). It consists of 10.6K unedited video from Flickr with 41.2K sentences aligned to unique moments in the video. The dataset is split into 80% train, 10% val and 10% test. Note that moment start and end points are aligned to five-second intervals and the maximum annotated video length is 30 seconds. <ref type="figure" target="#fig_0">(Xu et al., 2016b)</ref>, for text-based video retrieval on single-channel videos (video-only), includes YouTube videos collected from 257 popular video queries from 20 categories (e.g. music, sports, movie, etc.). It contains 200K unique video clip-caption pairs. We follow the same setup in <ref type="bibr">Yu et al. (2018b)</ref> to evaluate our model on MSR-VTT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSR-VTT</head><p>Evaluation Metrics Text-based Video-moment Retrieval can be decomposed into two sub-tasks: (i) Video Retrieval: retrieve the most relevant video clip described by the query; (ii) Moment Retrieval: localize the correct moment from the most relevant video clip. A model prediction is correct if: (i) its predicted video matches the ground-truth (in Video Retrieval); and (ii) its predicted span has high overlap with the ground-truth (in Moment Retrieval). Average recall at K (R@K) over all queries is used as the evaluation metric for TVR, How2R, Didemo and MSR-VTT. For TVR, How2R and Didemo, temporal Intersection over Union (tIoU) is used to measure the overlap between the predicted span and the ground-truth span. 18 TVQA and How2QA include 3 sub-tasks: QA on the grounded clip, question-driven moment localization, and QA on the full video clip. We only consider QA on the full video clip, as it is the most challenging setting among the three. Video clips in VIOLIN are constrained to a single, self-contained scene, hence no additional grounding annotation is provided. Accuracy is used to measure model performance on TVQA, How2QA and VIOLIN.  <ref type="formula" target="#formula_1">2018)</ref>) with text. Through these pre-training efforts, tremendous progress has been made for vision-and-language representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Video+Language Tasks Overview</head><p>Text-based Video-moment retrieval is one of the most popular video+language tasks currently studied. Anne <ref type="bibr">Hendricks et al. (2017b)</ref> and <ref type="bibr">Gao et al. (2017)</ref> introduce the task of Single Video Moment Retrieval (SVMR), which aims at retrieving a moment from a single video via a natural language query. <ref type="bibr">Escorcia et al. (2019)</ref> extends SVMR to Video Corpus Moment Retrieval (VCMR), extending searching pool from single video to large video corpus. TVR (Lei et al., 2020b) defines a new task, Video-Subtitle Corpus Moment Retrieval, which provides temporally aligned subtitle sentences along with the videos as inputs. For this new task, XML (Lei et al., 2020b) is proposed to compute similarity scores between the query and each modality separately (visual frames, subtitles) and then sum them together for final prediction.</p><p>Another popular task is Video Question Answering (QA), which aims to predict answers to natural language questions given a video as context. Most previous work focuses on QA pairs from one modality only. For example, MovieFIB (Maharaj et al., 2017) focuses on visual concepts, (a) User interface for query annotation. Each worker is provided with a video clip and required to select a singlescene clip from the video, then write a query in the text box.</p><p>(b) User interface for question/answer annotation. Each worker is provided with a segmented clip and required to write a question with four answers in the text boxes.  <ref type="formula" target="#formula_3">2020)</ref> recently proposed Video-and-Language Inference task along with VIOLIN dataset, which requires a model to draw inference on whether a written statement entails or contradicts a given video clip. This new task is challenging to solve, as a thorough interpretation of both visual and textual clues from videos is required to achieve in-depth understanding and inference for a complex video scenario.</p><p>There are also recent studies on video captioning <ref type="bibr">(Venugopalan et al., 2015;</ref><ref type="bibr">Pan et al., 2016;</ref><ref type="bibr">Gan et al., 2017;</ref><ref type="bibr">Zhou et al., 2018b</ref><ref type="bibr">Zhou et al., , 2019</ref>  <ref type="figure">Figure 6a</ref> and 6b present the interfaces used for collecting How2R and How2QA. For How2R, the annotator is asked to first select a video segment from the presented video clip using the sliding bar, and then enter a description about the selected video segment in the    <ref type="figure">Figure 6a</ref>). For How2QA, we reuse the selected video segments collected for How2R. The annotators are asked to write a question, a correct answer and 3 wrong answers in the five text boxes shown in <ref type="figure">Figure 6b</ref>. distribution of selected video segments is presented in <ref type="figure">Figure 7</ref>. The length of video segments varies from 5 to more than 30 seconds. The majority of them have length less than 15 seconds. <ref type="figure" target="#fig_12">Figure 8</ref> shows the length (in number of words) distribution of collected queries in How2R. The length of queries is diverse, ranging from 8 to 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Segment Length Distribution The length</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How2R Query Length Distribution</head><p>How2QA Question and Answer Distribution <ref type="figure" target="#fig_13">Figure 9</ref> and <ref type="figure" target="#fig_0">Figure 10</ref> show the length (in number of words) distribution of collected questions and answers in How2QA. Questions are relatively longer, with more than 10 words on average. Answers are relatively shorter, most of which have less than 7 words.</p><p>In addition, we analyze the types of collected question by plotting the distribution of their leading words in <ref type="figure" target="#fig_0">Figure 11</ref>. In total, we collected questions in 7 different types. Majority of them starts with "what", "why" and "when".</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>HERO Architecture (best viewed in color), consisting of Cross-Modal Transformer and Temporal Transformer, learned via four pre-training tasks hierarchically. Initial frame features are obtained by SlowFast and ResNet feature extractors, and word embeddings are learned via an embedding layer initialized from RoBERTa.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Lei et al., 2020b), TVQA (Lei et al., 2018), VIOLIN (Liu et al., 2020), TVC (Lei et al., 2020b), DiDeMo (Anne Hendricks et al., 2017a), and MSR-VTT (Xu et al., 2016b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Visualization of top-1 moment predictions by HERO model for Video-Subtitle Matching on: (a) TV Dataset; and (b) HowTo100M Dataset. Text inside the dashed boxes is the accompany subtitles, with sampled subtitle query highlighted in blue. Groundtruth is highlighted with the green bar under the video frames. Predicted moments are bounded with boxes in red. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>ans of all the answers for question i. The training objective is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>ed are the prediction scores of the start and end position, obtained by applying weighted-sum-across-answers attention to the Temporal Transformer output followed by two MLPs and a softmax layer. y st i , y ed i are the indices of the of interest' for the question, e.g.,TVQA+ (Lei et al., 2020a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of the attention maps learned by Cross-modal Transformers of HERO model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>HERO model adapted to downstream task: Text-based Video Moment Retrieval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>HERO model adapted to downstream task: Video Question Answering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>TVC performance is measured by standard captioning metrics, inlcuding BLEU@4(Papineni et al., 2002),METEOR (Denkowski and  Lavie, 2014),ROUGE-L (Lin, 2004), and CIDEr- D (Vedantam et al., 2015).A.7 Vision+Language Pre-training Overview Very recently, multimodal pre-training has gained increasing attention, especially in the image+text area. Pioneering works such as ViLBERT (Lu et al., 2019) and LXMERT (Tan and Bansal, 2019) propose to encode image and text modalities by two separate Transformers, with a third Transformer for later multimodal fusion. Compared to this twostream architecture, VL-BERT (Su et al., 2020), Unicoder-VL (Li et al., 2020a), B2T2 (Alberti et al., 2019), VisualBERT (Li et al., 2019), and UNITER (Chen et al., 2020b) advocate singlestream architecture, where image and text signals are fused together in early stage. In VLP (Zhou et al., 2020) and XGPT (Xia et al., 2020), image captioning is considered as additional downstream application, so is visual dialog in Murahari et al. (2020). More recently, ViLBERT is enhanced by multi-task learning (Lu et al., 2020), Oscar (Li et al., 2020b) enhances pre-training with image tags, and Pixel-BERT (Huang et al., 2020) proposes to align image pixels (instead of bottom-up features (Anderson et al.,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Data collection interface: (a) How2R; and (b) How2QA. MovieQA (Tapaswi et al., 2016) is based on text summaries, and TGIF-QA(Jang et al., 2017) depends on predefined templates for question generation on short GIFs. TVQA (Lei et al., 2018) designed a more realistic multimodal setting: collecting human-written QA pairs along with their associated video segments by providing the an-Distribution of video segment length. notators with both video clips and accompanying subtitles. Later on, Lei et al. (2020a) augmented TVQA with frame-level bounding box annotations for spatial-temporal video QA, and introduced the STAGE framework to jointly localize moments, ground objects, and answer questions. Inspired by natural language inference (Bowman et al., 2015; Williams et al., 2018) and visual entailment (Xie et al., 2019), Liu et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>, popular benchmarks including Youtube2Text (Guadarrama et al., 2013), MSR-VTT (Xu et al., 2016a), YouCook2 (Zhou et al., 2018a), ActivityNet Captions (Krishna et al., 2017) and VATEX (Wang et al., 2019). Unlike previous work mostly focusing on captions describing the visual content, a unique TVC (Lei et al., 2020b) dataset was released with captions that also describe dialogues/subtitles. A.9 How2R and How2QA Benchmarks Data Collection Interface</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>How2R query length distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>How2QA question length distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>How2QA answer length distribution. text box (shown at the bottom of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 11 :</head><label>11</label><figDesc>Distribution of questions categorized by their leading words in How2QA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>TVDataset (Lei et al., 2018)  was built on 6 popular TV shows across 3 genres: medical dramas, sitcoms and crime shows. It contains 21,793 video clips from 925 episodes. Each video clip is 60-90 seconds long, covering long-range scenes with complex character interactions and social/professional activities. Dialogue for each video clip is also provided.</figDesc><table><row><cell>HowTo100M Dataset (Miech et al., 2019) was</cell></row><row><cell>collected from YouTube, mostly instructional</cell></row><row><cell>videos. It contains 1.22 million videos, with ac-</cell></row><row><cell>tivities falling into 12 categories (e.g., Food &amp; En-</cell></row><row><cell>tertaining, Home &amp; Garden, Hobbies &amp; Crafts).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Evaluation on pre-training tasks and datasets. Dark and light grey colors highlight the top and second best results across all the tasks trained with TV Dataset. The best results are in bold.</figDesc><table><row><cell>Pre-training Data</cell><cell></cell><cell>Pre-training Tasks</cell><cell>TVR</cell><cell></cell><cell>TVQA</cell><cell></cell><cell>How2R</cell><cell></cell><cell>How2QA</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">R@1 R@10 R@100</cell><cell>Acc.</cell><cell cols="3">R@1 R@10 R@100</cell><cell>Acc.</cell></row><row><cell></cell><cell>1</cell><cell>MLM</cell><cell>2.92 10.66</cell><cell>17.52</cell><cell>71.25</cell><cell>2.06</cell><cell>9.08</cell><cell>14.45</cell><cell>69.79</cell></row><row><cell></cell><cell>2</cell><cell>MLM + MNCE</cell><cell>3.13 10.92</cell><cell>17.52</cell><cell>71.99</cell><cell>2.15</cell><cell>9.27</cell><cell>14.98</cell><cell>70.13</cell></row><row><cell>TV</cell><cell>3</cell><cell>MLM + MNCE + FOM</cell><cell>3.09 10.27</cell><cell>17.43</cell><cell>72.54</cell><cell>2.36</cell><cell>9.85</cell><cell>15.97</cell><cell>70.85</cell></row><row><cell></cell><cell>4</cell><cell>MLM + MNCE + FOM + VSM</cell><cell>4.44 14.69</cell><cell>22.82</cell><cell>72.75</cell><cell cols="2">2.78 10.41</cell><cell>18.77</cell><cell>71.36</cell></row><row><cell></cell><cell>5</cell><cell cols="2">MLM + MNCE + FOM + VSM + MFFR 4.44 14.29</cell><cell>22.37</cell><cell>72.75</cell><cell cols="2">2.73 10.12</cell><cell>18.05</cell><cell>71.36</cell></row><row><cell>Howto100M</cell><cell>6</cell><cell>MLM + MNCE + FOM + VSM</cell><cell>3.81 13.23</cell><cell>21.63</cell><cell>73.34</cell><cell cols="2">3.54 12.90</cell><cell>20.85</cell><cell>73.68</cell></row><row><cell cols="2">TV + HowTo100M 7</cell><cell>MLM + MNCE + FOM + VSM</cell><cell>5.13 16.26</cell><cell>24.55</cell><cell>74.80</cell><cell cols="2">3.85 12.73</cell><cell>21.06</cell><cell>73.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc>summarizes ablation results on TVR, TVQA, How2R and How2QA under different pre-training settings. Models are trained on TV dataset only for computational efficiency. Compared to using MLM only (L1 in Table 1), adding MNCE (L2) shows improvement on all downstream tasks. The best performance is achieved by MLM + MNCE + FOM + VSM (L4).</figDesc><table><row><cell>Effect of FOM and VSM When MLM, MNCE</cell></row><row><cell>and FOM are jointly trained (L3), there is a large</cell></row><row><cell>performance gain on TVQA, and significant im-</cell></row><row><cell>provement on How2R and How2QA. Comparable</cell></row><row><cell>results are achieved on TVR. This indicates that</cell></row><row><cell>FOM, which models sequential characteristics of</cell></row><row><cell>video frames, can effectively benefit downstream</cell></row><row><cell>tasks that rely on temporal reasoning (such as QA</cell></row><row><cell>tasks).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Results on multi-channel (video+subtitle) tasks: TVR 12 , How2R, TVQA, How2QA, VIOLIN and TVC.</figDesc><table><row><cell>Method \Task</cell><cell>TVR</cell><cell></cell><cell></cell><cell>How2R</cell><cell></cell><cell cols="3">TVQA How2QA VIOLIN</cell><cell></cell><cell>TVC</cell></row><row><cell></cell><cell cols="5">R@1 R@10 R@100 R@1 R@10 R@100</cell><cell>Acc.</cell><cell>Acc.</cell><cell>Acc.</cell><cell cols="3">Bleu Rouge-L Meteor Cider</cell></row><row><cell cols="2">SOTA Baseline 3.25 13.41</cell><cell>30.52</cell><cell>2.06</cell><cell>8.96</cell><cell>13.27</cell><cell>70.23</cell><cell>-</cell><cell>67.84</cell><cell>10.87</cell><cell>32.81</cell><cell>16.91 45.38</cell></row><row><cell>HERO</cell><cell>6.21 19.34</cell><cell>36.66</cell><cell cols="2">3.85 12.73</cell><cell>21.06</cell><cell>73.61</cell><cell>73.81</cell><cell>68.59</cell><cell>12.35</cell><cell>34.16</cell><cell>17.64 49.98</cell></row><row><cell cols="2">(a) Method \Task</cell><cell>DiDeMo</cell><cell></cell><cell cols="3">DiDeMo w/ ASR</cell><cell cols="2">MSR-VTT</cell><cell cols="3">MSR-VTT w/ ASR</cell></row><row><cell></cell><cell cols="11">R@1 R@10 R@100 R@1 R@10 R@100 R@1 R@5 R@10 R@1 R@5 R@10</cell></row><row><cell cols="2">SOTA Baseline 1.59</cell><cell>6.71</cell><cell>25.44</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">14.90 40.20 52.80</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HERO</cell><cell cols="2">2.14 11.43</cell><cell>36.09</cell><cell cols="2">3.01 14.87</cell><cell>47.26</cell><cell cols="5">16.80 43.40 57.70 20.50 47.60 60.90</cell></row><row><cell cols="12">(b) Results on DiDeMo and MSR-VTT with video-only inputs (single-channel), compared with ASR-augmented inputs (multi-</cell></row><row><cell>channel).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Results on the test set of six downstream tasks, compared to task-specific state-of-the-art (SOTA) mod-</cell></row><row><cell>els: XML (Lei et al., 2020b) for TVR, How2R and DiDeMo, HowTo100M (Miech et al., 2019) for MSR-VTT,</cell></row><row><cell>STAGE (Lei et al., 2020a) for TVQA (inapplicable to How2QA due to region-level features), Multi-stream (Liu</cell></row><row><cell>et al., 2020) for VIOLIN, and MMT (Lei et al., 2020b) for TVC.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 reports</head><label>3</label><figDesc>HERO results on the test splits of all downstream tasks. HERO is pre-trained on both TV and HowTo100M datasets, with the optimal pre-training setting: MLM + MNCE + FOM + VSM. We compare HERO with task-specific</figDesc><table><row><cell>SOTA models on each downstream task, includ-</cell></row><row><cell>ing: XML (Lei et al., 2020b) for TVR, Didemo</cell></row><row><cell>and How2R; HowTo100M (Miech et al., 2019) for</cell></row><row><cell>MSR-VTT; STAGE (Lei et al., 2020a) for TVQA;</cell></row><row><cell>Multi-stream (Liu et al., 2020) for VIOLIN; and</cell></row><row><cell>MMT (Lei et al., 2020b) for TVC. Note that we</cell></row><row><cell>cannot directly apply STAGE to How2QA, as it</cell></row><row><cell>was specifically designed to leverage region-level</cell></row><row><cell>features. Our HERO model achieves new state of</cell></row><row><cell>the art across all benchmarks.</cell></row><row><cell>Results on Multi-channel Tasks Table 3a</cell></row><row><cell>shows results on downstream tasks consisting of</cell></row><row><cell>multi-channel videos (video + subtitle). On TVR</cell></row><row><cell>R@1, HERO results nearly double those from</cell></row><row><cell>XML. 12 Further, without leveraging fine-grained</cell></row><row><cell>region-level features, HERO outperforms baseline</cell></row><row><cell>models by +3.28% on TVQA and +0.75% on VI-</cell></row><row><cell>OLIN. When evaluated on TVC, video and subti-</cell></row><row><cell>tles are encoded by HERO, then fed into a 2-layer</cell></row><row><cell>Transformer decoder to generate captions. Even</cell></row><row><cell>though no pre-training was applied to the decoder,</cell></row><row><cell>HERO surpasses SOTA baseline across all metrics,</cell></row><row><cell>especially +4.60% on Cider. In addition, HERO</cell></row><row><cell>establishes a strong baseline for new benchmarks</cell></row><row><cell>How2R and How2QA.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Results on the validation set of six multi-channel video downstream tasks, compared to task-specific SOTA models: XML (Lei et al., 2020b) for TVR and How2R, STAGE (Lei et al., 2020a) for TVQA (inapplicable to How2QA due to region-level features), Multi-stream (Liu et al., 2020) for VIOLIN, and MMT (Lei et al., 2020b) for TVC. † indicates re-implementation of the model using our visual frame features.</figDesc><table><row><cell cols="2">Downstream Task Pre-training</cell><cell>Video Ret.</cell><cell></cell><cell cols="2">Moment Ret. 18</cell><cell cols="2">Video Moment Ret. 18</cell></row><row><cell></cell><cell></cell><cell cols="6">R@1 R@10 R@100 R@1 R@10 R@100 R@1 R@10 R@100</cell></row><row><cell>TVR</cell><cell>No 10 Yes</cell><cell>19.44 52.43 30.11 62.69</cell><cell>84.94 87.78</cell><cell>3.76 4.02 10.38 9.59</cell><cell>61.77 62.93</cell><cell>2.98 10.65 5.13 16.26</cell><cell>18.25 24.55</cell></row><row><cell>How2R</cell><cell>No 10 Yes</cell><cell>11.15 39.78 14.73 47.69</cell><cell>59.62 68.37</cell><cell>4.94 12.73 6.48 15.69</cell><cell>67.90 70.38</cell><cell>2.21 3.78 12.96 9.52</cell><cell>15.17 20.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Detailed results on TVR and How2R val set, including the main-task (Video Moment Retrieval) and two sub-tasks (Video Retrieval and Moment Retrieval).</figDesc><table><row><cell>Attention Pattern Visualization Following Ko-</cell></row><row><cell>valeva et al. (2019) and Chen et al. (2020b), we</cell></row><row><cell>analyze observable patterns in the attention maps</cell></row><row><cell>of HERO.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>TVQA(Lei et al., 2018)  was introduced along with the TV dataset. Given a video clip and the accompanying subtitles, the goal is to answer a multiplechoice question about the video. Each video clip has 7 questions, with 5 answers per question. The start/end points of relevant moments are provided for each question. 17 VIOLIN (Liu et al., 2020) is a new Video-and-Language Inference task. Given a video clip with aligned subtitles as premise, a model needs to infer whether a natural language hypothesis is entailed or contradicted by the given video clip. It consists of 95.3K video-hypothesis pairs from 15.9K video clips, split into 80% train, 10% val and 10% test.TVC(Lei et al., 2020b)is a multimodal Video Captioning dataset extended from TVR, containing 262K descriptions paired with 108K video moments. 17 Note that it differs from traditional video captioning tasks in that models are allowed to utilize subtitle texts as input.DiDeMo (Anne Hendricks et al., 2017a) is de-16 https://github.com/NVIDIA/apex 17 Train, val and test video splits are the same as TVR.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">ASR can be applied when subtitles are unavailable. arXiv:2005.00200v2 [cs.CV] 29 Sep 2020</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">N is a natural number, M is the number of masked tokens, and m is the set of masked indices.5  Following BERT, we decompose the 15% randomly masked-out words into 10% random words, 10% unchanged, and 80% [MASK].</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Timestamps are used to perform local alignment, which are either included with video (e.g., TV) or generated by ASR (e.g., HowTo100M). Refer to Appendix A.4 for details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">For VIOLIN, we report results on test set for fair comparison, since no validation results are reported inLiu et al.  (2020).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">https://pytorch.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18">During evaluation, the average recalls are calculated with tIoU&gt;0.7. we apply non-maximal suppression (nms) with threshold 0.5 to TVR and How2R predictions followingLei  et al. (2020b).</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Additional Experiments</head><p>For further analysis, <ref type="table">Table 4</ref> provides comparison between HERO and task-specific SOTA models on the validation splits of each downstream task. <ref type="bibr">13</ref> For fair comparison, we re-run XML <ref type="bibr">(Lei et al., 2020b)</ref> and <ref type="bibr">MMT (Lei et al., 2020b)</ref> experiments using our visual frame features, which achieve slightly better performance than the reported results in <ref type="bibr">Lei et al. (2020b)</ref>. Note that we cannot directly apply our frame-level visual features to <ref type="bibr">STAGE (Lei et al., 2020a)</ref> and <ref type="bibr">Multi-stream (Liu et al., 2020)</ref>, which require region-level features for each video frame.</p><p>Overall, HERO achieves state-of-the-art results on all downstream tasks. Our model consistently outperforms XML on both TVR and How2R, with or without pre-training.  <ref type="formula">(2019)</ref>, we assess the embeddings learned in pre-training before any fine-tuning occurs. On How2R, HERO without fine-tuning achieves (2.11, 9.09, 14.83) for (R1, R10, R100). While the performance is significantly lower than the fine-tuned model (-1.62 for R1), it performs reasonably well without seeing any How2R query, indicating that HERO has learned to align videos and subtitles (pseudo-query) during pre-training.</p><p>Note that for TVQA, STAGE is trained with additional supervision on spatial grounding, which requires region-level features for each frame of the video. Without additional supervision on spatial grounding or fine-grained region-level features, HERO is able to achieve better performance than STAGE on TVQA dataset. We also observe that pre-training significantly boosts the performance of HERO across TVR, How2R and TVQA tasks.</p><p>On How2QA, since STAGE was specifically designed to leverage region-level features, we cannot directly apply STAGE. Thus, we only compare HERO performance w/o and with pre-training. Results exhibit consistent patterns observed on other downstream tasks: pre-training achieves better performance than w/o pre-training.</p></div>			</div>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
