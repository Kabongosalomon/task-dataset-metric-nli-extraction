<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quasi-Dense Similarity Learning for Multiple Object Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlu</forename><surname>Qiu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">ETH Zürich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
							<affiliation key="aff4">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">ETH Zürich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Quasi-Dense Similarity Learning for Multiple Object Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Similarity learning has been recognized as a crucial step for object tracking. However, existing multiple object tracking methods only use sparse ground truth matching as the training objective, while ignoring the majority of the informative regions on the images. In this paper, we present Quasi-Dense Similarity Learning, which densely samples hundreds of region proposals on a pair of images for contrastive learning. We can directly combine this similarity learning with existing detection methods to build Quasi-Dense Tracking (QDTrack) without turning to displacement regression or motion priors. We also find that the resulting distinctive feature space admits a simple nearest neighbor search at the inference time. Despite its simplicity, QDTrack outperforms all existing methods on MOT, BDD100K, Waymo, and TAO tracking benchmarks. It achieves 68.7 MOTA at 20.3 FPS on MOT17 without using external training data. Compared to methods with similar detectors, it boosts almost 10 points of MOTA and significantly decreases the number of ID switches on BDD100K and Waymo datasets. 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multiple Object Tracking (MOT) is a fundamental and challenging problem in computer vision, widely used in safety monitoring, autonomous driving, video analytics, and other applications. Contemporary MOT methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b45">46]</ref> mainly follow the tracking-by-detection paradigm <ref type="bibr" target="#b37">[38]</ref>. That is, they detect objects on each frame and then associate them according to the estimated instance similarity. Recent works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b55">56]</ref> show that if the detected objects are accurate, the spatial proximity between objects in consecutive frames, measured by Interaction of Unions (IoUs) or center distances, is a strong prior to associate the objects. However, this location heuristic only works well in simple scenarios. If the objects are occluded or the scenes are <ref type="bibr" target="#b0">1</ref> The code is available at https : / / github . com / SysCV / qdtrack crowded, this location heuristic can easily lead to mistakes. To remedy this problem, some methods introduce motion estimation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9]</ref> or displacement regression <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b36">37]</ref> to ensure accurate distance estimation.</p><p>However, object appearance similarity usually takes a secondary role <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b46">47]</ref> to strengthen object association or reidentify vanished objects. The search region is constrained to be local neighborhoods to avoid distractions because the appearance features can not effectively distinguish different objects. On the contrary, humans can easily associate the identical objects only through appearance. We conjecture this is because the image and object information is not fully utilized for learning object similarity. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, previous methods regard instance similarity learning as a post hoc stage after object detection or only use sparse ground truth bounding boxes as training samples <ref type="bibr" target="#b46">[47]</ref>. These processes ignore the majority of the regions proposed on the images. Because objects in an image are rarely iden-tical to each other, if the object representation is properly learned, a nearest neighbor search in the embedding space should associate and distinguish instances without bells and whistles.</p><p>We observe that besides the ground truths and detected bounding boxes, which sparsely distribute on the images, many possible object regions can provide valuable training supervision. They are either close to the ground truth bounding boxes to provide more positive training examples or in the background as negative examples. In this paper, we propose quasi-dense contrastive learning, which densely matches hundreds of regions of interest on a pair of images and learns the metric with contrastive loss. The quasi-dense samples can cover most of the informative regions on the images, providing both more box examples and hard negatives.</p><p>Because one sample has more than one positive samples on the reference image, we extend the contrastive learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b48">49]</ref> to multiple positive forms that makes the quasi-dense learning feasible. Each sample is thus trained to distinguish all proposals on the other image simultaneously. This contrast provides stronger supervision than using only the handful ground truth labels and enhances the instance similarity learning.</p><p>The inference process, which maintains the matching candidates and measures the instance similarity, also plays an important role in the tracking performance. Besides similarity, MOT also needs to consider false positives, id switches, new appeared objects, and terminated tracks. To tackle the missing targets with our similarity metric, we include backdrops, the unmatched objects in the last frame, for matching and use bi-directional softmax to enforce the bi-directional consistency. The no target objects lack the consistency thus has low similarity scores to any objects. To track the multiple targets, we also conduct duplicate removal to filter the matching candidates.</p><p>Quasi-dense contrastive learning can be easily used with most existing detectors since generating region of interests is widely used in object detection algorithms. In this paper, we apply our method to Faster R-CNN <ref type="bibr" target="#b38">[39]</ref> along with a lightweight embedding extractor and residual networks <ref type="bibr" target="#b16">[17]</ref> and build Quasi-Dense Tracking (QDTrack) models. We conduct extensive experiments on MOT <ref type="bibr" target="#b30">[31]</ref>, BDD100K <ref type="bibr" target="#b52">[53]</ref>, Waymo <ref type="bibr" target="#b42">[43]</ref>, and TAO <ref type="bibr" target="#b9">[10]</ref> tracking benchmarks. Despite its simplicity, QDTrack outperforms all existing methods without bells and whistles. It achieves 68.7 MOTA on MOT17 at 20.3 FPS without using external training data. Moreover, it boosts almost 10 points of MOTA and significantly decreases the number of ID switches on BDD100K and Waymo datasets, establishing solid records on these brandnew large-scale benchmarks. QDTrack allows end-to-end training, thereby simplifying the training and testing procedures of multi-object tracking frameworks. The simplicity and effectiveness shall benefit further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Recent developments in multiple object tracking <ref type="bibr" target="#b24">[25]</ref> follow the tracking-by-detection paradigm <ref type="bibr" target="#b37">[38]</ref>. These approaches present different methods to estimate the instance similarity between detected objects and previous tracks, then associate objects as a bipartite matching problem <ref type="bibr" target="#b32">[33]</ref>. Location and motion in MOT The spatial proximity has been proven effective to associate objects in consecutive frames <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. However, they cannot do well in complicated scenarios such as crowd scenes. Some methods use motion priors, such as Kalman Filter <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b53">54]</ref>, optical flow <ref type="bibr" target="#b49">[50]</ref>, and displacement regression <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b11">12]</ref>, to ensure accurate distance estimations. In contrast to the old paradigm that detects objects and predicts displacements separately, Detect &amp; Track <ref type="bibr" target="#b11">[12]</ref> is the first work that jointly optimizes object detection and tracking modules. It predicts the displacements of the objects in consecutive frames and associates the objects with the Viterbi algorithm. Tracktor <ref type="bibr" target="#b2">[3]</ref> directly adopts a detector as a tracker. CenterTrack <ref type="bibr" target="#b55">[56]</ref> and Chained-Tracker <ref type="bibr" target="#b36">[37]</ref> predict the object displacements with pair-wise inputs to associate the objects. Although these methods show promising results, they <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b2">3]</ref> still need an extra re-identification model as complementary to re-identify vanished objects, making the entire framework complicated. Appearance similarity in MOT To exploit instance appearance similarity to strengthen tracking and re-identify vanished objects, some methods directly use an independent model <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b2">3]</ref> or add an extra embedding head to the detector for end-to-end training <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b54">55]</ref>. However, they still learn the appearance similarity following the practice in image similarity learning, then measure the instance similarity by cosine distance. That is, they train the model either as a n-classes classification problem <ref type="bibr" target="#b46">[47]</ref> where n equals to the number of identities in the whole training set or using triplet loss <ref type="bibr" target="#b19">[20]</ref>. The classification problem is hard to extend to large-scale datasets, while the triplet loss only compares each training sample with two other identities. These rudimentary training samples and objectives leave instance similarity learning not fully explored in MOT. Meanwhile, they still heavily rely on motion models and displacement predictions to track objects, and the appearance similarity only takes the secondary role.</p><p>In contrast to these methods, QDTrack learns the instance similarity from dense-connected contrastive pairs and associates objects from the feature space with a simple nearest neighbor search. QDTrack has higher performance but with a simpler framework. The promising results prove the power of quasi-dense similarity learning in multiple object tracking. Contrastive learning Contrastive learning and its variants <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b43">44]</ref> have shown promising performance in self-supervised representation learning. However, it does not draw much attention when learning the instance similarity in multiple object tracking. In this pa-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiple Positive Contrastive Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparse GTs</head><p>Quasi-Dense Samples Key Frame</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference Frame Backbone</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample Generator</head><p>Embedding Head shared shared <ref type="figure">Figure 2</ref>: The training pipeline of our method. We apply dense matching between quasi-dense samples on the pair of images and optimize the network with multiple positive contrastive learning.</p><p>per, we supervise dense matched quasi-dense samples with multiple positive contrastive learning by the inspiration of <ref type="bibr" target="#b43">[44]</ref>. In contrast to these image-level contrastive methods, our method allows multiple positive training, while these methods can only handle the case when there is only one positive target. The promising results of our method shall draw the attention to contrastive learning in the multiple object tracking community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>We propose quasi-dense matching to learn the feature embedding space that can associate identical objects and distinguish different objects for online multiple object tracking. We define dense matching to be matching between box candidates at all pixels, and quasi-dense means only considering the potential object candidates at informative regions. Accordingly, sparse matching means the method only considers ground truth labels as matching candidates when learning object association. The main ingredients of using quasi-dense matching for multiple object tracking are object detection, instance similarity learning, and object association.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Object detection</head><p>Our method can be easily coupled with most existing detectors and admit end-to-end training. In this paper, we take Faster R-CNN <ref type="bibr" target="#b38">[39]</ref> with Feature Pyramid Network (FPN) <ref type="bibr" target="#b25">[26]</ref> as an example, while we can also apply other detectors with minor modifications. Faster R-CNN is a twostage detector that uses Region Proposal Network (RPN) to generate Region of Interests (RoIs). It then localizes and classifies the regions to obtain semantic labels and locations. Based on Faster R-CNN, FPN exploits lateral connections to build the top-down feature pyramid and tackles the scalevariance problem. The entire network is optimized with a multi-task loss function</p><formula xml:id="formula_0">L det = L rpn + λ 1 L cls + λ 2 L reg ,<label>(1)</label></formula><p>where the RPN loss L rpn , classification loss L cls , regression loss L reg remain the same as the original paper <ref type="bibr" target="#b38">[39]</ref>. The loss weights λ 1 and λ 2 are set to 1.0 by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Quasi-dense similarity learning</head><p>We use the region proposals generated by RPN to learn the instance similarity with quasi-dense matching. As shown in <ref type="figure">Figure 2</ref>, given a key image I 1 for training, we randomly select a reference image I 2 from its temporal neighborhood. The neighbor distance is constrained by an interval k, where k ∈ [−3, 3] in our experiments. We use RPN to generate RoIs from the two images and RoI Align <ref type="bibr" target="#b15">[16]</ref> to obtain their feature maps from different levels in FPN according to their scales <ref type="bibr" target="#b25">[26]</ref>. We add an extra lightweight embedding head, in parallel with the original bounding box head, to extract features for each RoI. An RoI is defined as positive to an object if they have an IoU higher than α 1 , or negative if they have an IoU lower than α 2 . α 1 and α 2 are 0.7 and 0.3 in our experiments. The matching of RoIs on two frames is positive if the two regions are associated with the same object and negative otherwise.</p><p>Assume there are V samples on the key frame as training samples and K samples on the reference frame as contrastive targets. For each training sample, we can use the non-parametric softmax <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b33">34]</ref> with cross-entropy to optimize the feature embeddings</p><formula xml:id="formula_1">L embed = −log exp(v · k + ) exp(v · k + ) + k − exp(v · k − ) ,<label>(2)</label></formula><p>where v, k + , k − are feature embeddings of the training sample, its positive target, and negative targets in K.  <ref type="figure">Figure 3</ref>: The testing pipeline of our method. We maintain the matching candidates and use bi-softmax to measure the instance similarity so that we can associate objects with a simple nearest neighbour search in the feature space.</p><p>embedding loss is averaged across all training samples, but we only illustrate one training sample for simplicity. We apply dense matching between RoIs on the pairs of images, namely, each sample on I 1 is matched to all samples on I 2 , in contrast to only using sparse sample crops, mostly ground truth boxes, to learn instance similarity in previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref>. Each training sample on the key frame has more than one positive targets on the reference frame, so Eq. 2 can be extended as</p><formula xml:id="formula_2">L embed = − k + log exp(v · k + ) exp(v · k + ) + k − exp(v · k − ) .<label>(3)</label></formula><p>However, this equation does not treat positive and negative targets fairly. Namely, each negative one is considered multiple times while only once for positive counterparts. Alternatively, we can first reformulate Eq. 2 as</p><formula xml:id="formula_3">L embed = log[1 + k − exp(v · k − − v · k + )].<label>(4)</label></formula><p>Then in the multi-positive scenario, it can be extended by accumulating the positive term as</p><formula xml:id="formula_4">L embed = log[1 + k + k − exp(v · k − − v · k + )].<label>(5)</label></formula><p>We further adopt L2 loss as an auxiliary loss</p><formula xml:id="formula_5">L aux = ( v · k ||v|| · ||k|| − c) 2 ,<label>(6)</label></formula><p>where c is 1 if the match of two samples is positive and 0 otherwise. Note the auxiliary loss aims to constrain the logit magnitude and cosine similarity instead of improving the performance.</p><p>The entire network is joint optimized under</p><formula xml:id="formula_6">L = L det + γ 1 L embed + γ 2 L aux ,<label>(7)</label></formula><p>where γ 1 and γ 2 are set to 0.25 and 1.0 by default in this paper. We sample all positive pairs and three times more negative pairs to calculate the auxiliary loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Object association</head><p>Tracking objects across frames purely based on object feature embeddings is not trivial. For example, if an object has no target or more than one target during matching, the nearest search will be ambiguous. In other words, an object should have only one target in the matching candidates. However, the actual tracking process is complex. The false positives, id switches, newly appeared objects, and terminated tracks all increase the matching uncertainty. We observe that our inference strategy, including ways of maintaining the matching candidates and measuring the instance similarity, can mitigate these problems. Bi-directional softmax Our main inference strategy is bidirectional matching in the embedding space. <ref type="figure">Figure 3</ref> shows our testing pipeline. Assume there are N detected objects in frame t with feature embeddings n, and M matching candidates with feature embeddings m from the past x frames, the similarity f between the objects and matching candidates is obtained by bi-directional softmax (bi-softmax):</p><formula xml:id="formula_7">f(i, j) = [ exp(n i · m j ) M −1 k=0 exp(n i · m k ) + exp(n i · m j ) N −1 k=0 exp(n k · m j ) ]/2.<label>(8)</label></formula><p>The high score under bi-softmax will satisfy a bi-directional consistency. Namely, the two matched objects should be each other's nearest neighbor in the embedding space. The instance similarity f can directly associate objects with a simple nearest neighbor search. No target cases Objects without a target in the feature space should not be matched to any candidates. Newly appeared objects, vanished tracks, and some false positives fall into this category. The bi-softmax can tackle this problem directly, as It is hard for these objects to obtain bi-directional consistency, leading to low matching scores. If a newly detected object has high detection confidence, it can start a new track. However, previous methods often directly drop the objects that do not match any tracks. We argue that despite most of them are false positives, they are still useful regions that the following objects are likely to match. We name these unmatched objects backdrops and keep them during matching. Experiments show that backdrops can reduce the number of false positives.</p><p>Multi-targets cases Most states of the art detectors only do intra-class duplicate removal by None Maximum Suppression (NMS). Consequently, some objects may have different categories but the same locations. In most cases, only one of these objects is true positive while the others not. This process can boost the object recall and contribute to a high mean Average Precision (mAP) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27]</ref>. However, it will create duplicate feature embeddings. To handle this issue, we do inter-class duplicate removal by NMS. The IoU threshold for NMS is 0.7 for objects with high detection confidence (larger than 0.5) and 0.3 for objects with low detection confidence (lower than 0.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We first conduct experiments on MOT <ref type="bibr" target="#b30">[31]</ref>, the most popular benchmark in multiple object tracking to compare with recent tracking works. We also like to test on larger scale and more diverse datasets to study the efficacy of quasi-dense similarity learning. Therefore, we conduct experiments on the brand-new large-scale BDD100K, Waymo, and TAO tracking benchmarks, and establish competitive records. We hope our efforts can facilitate further research in multiple object tracking. We also show the generalization ability of our method on the BDD100K segmentation tracking benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>MOT We perform experiments on two MOT benchmarks: MOT17 and MOT16 <ref type="bibr" target="#b30">[31]</ref>. The dataset contains 7 videos (5,316 images) for training and 7 videos (5,919 images) for testing. Only pedestrians are evaluated in this benchmark. The video frame rate is 14 -30 FPS. BDD100K We use BDD100K <ref type="bibr" target="#b52">[53]</ref>   <ref type="bibr" target="#b9">[10]</ref> annotates 482 classes in total, which are the subset of LVIS dataset <ref type="bibr" target="#b12">[13]</ref>. It has 400 videos, 216 classes in the training set, 988 videos, 302 classes in the validation set, and 1419 videos, 369 classes in the test set. The classes in train, validation, and test sets may not overlap. The videos are annotated in 1 FPS. The objects in TAO are in a long-tailed distribution that half of the objects are person and 1 / 6 of the objects are car.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>We use ResNet-50 <ref type="bibr" target="#b16">[17]</ref> as backbone. We select 128 RoIs from the key frame as training samples, and 256 RoIs from the reference frame with a positive-negative ratio of 1.0 as contrastive targets. We use IoU-balanced sampling <ref type="bibr" target="#b35">[36]</ref> to sample RoIs. We use 4conv-1fc head with group normalization <ref type="bibr" target="#b47">[48]</ref> to extract feature embeddings. The channel number of embedding features is set to 256 by default. We train our models with a total batch size of 16 and an initial learning rate of 0.02 for 12 epochs. We decrease the learning rate by 0.1 after 8 and 11 epochs.</p><p>Here, we first talk about the other common practice of our method if not specified mentioned below. We use the original scale of the images for training and inference. We do not use any other data augmentation methods except random horizontal flipping. We use a model pre-trained on ImageNet for training. When conducting online joint object detection and tracking, we initialize a new track if its detection confidence is higher than 0.8. Other confidence thresholds are all set as 0.5 in this paper if not mentioned. The objects can be associated only when they are classified as the same category. The feature embeddings of each identity 2 https://github.com/cheind/py-motmetrics are updated online with a momentum of 0.8.</p><p>For fair comparison with recent works, we follow the practice <ref type="bibr" target="#b45">[46]</ref> on MOT17 that randomly resizes and crops the longer side of the images to 1088 and does not change the aspect ratio at the training and inference time. Other data augmentation includes random horizontal flipping and color jittering, which is the common practice in <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b36">37]</ref>. We do not use extra data for training except a pre-trained model from COCO. Note that COCO is not considered as additional training data by the official rules and widely used in most methods.</p><p>On TAO, we randomly select a scale between 640 to 800 to resize the shorter side of images during training. At inference time, the shorter side of the images are resized to 800. We use a LVIS <ref type="bibr" target="#b12">[13]</ref> pre-trained model, consistent with the implementation of <ref type="bibr" target="#b9">[10]</ref>. However, we observe severe over-fitting problem when training on the training videos of TAO, which hurts the detection performance. So we freeze the detection model and only fine-tune the embedding head to extract instance representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Main results</head><p>Our method outperforms all existing methods on aforementioned benchmarks without bells and whistles. The per-   <ref type="table" target="#tab_3">Table 2</ref>. The mMOTA and mIDF1, which represent object coverage and identity consistency respectively, are 36.6% and 50.8% on the validation set, and 35.5% and 52.3% on the testing set. On the two sets, our method outperforms the baseline benchmark method by 10.7 points and 9.2 points in terms of mMOTA, and 6.3 points and 7.6 points in terms of mIDF1 respectively. We also outperform the champion of BDD100K 2020 MOT Challenge (madamada) by a large margin but with a simpler detector. The significant advancements demonstrate that our method enables more stable object tracking.</p><p>Waymo <ref type="table" target="#tab_4">Table 3</ref> shows our main results on Waymo open dataset. We report the results on the validation set following the setup of RetinaTrack <ref type="bibr" target="#b28">[29]</ref>, which only conduct experiments on the vehicle class. We also report the overall performance for future comparison. We report the results on the test set via official rules. Our method outperforms all baselines on both validation set and test set. We obtain a MOTA of 44.0% and a IDF1 of 56.8% on the validation set. We also obtain a MOTA/L1 of 49.40% and a MOTA/L2 of 43.88% on the test set. The performance of vehicle on the validation set is 10.7, 13.0, and 17.4 points higher than RetinaTrack <ref type="bibr" target="#b28">[29]</ref>, Tracktor++ <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29]</ref>, and IoU baseline <ref type="bibr" target="#b28">[29]</ref>, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation studies</head><p>We conduct ablation studies on BDD100K validation set, where we investigate the importance of the major model components for training and testing procedures. Importance of quasi-dense matching The results are presented in the top sub <ref type="table" target="#tab_5">-table of Table 4</ref>. MOTA and IDF1 are calculated over all instances without considering categories as overall evaluations. We use cosine distance to calculate the similarity scores during the inference procedure. Compared to learning with sparse ground truths, quasi-dense tracking improves the overall IDF1 by 4.8 points (63.0% to 67.8%). The significant improvement on IDF1 indicates quasi-dense tracking greatly improves the feature embeddings and enables more accurate associations.</p><p>We then analyze the improvements in detail. In the table, we can observe that when we match each training sample  to more negative samples and train the feature space with Equation 2, the IDF1 is significantly improved by 3.4 points. This improvement contributes 70% to the total improved 4.8 points IDF1. This experiment shows that more contrastive targets, even most of them are negative samples, can improve the feature learning process. The multiple-positive contrastive learning following Equation 5 further improves the IDF1 by 1 point (66.8% to 67.8%). Importance of bi-softmax We investigate how different inference strategies influence the performance. As shown in the bottom part of <ref type="table" target="#tab_5">Table 4</ref>, replacing cosine similarity by bi-softmax improves overall IDF1 by 2.2 points and the IDF1 of pedestrian by 4.5 points. This experiment also shows that the one-to-one constraint further strengthens the estimated similarity. Importance of matching candidates Duplicate removal and backdrops improve IDF1 by 1.5 points. Overall, our training and inference strategies improve the IDF1 by 8.5 points (63.0% to 71.5%). The total number of ID switches is decreased by 30%. Especially, the MOTA and IDF1 of pedestrian are improved by 9.1 points and 10.5 points respectively, which further demonstrate the power of quasi-dense contrastive learning. Combinations with motion and location Finally, we try to add the location and motion priors to understand whether they are still helpful when we have good feature embeddings for similarity measure. These experiments follow the procedures in Tracktor <ref type="bibr" target="#b2">[3]</ref> and use the same detector for fair comparisons. As shown in <ref type="table" target="#tab_6">Table 5</ref>, without appearance features, the tracking performance is consistently improved with the introduction of additional information. However, these cues barely enhance the performance of our approach. Our method yields the best results when only using appearance embeddings. The results indicate that our instance feature embeddings are sufficient for multiple object tracking with the effective quasi-dense matching, which greatly simplify the testing pipeline. Inference speed To understand the runtime efficiency, we profile our method on NVIDIA Tesla V100. Because it only adds a lightweight embedding head to Faster R-CNN, our method only bring marginal inference cost overhead. With an input size of 1296 × 720 and a ResNet-50 backbone on BDD100K, the inference FPS is 16.4. With an input size </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Embedding visualizations</head><p>We visualize the instance embeddings that trained with sparse matching and quasi-dense matching using t-SNE in <ref type="figure" target="#fig_2">Figure 4</ref>. The same instance is shown with the same color. We observe that it is easier to separate objects in the feature space of quasi-dense matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Segmentation tracking</head><p>We show the generalization ability of our method by extending it to instance segmentation tracking. BDD100K provides a subset for the segmentation tracking task. There are 154 videos (30,817 images) in the training set and 32 videos (6,475 images) in the validation set. <ref type="table" target="#tab_8">Table 6</ref> shows the results on BDD100K segmentation tracking task. "-fix" means adopting the pretrained model from the BDD100K tracking set, fixing the existing parts, and only training the added mask head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present QDTrack, a tracking method based on quasidense matching for instance similarity learning. In contrast to previous methods that use sparse ground-truth matching as similarity supervision, we learn instance similarity from hundreds of region proposals on pairs of images, and train the feature embeddings with multiple positive contrastive learning. In the resulting feature space, a simple nearest neighbor search can distinguish instances without bells and whistles. Our method can be easily coupled with most of the existing detectors and trained end-to-end for multiple object tracking and segmentation tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>In this appendix, we present additional experiments, investigate oracle performance, analyze failure cases, and show some patch visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Supplementary experiments</head><p>MOT17 with public detectors Following the strategy in Tracktor <ref type="bibr" target="#b2">[3]</ref> and CenterTrack <ref type="bibr" target="#b55">[56]</ref>, we evaluate our method with public detectors on MOT17. That is, a new trajectory is only initialized from a public detection bounding box. As shown in <ref type="table" target="#tab_10">Table 8</ref>, our method outperforms existing results by a large margin. Our method outperforms CenterTrack by 3.1 points on MOTA and 5.5 points on IDF1. TAO <ref type="table" target="#tab_9">Table 7</ref> presents detailed results on the TAO <ref type="bibr" target="#b9">[10]</ref> dataset. Although QDTrack does not perform zero-shot and few-shot learning for the long-tail categories, our method is still a stronger baseline method on this dataset and paves the way for future studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Oracle analysis</head><p>We investigate the performances of two types of oracles: detection oracle and tracking oracle on BDD100K tracking validation set. For detection oracle, we directly extract feature embeddings of the ground truth objects in each frame and associate them using our method. For tracking oracle, we use ground truth tracking labels to associate the detected objects. Detection oracle The results are shown in <ref type="table" target="#tab_11">Table 9</ref>. We can observe that all MOTAs are higher than 94%, and some of them are even close to 100%. This is because we use the ground truth boxes directly so that the number of false negatives and false positives are close to 0. The metric IDF1 and ID Switches can measure the performance of identity consistency. The average IDF1 over the 8 classes is 88.8%, which is 38 points higher than our result. The gaps on classes "car" and "pedestrain" are only 11.1 points and 19.3 points between oracle results and our results respectively, while gaps on other classes are exceeding 30 points. These results show that if highly accurate detection results are provided, our method can obtain robust feature embeddings and associate objects effectively. However, the huge performance gaps also indicate the demand of promoting detection algorithms in the video domain. We also notice that the total number of ID switches in the oracle experiment is higher than ours. This is due to the high object recalls in the oracle experiments, as more detected instances may introduce more ID switches accordingly. Tracking oracle The results are shown in <ref type="table" target="#tab_1">Table 10</ref>. We can observe that when associating object directly with tracking labels, the mIDF1 is only boosted by 4.3 points. This promising oracle analysis shows the effectiveness of our method and indicates that our method is bounded more by detection performance than tracking performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Failure case analysis</head><p>Our method can distinguish different instances even they are similar in appearance. However, there are still some failure cases. We show them below with figures, in which we use yellow color to represent false negatives, red color to represent false positives, and cyan color to represent ID switches. The float number at the corner of each box indicates the detection score, while the integer indicates the object identity number. We use green dashed box to highlight the objects we want to emphasize. Object classification Inaccurate classification confidence is the main distraction for the association procedure because false negatives and false positives destroy the one-to-one matching constraint. As shown in <ref type="figure" target="#fig_3">Figure 5</ref>, the false negatives are mainly small objects or occluded objects under crowd scenes. The false positives are objects that have similar appearances to annotated objects, such as persons in the mirror or advertising board, etc.</p><p>Inaccurate object category is a less frequent distraction   caused by classification. The class of the instance may switch between different categories, which mostly belong to the same super-category. <ref type="figure" target="#fig_4">Figure 6</ref> shows an example. The category of the highlighted object changes from "rider" to "pedestrian" when the bicycle is occluded. Our method fails in this case because we require the associated objects have the same category. These failure cases caused by object classification suggest the improvements on video object detection algorithms. We can exploit temporal or tracking information to improve the detectors, thus obtaining better tracking performance. Object truncation/occlusion Object truncation/occlusion causes inaccurate object localization. As shown in <ref type="figure" target="#fig_5">Figure 7</ref>, the highlighted objects are truncated by other objects. The detector detects two objects. One of them is a false positive box that only covers a part of the object. The other one is a box with a lower detection score but covers the entire object. This case may influence the association process if the two boxes have similar feature embeddings. An instance may have totally different appearances before and after occlusion that result in low similarity scores. As shown in <ref type="figure" target="#fig_7">Figure 8</ref>, only the front of the car appears before occlusion, while only the rear of the car appears after occlusion. Our method can associate two boxes if they cover the same discriminative regions of an object, not necessarily the exact same region. However, if two boxes cover totally different regions of the object, they will have a low matching score.</p><p>Another corner case is the extreme high-level truncation. As shown in <ref type="figure">Figure 9</ref>, the highly truncated objects only appear a little when they just enter or leave the camera view. We cannot distinguish different instances effectively according to the limited appearance information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Visualizations</head><p>We show the visualizations of different instance patches during the testing procedure in <ref type="figure" target="#fig_0">Figure 10</ref>. The detected objects in each frame are matched to prior objects via bidirectional softmax. The prior objects include tracks in the consecutive frame, vanished tracks, and backdrops. We annotate them with different colors. Each detected object is enclosed by the same color of its matched object. We can observe that most false positives in the current frame are matched to backdrops, which demonstrates keeping backdrops during the matching procedure helps reduce the number of false positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Qualitative results</head><p>We show some qualitative results of our method on BDD100K dataset and MOT17 dataset in <ref type="figure" target="#fig_0">Figure 11</ref> and       </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Traditional ReID model that decouples with detector and learns with sparse ID loss; (b) joint learning ReID model with sparse ID loss; (c) joint learning ReID model with sparse triplet loss; (d) our quasi-dense similarity learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Visualizations of instance embeddings with (a) sparse matching and (b) quasi-dense matching using t-SNE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Failure cases caused by inaccurate classification confidences. The objects enclosed by yellow rectangles are false negatives, and the objects enclosed by red rectangles are false positives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Failure case caused by inaccurate object category. The category of the highlighted object changes from "rider" to "pedestrian" due to the occlusion of the bicycle. They cannot be associated because they do not satisfy the category consistency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Inaccurate object localization caused by truncation. The red false positive box only covers part of the object, while the yellow box covers the entire object. They may have similar feature embeddings thus influencing the association procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 12</head><label>12</label><figDesc>respectively. The results are sampled from a certain interval for illustrative purposes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Two detected objects in different frames cover totally different regions of the object thus having low appearance similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :Figure 10 :Figure 11 :</head><label>91011</label><figDesc>Our method cannot distinguish different instances effectively according to the limited appearance information in highly truncated objects. The visualizations of different instance patches during the testing procedure. The detected objects in the current frame are matched to tracklets in the consecutive frame, vanished tracklets, and backdrops via bi-directional softmax Qualitative results of our method on BDD100K dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Qualitative results of our method on MOT17 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The overall</figDesc><table><row><cell></cell><cell>Detections</cell><cell>Tracks</cell><cell cols="2">Vanished Tracks</cell><cell>Backdrops</cell><cell>Matching Candidates</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Consistent</cell><cell></cell></row><row><cell>Reference Frames</cell><cell>shared</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>High Similarity</cell><cell>w/o Backdrops</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Vanished Object</cell></row><row><cell></cell><cell></cell><cell cols="2">Bi-directional Softmax</cell><cell>Inconsistent</cell><cell></cell></row><row><cell></cell><cell>Embedding</cell><cell></cell><cell></cell><cell></cell><cell>New Object</cell><cell>Low Similarity</cell></row><row><cell>Key Frame</cell><cell>Extractor</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>w/ Backdrops</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results on MOT16 and MOT17 object tracking benchmark test set. Note that we do not use extra data for training. ↑ means higher is better, ↓ means lower is better. * means external data besides COCO and ImageNet is used.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>MOTA ↑</cell><cell>IDF1 ↑</cell><cell>MOTP ↑</cell><cell>MT ↑</cell><cell>ML ↓</cell><cell>FP ↓</cell><cell>FN ↓</cell><cell>IDs ↓</cell></row><row><cell></cell><cell>TAP [57]</cell><cell>64.8</cell><cell>73.5</cell><cell>78.7</cell><cell>292</cell><cell>164</cell><cell>12980</cell><cell>50635</cell><cell>571</cell></row><row><cell></cell><cell>CNNMTT [30]</cell><cell>65.2</cell><cell>62.2</cell><cell>78.4</cell><cell>246</cell><cell>162</cell><cell>6578</cell><cell>55896</cell><cell>946</cell></row><row><cell>MOT16</cell><cell>POI  *  [54] TubeTK_POI  *  [35]</cell><cell>66.1 66.9</cell><cell>65.1 62.2</cell><cell>79.5 78.5</cell><cell>258 296</cell><cell>158 122</cell><cell>5061 11544</cell><cell>55914 47502</cell><cell>3093 1236</cell></row><row><cell></cell><cell>CTrackerV1 [37]</cell><cell>67.6</cell><cell>57.2</cell><cell>78.4</cell><cell>250</cell><cell>175</cell><cell>8934</cell><cell>48305</cell><cell>1897</cell></row><row><cell></cell><cell>Ours</cell><cell>69.8</cell><cell>67.1</cell><cell>79.0</cell><cell>316</cell><cell>150</cell><cell>9861</cell><cell>44050</cell><cell>1097</cell></row><row><cell></cell><cell>Tracktor++v2 [3]</cell><cell>56.3</cell><cell>55.1</cell><cell>78.8</cell><cell>498</cell><cell>831</cell><cell>8866</cell><cell>235449</cell><cell>1987</cell></row><row><cell></cell><cell>Lif_T  *  [21]</cell><cell>60.5</cell><cell>65.6</cell><cell>78.3</cell><cell>637</cell><cell>791</cell><cell>14966</cell><cell>206619</cell><cell>1189</cell></row><row><cell>MOT17</cell><cell>TubeTK  *  [35] CTrackerV1 [37]</cell><cell>63.0 66.6</cell><cell>58.6 57.4</cell><cell>78.3 78.2</cell><cell>735 759</cell><cell>468 570</cell><cell>27060 22284</cell><cell>177483 160491</cell><cell>4137 5529</cell></row><row><cell></cell><cell>CenterTrack  *  [56]</cell><cell>67.8</cell><cell>64.7</cell><cell>78.4</cell><cell>816</cell><cell>579</cell><cell>18498</cell><cell>160332</cell><cell>3039</cell></row><row><cell></cell><cell>Ours</cell><cell>68.7</cell><cell>66.3</cell><cell>79.0</cell><cell>957</cell><cell>516</cell><cell>26589</cell><cell>146643</cell><cell>3378</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>detection training set and tracking training set for training, and tracking validation/testing set for testing. It annotates 8 categories for evaluation. The detection set has 70,000 images. The tracking set has 1,400 videos (278k images) for training, 200 videos (40k images) for validation, and 400 videos (80k images) for testing. The images in the tracking set are annotated per 5 FPS with a 30 FPS video frame rate. Waymo Waymo open dataset [43] contains images from 5 cameras associated with 5 different directions: front, front left, front right, side left, and side right. There are 3,990 videos (790k images) for training, 1,010 videos (200k images) for validation, and 750 videos (148k images) for testing. It annotates 3 classes for evaluation. The videos are annotated in 10 FPS.</figDesc><table /><note>TAO TAO dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on BDD100K tracking validation and test set. mMOTA and mIDF1 are averages over 8 categories while MOTA and IDF1 indicates the overall performance without considering categories. Our method outperforms the champion of BDD100K 2020 Tracking Challenge (madamada) only with a simple single model.MethodSplitmMOTA ↑ mIDF1 ↑ MOTA ↑ IDF1 ↑ FN ↓ FP ↓ ID Sw. ↓ MT ↑ ML ↓ mAP ↑</figDesc><table><row><cell>Yu et al. [53]</cell><cell>val</cell><cell>25.9</cell><cell>44.5</cell><cell>56.9</cell><cell>66.8</cell><cell>122406 52372</cell><cell>8315</cell><cell>8396</cell><cell>3795</cell><cell>28.1</cell></row><row><cell>Ours</cell><cell>val</cell><cell>36.6</cell><cell>50.8</cell><cell>63.5</cell><cell>71.5</cell><cell>108614 46621</cell><cell>6262</cell><cell>9481</cell><cell>3034</cell><cell>32.6</cell></row><row><cell cols="2">Yu et al. [53] test</cell><cell>26.3</cell><cell>44.7</cell><cell>58.3</cell><cell>68.2</cell><cell>213220 100230</cell><cell>14674</cell><cell cols="2">16299 6017</cell><cell>27.9</cell></row><row><cell>DeepBlueAI</cell><cell>test</cell><cell>31.6</cell><cell>38.7</cell><cell>56.9</cell><cell>56.0</cell><cell>292063 35401</cell><cell>25186</cell><cell cols="2">10296 12266</cell><cell>-</cell></row><row><cell>madamada</cell><cell>test</cell><cell>33.6</cell><cell>43.0</cell><cell>59.8</cell><cell>55.7</cell><cell>209339 76612</cell><cell>42901</cell><cell cols="2">16774 5004</cell><cell>-</cell></row><row><cell>Ours</cell><cell>test</cell><cell>35.5</cell><cell>52.3</cell><cell>64.3</cell><cell>72.3</cell><cell>201041 80054</cell><cell>10790</cell><cell cols="2">17353 5167</cell><cell>31.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results on Waymo tracking validation set using py-motmetrics library (top) 2 and test set using official evaluation. * indicates methods using undisclosed detectors.</figDesc><table><row><cell>Method</cell><cell cols="2">Split Category</cell><cell>MOTA ↑</cell><cell>IDF1 ↑</cell><cell>FN ↓</cell><cell>FP ↓</cell><cell>ID Sw. ↓</cell><cell>MT ↑</cell><cell>ML ↓</cell><cell>mAP ↑</cell></row><row><cell>IoU baseline [29]</cell><cell>val</cell><cell>Vehicle</cell><cell>38.25</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>45.78</cell></row><row><cell>Tracktor++ [3, 29]</cell><cell>val</cell><cell>Vehicle</cell><cell>42.62</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>42.41</cell></row><row><cell>RetinaTrack [29]</cell><cell>val</cell><cell>Vehicle</cell><cell>44.92</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>45.70</cell></row><row><cell>Ours</cell><cell>val</cell><cell>Vehicle</cell><cell>55.6</cell><cell>66.2</cell><cell>514548</cell><cell>214998</cell><cell>24309</cell><cell>17595</cell><cell>5559</cell><cell>49.5</cell></row><row><cell>Ours</cell><cell>val</cell><cell>All</cell><cell>44.0</cell><cell>56.8</cell><cell>674064</cell><cell>264886</cell><cell>30712</cell><cell>21410</cell><cell>7510</cell><cell>40.1</cell></row><row><cell>Tracktor [23, 43]</cell><cell>test</cell><cell>Vehicle</cell><cell>34.80</cell><cell>10.61</cell><cell>14.88</cell><cell>39.71</cell><cell>28.29</cell><cell>8.63</cell><cell>12.10</cell><cell>50.98</cell></row><row><cell>CascadeRCNN-SORTv2*</cell><cell>test</cell><cell>All</cell><cell>50.22</cell><cell>7.79</cell><cell>2.71</cell><cell>39.28</cell><cell>44.15</cell><cell>6.94</cell><cell>2.44</cell><cell>46.46</cell></row><row><cell>HorizonMOT*</cell><cell>test</cell><cell>All</cell><cell>51.01</cell><cell>7.52</cell><cell>2.44</cell><cell>39.03</cell><cell>45.13</cell><cell>7.13</cell><cell>2.25</cell><cell>45.49</cell></row><row><cell>Ours (ResNet-50)</cell><cell>test</cell><cell>All</cell><cell>49.40</cell><cell>7.41</cell><cell>1.46</cell><cell>41.74</cell><cell>43.88</cell><cell>7.10</cell><cell>1.31</cell><cell>48.21</cell></row><row><cell cols="2">Ours (ResNet-101 + DCN) test</cell><cell>All</cell><cell>51.18</cell><cell>7.64</cell><cell>1.45</cell><cell>39.73</cell><cell>45.09</cell><cell>7.20</cell><cell>1.31</cell><cell>46.41</cell></row></table><note>Method Split Category MOTA/L1 ↑ FP/L1 ↓ MisM/L1 ↓ Miss/L1 ↓ MOTA/L2 ↑ FP/L2 ↓ MisM/L2 ↓ Miss/L2 ↓</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies on quasi-dense matching and the inference strategy. All models are comparable on detection performance. D. R. means duplicate removal. (P) means results of the class "pedestrian".</figDesc><table><row><cell cols="2">Quasi-Dense one-positive multi-positive</cell><cell>Metric</cell><cell cols="8">Matching candidates MOTA ↑ IDF1 ↑ mMOTA ↑ mIDF1 ↑ MOTA(P) ↑ IDF1(P) ↑ D. R. Backdrops</cell></row><row><cell>-</cell><cell>-</cell><cell>cosine</cell><cell>-</cell><cell>-</cell><cell>60.4</cell><cell>63.0</cell><cell>34.0</cell><cell>47.9</cell><cell>37.6</cell><cell>49.7</cell></row><row><cell></cell><cell>-</cell><cell>cosine</cell><cell>-</cell><cell>-</cell><cell>61.5</cell><cell>66.8</cell><cell>35.5</cell><cell>50.0</cell><cell>40.5</cell><cell>52.7</cell></row><row><cell>-</cell><cell></cell><cell>cosine</cell><cell>-</cell><cell>-</cell><cell>62.5</cell><cell>67.8</cell><cell>36.2</cell><cell>50.0</cell><cell>44.0</cell><cell>54.3</cell></row><row><cell>-</cell><cell></cell><cell>bi-softmax</cell><cell>-</cell><cell>-</cell><cell>62.9</cell><cell>70.0</cell><cell>35.4</cell><cell>48.5</cell><cell>45.5</cell><cell>58.8</cell></row><row><cell>-</cell><cell></cell><cell>bi-softmax</cell><cell></cell><cell>-</cell><cell>63.2</cell><cell>70.1</cell><cell>36.4</cell><cell>50.4</cell><cell>45.5</cell><cell>58.3</cell></row><row><cell>-</cell><cell></cell><cell>bi-softmax</cell><cell></cell><cell></cell><cell>63.5</cell><cell>71.5</cell><cell>36.6</cell><cell>50.8</cell><cell>46.7</cell><cell>60.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+3.1</cell><cell>+8.5</cell><cell>+2.6</cell><cell>+2.9</cell><cell>+9.1</cell><cell>10.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablations studies on location and motion cues on the BDD100K tracking validation set. All experiments use the same detector for fair comparisons.</figDesc><table><row><cell cols="5">Appearance IoU Motion Regression mMOTA ↑ mIDF1 ↑</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>26.3</cell><cell>36.0</cell></row><row><cell>-</cell><cell></cell><cell>-</cell><cell>27.7</cell><cell>38.5</cell></row><row><cell>-</cell><cell>-</cell><cell></cell><cell>28.6</cell><cell>39.3</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>36.6</cell><cell>50.8</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell>36.3</cell><cell>49.8</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>36.4</cell><cell>49.9</cell></row><row><cell></cell><cell>-</cell><cell></cell><cell>36.4</cell><cell>50.1</cell></row><row><cell cols="5">formance are evaluated according to the official metrics of</cell></row><row><cell>each benchmark.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">MOT The results on MOT17 and MOT16 benchmark are</cell></row><row><cell cols="5">shown in Table 1. Our model achieves the best MOTA of</cell></row><row><cell cols="5">68.7% and IDF1 of 66.3% on the MOT17 and best MOTA</cell></row><row><cell cols="5">of 69.8% on MOT16 without using external training data.</cell></row><row><cell cols="5">We outperform the recent state of the art tracker Center-</cell></row><row><cell cols="5">Track [56] by 0.9 points on MOTA and 1.6 points on IDF1</cell></row><row><cell cols="5">on MOT17 benchmark, showing the effectiveness of our</cell></row><row><cell>method.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">BDD100K The main results on BDD100K tracking valida-</cell></row><row><cell cols="2">tion and testing sets are in</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>TAO We obtain 16.1 points and 12.4 points of AP50 on the validation and test set, respectively. The results are 2.9 points and 2.2 points higher than TAO's solid baseline, which are 13.2 points and 10.2 points respectively. Although we only boost the overall performance by 2 -3 points, we observe that we outperform the baseline by a large margin on frequent classes, that is, 38.6 points vs. 18.5 points on person. This improvement is buried by the average across the entire hundreds of classes. It shows that the crucial part on TAO is still how to improve the tracking on tail classes, which should be a meaningful direction for further research.</figDesc><table /><note>Our model with ResNet-101 and deformable convolution (DCN) has the state-of-the-art performance on the test benchmark which is on par with the champion of Waymo 2020 2D Tracking Challenge (HorizonMOT) but only with a simple single model.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Results on the BDD100K segmentation tracking val set. I: ImageNet. C: COCO. S: Cityscapes. B: BDD100K.</figDesc><table><row><cell>Method</cell><cell cols="6">Pretrained mAP ↑ mMOTSA ↑ mMOTSP ↑ mIDF1 ↑ ID sw. ↓</cell></row><row><cell>SortIoU</cell><cell>I, C, S</cell><cell>22.2</cell><cell>10.3</cell><cell>59.9</cell><cell>21.8</cell><cell>15951</cell></row><row><cell>Yang et al. [52]</cell><cell>I, C, S</cell><cell>22.0</cell><cell>12.3</cell><cell>59.9</cell><cell>26.2</cell><cell>9116</cell></row><row><cell>Ours</cell><cell>I, C, S</cell><cell>22.4</cell><cell>18.2</cell><cell>59.6</cell><cell>35.1</cell><cell>2305</cell></row><row><cell>Ours-fix</cell><cell>I, B</cell><cell>25.5</cell><cell>22.5</cell><cell>65.8</cell><cell>43.9</cell><cell>1591</cell></row><row><cell cols="7">of 1088 × 608 and a ResNet-50 backbone on MOT17, the</cell></row><row><cell cols="3">inference FPS is 20.3.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Results on TAO challenge benchmark.</figDesc><table><row><cell>Method</cell><cell cols="7">Split AP50 AP75 AP AP50(S) AP50(M) AP50(L)</cell></row><row><cell>SORT_TAO [10]</cell><cell>val</cell><cell>13.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell>val</cell><cell>16.1</cell><cell>5.0</cell><cell>7.0</cell><cell>2.4</cell><cell>4.6</cell><cell>9.6</cell></row><row><cell cols="2">SORT_TAO [10] test</cell><cell>10.2</cell><cell>4.4</cell><cell>4.9</cell><cell>7.7</cell><cell>8.2</cell><cell>15.2</cell></row><row><cell>Ours</cell><cell>test</cell><cell>12.4</cell><cell>4.5</cell><cell>5.2</cell><cell>3.7</cell><cell>8.3</cell><cell>18.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Results on MOT17 test set with public detector. Note that we do not use extra data for training. ↑ means higher is better, ↓ means lower is better. * means external data besides COCO and ImageNet is used.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell cols="3">MOTA ↑ IDF1 ↑ MOTP ↑</cell><cell>MT ↑</cell><cell>ML ↓</cell><cell>FP ↓</cell><cell>FN ↓</cell><cell>IDs ↓</cell></row><row><cell></cell><cell>Tracktor++v2 [3]</cell><cell>56.3</cell><cell>55.1</cell><cell>78.8</cell><cell cols="4">498 (21.1) 831 (35.3) 8866 235449 1987 (34.1)</cell></row><row><cell></cell><cell>GSM_Tracktor [28]</cell><cell>56.4</cell><cell>57.8</cell><cell>77.9</cell><cell cols="4">523 (22.2) 813 (34.5) 14379 230174 1485 (25.1)</cell></row><row><cell>Public MOT17</cell><cell>MPNTrack  *  [7] Lif_T  *  [21]</cell><cell>58.8 60.5</cell><cell>61.7 65.6</cell><cell>78.6 78.3</cell><cell cols="4">679 (28.8) 788 (33.5) 17413 213594 1185 (19.1) 637 (27.0) 791 (33.6) 14966 206619 1189 (18.8)</cell></row><row><cell></cell><cell>CenterTrackPub  *  [56]</cell><cell>61.5</cell><cell>59.6</cell><cell>78.9</cell><cell cols="4">621 (26.4) 752 (31.9) 14076 200672 2583 (40.1)</cell></row><row><cell></cell><cell>Ours</cell><cell>64.6</cell><cell>65.1</cell><cell>79.6</cell><cell cols="4">761 (32.3) 666 (28.3) 14103 182998 2652 (39.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Detection oracle analysis. The numbers in the round brackets mean the gaps between oracle results and our results.</figDesc><table><row><cell>Category</cell><cell>Set</cell><cell>MOTA ↑</cell><cell>IDF1 ↑</cell><cell>MOTP ↑</cell><cell>FN ↓</cell><cell>FP ↓</cell><cell>ID Sw. ↓</cell><cell>MT ↑</cell><cell>ML ↓</cell></row><row><cell>Pedestrian</cell><cell>val</cell><cell>94.3</cell><cell>79.5 (+19.3)</cell><cell>99.8</cell><cell>1</cell><cell>1</cell><cell>3226</cell><cell>3506</cell><cell>0</cell></row><row><cell>Rider</cell><cell>val</cell><cell>95.8</cell><cell>88.5 (+40.4)</cell><cell>99.9</cell><cell>0</cell><cell>0</cell><cell>107</cell><cell>134</cell><cell>0</cell></row><row><cell>Car</cell><cell>val</cell><cell>97.7</cell><cell>86.1 (+11.1)</cell><cell>99.9</cell><cell>0</cell><cell>0</cell><cell>7716</cell><cell>13189</cell><cell>0</cell></row><row><cell>Bus</cell><cell>val</cell><cell>99.2</cell><cell>93.0 (+31.2)</cell><cell>100.0</cell><cell>0</cell><cell>0</cell><cell>72</cell><cell>196</cell><cell>0</cell></row><row><cell>Truck</cell><cell>val</cell><cell>98.8</cell><cell>90.3 (+33.8)</cell><cell>100.0</cell><cell>0</cell><cell>0</cell><cell>340</cell><cell>726</cell><cell>0</cell></row><row><cell>Bicycle</cell><cell>val</cell><cell>88.2</cell><cell>79.5 (+31.8)</cell><cell>98.7</cell><cell>8</cell><cell>8</cell><cell>470</cell><cell>243</cell><cell>0</cell></row><row><cell>Motorcycle</cell><cell>val</cell><cell>97.0</cell><cell>94.5 (+37.8)</cell><cell>99.8</cell><cell>0</cell><cell>0</cell><cell>27</cell><cell>44</cell><cell>0</cell></row><row><cell>Train</cell><cell>val</cell><cell>99.4</cell><cell>98.7 (+98.7)</cell><cell>100.0</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>6</cell><cell>0</cell></row><row><cell>All (average)</cell><cell>val</cell><cell>96.3</cell><cell>88.8 (+38.0)</cell><cell>99.8</cell><cell>9</cell><cell>9</cell><cell>11960</cell><cell>18044</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Tracking oracle analysis. The numbers in the round brackets mean the gaps between oracle results and our results.</figDesc><table><row><cell>Category</cell><cell>Set</cell><cell>MOTA ↑</cell><cell>IDF1 ↑</cell><cell>MOTP ↑</cell><cell>FN ↓</cell><cell>FP ↓</cell><cell>ID Sw. ↓</cell><cell>MT ↑</cell><cell>ML ↓</cell></row><row><cell>Pedestrian</cell><cell>val</cell><cell>54.7</cell><cell>71.2 (+11.0)</cell><cell>77.6</cell><cell>14990</cell><cell>10095</cell><cell>755</cell><cell>1835</cell><cell>367</cell></row><row><cell>Rider</cell><cell>val</cell><cell>31.4</cell><cell>52.6 (+4.5)</cell><cell>76.6</cell><cell>1390</cell><cell>242</cell><cell>115</cell><cell>16</cell><cell>56</cell></row><row><cell>Car</cell><cell>val</cell><cell>74.3</cell><cell>82.9 (+7.9)</cell><cell>84.1</cell><cell>54585</cell><cell>31014</cell><cell>2309</cell><cell>8759</cell><cell>1141</cell></row><row><cell>Bus</cell><cell>val</cell><cell>38.2</cell><cell>65.8 (+4.0)</cell><cell>86.1</cell><cell>3532</cell><cell>2031</cell><cell>57</cell><cell>61</cell><cell>41</cell></row><row><cell>Truck</cell><cell>val</cell><cell>37.0</cell><cell>60.9 (+4.4)</cell><cell>84.7</cell><cell>12719</cell><cell>4259</cell><cell>247</cell><cell>149</cell><cell>239</cell></row><row><cell>Bicycle</cell><cell>val</cell><cell>30.6</cell><cell>55.6 (+7,9)</cell><cell>75.4</cell><cell>2031</cell><cell>714</cell><cell>125</cell><cell>60</cell><cell>58</cell></row><row><cell>Motorcycle</cell><cell>val</cell><cell>14.6</cell><cell>51.7 (-5.0)</cell><cell>76.4</cell><cell>443</cell><cell>292</cell><cell>35</cell><cell>10</cell><cell>18</cell></row><row><cell>Train</cell><cell>val</cell><cell>-0.6</cell><cell>0.0 (+0.0)</cell><cell>0.0</cell><cell>308</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>6</cell></row><row><cell>All (average)</cell><cell>val</cell><cell>35.0</cell><cell>55.1 (+4.3)</cell><cell>70.1</cell><cell>89998</cell><cell>48649</cell><cell>3643</cell><cell>10890</cell><cell>1926</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-target tracking by continuous energy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andriyenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1265" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15535" to="15545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.05625</idno>
		<title level="m">Tracking without bells and whistles</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">High-speed trackingby-detection without using image information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bochinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Eiselein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a neural solver for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brasó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiple target tracking in world coordinate with single, minimally calibrated camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tao: A large-scale benchmark for tracking any object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khurana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mask rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to track at 100 FPS with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>De Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<title level="m">Data-efficient image recognition with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lifted disjoint paths with application in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornakova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swoboda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiple hypothesis tracking revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-object tracking with neural gating using bilinear LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning by tracking: Siamese CNN for robust target association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Tracking the trackers: an analysis of the state of the art in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02781</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gsm: Graph similarity model for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</title>
		<editor>C. Bessiere</editor>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="530" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Retinatrack: Online single stage joint detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Votel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ahadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rahmati</surname></persName>
		</author>
		<title level="m">Multi-target tracking using cnn-based features: Cnnmtt. Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="7077" to="7096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<title level="m">Mot16: A benchmark for multi-object tracking</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Online multi-target tracking using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Algorithms for the assignment and transportation problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munkres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Society for Industrial and Applied Mathematics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tubetk: Adopting tubes to track multi-object in a one-step training model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Libra r-cnn: Towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="821" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Chained-tracker: Chaining paired attentive regression results for end-to-end joint multipleobject detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Finding and tracking people from the bottom up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2003" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
	<note>pages II-II</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tracking the untrackable: Learning to track multiple cues with long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-object tracking with quadruplet convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Timofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krivokon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<title level="m">Scalability in perception for autonomous driving: Waymo open dataset</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10857</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12605</idno>
		<title level="m">Towards real-time multi-object tracking</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">An online learned CRF model for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Bdd100k: A diverse driving dataset for heterogeneous multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">POI: multiple object tracking with high performance detection and appearance feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Fairmot: On the fairness of detection and re-identification in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01888</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Tracking objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Online multi-target tracking with tensor-based high-order graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 24th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
