<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 MEMC-Net: Motion Estimation and Motion Compensation Driven Neural Network for Video Interpolation and Enhancement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 MEMC-Net: Motion Estimation and Motion Compensation Driven Neural Network for Video Interpolation and Enhancement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Motion Estimation</term>
					<term>Motion Compensation</term>
					<term>Convolutional Neural Network</term>
					<term>Adaptive Warping !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Motion estimation (ME) and motion compensation (MC) have been widely used for classical video frame interpolation systems over the past decades. Recently, a number of data-driven frame interpolation methods based on convolutional neural networks have been proposed. However, existing learning based methods typically estimate either flow or compensation kernels, thereby limiting performance on both computational efficiency and interpolation accuracy. In this work, we propose a motion estimation and compensation driven neural network for video frame interpolation. A novel adaptive warping layer is developed to integrate both optical flow and interpolation kernels to synthesize target frame pixels. This layer is fully differentiable such that both the flow and kernel estimation networks can be optimized jointly. The proposed model benefits from the advantages of motion estimation and compensation methods without using hand-crafted features. Compared to existing methods, our approach is computationally efficient and able to generate more visually appealing results. Furthermore, the proposed MEMC-Net architecture can be seamlessly adapted to several video enhancement tasks, e.g., super-resolution, denoising, and deblocking. Extensive quantitative and qualitative evaluations demonstrate that the proposed method performs favorably against the state-of-the-art video frame interpolation and enhancement algorithms on a wide range of datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>V IDEO frame interpolation aims to synthesize nonexistent frames between original input frames, which has been applied to numerous applications such as video frame rate conversion <ref type="bibr" target="#b0">[1]</ref>, novel view synthesis <ref type="bibr" target="#b1">[2]</ref>, and frame recovery in video streaming <ref type="bibr" target="#b2">[3]</ref>, to name a few. Conventional approaches <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> are generally based on motion estimation and motion compensation (MEMC), and have been widely used in various display devices <ref type="bibr" target="#b5">[6]</ref>. A few deep learning based frame interpolation approaches <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> have been developed to address this classical topic. In this paper, we analyze the MEMC-based and learning-based approaches of video frame interpolation and exploit the merits of both paradigms to propose a high-quality frame interpolation processing algorithm.</p><p>Conventional MEMC-based approaches entail both motion estimation <ref type="bibr" target="#b12">[13]</ref> and motion compensation <ref type="bibr" target="#b13">[14]</ref> for video interpolation. Motion estimation is used to determine the block-wise or pixel-wise motion vectors between two frames. The block-based methods <ref type="bibr" target="#b3">[4]</ref> assume that the pixels within a block share the same motion and use search strategies <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> and selection criteria <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b16">[17]</ref> to obtain the optimal motion vector. On the other hand, the methods based on pixel-based motion, i.e., optical flow, estimate a motion/flow vector for each pixel of the frames and thus entail heavy computational loads.  <ref type="bibr" target="#b17">[18]</ref>, nearest neighbor field search <ref type="bibr" target="#b18">[19]</ref>, cost volume filtering <ref type="bibr" target="#b19">[20]</ref>, and deep convolutional neural networks (CNNs) <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b20">[21]</ref>. However, estimating optical flow remains a challenging problem due to fast-moving and thin objects, occlusion and dis-occlusion, brightness change and motion blur. To account for inaccurate flow and occluded pixels, motion compensated interpolation methods usually use sophisticated filters to reduce visual artifacts of the generated frames <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b21">[22]</ref>. In addition, these schemes do not perform well where objects in the intermediate frame are invisible in both forward and backward reference frames (e.g., some pixels cannot be compensated), and require further post-processing procedures to fill in missing or remove unreliable pixels <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Numerous learning-based frame interpolation methods based on deep CNNs have been recently proposed <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. The training datasets for learning-based methods typically contain image triplets from raw video sequences, with the first and third frame feeding into the network as inputs and the intermediate second frame acting as ground truth <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> for output. By imposing loss functions such as L pnorm on the difference between the network output and ground truth frame pixels, the model parameters can be iteratively updated via a gradient descent scheme.</p><p>The conventional MEMC-based methods are computationally efficient due to the block-wise setting <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. However, these block-based methods do not achieve the state-of-the-art results as hand-crafted features are typically used in the ME and MC stages. In contrast, the learningbased methods are developed based on the massive amount of raw video data. However, the state-of-the-art learning-arXiv:1810.08768v2 [cs.CV] 5 Sep 2019 (a) Overlay (b) MIND <ref type="bibr" target="#b6">[7]</ref> (c) ToFlow <ref type="bibr" target="#b8">[9]</ref> (d) EpicFlow <ref type="bibr" target="#b9">[10]</ref> (e) SPyNet <ref type="bibr" target="#b10">[11]</ref> (f) SepConv-L f <ref type="bibr" target="#b11">[12]</ref> (g) SepConv-L 1 <ref type="bibr" target="#b11">[12]</ref> (h) MEMC-Net (i) MEMC-Net* (j) Ground Truth based approaches <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b26">[27]</ref> focus on motion estimation, which often leads to blurry results due to bilinear interpolation process. While other approaches <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b27">[28]</ref> are developed to consider the effect of interpolation kernels, such schemes are sensitive to large motion.</p><p>In this paper, we propose to exploit motion estimation and motion compensation in a neural network for video frame interpolation. Both the motion vectors and compensation filters are estimated through CNNs. We further propose an adaptive warping layer based on optical flow and compensation filters for synthesizing new pixels. This novel warping layer is fully differentiable such that the gradients can be back-propagated to both the ME and MC networks. To account for the occlusions, we estimate occlusion masks to adaptively blend the warped frames. Furthermore, the missing pixels in holes and unreliable pixels of the warped frames are processed by a post-processing CNN. Our entire model, MEMC-Net, is motivated by the architecture of conventional methods but realized via the most recent learningbased approaches. <ref type="figure" target="#fig_0">Fig. 1</ref> shows an interpolated frame of our methods (MEMC-Net and MEMC-Net*) and existing algorithms <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, where the proposed methods predict the moving ball with clearer contours and sharper edges.</p><p>The contributions of this paper are summarized as follows:</p><p>(1) We propose a motion estimation and compensation driven neural network for robust and high-quality video frame interpolation. (2) We integrate the optical flow warping with learned compensation filters into an adaptive warping layer. The proposed adaptive warping layer is fully differentiable and applicable to several video processing tasks, e.g., video super-resolution, video denoising, and video deblocking. <ref type="bibr" target="#b2">(3)</ref> We demonstrate that the proposed method performs favorably against the state-of-the-art frame interpolation algorithms on several benchmark datasets, including the Middlebury <ref type="bibr" target="#b28">[29]</ref>, UCF101 <ref type="bibr" target="#b29">[30]</ref>, and Vimeo90K <ref type="bibr" target="#b8">[9]</ref> datasets. Our model requires less memory to predict the compensation filters and executes efficiently.</p><p>(4) We extend our network to the other video enhancement tasks including super-resolution, denoising, and deblocking as the model is general and applicable to motion compensation based tasks. Our methods obtain more favorable results against the state-of-the-art algorithms on each of these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we discuss the conventional MEMC-based and recent learning-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Conventional MEMC-based Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2(a)</head><p>shows the typical framework of conventional MEMC-based video frame interpolation methods. First, motion vectors between the forward and reference frames are estimated. Along the motion trajectories, pixels of the reference frames are used to interpolate the intermediate frame.</p><p>Conventional ME methods use block-based algorithms such as the 3D recursive search <ref type="bibr" target="#b3">[4]</ref>, which are hardware-friendly and computationally efficient. The block-based methods typically divide the image frames into small pixel blocks and exploit certain search strategies such as spatial/temporal search <ref type="bibr" target="#b3">[4]</ref>, hierarchical search <ref type="bibr" target="#b30">[31]</ref>, based on selection criteria such as the minimum sum of absolute block difference to compute their motion vectors. For motion compensated interpolation, overlapped blocks are usually utilized to cope with the erroneous motion vectors of pixel blocks <ref type="bibr" target="#b13">[14]</ref>. Recently, several methods <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b31">[32]</ref> exploit optical flow for the truthfulness of flow fields. Compensation filters via image fusion <ref type="bibr" target="#b23">[24]</ref> or overlapped patch reconstruction <ref type="bibr" target="#b31">[32]</ref> are developed to deal with occlusion or blocky effects.</p><p>Aside from the motion estimation and motion compensation procedures, a post-processing step is often required to minimize artifacts and improve visual qualities <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Due to relative motions and occlusion between objects with different depth, the estimated flow vectors may lead to incorrect interpolation results with hole regions. <ref type="bibr">Kim</ref>   <ref type="figure">Fig. 2</ref>. Frameworks of (a) the conventional MEMC-based approaches, (b) the flow-based and (c) the kernel-based models. The black, red, and blue text boxes correspond to the conventional modules, network modules, and network layers respectively. a trilateral filtering method to fill the holes and smooth the compensation errors in both the spatial and temporal domains. The proposed algorithm differs from the conventional MEMC methods in that we develop a data-driven end-to-end trainable model with deep features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning-based Methods</head><p>Video frame interpolation based on deep learning algorithms can be categorized into the direct method, phase-based, flow-based, and kernel-based approaches. Long et al. <ref type="bibr" target="#b6">[7]</ref> train a deep CNN to directly predict the interpolated frames. The outputs are usually blurry and contain fewer details as this deep model is not able to capture the multi-modal distribution of natural images and videos. The phase-based method <ref type="bibr" target="#b32">[33]</ref> manipulates the pixel phase information within a multi-scale pyramid for frame interpolation. However, this approach is less effective in handling large motion in complicated scenes. In the following, we focus our discussion on recent flow-based and kernel-based methods.</p><p>Flow-based methods. With the advances in optical flow estimation by deep CNNs <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, several methods based on end-to-end deep models have been developed for frame interpolation. These approaches either predict bidirectional flow <ref type="bibr" target="#b8">[9]</ref> or use the bilinear warping operation to align input frames based on linear motion models <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>. To synthesize an output image, a common technique is to estimate an occlusion mask to adaptively blend the warped frames. As the bilinear warping blend neighbor pixels based on the sub-pixel shifts, the flow-based methods inevitably generate ghost or blurry artifacts when the input frames are not aligned well. The pipeline for the flow-based methods is illustrated in <ref type="figure">Fig. 2(b)</ref>. Instead of using the fixed bilinear coefficients for interpolation, our approach learns spatially-varying interpolation kernels for each pixel. The learned kernels have larger spatial support (e.g., 4 × 4) than the bilinear interpolation and thus better account for occlusion and dis-occlusion.</p><p>Kernel-based methods. Instead of relying on pixelwise optical flow, frame interpolation can be formulated as convolution operations over local patches <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>. Niklaus et al. <ref type="bibr" target="#b27">[28]</ref> propose the AdaConv model to estimate spatially-adaptive convolutional kernels for each output pixel. We show the pipeline of kernel-based methods in <ref type="figure">Fig. 2</ref>(c). In these methods, a large kernel size is used   <ref type="bibr" target="#b11">[12]</ref> assumes that the convolutional kernels are separable and uses a pair of 1D kernels (one vertical and one horizontal kernel) to approximate the 2D kernels. This strategy significantly reduces the memory consumption from O(R 2 ) to O(2R) and further improves interpolation results. However, both the AdaConv and SepConv methods cannot handle motion larger than the pre-defined kernel size. While our approach also learns adaptive local kernels for interpolation, the proposed method is not limited by the assumption of fixed motion range as optical flow warping is integrated. That is, our method uses smaller kernels, requires a low amount of memory, and performs robustly to frames with large motion. We list the main difference with flow-based methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b35">[36]</ref> and kernel-based approaches <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b27">[28]</ref> in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MOTION ESTIMATION AND MOTION COMPENSA-TION DRIVEN NEURAL NETWORK</head><p>In this section, we describe the design methodology of the proposed MEMC-Net framework, adaptive warping layer, and flow projection layer used in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MEMC-Net Framework</head><p>Following the conventional MEMC-based and recent learning-based methods, there are different ways to design a MEMC-Net model for video frame interpolation. A straightforward method is to combine the motion estimation, motion compensation, and post-processing sequentially. That is, the reference frames are first aligned with the motion estimation, bilinear warping is applied to account for large motion, and small convolutional kernels for the warped frames are estimated to synthesize a final frame. As in the conventional MEMC-based framework, a postprocessing network is also added to the sequential model to reduce the possible pixel outliers. <ref type="figure" target="#fig_1">Fig. 3</ref>  sequential model. However, according to our experiments, the warped frames (Î t−1 andÎ t+1 ) are usually of low quality due to the imperfect optical flow estimated by existing methods. Consequently, the lateral kernel estimation, kernel convolution, and post-processing are not able to generate visually pleasing results from the corrupted frames.</p><p>In this paper, we develop a novel algorithm to simultaneously estimate the flow and compensation kernels with respect to the original reference frames. This approach requires frame interpolation to be carried out within a warping layer based on both the flow and compensation kernel. This new warping layer is expected to tightly couple the motion estimation and kernel estimation networks so that both networks can be optimized through the enormous video data. <ref type="figure" target="#fig_1">Fig. 3</ref>(b) shows the proposed framework for video frame interpolation.</p><p>We present the network architecture of the proposed MEMC-Net for the video frame interpolation in <ref type="figure" target="#fig_2">Fig. 4</ref>. In this work, we propose a novel adaptive warping layer to assemble the bilinear warping and kernel convolution in one single step. The layer takes in the optical flow, interpolation kernel to warp the input frame pixels. For the video frame interpolation task, since the intermediate frame is not originally available, we estimate the flow between the forward and backward reference frames, and then project it to simulate the flow between the intermediate and reference frames. This operation is achieved by our proposed flow projection layer.</p><p>The adaptive warping and the flow projection layers are the two major technical innovations of our algorithm. We summarize the benefits of the proposed layer from two aspects. First, the conventional MEMC-based approaches rely on hand-crafted features (e.g., SIFT <ref type="bibr" target="#b40">[41]</ref> for motion estimation or Gaussian-like weighting maps <ref type="bibr" target="#b41">[42]</ref> for motion compensation), while the proposed adaptive warping layer allows us to extract data-driven features for joint motion estimation and motion compensation. Therefore, the proposed model has a better generalization capability to handle various scenarios for video frame interpolation and enhancement tasks. Second, the adaptive warping layer tightly integrates two learning-based methodologies, namely the flow-based and kernel-based ones, and inherits their merits in that: 1) Compared to the flow-based methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b35">[36]</ref> that rely on simple bilinear coefficients, our method is able to improve the interpolation accuracy by using data-driven kernel coefficients.</p><p>2) Compared to the kernel-based approaches <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b27">[28]</ref>, our method obtains higher computational efficiency by largely reducing the kernel size through pre-aligning the pixels with learned optical flows.</p><p>We present the forward inference and back-propagation details of the two novel layers in Section 3.2 and Section 3.3 respectively. The pipeline of our method in <ref type="figure" target="#fig_2">Fig. 4</ref>, as well as the detailed network configuration of the used motion estimation, kernel estimation, mask estimation, and postprocessing networks, are described in Section 4. We will also introduce an additional context extraction network toward the enhanced MEMC-Net* model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adaptive Warping Layer</head><p>The proposed adaptive layer warps images or features based on the given optical flow and local convolutional kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Forward pass. Let</head><formula xml:id="formula_0">I(x) : Z 2 → R 3 denote the RGB image where x ∈ [1, H] × [1, W ], f (x) := (u(x), v(x)) represent the optical flow field and k l (x) = [k l r (x)] H×W (r ∈ [−R + 1, R] 2 )</formula><p>indicate the interpolation kernel where R is the kernel size. The adaptive warping layer synthesizes an output image by:</p><formula xml:id="formula_1">I(x) = r∈[−R+1,R] 2 k r (x)I(x + f (x) + r),<label>(1)</label></formula><p>where the weight k r = k l r k d r is determined by both the learned interpolation kernel k l r and bilinear coefficient k d r . We train a kernel estimation network to predict the weights for the interpolation kernels. For each 2D spatial location in the image grid [1, W ] × [1, H], the kernel estimation network generates a 16-channel feature vector. We then map the feature vector into a 4 × 4 square matrix as the On the other hand, the bilinear coefficient ( <ref type="figure" target="#fig_3">Fig. 5(c)</ref>) is defined by:</p><formula xml:id="formula_2">k d r =          [1 − θ(u)][1 − θ(v)], r u ≤ 0, r v ≤ 0, θ(u)[1 − θ(v)], r u &gt; 0, r v ≤ 0, [1 − θ(u)]θ(v), r u ≤ 0, r v &gt; 0, θ(u)θ(v), r u &gt; 0, r v &gt; 0,<label>(2)</label></formula><p>where θ(u) = u − u denotes the fractional part of a float point number, and the subscript u, v of the 2-D vector r represent the horizontal and vertical components, respectively. The bilinear coefficient allows the layer to back-propagate the gradients to the optical flow estimation network. In this case, we aim to compute a local interpolation kernel that combines the bilinear coefficients and the learned coefficients from the kernel prediction network. To apply the bilinear coefficients to kernels of any size, we first compute the bilinear coefficients for the nearest four neighbor pixels, i.e., P 11 , P 12 , P 21 , and P 22 , and then replicate the coefficients to the pixels at the same corner. Therefore, the pixels with the same color in <ref type="figure" target="#fig_3">Fig. 5</ref>(c) have the same bilinear coefficient. Finally, we multiply the bilinear coefficients with the learned kernel coefficients as our local adaptive kernels.</p><p>Backward pass. We compute the gradient with respect to the optical flow and interpolation kernels, respectively. The derivative with respect to the optical flow field f is computed by (using the horizontal component u for example):</p><formula xml:id="formula_3">∂Î(x) ∂u(x) = r k l r (x) · I (x + f (x) + r) · ∂k d r ∂u ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">∂k d r ∂u =          − [1 − θ(v)], r u ≤ 0, r v ≤ 0, [1 − θ(v)], r u &gt; 0, r v ≤ 0, − θ(v), r u ≤ 0, r v &gt; 0, θ(v), r u &gt; 0, r v &gt; 0.<label>(4)</label></formula><p>The derivative with respect to the vertical component v can be derived in a similar way. The derivative with respect to the interpolation kernel k l r is:</p><formula xml:id="formula_5">∂Î ∂k l r (x) = k d r (x) · I (x + f (x) + r) .<label>(5)</label></formula><p>The integration with the spatially-varying kernels alleviates the limitation of bilinear interpolation to synthesize pixel values from a broader neighborhood. In addition, this approach facilitates the warping layer to perform more robustly to inaccurate optical flow and better account for occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Flow Projection Layer</head><p>As the intermediate frame is not available, we transform the flow between the forward and backward reference frames and then project it to simulate the flow between the intermediate frame and the reference frames. Let f t→t−1 (x) be the motion vector field of frame I t to I t−1 . Similarly, f t−1→t+1 (y) represents the motion vector field of frame I t−1 to I t+1 . Note that we use y to index the 2-D coordinate at time step t − 1, as distinguished to x at t. Our flow projection layer is designed to transform an estimated flow f t−1→t+1 (y) to f t→t−1 (x). Here we assume that the local motion between consecutive frames is linear and invert the flow between I t−1 and I t+1 to approximate the intermediate flow fields.</p><p>As there may exist multiple flow vectors projected to the same location in the intermediate frame, we average all the projected flow vectors at the same location. On the other hand, there may exist holes where no flow is projected. Thus, we use the outside-in strategy <ref type="bibr" target="#b28">[29]</ref> to fill-in these holes in the intermediate frame. We denote the set of flow vectors mapped to location </p><formula xml:id="formula_6">f t→t−1 (x) =            −1 |S(x)| y∈S(x) f t−1→t+1 (y) 2 , if |S(x)| &gt; 0, 1 |N (x)| x ∈N (x) f t→t−1 (x ), if |S(x)| = 0. (6)</formula><p>The backward pass computes the derivative with respect to the input optical flow f t−1→t+1 (y):</p><formula xml:id="formula_7">∂f t→t−1 (x) ∂f t−1→t+1 (y) =    −1 2|S(x)| , for y ∈ S(x) if |S(x)| &gt; 0, 0, for y / ∈ S(x) or |S(x)| = 0.<label>(7)</label></formula><p>We use a graph to illustrate the outside-in strategy in <ref type="figure">Fig. 6</ref>. We use a soft blending way in the proposed flow projection layer by averaging the 4-directional available flow vectors from the neighboring non-hole regions. The spatial position at X has its 4-directional non-hole neighbors A, B, C, and D. Therefore, the flow vector f X is approxi-</p><formula xml:id="formula_8">mated by f X = (f A + f B + f C + f D )/4.</formula><p>An alternative is to fill in the flow holes with zero vectors, which is only suitable for stationary objects. We show an example in <ref type="figure">Fig. 7</ref> to compare the two strategies. The outside-in strategy can reduce interpolation artifacts significantly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VIDEO FRAME INTERPOLATION</head><p>We provide an overview of the proposed MEMC-Net in <ref type="figure" target="#fig_2">Fig. 4</ref> and describe the detailed architecture design of each component below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motion estimation.</head><p>Given two input frames I t−1 and I t+1 , we first estimate the forward flow f t−1→t+1 and backward flow f t+1→t−1 by passing I t−1 and I t+1 into the flow estimation network twice with a reverse order. In this work, we use the FlowNetS <ref type="bibr" target="#b20">[21]</ref> model for optical flow estimation. Then we use the proposed flow projection layer as described by Eq.(6) to project the forward flow f t−1→t+1 and backward flow f t+1→t−1 into f t→t−1 and f t→t+1 for the intermediate frame, respectively.</p><p>Kernel estimation. We use the U-Net <ref type="bibr" target="#b42">[43]</ref> as our kernel estimation network, which has an encoder with five maxpooling layers, a decoder with five un-pooling layers, and skip connections from the encoder to the decoder. The kernel prediction network takes two video frames as input and generates R 2 coefficient maps, denoted by K t−1 and K t+1 . We then reshape the coefficient maps to R×R convolutional kernels for each pixel, as shown in <ref type="figure" target="#fig_3">Fig. 5</ref> Mask estimation. Due to the depth variation and relative motion of objects, there are occluded pixels between the two reference frames. To select valid pixels from the two warped reference frames, we learn a mask estimation network to predict the occlusion masks. The mask estimation network has the same U-Net architecture as our kernel estimation network, but the last convolutional layer outputs a 2-channel feature map as the occlusion masks M t−1 and M t+1 . The blended frame is generated by:</p><formula xml:id="formula_9">I t = M t−1 ⊗Î t−1 + M t+1 ⊗Î t+1 ,<label>(8)</label></formula><p>where ⊗ denotes the channel-wise multiplication operation.</p><p>Context extraction. We also use the contextual information <ref type="bibr" target="#b26">[27]</ref> in the post-processing module to better deal with occlusion. We extract the conv1 features of the input reference frames from a pre-trained ResNet18 <ref type="bibr" target="#b43">[44]</ref> as the contextual maps. The contextual maps are then warped by the optical flow and the interpolation kernels via the adaptive warping layer. The warped contextual maps, denoted aŝ C t−1 andĈ t+1 , are fed as inputs to the following postprocessing .</p><p>Post-processing. Since the blended imageÎ t usually contains artifacts caused by inaccurate flow estimation or masks, we introduce a post-processing network to improve the visual quality. The post-processing module takes as input the blended frameÎ t , estimated flows f t→t+1 and f t→t−1 , coefficient maps of the interpolation kernels K t−1 and K t+1 , occlusion masks M t−1 and M t+1 , and the warped context featuresĈ t−1 andĈ t+1 . Our post-processing network contains 8 convolutional layers as shown in <ref type="figure" target="#fig_6">Fig. 8</ref>.</p><p>Except for the last one, each convolutional layer has a filter size of 3 × 3 with 64 output channels and is followed by a Rectified Linear Unit (ReLU). The last convolutional layer outputs a 3-channel RGB image. As the output and input of this module are highly similar (i.e., both are the interpolated frame at t), we enforce the network to output the residual between the blended frameÎ t and the ground-truth frame. Therefore, the post-processing module learns to enhance the details and remove the artifacts in the blended frame. We present an example in <ref type="figure">Fig. 9</ref> to demonstrate the effect of the post-processing module. The blurry edges and lines are sharpened by our method. The proposed model generates I t as the final interpolated frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MEMC-Net.</head><p>We provide two variants of the proposed model. The first one does not use the contextual information, where we refer the model as MEMC-Net. The second one includes the context information as the inputs to the postprocessing module, where we refer the model as MEMC-Net*.</p><p>Before post-processing After post-processing Ground-truth <ref type="figure">Fig. 9</ref>. Effectiveness of the proposed post-processing network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">IMPLEMENTATION DETAILS</head><p>In this section, we discuss the implementation details including the loss function, datasets, and hyper-parameter settings of the proposed MEMC-Net and MEMC-Net*.</p><p>Loss Function. We use a robust loss function between the restored framesĨ t ,Î t and the corresponding ground truth frame I GT t . We also regularize the sum of two masks to be 1.0. The combined loss function is given by:</p><formula xml:id="formula_10">L = x Φ Ĩ t − I GT t + α x Φ Î t − I GT t + β x Φ (M t−1 + M t+1 − 1.0)<label>(9)</label></formula><p>where Φ(x) = √ x 2 + 2 is the Charbonnier penalty function <ref type="bibr" target="#b44">[45]</ref> with to be 1e − 6. We empirically set α and β to be 1e − 3 and 2e − 3 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets.</head><p>We use the training set of the Vimeo90K dataset <ref type="bibr" target="#b8">[9]</ref> to learn the proposed frame interpolation model. There are 51,312 triplets, and each image is of 448×256 pixels. During the training process, we use the data augmentation with random horizontal and vertical flipping as well as reversing the temporal order of input sequences.</p><p>Hyper-parameter settings. We initialize the network parameters with the method of He et al. <ref type="bibr" target="#b45">[46]</ref>. We set the initial learning rate of the kernel prediction, mask estimation and post-processing networks to be 0.001 while using a smaller learning rate of 0.00001 for fine-tuning the flow estimation network. We decrease the learning rates by a factor of 0.2 if the validation loss does not decrease during 5 epochs. We use a batch size of 4 and use the Adam <ref type="bibr" target="#b46">[47]</ref> optimizer with β 1 of 0.9 and β 2 of 0.999 for training our model. In addition, we use a weight decay of 1e − 6. The entire network is trained for 100 epochs. Except for the last output layer, the convolutional layers of the FlowNetS network are activated by the leaky ReLU <ref type="bibr" target="#b47">[48]</ref>, while those of the other three networks are activated by the ReLU <ref type="bibr" target="#b48">[49]</ref> layer. We use the batch normalization <ref type="bibr" target="#b49">[50]</ref> layer in the kernel prediction and mask estimation networks.</p><p>The source code, trained model, and video frame interpolation results generated by all the evaluated methods are available on our project website: https://sites.google.com/ view/wenbobao/memc-net. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">VIDEO FRAME ENHANCEMENT</head><p>In addition to video frame interpolation, we show that the proposed framework can be generalized to several video frame enhancement tasks, including video super-resolution, video denoising, and video deblocking. In these tasks, multiple consecutive frames are used to extract useful texture cues to reduce the distortions like low-resolution, noise, blockiness, etc. Existing learning-based methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b50">[51]</ref> often align the consecutive frames based on the estimated optical flow and bilinear interpolation. However, as discussed in Section 3.1 and 4, the bilinear interpolation process may result in blurred pixels. In contrast, the proposed adaptive warping layer is able to compensate for more accurate frame pixels.</p><p>Here we discuss how the proposed method can be extended for the video super-resolution problem. Given 2L+1 consecutive low-resolution frames {I LR k } t+L k=t−L , our goal is to recover a high-resolution frame I HR t at the time step t. We first use bicubic interpolation to up-sample all the low-resolution frames to the target resolution, denoted by {İ HR k } t+L k=t−L . For each pair ofİ HR k (k = t) andİ HR t , we estimate the optical flow f k (k = t) and compensation kernel K k (k = t) via our flow estimation and kernel estimation networks. Then, we use the proposed adaptive warping layer to warp all the neighboring frames to align withİ HR t at the time step t, denoted byÎ HR k (k = t). Alongside the frame pixels, we also extract and warp the context information from a pre-trained ResNet18 <ref type="bibr" target="#b43">[44]</ref> model. Finally, all the generated motions f k , kernel K k , contextĈ k , warped framê I HR k as well as up-sampled blurry frameİ HR t are fed into a frame enhancement network. Our frame enhancement network has a similar architecture to the single-image super-resolution method, EDSR <ref type="bibr" target="#b51">[52]</ref>.</p><p>The frame enhancement network is deeper than the postprocessing network for video frame interpolation. Since the input frames are heavily degraded by low resolution, noise, or blockiness, the frame enhancement network thus requires more complex architecture to restore high-quality results. In the frame enhancement network shown in <ref type="figure" target="#fig_0">Fig. 10(a)</ref>, we first use one convolutional layer with a ReLU activation. Then, we adopt 10 residual blocks, where each residual block contains two convolutional layers, one ReLU layer, and a skip connection as shown in <ref type="figure" target="#fig_0">Fig. 10(b)</ref>. Finally, the last convolutional layer generates the residuals between the input and output images. All the convolutional layers have 128 channels and a 3 × 3 kernel size. The final output frame of this network is denoted byĨ t . We name the entire video super-resolution network as MEMC-Net SR as shown in <ref type="figure" target="#fig_0">Fig. 11</ref>.</p><p>The model difference between the MEMC-Net SR and MEMC-Net* is twofold. First, the MEMC-Net SR does not require the flow projection layer as we can directly estimate flow for the target frame. Second, since each pixel of the target frame has a valid flow vector, we discard the mask estimation module in MEMC-Net SR. We use the same network architecture as MEMC-Net SR for video denoising and deblocking. The extended model for denoising and deblocking are referred to as MEMC-Net DN and MEMC-Net DB, respectively.</p><p>For each of the three video enhancement tasks, we train our network on the corresponding training set from the Vimeo90K dataset <ref type="bibr" target="#b8">[9]</ref>, namely Vimeo90K-SR, Vimeo90K-DN, and Vimeo90K-DB. Each of the training sets consists of 91,701 7-frame sequences with an image resolution of 448 × 256 pixels. Note that the input images of the Vimeo90K-SR set are first downsampled to a resolution of 224 × 128 and then upsampled to 448 × 256 with the bicubic interpolation. In each training iteration, a batch contains one sequence with 7 consecutive frames. The learning rate is initialized to 0.0005 for the first 10 epochs and then is dropped to 0.0001 for the following 5 epochs. Similar to the video frame interpolation task, we initialized the parameters with the method of He et al. <ref type="bibr" target="#b45">[46]</ref> and use the Adamax optimizer <ref type="bibr" target="#b46">[47]</ref> to update the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXPERIMENTAL RESULTS</head><p>In this section, we first analyze and discuss the contributions of each sub-module, processing speed, and model parameters. We then present the experimental results on video frame interpolation and the other three video frame enhancement tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Analysis and Discussion</head><p>We first describe the evaluated datasets and conduct experiments to analyze the contribution of each component in the proposed model, especially on flow and kernel estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Datasets</head><p>We evaluate the proposed frame interpolation approach on a wide variety of video datasets.</p><p>Middlebury. The Middlebury dataset <ref type="bibr" target="#b28">[29]</ref> is widely used for evaluation of optical flow estimation, stereo image matching, and frame interpolation methods. There are 12 sequences in the OTHER set and 8 sequences in the EVALU-ATION set with a resolution of 640 × 480 pixels. We use the evaluation protocol to compute the Interpolation Error (IE) and Normalized Interpolation Error (NIE).</p><p>UCF101. The UCF101 dataset <ref type="bibr" target="#b29">[30]</ref> contains a large variety of human actions. We use 379 triplets from the UCF101 test set, where the image resolution is 256 × 256 of pixels.</p><p>Vimeo90K. Xue et al. <ref type="bibr" target="#b8">[9]</ref> develop a high-quality dataset with videos from Vimeo (https://vimeo.com). There are 3,782 triplets for evaluation with the image resolution of 448×256 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HD videos.</head><p>In this work, we collect 7 HD (High Definition) videos from the Xiph website (https://media.xiph. org/video/derf/), and interpolate the first 50 even frames for each of the videos. We also evaluate on four short video clips from the Sintel dataset <ref type="bibr" target="#b52">[53]</ref>, where each image is of 1280 × 544 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Ablation Studies</head><p>Flow-based analysis. We first construct a baseline model by using the optical flow estimation network and bilinear interpolation for warping images. Similar to the ToFlow <ref type="bibr" target="#b8">[9]</ref> method, the baseline model does not contain the kernel estimation, mask estimation, and post-processing networks. <ref type="table" target="#tab_5">Table 2</ref> shows the performance of this baseline model is similar to that by the ToFlow method. We then include the mask estimation and post-processing (abbreviated by post-proc. in <ref type="table" target="#tab_5">Table 2</ref>) networks, where both modules clearly contribute to the performance on all the test datasets. By replacing the fixed bilinear kernel module with the proposed spatially-adaptive kernel estimation network, a significant performance gain can be achieved. Our final model with all the components achieves the state-of-the-art performance on all three benchmark datasets. The FlowNetS contains 38.6M parameters, which take 57.4% of the parameters in our MEMC-Net model. We conduct an experiment to use a fixed flow estimation network. The performance of this variant drops a lot on all the datasets, as shown in the last two rows of <ref type="table" target="#tab_5">Table 2</ref>. Without fine-tuning the flow estimation network, the projected flow is a simple approximation of the flow from the intermediate frame to the reference frame, which is not able to synthesize high-quality intermediate frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kernel-based analysis.</head><p>We conduct experiments to analyze the contribution of the learned interpolation kernels in the proposed method. We train a baseline model by removing the optical flow estimation network and only learn 4 × 4 spatially-adaptive kernels for interpolation. This baseline model is similar to the SepConv method <ref type="bibr" target="#b11">[12]</ref> but with a much smaller kernel. In <ref type="table" target="#tab_6">Table 3</ref>, it is interesting to see that this baseline model already outperforms the SepConv method on the UCF101 dataset. It is sufficient to use a 4 × 4 interpolation kernel for all videos as the image resolution is low and object motion is small. However, it does not perform well on the Vimeo90K and Middlebury datasets, which contain much larger motion. By introducing the flow   estimation network, the proposed method performs better than the evaluated models on the Vimeo90K and Middlebury datasets. The results demonstrate the importance of integrating optical flow and learned interpolation kernels to deal with large motions in frame interpolation.</p><p>Model parameters. Since modern mobile devices typically have limited memory size, we present a smaller model with fewer parameters but maintaining the same MEMC framework. We first replace the FlowNetS <ref type="bibr" target="#b20">[21]</ref> with the SPyNet <ref type="bibr" target="#b10">[11]</ref> model, which reduces about 97% parameters in the flow estimation network. We then simplify the kernel prediction network by removing one convolutional layer before each max-pooling and un-pooling layer and discard the occlusion estimation network. This reduced model, denoted by MEMC-Net s, has only 7,204,367 trainable parameters, which is 89.3% smaller than our full network. We compare the performance of the full and small models in <ref type="table" target="#tab_7">Table 4</ref>. As the SPyNet performs better than the FlowNetS <ref type="bibr" target="#b20">[21]</ref> on small motion <ref type="bibr" target="#b10">[11]</ref>, the performance of MEMC-Net s is slightly better than our full model on the Vimeo90K dataset. However, MEMC-Net s does not perform as well on the Middlebury dataset as it contains large displacement. Execution speed. We evaluate the runtime of the proposed algorithm on an NVIDIA Titan X (Pascal) GPU. We compare the execution time of the proposed method and the state-of-the-art algorithms on 640 × 480p, 1280 × 720p and 1920 × 1080p videos in <ref type="table">Table 5</ref>. Our MEMC-Net model can process 1920 × 1080p videos with the runtime of 0.41 second per frame. Moreover, when using four GPU cards to process a 1920 × 1080p videos in parallel by splitting input frames into 270 × 240 non-overlapped patches, our method is able to process 30 frames per second. We note that the small model MEMC-Net s does not necessarily have better runtime performance as the SPyNet applies several convolutional layers on the input resolution (which results in larger feature maps and a higher computational cost). On the other hand, the operations of the FlowNetS model are mostly applied on the 1/4 or smaller resolution space. The SPyNet uses 97% fewer parameters but has 1.35 times more FLOPs than the FlowNetS. In <ref type="table" target="#tab_8">Table 6</ref>, we show the runtime of each component in the proposed models. The small model can be used for memory-constrained devices while the full model is preferable for applications that require prompt response time. The proposed MEMC-Net* can be used for cases where the interpolation quality is of most importance. Our MEMC-Net s consists of only 7.2M parameters but performs better than a larger SepConv model which contains 21.7M parameters. Furthermore, the amount of parameters is not the only factor to be considered. Although the ToFlow+Mask model uses fewer parameters, it runs slower than the proposed and SepConv methods. We present the evaluation results of these algorithms in <ref type="table" target="#tab_9">Table 7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Video Frame Interpolation</head><p>We first provide the comparison with the sequential model and then present quantitative and qualitative evaluations with the state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Comparisons with the Sequential Model</head><p>We train the sequential model and present the quantitative results in <ref type="table">Table 8</ref>. Compared to the proposed approach, the performance of the sequential model is 0.61dB and 1.08dB lower on the UCF101 and Vimeo90K datasets, respectively. We attribute the performance difference to the flow warping errors on the motion boundary (which has occlusion and dis-occlusion). The proposed model avoid estimating kernels from warped images and leads to better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Comparisons with the State-of-the-arts</head><p>We evaluate the proposed MEMC-Net and MEMC-Net* against the kernel-based method (SepConv <ref type="bibr" target="#b11">[12]</ref>), flow-based algorithms (DVF <ref type="bibr" target="#b35">[36]</ref>, ToFlow <ref type="bibr" target="#b8">[9]</ref>, and CtxSyn <ref type="bibr" target="#b26">[27]</ref>), and a direct interpolation approach (MIND <ref type="bibr" target="#b6">[7]</ref>). The ToFlow method <ref type="bibr" target="#b8">[9]</ref> generates two results with and without learning occlusion masks. Two pre-trained models of the SepConv approach <ref type="bibr" target="#b11">[12]</ref> are available: the SepConv-L 1 model is optimized with a L 1 loss function while the SepConv-L f model uses both the L 1 loss and the perceptual loss <ref type="bibr" target="#b55">[56]</ref> for generating more realistic results. As no pre-trained model of the MIND method <ref type="bibr" target="#b6">[7]</ref> is available, we train their network on the Vimeo90K training set for evaluations. In addition to the above learning-based frame interpolation methods, we also use existing optical flow algorithms (SPyNet <ref type="bibr" target="#b10">[11]</ref> and EpicFlow <ref type="bibr" target="#b9">[10]</ref>) to directly interpolate frames. We show the interpolation results of the Middlebury EVALUATION set in <ref type="table" target="#tab_10">Table 9</ref>. These results are also publicly available on the Middlebury benchmark website (http://vision.middlebury.edu/flow/eval/results/ results-i1.php). For the sequences with smaller motions (i.e., the maximum flow magnitude is less than 10 pixels) or fine textures, such as the Mequon, Teddy and Schefflera, the CtxSyn method <ref type="bibr" target="#b26">[27]</ref> obtains the best results in terms of both IE and NIE metrics. In contrast, our MEMC-Net and MEMC-Net* perform well on the videos with complicated motion, e.g., the Backyard sequence with dancing feet and the Basketball video with moving arms and fingers. Notably, the SuperSlomo method <ref type="bibr" target="#b37">[38]</ref> generates the best results for the synthetic Urban sequence. On average, the proposed models perform favorably against the state-of-the-art approaches on the Middlebury dataset. In <ref type="table" target="#tab_2">Table 10</ref>, we present the average IE and NIE values with standard variances on the Middlebury dataset. Also in <ref type="figure" target="#fig_0">Fig. 13</ref>, we present error bars on the IE and NIE metrics to show the statistical comparison between different methods. The MEMC-Net* obtains lower interpolation error at smaller variance on different scenarios. <ref type="table" target="#tab_2">Table 11</ref> shows that the proposed methods perform favorably against the state-of-the-art approaches on the UCF101 <ref type="bibr" target="#b29">[30]</ref>, Vimeo90K <ref type="bibr" target="#b8">[9]</ref>, and Middlebury <ref type="bibr" target="#b28">[29]</ref> datasets. The numbers in red depict the best performance, while the numbers in blue depict the second-best performance. The diverse scenarios in these video datasets demonstrate that our model generalizes well to different types of motion. On the other hand, the MIND model <ref type="bibr" target="#b6">[7]</ref> trained on the same Vimeo90k training set does not perform well on the UCF101 and Middlebury datasets.</p><p>In <ref type="table" target="#tab_2">Table 12</ref>, we present the evaluation results on the HD videos, which typically contain much larger motion. Our approach consistently performs well against the state-ofthe-art methods on different resolutions. The performance gap between the proposed MEMC-Net and SepConv <ref type="bibr" target="#b11">[12]</ref> becomes larger especially on 1080p videos, which demonstrates that it is not feasible to handle large motion with the fixed kernel size (e.g., 51 × 51). Our MEMC-Net* with context information and residual blocks performs favorably against the existing methods with significant improvement up to 0.9dB (e.g., Alley2 and ParkScene).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.3">Qualitative Results</head><p>We present sample interpolation results from the evaluated datasets in <ref type="figure" target="#fig_0">Fig. 12, 14, 15, and 16</ref>. On the first row of <ref type="figure" target="#fig_0">Fig. 12</ref>, the EpicFlow <ref type="bibr" target="#b9">[10]</ref> and SPyNet <ref type="bibr" target="#b10">[11]</ref> methods do not reconstruct the straight lamppost due to inaccurate optical flow estimation. Both the SepConv-L 1 and SepConv-L f <ref type="bibr" target="#b11">[12]</ref> models cannot interpolate the lamppost well as the motion is larger than the size of the interpolation kernels. In contrast, our method reconstructs the lamppost well. On the second row of <ref type="figure" target="#fig_0">Fig. 12</ref>, the proposed method interpolates the falling ball with a clear shape with fewer artifacts on the leg of the girl.</p><p>In <ref type="figure" target="#fig_0">Fig. 15</ref>, the ToFlow <ref type="bibr" target="#b8">[9]</ref> and SepConv <ref type="bibr" target="#b11">[12]</ref> methods generate ghost effect around the hand. Due to large and non-rigid motion, flow-based methods are less effective in estimating accurate optical flow for interpolation, while kernel-based approaches are not able to infer motion beyond  <ref type="table">IE  NIE  IE  NIE  IE  NIE  IE  NIE  IE  NIE  IE  NIE  IE  NIE  IE  NIE  IE</ref>     <ref type="figure" target="#fig_0">Fig. 13</ref>. Error bars on the Middlebury sequences.</p><p>the size of local kernels. In contrast, our model reconstructs the hand with fewer visual artifacts. As shown in <ref type="figure" target="#fig_0">Fig. 16</ref>, the SepConv [12] method is not able to interpolate the scene well due to large motion. In contrast, the proposed method interpolates frames well with visually pleasing results when optical flow is not accurately estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Video Frame Enhancement</head><p>We use the Vimeo90K dataset <ref type="bibr" target="#b8">[9]</ref> to evaluate the proposed method on the video denoising, video super-resolution, and video deblocking tasks. There are 7,824 sequences in the Vimeo90k test set, and each contains 7 consecutive frames. The qualitative results for super-resolution, denoising and deblocking tasks are presented in <ref type="table" target="#tab_2">Table 13</ref>, <ref type="table" target="#tab_2">Table 14</ref> and <ref type="table" target="#tab_2">Table 15</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Super-Resolution.</head><p>We evaluate the proposed method on the widely used video super-resolution dataset developed by Liu et al. <ref type="bibr" target="#b56">[57]</ref>, which is denoted by BayeSR in <ref type="table" target="#tab_2">Table 13</ref>. The low-resolution image distortion for both the Vimeo90K and BayesSR datasets are generated by down-sampling the original high-resolution frames at the scaling ratio of 4 (use the MATLAB function imresize with the bicubic mode). And the evaluated algorithms are to up-sample the middle frame of a sequence in Vimeo90K dataset or each frame of a video in BayesSR dataset by a factor of 4. The DeepSR <ref type="bibr" target="#b50">[51]</ref> and ToFlow <ref type="bibr" target="#b8">[9]</ref> methods are CNN-based approaches for video super-resolution. In addition, we also compare with   <ref type="bibr" target="#b6">[7]</ref> (c) ToFlow <ref type="bibr" target="#b8">[9]</ref> (d) EpicFlow <ref type="bibr" target="#b9">[10]</ref> (e) SPyNet <ref type="bibr" target="#b10">[11]</ref> (f) SepConv-L f <ref type="bibr" target="#b11">[12]</ref> (g) SepConv-L 1 <ref type="bibr" target="#b11">[12]</ref> (h) MEMC-Net (i) MEMC-Net* (j) Ground Truth <ref type="figure" target="#fig_0">Fig. 14. Visual comparisons on Middlebury [29]</ref>. The sequences are from the OTHER set.</p><p>the BayesSR <ref type="bibr" target="#b56">[57]</ref> method. Since the single-image superresolution (SISR) is also a well-studied task, we include the state-of-the-art SISR method, EDSR <ref type="bibr" target="#b51">[52]</ref>, for evaluations.</p><p>We present the quantitative results on video superresolution in <ref type="table" target="#tab_2">Table 13</ref>. Our method performs favorably against the state-of-the-art approaches on both benchmark datasets. Compared to the state-of-the-art SISR method <ref type="bibr" target="#b51">[52]</ref>, MEMC-Net SR has fewer residual blocks and a smaller number of filters but obtains higher PSNRs on both the Vimeo90K and BayesSR datasets. Compared to existing video super-resolution approaches <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b56">[57]</ref>, our method is more favorable, especially on the BayesSR dataset. In <ref type="figure" target="#fig_0">Fig. 17</ref>, we present the video super-resolution results. In the first row, the EDSR <ref type="bibr" target="#b51">[52]</ref> does not restore the correct shape of the number "31" on the calendar. The results by the ToFlow <ref type="bibr" target="#b8">[9]</ref> and BayesSR <ref type="bibr" target="#b56">[57]</ref> methods contain artifacts and blurry pixels. In contrast, the proposed MEMC-Net SR model is able to restore sharper video frames.</p><p>Denoising. We evaluate our method with the ToFlow <ref type="bibr" target="#b8">[9]</ref> and V-BM4D <ref type="bibr" target="#b57">[58]</ref> algorithms. In addition, we train a single frame denoising model as the baseline. The model architecture is the same as the EDSR network except that the (a) MIND <ref type="bibr" target="#b6">[7]</ref> (b) ToFlow <ref type="bibr" target="#b8">[9]</ref> (c) SepConv-L f <ref type="bibr" target="#b11">[12]</ref> (d) SepConv-L 1 <ref type="bibr" target="#b11">[12]</ref> (e) MEMC-Net (f) Ground Truth <ref type="figure" target="#fig_0">Fig. 15</ref>. Visual comparisons on the Vimeo90K dataset <ref type="bibr" target="#b8">[9]</ref>.</p><p>(a) ToFlow <ref type="bibr" target="#b8">[9]</ref> (b) SepConv-L f <ref type="bibr" target="#b11">[12]</ref> (c) SepConv-L 1 <ref type="bibr" target="#b11">[12]</ref> (d) MEMC-Net (e) MEMC-Net* (f) Ground Truth <ref type="figure" target="#fig_0">Fig. 16</ref>. Visual comparisons on HD videos.</p><p>(a) Bicubic (b) EDSR <ref type="bibr" target="#b51">[52]</ref> (c) ToFlow <ref type="bibr" target="#b8">[9]</ref> (d) BayesSR <ref type="bibr" target="#b56">[57]</ref> (e) MEMC-Net SR (f) Ground Truth <ref type="figure" target="#fig_0">Fig. 17</ref>. Visual comparisons of video super-resolution methods.</p><p>(a) Noisy (b) EDSR DN <ref type="bibr" target="#b51">[52]</ref> (c) ToFlow <ref type="bibr" target="#b8">[9]</ref> (d) V-BM4D <ref type="bibr" target="#b57">[58]</ref> (e) MEMC-Net DN (f) Ground Truth <ref type="figure" target="#fig_0">Fig. 18</ref>. Visual comparisons of video denoising methods.</p><p>(a) Blocky (b) EDSR DB <ref type="bibr" target="#b51">[52]</ref> (c) ToFlow <ref type="bibr" target="#b8">[9]</ref> (d) V-BM4D <ref type="bibr" target="#b57">[58]</ref> (e) MEMC-Net DB (f) Ground Truth <ref type="figure" target="#fig_0">Fig. 19</ref>. Visual comparisons of video deblocking methods.</p><p>input images are with noise instead of low-resolution, and referred to as EDSR DN. We evaluate on the Vimeo90k test set as well as the dataset developed by Maggioni et al. <ref type="bibr" target="#b57">[58]</ref>. For the denoising experiments, we add Gaussian noise with  σ = 20 to synthesize noisy input frames. The quantitative results for video denoising are presented in <ref type="table" target="#tab_2">Table 14</ref>. Our method performs well on both datasets. The PSNR gains of MEMC-Net DN over the second best method are 1.24dB and 1.95dB on the Vimeo90K and V-BM4D datasets, respectively. In <ref type="figure" target="#fig_0">Fig. 18</ref>, the fine textures on the clothes and street are not well restored by the EDSR DN, ToFlow, and V-BM4D methods. In contrast, our MEMC-Net DN preserves these textures well.</p><p>Deblocking. For the video deblocking task, we use the same videos as in the denoising task. The images encoded by the widely used H.264 <ref type="bibr" target="#b58">[59]</ref> standard may generate blockiness due to the block-based approach. We use the FFmpeg software to encode the images in the Vimeo90K and V-BM4D datasets with libx264 and the quality parameter qp of 37, and disable the in-loop deblocking of the codec. We compare the proposed algorithm with the EDSR DB <ref type="bibr" target="#b51">[52]</ref>, ToFlow <ref type="bibr" target="#b8">[9]</ref> and V-BM4D <ref type="bibr" target="#b57">[58]</ref> methods.</p><p>The quantitative evaluation results for video deblocking are presented in <ref type="table" target="#tab_2">Table 15</ref>. Overall, the proposed model performs favorably against all the evaluated algorithms. In <ref type="figure" target="#fig_0">Fig. 19</ref>, the blocky regions around the hand and eye are sufficiently reduced by both MEMC-Net DB and V-BM4D <ref type="bibr" target="#b57">[58]</ref> methods. The ToFlow <ref type="bibr" target="#b8">[9]</ref> and EDSR DB schemes, however, do not reduce the blocky pixels well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS</head><p>In this work, we propose the motion estimation and motion compensation driven neural network for learning video frame interpolation and enhancement. Our model exploits the merits of the MEMC framework to handle large motion as well as the data-driven learning-based methods to extract effective features. Two network layers, namely the adaptive warping layer and flow projection layers, are proposed to tightly integrate all the sub-networks to make our model end-to-end trainable. The generalized motion compensated alignment of the proposed MEMC framework enables it to be extended to various video enhancement tasks such as video super-resolution, denoising, and deblocking. Quantitative and qualitative evaluations on the various benchmark datasets show that the proposed methods perform favorably against the state-of-the-art algorithms in video interpolation and enhancement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Visual comparisons with existing frame interpolation approaches. The proposed method MEMC-Net synthesizes the intermediate frame with clear edges and shape. With the context information and residual blocks used, the improved model MEMC-Net* obtains better outcome with fine details around motion boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Frameworks of (a) the sequential MEMC-Net model and (b) our proposed MEMC-Net model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Network architecture of the proposed MEMC-Net and MEMC-Net*. The context extraction module and its generated contextual features and warped contextual features are for MEMC-Net*.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Learned interpolation kernel k l r and bilinear kernel k d r . The k l r is re-organized from the output feature blob generated by kernel estimation network.kernel coefficients for sampling the local patch. As shown inFig. 5(a) and (b), the colors on the feature vector and the patch pixels show the mapping of the 16 channels. The red point inFig. 5(b)indicates the sub-pixel location shifted by the optical flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>x of time step t by S(x) := {y : round y + f t−1→t+1 (y)/2 = x, ∀ y ∈ [1, H] × [1, W ]} and denote the 4-directional nearest available flow vectors of a hole by N (x) := {x : |S(x )| &gt; 0}. The forward pass of the proposed projection layer is defined by:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Outside-in strategy for filling the flow holes. The green regions indicate a hole, where the flow vectors are approximated by the average of 4-directional available flow vectors from the non-hole regions. (a) zero filling (b) outside-in (c) zero filling (d) outside-in (IE = 2.76) (IE = 2.66) Effectiveness of the used outside-in strategy for hole filling. (a) and (b) are the flow maps by zero filling and outsidein strategy. (c) and (d) are the generated frames by them. Less artifact is generated by the outside-in hole filling strategy. IE is short for interpolation error. The lower, the better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Proposed post-processing network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Network architecture for frame enhancement. Network architecture for video frame superresolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>SepConv-L f (e) SepConv-L 1 (f) MEMC-Net (g) MEMC-Net* Visual comparisons on Middlebury<ref type="bibr" target="#b28">[29]</ref>. The sequences are from the EVALUATION set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The recent years have witnessed</figDesc><table /><note>• Wenbo Bao, Xiaoyun Zhang and Zhiyong Gao are with the Department of Electrical Engineering, Shanghai Jiao Tong University, Shanghai, 200240. Email: {baowenbo|xiaoyun.zhang|zhiyong.gao}@sjtu.edu.cn • Wei-Sheng Lai and Ming-Hsuan Yang are with the Department of Electrical Engineering and Computer Science, University of California, Merced, CA, 95340. Email:{wlai24|mhyang}@ucmerced.edu significant advances in optical flow estimation via varia- tional optimization</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 . CNN-based frame interpolation methods.</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>Optical</cell><cell>Occlusion</cell><cell>Interpolation</cell><cell>Kernel</cell></row><row><cell></cell><cell>flow</cell><cell>mask</cell><cell>coefficients</cell><cell>size</cell></row><row><cell>Flow-based [9], [36]</cell><cell></cell><cell></cell><cell>fixed</cell><cell>2 × 2</cell></row><row><cell>Kernel-based [12], [28]</cell><cell>-</cell><cell>-</cell><cell>adaptive</cell><cell>41 × 41, 51 × 51</cell></row><row><cell>MEMC-Net (Ours)</cell><cell></cell><cell></cell><cell>adaptive</cell><cell>4 × 4</cell></row><row><cell cols="5">to handle large motion, which requires a large amount of</cell></row><row><cell cols="5">memory to process high-resolution images. For an input</cell></row><row><cell cols="5">frame of H × W pixels, the AdaConv model needs to</cell></row><row><cell cols="5">estimate H × W × R × R coefficients for interpolation,</cell></row><row><cell cols="5">where R is the size of the local kernels. To reduce memory</cell></row><row><cell cols="3">requirements, the SepConv method</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>(a) illustrates this Flow Estimation Context Extraction Mask Estimation Optical Flows Occlusion Masks Interpolation Kernels Contextual Features Kernel Estimation Post-processing Frame − 1 Flow Projection Layer Adaptive Warping Layer</head><label></label><figDesc></figDesc><table><row><cell cols="2">Projected Flows</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Blended Frame</cell></row><row><cell></cell><cell cols="2">Warped Frames</cell><cell>Blended Frame</cell><cell>concatenate</cell><cell>+</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Interpolation Kernels</cell><cell>Interpolated Frame</cell></row><row><cell>Frame + 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Interpolation Kernels</cell><cell>Projected Flows</cell><cell></cell><cell></cell></row><row><cell cols="2">Adaptive</cell><cell>-</cell><cell></cell></row><row><cell cols="2">Warping Layer</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Warped Contexture Features</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>(b). Two pairs of intermediate flow and the kernel coefficients, {f t→t−1 , K t−1 } and {f t→t+1 , K t+1 } are then fed into the proposed adaptive warping layer to warp the input frames by Eq. (1)</figDesc><table /><note>and generate two warped framesÎ t−1 andÎ t+1 .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 2 . Analysis on flow-based methods.</head><label>2</label><figDesc>The red numbers indicate the best performance.</figDesc><table><row><cell>Methods</cell><cell></cell><cell>Sub-networks</cell><cell></cell><cell></cell><cell cols="2">UCF101 [30]</cell><cell cols="2">Vimeo90K [9]</cell><cell>Middlebury [29]</cell><cell>#Param.</cell></row><row><cell></cell><cell>flow</cell><cell>kernel (size)</cell><cell cols="2">mask post-proc.</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>IE (oth.)</cell><cell></cell></row><row><cell>DVF [36] ToFlow+Mask [9]</cell><cell>Enc-Dec SPyNet</cell><cell>bilinear (2) bilinear (2)</cell><cell>√ √</cell><cell>× √</cell><cell>34.12 34.58</cell><cell>0.9631 0.9667</cell><cell>31.54 33.73</cell><cell>0.9462 0.9682</cell><cell>7.75 2.51</cell><cell>1,604,547 1,074,635</cell></row><row><cell>MEMC-Net</cell><cell>FlowNetS FlowNetS FlowNetS FlowNetS FlowNetS FlowNetS FlowNetS (fixed) FlowNetS</cell><cell>bilinear (2) bilinear (2) bilinear (2) bilinear (2) learned (4) learned (4) learned (4) learned (4)</cell><cell>× × √ √ × √ √ √</cell><cell>× √ × √ √ × √ √</cell><cell>34.65 34.70 34.69 34.76 34.77 34.88 33.15 34.95</cell><cell>0.9664 0.9667 0.9667 0.9671 0.9669 0.9669 0.9632 0.9679</cell><cell>32.73 33.25 32.94 33.40 33.29 33.51 32.05 34.02</cell><cell>0.9606 0.9646 0.9626 0.9661 0.9663 0.9670 0.9580 0.9704</cell><cell>2.81 2.50 2.72 2.47 2.42 2.37 3.26 2.24</cell><cell>38,676,496 38,922,323 52,842,818 53,088,645 53,092,995 67,013,490 28,583,973 67,260,469</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3 . Analysis on kernel-based methods.</head><label>3</label><figDesc></figDesc><table><row><cell>Methods</cell><cell></cell><cell cols="2">Sub-networks</cell><cell></cell><cell cols="2">UCF101 [30]</cell><cell cols="2">Vimeo90K [9]</cell><cell>Middlebury [29]</cell><cell>#Param.</cell></row><row><cell></cell><cell>flow</cell><cell>kernel (size)</cell><cell>mask</cell><cell>post-proc.</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>IE (oth.)</cell><cell></cell></row><row><cell>SepConv-L f [12]</cell><cell>×</cell><cell>learned (51)</cell><cell>×</cell><cell>×</cell><cell>34.69</cell><cell>0.9655</cell><cell>33.45</cell><cell>0.9674</cell><cell>2.44</cell><cell>21,675,452</cell></row><row><cell>SepConv-L 1 [12]</cell><cell>×</cell><cell>learned (51)</cell><cell>×</cell><cell>×</cell><cell>34.78</cell><cell>0.9669</cell><cell>33.79</cell><cell>0.9702</cell><cell>2.27</cell><cell>21,675,452</cell></row><row><cell>MEMC-Net</cell><cell cols="2">× × FlowNetS learned (4) learned (4) learned (4)</cell><cell>× × √</cell><cell>× √ √</cell><cell>34.89 34.97 34.95</cell><cell>0.9682 0.9682 0.9679</cell><cell>32.73 33.31 34.02</cell><cell>0.9581 0.9633 0.9704</cell><cell>2.74 2.57 2.24</cell><cell>14,710,415 14,720,783 67,260,469</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4 .</head><label>4</label><figDesc></figDesc><table><row><cell>Methods</cell><cell cols="4">UCF101 [30] Vimeo90K [9] M.B. [29] #Param.</cell></row><row><cell></cell><cell>PSNR SSIM</cell><cell>PSNR SSIM</cell><cell>IE (oth.)</cell></row><row><cell>MEMC-Net s</cell><cell cols="2">34.83 0.9676 33.97 0.9721</cell><cell>2.44</cell><cell>7.2M</cell></row><row><cell>MEMC-Net</cell><cell cols="2">34.95 0.9679 34.02 0.9704</cell><cell>2.24</cell><cell>67.2M</cell></row><row><cell>MEMC-Net*</cell><cell cols="2">35.01 0.9683 34.40 0.9742</cell><cell>2.10</cell><cell>70.3M</cell></row><row><cell>Methods</cell><cell>640 × 480p</cell><cell>1280 × 720p</cell><cell cols="2">1920 × 1080p</cell></row><row><cell>AdaConv [28]</cell><cell>2.80</cell><cell>-</cell><cell>-</cell></row><row><cell>ToFlow [9]</cell><cell>0.43</cell><cell>1.01</cell><cell>1.90</cell></row><row><cell>SepConv [12]</cell><cell>0.20</cell><cell>0.50</cell><cell>0.90</cell></row><row><cell>MEMC-Net s</cell><cell>0.13</cell><cell>0.33</cell><cell>0.67</cell></row><row><cell>MEMC-Net</cell><cell>0.06</cell><cell>0.20</cell><cell>0.41</cell></row><row><cell>MEMC-Net*</cell><cell>0.12</cell><cell>0.36</cell><cell>0.64</cell></row></table><note>Evaluation on models with fewer model parameters.M.B. stands for Middlebury.TABLE 5. Runtime of frame interpolation methods (seconds).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 6 .</head><label>6</label><figDesc>Runtime</figDesc><table><row><cell>Networks</cell><cell cols="3">flow kernel mask context post-proc.</cell><cell>Total</cell></row><row><cell>MEMC-Net s</cell><cell>0.103 0.005 -</cell><cell>-</cell><cell>0.020</cell><cell>0.13</cell></row><row><cell>MEMC-Net</cell><cell>0.024 0.008 0.008</cell><cell>-</cell><cell>0.020</cell><cell>0.06</cell></row><row><cell>MEMC-Net*</cell><cell cols="2">0.024 0.008 0.008 0.001</cell><cell>0.080</cell><cell>0.12</cell></row></table><note>of the proposed models (seconds). We evaluate these models on 640 × 480 videos.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 7 . Quantitative evaluation on UCF101, Vimeo90K, and Middlebury datasets.</head><label>7</label><figDesc>The abbreviations oth. and eval. represent the OTHER and EVALUATION sets in the Middlebury dataset. The runtime is evaluated on the 640 × 480 sequences.</figDesc><table><row><cell>Methods</cell><cell></cell><cell>#Parameters (million)</cell><cell>Runtime (seconds)</cell><cell cols="2">UCF101 [30] PSNR SSIM</cell><cell cols="2">Vimeo90K [9] PSNR SSIM</cell><cell cols="2">Middlebury [29] IE (oth.) IE (eval.)</cell></row><row><cell cols="2">ToFlow+Mask [9]</cell><cell>1.07</cell><cell>0.43</cell><cell>34.58</cell><cell>0.9667</cell><cell>33.73</cell><cell>0.9682</cell><cell>2.51</cell><cell>-</cell></row><row><cell cols="2">SepConv-L 1 [12]</cell><cell>21.6</cell><cell>0.20</cell><cell>34.78</cell><cell>0.9669</cell><cell>33.79</cell><cell>0.9702</cell><cell>2.27</cell><cell>5.61</cell></row><row><cell cols="2">MEMC-Net s</cell><cell>7.20</cell><cell>0.13</cell><cell>34.83</cell><cell>0.9676</cell><cell>33.97</cell><cell>0.9721</cell><cell>2.44</cell><cell>-</cell></row><row><cell cols="2">MEMC-Net</cell><cell>67.2</cell><cell>0.06</cell><cell>34.95</cell><cell>0.9679</cell><cell>34.02</cell><cell>0.9704</cell><cell>2.24</cell><cell>5.35</cell></row><row><cell cols="2">MEMC-Net*</cell><cell>70.3</cell><cell>0.12</cell><cell>35.01</cell><cell>0.9683</cell><cell>34.40</cell><cell>0.9742</cell><cell>2.10</cell><cell>5.00</cell></row><row><cell cols="5">TABLE 8. Quantitative comparisons with the sequential</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">model. M.B. stands for Middlebury.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="4">UCF101 [30] Vimeo90K [9] M.B. [29] #Param.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>PSNR SSIM</cell><cell>PSNR SSIM</cell><cell>IE (oth.)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sequential</cell><cell cols="2">34.34 0.9652 32.94 0.9639</cell><cell>2.47</cell><cell>67.2M</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MEMC-Net</cell><cell cols="2">34.95 0.9679 34.02 0.9704</cell><cell>2.24</cell><cell>67.2M</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9 .</head><label>9</label><figDesc>Quantitative results on the Middlebury EVALUATION set. The red numbers indicate that corresponding method takes the 1st place among all the evaluated algorithms.</figDesc><table><row><cell>Methods</cell><cell>Mequon</cell><cell>Schefflera</cell><cell>Urban</cell><cell>Teddy</cell><cell>Backyard</cell><cell>Basketball</cell><cell>Dumptruck</cell><cell>Evergreen</cell><cell>Average</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>.62 3.79 0.70 4.28 1.06 6.37 1.09 11.2 1.18 6.23 1.10 8.11 1.00 8.76 1.04 6.49 0.97 MDP-Flow2 [54] 2.89 0.59 3.47 0.62 3.66 1.24 5.20 0.94 10.2 0.98 6.13 1.09 7.36 0.70 7.75 0.78 5.83 0.87 DeepFlow2 [55] 2.98 0.62 3.88 0.74 3.62 0.86 5.39 0.99 11.0 1.04 5.91 1.02 7.14 0.63 7.80 0.96 5.97 0.86 SepConv-L 1 [12] 2.52 0.54 3.56 0.67 4.17 1.07 5.41 1.03 10.2 0.99 5.47 0.96 6.</figDesc><table><row><cell></cell><cell></cell><cell>NIE</cell></row><row><cell>EpicFlow [10]</cell><cell>3.17 088 0.68</cell><cell>6.63 0.70 5.61 0.83</cell></row><row><cell>SuperSlomo [38]</cell><cell>2.51 0.59 3.66 0.72 2.91 0.74 5.05 0.98 9.56 0.94 5.37 0.96 6.69 0.60</cell><cell>6.73 0.69 5.31 0.78</cell></row><row><cell>CtxSyn [27]</cell><cell>2.24 0.50 2.96 0.55 4.32 1.42 4.21 0.87 9.59 0.95 5.22 0.94 7.02 0.68</cell><cell>6.66 0.67 5.28 0.82</cell></row><row><cell>MEMC-Net</cell><cell>2.83 0.64 3.84 0.73 4.16 0.84 5.75 0.99 8.57 0.93 4.99 0.96 5.86 0.60</cell><cell>6.83 0.69 5.35 0.80</cell></row><row><cell>MEMC-Net*</cell><cell>2.39 0.59 3.36 0.64 3.37 0.80 4.84 0.88 8.55 0.88 4.70 0.85 6.40 0.64</cell><cell>6.37 0.63 5.00 0.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 10 . Average IE and NIE values with standard variances on Middlebury benchmark.</head><label>10</label><figDesc></figDesc><table><row><cell>Method</cell><cell>IE</cell><cell>NIE</cell></row><row><cell>EpicFlow [10]</cell><cell>6.48 ± 2.75</cell><cell>0.97 ± 0.20</cell></row><row><cell>MDP Flow2 [54]</cell><cell>5.83 ± 2.52</cell><cell>0.86 ± 0.23</cell></row><row><cell>DeepFlow2 [55]</cell><cell>5.96 ± 2.65</cell><cell>0.85 ± 0.17</cell></row><row><cell>SepConv L 1 [12]</cell><cell>5.60 ± 2.37</cell><cell>0.83 ± 0.20</cell></row><row><cell>SuperSlomo [38]</cell><cell>5.31 ± 2.34</cell><cell>0.77 ± 0.16</cell></row><row><cell>CtxSyn [27]</cell><cell>5.27 ± 2.39</cell><cell>0.82 ± 0.29</cell></row><row><cell>MEMC-Net</cell><cell>5.35 ± 1.81</cell><cell>0.79 ± 0.15</cell></row><row><cell>MEMC-Net*</cell><cell cols="2">4.99 ± 2.02 0.73 ± 0.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 11 . Quantitative evaluation on UCF101, Vimeo90K, and Middlebury datasets.</head><label>11</label><figDesc>The abbreviations oth. and eval. represent the OTHER and EVALUATION sets in the Middlebury dataset. The numbers in red depict the best performance, while the numbers in blue depict the second-best performance.</figDesc><table><row><cell>Methods</cell><cell cols="2">UCF101 [30]</cell><cell cols="2">Vimeo90K [9]</cell><cell cols="2">Middlebury [29]</cell></row><row><cell></cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>IE (oth.)</cell><cell>IE (eval.)</cell></row><row><cell>SPyNet [11]</cell><cell>33.67</cell><cell>0.9633</cell><cell>31.95</cell><cell>0.9601</cell><cell>2.49</cell><cell>-</cell></row><row><cell>EpicFlow [10]</cell><cell>33.71</cell><cell>0.9635</cell><cell>32.02</cell><cell>0.9622</cell><cell>2.47</cell><cell>6.48</cell></row><row><cell>MIND [7]</cell><cell>33.93</cell><cell>0.9661</cell><cell>33.50</cell><cell>0.9429</cell><cell>3.35</cell><cell>-</cell></row><row><cell>DVF [36]</cell><cell>34.12</cell><cell>0.9631</cell><cell>31.54</cell><cell>0.9462</cell><cell>7.75</cell><cell>-</cell></row><row><cell>ToFlow [9]</cell><cell>34.54</cell><cell>0.9666</cell><cell>33.53</cell><cell>0.9668</cell><cell>-</cell><cell>-</cell></row><row><cell>ToFlow+Mask [9]</cell><cell>34.58</cell><cell>0.9667</cell><cell>33.73</cell><cell>0.9682</cell><cell>2.51</cell><cell>-</cell></row><row><cell>SepConv-L f [12]</cell><cell>34.69</cell><cell>0.9655</cell><cell>33.45</cell><cell>0.9674</cell><cell>2.44</cell><cell>-</cell></row><row><cell>SepConv-L 1 [12]</cell><cell>34.78</cell><cell>0.9669</cell><cell>33.79</cell><cell>0.9702</cell><cell>2.27</cell><cell>5.61</cell></row><row><cell>MEMC-Net s</cell><cell>34.83</cell><cell>0.9676</cell><cell>33.97</cell><cell>0.9721</cell><cell>2.43</cell><cell>-</cell></row><row><cell>MEMC-Net</cell><cell>34.95</cell><cell>0.9679</cell><cell>34.02</cell><cell>0.9704</cell><cell>2.24</cell><cell>5.35</cell></row><row><cell>MEMC-Net*</cell><cell>35.01</cell><cell>0.9683</cell><cell>34.40</cell><cell>0.9742</cell><cell>2.10</cell><cell>5.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 12 . Quantitative evaluation on HD videos.</head><label>12</label><figDesc></figDesc><table><row><cell>Video</cell><cell>Resolution</cell><cell cols="2">ToFlow+Mask [9]</cell><cell cols="2">SepConv-L f [12]</cell><cell cols="2">SepConv-L 1 [12]</cell><cell cols="2">MEMC-Net</cell><cell cols="2">MEMC-Net*</cell></row><row><cell></cell><cell></cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell>Alley2</cell><cell>544p</cell><cell>26.30</cell><cell>0.7997</cell><cell>28.26</cell><cell>0.8462</cell><cell>28.52</cell><cell>0.8646</cell><cell>29.57</cell><cell>0.8845</cell><cell>29.60</cell><cell>0.8920</cell></row><row><cell>Market5</cell><cell>544p</cell><cell>18.21</cell><cell>0.7324</cell><cell>20.59</cell><cell>0.7878</cell><cell>20.57</cell><cell>0.8012</cell><cell>21.16</cell><cell>0.8074</cell><cell>21.64</cell><cell>0.8105</cell></row><row><cell>Temple1</cell><cell>544p</cell><cell>25.20</cell><cell>0.9174</cell><cell>26.42</cell><cell>0.9295</cell><cell>26.69</cell><cell>0.9370</cell><cell>27.25</cell><cell>0.9354</cell><cell>27.18</cell><cell>0.9390</cell></row><row><cell>Temple2</cell><cell>544p</cell><cell>19.90</cell><cell>0.8246</cell><cell>21.74</cell><cell>0.8471</cell><cell>21.93</cell><cell>0.8533</cell><cell>22.72</cell><cell>0.8628</cell><cell>22.94</cell><cell>0.8532</cell></row><row><cell>Parkrun</cell><cell>720p</cell><cell>27.77</cell><cell>0.8841</cell><cell>28.69</cell><cell>0.9083</cell><cell>29.03</cell><cell>0.9158</cell><cell>29.07</cell><cell>0.9125</cell><cell>29.15</cell><cell>0.9145</cell></row><row><cell>Shields</cell><cell>720p</cell><cell>34.10</cell><cell>0.8884</cell><cell>34.55</cell><cell>0.9093</cell><cell>34.91</cell><cell>0.9188</cell><cell>35.21</cell><cell>0.9206</cell><cell>35.49</cell><cell>0.9251</cell></row><row><cell>Stockholm</cell><cell>720p</cell><cell>33.53</cell><cell>0.8534</cell><cell>33.99</cell><cell>0.8669</cell><cell>34.27</cell><cell>0.8826</cell><cell>34.64</cell><cell>0.8894</cell><cell>34.89</cell><cell>0.8931</cell></row><row><cell>Kimono</cell><cell>1080p</cell><cell>33.34</cell><cell>0.9107</cell><cell>34.07</cell><cell>0.9168</cell><cell>34.31</cell><cell>0.9287</cell><cell>34.93</cell><cell>0.9341</cell><cell>34.99</cell><cell>0.9363</cell></row><row><cell>ParkScene</cell><cell>1080p</cell><cell>33.49</cell><cell>0.9233</cell><cell>35.27</cell><cell>0.9374</cell><cell>35.51</cell><cell>0.9451</cell><cell>36.20</cell><cell>0.9491</cell><cell>36.64</cell><cell>0.9521</cell></row><row><cell>Sunflower</cell><cell>1080p</cell><cell>33.75</cell><cell>0.9476</cell><cell>34.88</cell><cell>0.9539</cell><cell>35.02</cell><cell>0.9605</cell><cell>35.42</cell><cell>0.9616</cell><cell>35.59</cell><cell>0.9638</cell></row><row><cell>Bluesky</cell><cell>1080p</cell><cell>37.53</cell><cell>0.9673</cell><cell>38.32</cell><cell>0.9730</cell><cell>38.83</cell><cell>0.9775</cell><cell>39.28</cell><cell>0.9791</cell><cell>39.55</cell><cell>0.9801</cell></row><row><cell>Average</cell><cell></cell><cell>29.37</cell><cell>0.8772</cell><cell>30.61</cell><cell>0.8978</cell><cell>30.87</cell><cell>0.9077</cell><cell>31.40</cell><cell>0.9124</cell><cell>31.60</cell><cell>0.9145</cell></row><row><cell cols="2">(a) Overlay</cell><cell cols="2">(b) MIND</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 13 . Quantitative evaluation for video super-resolution.TABLE 14 . Quantitative evaluation for video denoising.</head><label>1314</label><figDesc></figDesc><table><row><cell>Frame #Num.</cell><cell>Methods</cell><cell cols="2">Vimeo90K [9] PSNR SSIM</cell><cell cols="2">BayesSR [57] PSNR SSIM</cell></row><row><cell>1</cell><cell>Bicubic EDSR [52]</cell><cell>29.79 33.08</cell><cell>0.9036 0.9411</cell><cell>22.17 23.93</cell><cell>0.7391 0.8113</cell></row><row><cell></cell><cell>DeepSR [51]</cell><cell>25.55</cell><cell>0.8498</cell><cell>21.85</cell><cell>0.7535</cell></row><row><cell></cell><cell>BayesSR [57]</cell><cell>24.64</cell><cell>0.8205</cell><cell>21.95</cell><cell>0.7369</cell></row><row><cell>7</cell><cell>ToFlow [9]</cell><cell>33.08</cell><cell>0.9417</cell><cell>23.54</cell><cell>0.8070</cell></row><row><cell></cell><cell>MEMC-Net SR</cell><cell>33.47</cell><cell>0.9470</cell><cell>24.37</cell><cell>0.8380</cell></row><row><cell>Frame #Num.</cell><cell>Methods</cell><cell cols="2">Vimeo90K [9] PSNR SSIM</cell><cell cols="2">V-BM4D [58] PSNR SSIM</cell></row><row><cell>1</cell><cell>Noisy EDSR DN [52]</cell><cell>22.63 35.11</cell><cell>0.5007 0.9513</cell><cell>22.28 32.02</cell><cell>0.4715 0.8828</cell></row><row><cell></cell><cell>ToFlow [9]</cell><cell>32.66</cell><cell>0.9198</cell><cell>30.19</cell><cell>0.8699</cell></row><row><cell></cell><cell>V-BM4D [58]</cell><cell>34.39</cell><cell>0.9217</cell><cell>32.27</cell><cell>0.8913</cell></row><row><cell>7</cell><cell>MEMC-Net DN</cell><cell>36.35</cell><cell>0.9642</cell><cell>34.22</cell><cell>0.9310</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE 15 . Quantitative evaluation for video deblocking.</head><label>15</label><figDesc></figDesc><table><row><cell>Frame #Num.</cell><cell>Methods</cell><cell cols="2">Vimeo90K [9] PSNR SSIM</cell><cell cols="2">V-BM4D [58] PSNR SSIM</cell></row><row><cell>1</cell><cell>Blocky EDSR DB [52]</cell><cell>31.99 32.87</cell><cell>0.9179 0.9319</cell><cell>29.38 29.66</cell><cell>0.8302 0.8362</cell></row><row><cell></cell><cell>ToFlow [9]</cell><cell>32.57</cell><cell>0.9292</cell><cell>29.59</cell><cell>0.8390</cell></row><row><cell>7</cell><cell>V-BM4D [58]</cell><cell>32.74</cell><cell>0.9293</cell><cell>29.94</cell><cell>0.8435</cell></row><row><cell></cell><cell>MEMC-Net DB</cell><cell>33.37</cell><cell>0.9388</cell><cell>30.14</cell><cell>0.8498</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A method for motion adaptive frame rate up-conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Castagno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haavisto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ramponi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="436" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deepstereo: Learning to predict new views from the world&apos;s imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Neulander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling and optimization of high frame rate video transmission over wireless networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-M</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Wireless Communications</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2713" to="2726" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Truemotion estimation with 3-D recursive search block matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Biezen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huijgen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Ojo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="368" to="379" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">High-order model and dynamic filtering for frame rate up-conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3813" to="3826" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enabling adaptive high-frame-rate video streaming in mobile cloud gaming applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-M</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning image matching by simply watching video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kneip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Video enhancement with task-oriented flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09078</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Epicflow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bayesian estimation of motion vector fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dubois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="910" to="927" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Overlapped block motion compensation: An estimation-theoretic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="693" to="699" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A new diamond search algorithm for fast block-matching motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="287" to="290" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A multilevel successive elimination algorithm for block matching motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="501" to="504" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Frame rate upconversion using trilateral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large displacement optical flow from nearest neighbor fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2443" to="2450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accurate optical flow via direct cost volume processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1289" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Motioncompensated frame interpolation using bilateral motion estimation and adaptive overlapped block motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="407" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On handling of occlusion for frame rate up-conversion using video in-painting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Namboodiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="785" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Frame rate up conversion based on variational image fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Ra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="399" to="412" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">New frame rate up-conversion algorithms with low computational complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Sunwoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="384" to="393" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A low complexity motion compensated frame interpolation method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and System</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="4927" to="4930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Context-aware synthesis for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12- 01</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A fast hierarchical motion vector estimation algorithm using mean pyramid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="344" to="351" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Frame rate upconversion using optical flow and patch-based reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Kaviani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1581" to="1594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Phase-based frame interpolation for video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for optical flow with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Frame interpolation with multi-scale deep loss functions and generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06045</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Super slomo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A spatiotemporal autoregressive frame rate up conversion scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A spatio-temporal auto regressive model for frame rate upconversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1289" to="1301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Large displacement optical flow: descriptor matching in variational motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Motioncompensated frame rate up-conversionpart ii: New algorithms for frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blanchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klepko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Broadcasting</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Two deterministic half-quadratic regularization algorithms for computed imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Charbonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Blanc-Féraud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Aubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barlaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ADAM: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Video super-resolution via deep draft-ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Motion detail preserving optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A bayesian approach to adaptive video super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Video denoising, deblocking, and enhancement through separable 4-d nonlocal spatiotemporal transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boracchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Overview of the H.264/AVC video coding standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bjontegaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luthra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="560" to="576" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Wenbo Bao is a Ph.D. candidate of Electrical</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
