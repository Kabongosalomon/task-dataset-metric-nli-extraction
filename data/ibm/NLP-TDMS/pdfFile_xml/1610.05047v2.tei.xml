<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning Prototype Domains for Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Schumann</surname></persName>
							<email>arne.schumann@iosb.fraunhofer.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schuchert</surname></persName>
							<email>tobias.schuchert@iosb.fraunhofer.de</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer IOSB</orgName>
								<address>
									<settlement>Karlsruhe</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Shaogang Gong Queen</orgName>
								<orgName type="institution">Mary University of London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Fraunhofer IOSB</orgName>
								<address>
									<settlement>Karlsruhe</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning Prototype Domains for Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person re-identification (re-id) is the task of matching multiple occurrences of the same person from different cameras, poses, lighting conditions, and a multitude of other factors which alter the visual appearance. Typically, this is achieved by learning either optimal features or matching metrics which are adapted to specific pairs of camera views dictated by the pairwise labelled training datasets. In this work, we formulate a deep learning based novel approach to automatic prototype-domain discovery for domain perceptive (adaptive) person re-id (rather than camera pair specific learning) for any camera views scalable to new unseen scenes without training data. We learn a separate re-id model for each of the discovered prototype-domains and during model deployment, use the person probe image to select automatically the model of the closest prototypedomain. Our approach requires neither supervised nor unsupervised domain adaptation learning, i.e. no data available from the target domains. We evaluate extensively our model under realistic re-id conditions using automatically detected bounding boxes with low-resolution and partial occlusion. We show that our approach outperforms most of the state-of-the-art supervised and unsupervised methods on the latest CUHK-SYSU and PRW benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of re-identifying the same person across different cameras has attracted much interest in recent years. Person re-identification is at its core a cross-domain recognition problem. Datasets are usually recorded in a camera network setting with a fixed set of cameras and viewing angles. Consequently, most approaches interpret each camera as a separate visual domain and focus on developing features or metrics that can robustly recognize a person within such camera-view-perspective domains. In this work, we <ref type="bibr">Figure 1</ref>. We use the latest PRW <ref type="bibr" target="#b37">[37]</ref> (top row) and CUHK-SYSU <ref type="bibr" target="#b33">[33]</ref> (bottom row) datasets as target (test) domains for evaluation, unavailable to model learning. Both datasets provide many camera views and unsegmented video frames which requires auto-persondetection for a more realistic person re-id evaluation. consider other camera-view-independent factors, such as pose, illumination, occlusions, and background influence the visual appearance of a person, and we wish to explore them as visual domains in constructing camera-view independent re-id models for better scalability to unknown camera views.</p><p>In this work, we propose a two-stage approach to automatically discover visual domains in large amounts of diverse data and use them to learn feature embeddings for person re-identification (see <ref type="figure">Figure 1</ref>). In the first stage, we pool data from a large amount of re-identification datasets, independent from the test domains, to capture a large degree of visual variation in the training data. We then explore clustering based on feature learning in convolutional neural networks (CNNs) to automatically discover dominant (prototype) visual domains. In the second stage, we again apply CNNs to learn feature embeddings in each of the prototype arXiv:1610.05047v2 [cs.CV] <ref type="bibr" target="#b19">19</ref> Sep 2017 domains in order to support domain perceptive (sensitive) person re-id during testing with automatic domain selection. We learn one embedding per domain. This allows the model to learn specific details about each individual prototype domain while ignoring others. For example, an embedding learned for a domain which predominantly contains people of dark-dress does not need to encode information relevant to distinguishing a person dressed in light blue colors from a person dressed in white clothes. By doing so, the domain perceptive embedding focuses on learning subtle discriminative characteristics among similar visual appearances. On testing, a probe image is first matched to its most likely domain. Then, the feature embedding learned on that domain is used to perform re-identification. Note, this approach is purely inductive. It does not require any training data (labelled or unlabelled) from the target (test) domains, and the model is designed to scale to any new target domains. Our approach is particularly well suited to scenarios in which no fixed set of camera views is available (i.e. no fixed domain borders are specified). We thus evaluate it on the latest CUHK-SYSU and PRW datasets, which contain images from diverse sources of mobile cameras, movies and fixed view cameras, with multitude of view angles, backgrounds, resolutions and poses. Our approach yields the state-of-theart accuracy on CUHK-SYSU and is competitive on PRW. This is without using target domain data in our model training whilst all other methods compared in the evaluation exploit target domain data in their model learning.</p><p>Our contributions are: (1) We formulate a novel approach to automatic discovery of prototype-domains, characterising person visual appearance with domain perceptive awareness. (2) We develop a deep learning model for domain perceptive (DLDP) selection and re-id matching in a single automatic process without any supervised nor unsupervised domain transfer learning. (3) We show the significant advantage of our model by outperforming the stateof-the-art on the CUHK-SYSU benchmark <ref type="bibr" target="#b33">[33]</ref> with up to 5.6% at Rank-1 re-id, and being competitive on the PRW benchmark <ref type="bibr" target="#b37">[37]</ref> of 45.4% Rank-1 re-id compared to the 47.7% state-of-the-art, notwithstanding that the latter benefited from model learning on target domain data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Most re-id approaches can be grouped into two categories: feature based approaches and metric based approaches. The former type aims to develop a robust feature representation. The latter approach focuses on optimizing a distance metric that, given any feature, yields small distances for matching person images and large distances of images of different people. In recent years, deep learning methods have gained significant advantages on image classification, and have been applied to person re-id. Many deep learning approaches focus on feature learning. Yi et al. <ref type="bibr" target="#b35">[35]</ref> split person images into three regions and learn separate feature maps which are combined into a final feature through a fully connected layer. Cosine distance is used to perform re-id. Ding et al. <ref type="bibr">[7]</ref> apply a triplet loss in order to train a feature whose Euclidean distance of a matching image pair is smaller than that of a pair of images of different people. Xiao et al. <ref type="bibr" target="#b32">[32]</ref> propose to use dataset specific dropout to learn features over multiple smaller datasets simultaneously. Cheng et al. <ref type="bibr">[4]</ref> propose an improved triplet loss function which emphasises small distances for similar image pairs. We et al. <ref type="bibr" target="#b31">[31]</ref> combine hand-crafted features with CNN features for re-id metric learning. Other deep learning approaches focus on studying network layers specifically designed for person re-id. Li et al. <ref type="bibr" target="#b18">[18]</ref> describe a filter pairing network to model translation, occlusion and background clutter in its architecture. Ahmend et al. <ref type="bibr">[1]</ref> introduce a neighborhood matching layer for improving robustness to translation and pose change. This layer is also applied by Wu et al. <ref type="bibr" target="#b30">[30]</ref> to train an end-to-end re-id net which directly outputs a (dis-) similarity decision without relying on a separate distance function. Xiao et al. <ref type="bibr" target="#b33">[33]</ref> propose an approach which combines person detection and re-id into a single CNN for simultaneous person detection and computing re-id feature for each detection.</p><p>A few studies have addressed cross-domain re-id by using target domain data for supervised <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b29">29]</ref> or unsupervised <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b25">25]</ref> domain adaptation. Others have evaluated their models on datasets without any adaptation to the target domain <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b35">35]</ref>. To our knowledge, the proposed model in this work, for the first time, does not rely on domain adaptation using target domain data whilst learning domain perceptive re-id for unknown target domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The central objective of our approach is to learn a domain adaptive re-id model (domain perceptive) which is scalable to new and unseen data without requiring any manual labelling for model training on the new target domains. We propose a two-stage approach to achieve this: (1) In the first stage, characteristic and dominant (prototype) domains are automatically discovered in large amounts of diverse data (Section 3.1.2); (2) In the second stage, this information is used to train a number of domain specific embeddings by deep learning for person re-id (Section 3.2.2). An overview of our approach is given in <ref type="figure">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Automatic Domain Discovery</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Divergent Data Sampling</head><p>A key requirement for a meaningful domain discovery is divergent data sampling which aims to provide a large range of realistic visual variation. In order to achieve such a high degree of variation, we pool a number of publicly available  <ref type="table">Table 1</ref> shows the sources used to construct the DLDP domain discovery dataset. We resize all bounding boxes to a uniform size of 160 × 80 pixels. This DLDP divergent data sampling allows us to discover domains which cover a large and diverse spectrum of possible variation in person visual appearance. We show in our experiments its suitability for generalisation to person re-id in new/unseen camera domains. <ref type="bibr">1</ref> The DLDP dataset will be made publically available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Prototype-Domain Discovery</head><p>We wish to explore deep learning based clustering to discover dominant (prototype) visual domains from the multisource pooled DLDP dataset. In particular, we exploit the concept of unsupervised deep embedding space learning proposed in <ref type="bibr" target="#b34">[34]</ref>, but importantly, adopted to utilise the available person id labels from the re-id datasets. Our supervised deep learning clustering model alternates between (1) training a CNN to learn a feature embedding from the re-id image datasets and (2) applying conventional k-means clustering in the embedding space to find clusters. To initialize the weights of our feature embedding CNN, we train the model using the person ID labels available in the data. Our model architecture is given in <ref type="table">Table 2</ref> (Section 3.2.2 for more details). We set the last, fully connected layer of the network to 4,786 dimensions and train using person ID labels in a one-hot encoding and a softmax loss for person ID classification. The multi-source dataset ensures that the influence of any particular dataset's bias on the initial feature embedding is reduced. Moreover, we apply data augmentation by cropping and flipping the images, and also ensure unbiased sampling by selecting images from different data sources in DLDP with equal frequency. Image cropping is performed by resizing an image to 30 × 10 pixels larger than the net requires and randomly cropping it down to the correct size. Through data augmentation, the hypothetical data pool size is increased by a factor of 600. We ensure unbiased sampling by selecting images from different data sources in DLDP with equal frequency. This results in more data augmentation on the smaller data sources (among the ten sources in the DLDP dataset) so to prevent the CNN from overfitting to the larger data sources. For the domain discovery part (k-means clustering) the person ID softmax loss layer is replaced by a softmax loss which corresponds to the number of clusters, set to eight in our current model 2 . Thus, after a supervised initialization, the domain discovery continues in an unsupervised manner. The initialization from a person re-identification net is crucial to the success of our prototype domain discovery. The re-identification training ensures that the initial model does not react strongly to the dataset biases present in our feature pool. This prevents the clustering from simply discovering trivial dataset boundaries as prototype domain boundaries and instead, lets the model focuses more on the content of each person bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Training Strategy</head><p>For training of our deep clustering model we use a low initial learning rate of 0.001. This ensures that the cluster embedding does not deviate too quickly from its re-id label constrained initialization. Given the initial embedding, we perform 25 runs of k-means clustering in the embedding space and select the best result for the next refinement of the embedding (i.e. step 2 in Section 3.1.2). This ensures stability of the iterative training process. The refinement (fine-tuning) of the embedding CNN is then performed for a further 10,000 training iterations (i.e. step 1 in Section 3.1.2). We divide the learning rate of the embedding by 10 every two iterations of the discovery process. This iterative process is repeated until less than 1% of images change their cluster assignments. Some examples of learned prototype domains (i.e. clusters in the embedding feature space) with their corresponding images are shown in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Deep Learning Domain Perceptive Re-Id Model</head><p>The second stage of our DLDP re-id model consists of learning a domain-sensitive re-id model for each prototype domain. That is, we train one feature embedding with all person ID labels for each of the discovered clusters in the feature embedding space resulted from the first stage. To that end, we start by training a common generic baseline re-id model on all available data without considering the domains. The individual domain models are then trained by fine-tuning this baseline model. The same baseline model is also used as initialization for the domain discovery approach described in Section 3.1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Baseline Generic Model</head><p>As a baseline approach we train a model of the architecture given in <ref type="table">Table 2</ref> (Section 3.2.2 for more details) on all <ref type="bibr">Figure 3</ref>. Example domains discovered by our approach using the proposed initialization with a re-id net (top 3 rows, supervised initialization) and initialization by weights learned through autoencoding (AE) (bottom 3 rows, unsupervised initialization). The re-id initialization leads to more semantically meaningful domain (e.g. light-colored, yellow and blue clothing). The AE initialization is strongly influenced by dataset bias and learns domains corresponding to datasets (e.g. CAVIAR4REID, 3DPeS, PRID).</p><p>available training data to learn a generic feature embedding without domain specific adaptation. We train the baseline model for 60,000 iterations. The initial learning rate is set to 0.1 and divided by 10 after every 20,000 iterations. We use the output of the 512 dimensional layer (fc feat in Table 2) just before the loss as our feature embedding for person re-id. The resulting features are compared using cosine distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Domain Embeddings</head><p>In order to learn feature embedding focused on each of the domains we need to first create suitable domain-specific training data. For any person ID in a given domain we thus select all of that person's images and add them to the training data for the domain. This data sampling method allows the domain models to specialize and focus particularly on the visual cues relevant to persons from their domain while not having to also learn how to distinguish persons from different domains.</p><p>The architecture <ref type="table">(Table 2)</ref> we use to learn the domainspecific feature embedding is motivated by a number of recent studies. It consists of an inital set of four convolutional layers with filter sizes of 3 × 3. This configuration of multiple layers with small filter sizes was shown to perform well for image classification in the VGG nets <ref type="bibr" target="#b26">[26]</ref>. We further adopt insights from <ref type="bibr" target="#b27">[27]</ref> and <ref type="bibr" target="#b28">[28]</ref> to add multiple (four) inception layers to our network. We modify the original inception architecture by replacing the 5 × 5 layer with two 3 × 3 layers, reducing the grid size and expanding filter banks as suggested in <ref type="bibr" target="#b28">[28]</ref>. We apply batch normalization <ref type="bibr" target="#b12">[13]</ref> after each layer and use a softmax loss based on the person ID labels for training. Our feature embeddings are of size 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Training Strategy</head><p>We begin training by disregarding the identified domain borders and combining all available person IDs into one softmax layer. We train this net for an initial 60,000 iterations with a learning rate on 0.1 which is divided by 10 every 20,000 iterations. After this, we continue to train individually for each domain relying only the corresponding data pool. The dimension of the softmax layers is adapted accordingly. For each domain we continue training for 30,000 iterations at an inital learning rate of 0.001. Our input images are resized to a size of 210 × 70. Data augmentation is then performed by randomly flipping images and randomly cropping them to a final input size of 180 × 60. Similar to <ref type="bibr" target="#b31">[31]</ref> we apply hard negative mining by selecting misclassified training images and fine-tuning each net for a further 10,000 iterations at a reduced learning rate of 0.00001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Automatic Domain Selection</head><p>After model training and during model deployment, a probe person image is first matched to its most likely domain by the deep clustering model (Section 3.1). The corresponding domain embedding (domain specific re-id model) is then used to rank the gallery images by computing the corresponding 512 dimensional embedding and using cosine distance for matching the probe image. Note, the camera view and the target domain of the probe image is new, i.e. unseen and independent from any of the multi-sources used to construct the DLDP dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets: We evaluate our model on two publicly available large re-id datasets: CUHK-SYSU <ref type="bibr" target="#b33">[33]</ref> and PRW <ref type="bibr" target="#b37">[37]</ref>, both of which are independent/unseen from the ten multi-source data pool used to construct our DLDP domain discovery training dataset. Both datasets contain a large number of viewing angles. CUHK-SYSU consists of pedestrian images collected by handheld cameras as well as scenes from movies and the PRW dataset was collected with six cameras on a campus environment. The datasets contain 8432 and 932 person ids and 99,809 and 34,304 bounding boxes, respectively. Both datasets provide full images to enable automatically detected person bounding boxes to be evaluated in person re-id, subject to occlusion, bbox misalignment, and large changes in resolution/low-resolution. Some example images of both datasets are depicted in <ref type="figure">Figure 1</ref>. These characteristics of the two datasets allow us to investigate the generalization capability of our approach, its ability to handle large amounts of varying views and to evaluate its performance against automatically detected person bboxes for more realistic evaluation. Evaluation protocol: A central objective of our approach is not to require any training data on the target domain for the re-id task. To that end, in the experiments we only used the test part of both datasets. The CUHK-SYSU dataset contains a fixed set of 2,900 query persons and gallery sets of multiple sizes (at most 6,978 images). The PRW dataset contains a fixed query set of 2,057 bounding boxes and a gallery size of 6,112 test images. Note that in both datasets each gallery image contains multiple persons and an automatic person detector may generate additional false posistive bounding boxes. We follow the exact evaluation protocols specified in <ref type="bibr" target="#b33">[33]</ref> and <ref type="bibr" target="#b37">[37]</ref> respectively, and used the provided evaluation code where applicable. Also note, both datasets contain many persons without id in the galleries, i.e. the re-id tasks in these datasets are potentially open-set given the unknown distractors in the target population. To give a direct comparison to the reported results in <ref type="bibr" target="#b33">[33]</ref> and <ref type="bibr" target="#b37">[37]</ref>, we also adopt mean Averaged Precision (mAP) and Rank-1 accuracy as evaluation metrics.</p><p>Comparison with the state-of-the-art: To demonstrate the effectiveness of our approach, we compared our model directly to the state-of-the-art reported in <ref type="bibr" target="#b33">[33]</ref> and <ref type="bibr" target="#b37">[37]</ref>, using both manually labelled person bounding boxes (ground truth) and automatically detected bounding boxes. Results on the CUHK-SYSU dataset for gallery sizes of 100 images   are given in <ref type="table" target="#tab_3">Table 3</ref>. Our baseline generic re-id model (Section 3.2.1) given manually labelled person bounding boxes (ground truth) as input outperforms not only <ref type="bibr" target="#b33">[33]</ref> using conventional image features but also the deep IDNet model which has the advantage of being trained on the CUHK-SYSU dataset itself at Rank-1 by 1.9%. The reason is likely a combination of our deeper 10 layer network architecture, the use of inception layers and batch normalization. For our domain adaptive model given manually labelled person bounding boxes, our model outperforms <ref type="bibr" target="#b33">[33]</ref> by 7.5% and 5.6% in mAP and Rank-1 respectively, a further improvement of 6% in both mAP and Rank-1 over our generic baseline model. This suggests that the DLDP model learning for prototype-domain adaptive re-id is more effective than the "blind" generic model. For automatic detection generated person bounding boxes, we adopt the SSD VOC500 person detector <ref type="bibr" target="#b20">[20]</ref>. For re-id given these automatic detections, our prototypedomain adaptive model outperforms the state-of-the-art person search deep model <ref type="bibr" target="#b33">[33]</ref> by 2.06% and 1.89% on mAP and Rank-1 respectively, despite a very critical difference that the person search deep model <ref type="bibr" target="#b33">[33]</ref> was trained jointly for person detection and re-identification using part of the CUHK-SYSU dataset, i.e. both their detector and their re-id matching model were trained and fine-tuned on the target domain. In contrast, our DLDP model did not benefit from training detectors in the target domain, nor fine-tuning re-id model on the target domain data.</p><p>For the evaluation on the PRW benchmark, we compared DLDP to a baseline using BoW features and XQDA metric learing <ref type="bibr" target="#b19">[19]</ref> and two deep feature embeddings IDE and IDE det from <ref type="bibr" target="#b37">[37]</ref> which are based on the AlexNet <ref type="bibr" target="#b14">[15]</ref> architecture, trained on ImageNet and fine-tuned for re-id on PRW. For person detection, we used both the DPM person detector <ref type="bibr">[8]</ref> trained on the INRIA dataset <ref type="bibr">[6]</ref> provided by <ref type="bibr" target="#b37">[37]</ref> and the SSD detectors for a fair comparison. Our results are shown in <ref type="table" target="#tab_4">Table 4</ref>. All reported results were obtained by considering five bounding boxes per gallery image which is the value at which the methods reported in <ref type="bibr" target="#b37">[37]</ref> perform best. It is evident that the SSD detectors decrease re-id performance for all models as the SSD detectors seem to perform poorly on the PRW dataset. Regardless, our model outperforms both the BOW+XQDA baseline and the deep IDE feature embedding reported in <ref type="bibr" target="#b37">[37]</ref> when identical DPM person detector was used, by 2.2% and 7.4% in mAP and Rank-1, respectively (for the more accurate DPM detections). The improved deep IDE det embedding of <ref type="bibr" target="#b37">[37]</ref> is trained by fine-tuning the AlexNet first for person/background classification followed by further finetuning for re-id. It outperforms DLDP by 2.9% and 2.3% in mAP and Rank-1 accuracy. However, our performance remains competitive and has its unique advantage over IDE det embedding. This is because that IDE det was not only trained directly on PRW but also specifically adapted on the PRW for sensitivity to false positive person detections. DLDP benefited from none of that.</p><p>In summary, our DLDP model (given the output from comparable/identical person detectors) outperforms most of the state-of-the-art person re-identification methods on both the CUHK-SYSU and the PRW benchmark datasets. It is even more significant that our results are obtained without any labelled data training on the target test domains whilst all other methods require training data from the target domains. Qualitative examples on both datasets are shown in <ref type="figure">Figure 8</ref>, including two failure cases in the last row. Note that the incorrect results for all queries have a color composition or clothing configuration that is reasonably similar to the query image. In particular, DLPD understandably ranks near-identically looking people (PRW, row 2) very high. In the failure case on PRW our model appears to focus on the structural pattern created by the bikes in combination with white-dressed persons. Effects from gallery size increase: The CUHK-SYSU dataset offers multiple gallery sets of varying sizes. This  <ref type="table">Table 5</ref>. Comparison of DLDP to <ref type="bibr" target="#b33">[33]</ref> for different gallery sizes on CUHK-SYSU. Results of <ref type="bibr" target="#b33">[33]</ref> were obtained using the provided code.</p><p>Deep+Kissme <ref type="bibr" target="#b33">[33]</ref>  allows us to evaluate the influence of larger numbers of detractors on re-identification accuracy in a more realistic open-set setting. <ref type="table">Table 5</ref> shows results of our DLDP model in comparison to those achieved by the end-to-end person search deep network model <ref type="bibr" target="#b33">[33]</ref>, where results were obtained by running the code provided by the authors. Our obtained results closely match those reported in <ref type="bibr" target="#b33">[33]</ref>  <ref type="figure" target="#fig_2">(Figure 7  b)</ref>). Overall, our DLDP model consistently ourperforms the end-to-end person search deep model by a constant 2% in mAP regardless gallery size; 3% in Rank-1 for low gallery sizes of 50 images (correspoding to 256 bounding boxes) and up to 5.4% in Rank-1 for the largest possible gallery of all 6978 images (36984 bounding boxes). This suggests that the DLDP model is less sensitive to increase in gallery size, even without benefiting from learning on target domains. Effects from occlusion and low-resolution: Finally, we evaluated the effects of occlusion and low-resolution probe images. The CUHK-SYSU dataset provides two probe subsets for this purpose, which were created by sampling heavily occluded probe images and the 10% probe images with the lowest resolutions, respectively. Gallery sizes for this evaluation are fixed at 100 images. We report results using the SSD VOC500 person detection in <ref type="table" target="#tab_6">Table 6</ref> and compared to the end-to-end person search deep network model. Consistent with the observation made by <ref type="bibr" target="#b33">[33]</ref>, an occluded probe image causes more difficulty for re-id than that of low-resolution imagery. For low-resolution, our DLDP model suffers only a 15% loss in mAP and Rank-1, as compared to a 20% decrease for the end-to-end person search deep model. On occlusion, the reported results on the end-to-end person search model are less affected (reduced by 16.0% mAP and 19.4% Rank-1) than our DLDP model whose performance is reduced by 18.8% in mAP and 25.6% in Rank-1. However, using the author provided code, we could not re-create the same results as reported in <ref type="bibr" target="#b33">[33]</ref> for the occlusion test. Instead, we obtained the result on the end-to-end person search model for occlusion test 5% lower than reported, almost identical to that of DLDP.</p><p>To gain more insight, we further report the re-id results from our DLDP model on the occlusion and low-resolution tests but this time using manually labelled person bounding boxes ( <ref type="table" target="#tab_7">Table 7)</ref>. The overall results are much improved by relying on ground truth detections. This may partly be due to that ground truth labelled bboxes resemble more closely to data the model was trained on, therefore the negative impact of low-resolution query images is less severe. This suggests that the resolution gap between probe and gallery can be handled well by DLDP provided person bbox detection is reasonably accurate without significant misalignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we presented a novel approach to domain sensitive person re-identification by deep learning without the need for training data from the target (test) domains. The new model DLDP automatically discovers prototype domains from independent diverse datasets and learns specific feature embedding for each of the discovered domains. In model deployment, each query image is used to select for the most suitable feature embedding with its corresponding domain best fitting the query before the ranking match against the gallery candidates. Our approach has a singificant unique advantage, over all existing models, of not requiring any target domain data for model learning. Our extensive comparative evaluation on two latest benchmark datasets demonstrate clearly that the proposed DLDP model outperforms the state-of-the-art or is competitive, notwithstanding that all other models benefit from having been trained on the target domain data. It is also evident that the proposed new DLDP model copes well with real-world re-id conditions when automatic person detection, occlusion, low resolution and very large gallery sizes (i.e. open world) are unavoidable in model deployment. Future work includes investigating in more detail the impact of the num-  ber of domains on the accuracy of our approach as well as ways of coupling the domain discovery and learning of domain embeddings more directly in an end-to-end approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>This work was supported in part by the research travel grant of the Karlsruhe House of Young Scientists (KHYS).</p><p>We additionally evaluted DLDP on the Market-1501 <ref type="bibr" target="#b36">[36]</ref> dataset. This dataset does not provide full images but was created using the DPM detector instead of manual annotations. Results of DLDP in comparison to state-of-theart approaches are given in <ref type="table" target="#tab_8">Table 8</ref>. DLDP outperforms many recent approaches and performs en-par with the approach of <ref type="bibr">[1]</ref> (-0.12% mAP and -0.44 Rank-1). DLPD is clearly outperformed by <ref type="bibr">[9]</ref> and <ref type="bibr">[7]</ref> which both achieve a significant improvement over the previous state-of-the-art and beat DLPD by up to 13.32% in mAP and 14.42% in Rank-1. All approaches given in <ref type="table" target="#tab_8">Table 8</ref> make full use of the Market training dataset with the noteable exception of <ref type="bibr">[5]</ref> which uses only 13.58% of the training data to adapt model pretrained on other data. DLPD clearly outperforms this result without using any of the training data.</p><p>A qualitative impression of the results of DLPD on the Market-1501 dataset is depicted in <ref type="figure">Figure 8</ref>. Again, it can be observed that most of the incorrect results are quite reasonable and share one or more salient features with the query person (e.g. the black backpack in the second row or either a bag-strap or gray shirt in the fourth row). The last row shows a query for which only one out of five true matches is returned among the top 15 results. We consider this a near failure case. A success implies that the model finds all of the existing true matches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-Truth Detections on PRW</head><p>In <ref type="table">Table 9</ref> we show the performance of DLDP and our baseline model (see Section 3.2.1 in the main paper) on the PRW dataset. We compare to the BoW+XQDA baseline which is provided with the evaluation code of <ref type="bibr" target="#b37">[37]</ref>. Compared to the CUHK-SYSU dataset (compare <ref type="table" target="#tab_3">Table 3</ref>, main paper) the improvement in accuracy achieved by relying on ground-truth is much less pronounced for all approaches. This is likely due to the fact that the ground-truth on PRW was obtained in part using the DPM-Inria person detector, thus giving that detector an unusually high localization accuracy on the dataset. This also explaines the comparatively weak performance of the SSD detectors observed in <ref type="table" target="#tab_4">Table  4</ref> of the main paper. DLDP is able to maintain its advantage over the BoW+XQDA baseline on ground-truth and outperforms it by 1.8% mAP and 8.5% Rank-1.  <ref type="table">Table 9</ref>. DLDP re-id performance on the PRW dataset using ground-truth annotations instead of automatic detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full CMC-Curves</head><p>In <ref type="figure">Figures 5, 6</ref> and 7 we give the full CMC curves for our main experiments on CUHK-SYSU, PRW, and Market-1501, respectively. All curves were generated using the evaluation code provided with the datasets and are compared to the strongest baseline approaches for which code was provided.</p><p>In <ref type="figure">Figure 5</ref> we show DLDP's average accuracy over the first 50 ranks on the CUHK-SYSU dataset. DLDP shows a consistent improvement of more than 5% over the baseline model and narrowly but consistently outperforms the deep PersonSearch approach which integrates detection and re-id into one CNN. <ref type="figure">Figure 5</ref>. CMC curve of DLDP on CUHK-SYSU compared to our baseline model and the PersonSearch approach presented in <ref type="bibr" target="#b33">[33]</ref>. Results are obtained using the default gallery size of 100 images and the SSD-VOD500 detector for DLDP and the baseline model. <ref type="figure">Figure 6</ref> shows the first 50 ranks on the PRW dataset.</p><p>Our baseline model performs en-par with the BoW+XQDA baseline. Both appraoches are again consistently outperformed by more than 5% by DLDP. <ref type="figure">Figure 6</ref>. CMC curve of DLDP on PRW compared to our baseline model and the BoW+XQDA baseline provided with the evaluation code. Results were obtained by considering the top 5 detections in each image. All approaches are evaluated on the provided detections of the DPM-Inria detector.</p><p>The average accuracy over the first 50 ranks on the Market-1501 dataset is depicted in <ref type="figure" target="#fig_2">Figure 7</ref>. The difference between DLDP, our baseline and the BoW+KISSME baseline is less significant on this dataset. However, in the important segment of ranks 1-20 DLDP has a clear advantage. For ranks above 38 the BoW+KISSME approach actually outperforms both our baseline model and DLDP narrowly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix-Literature</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>(a) The top 8 re-id matches by the DLDP model on the CUHK-SYSU test data for a set of five randomly chosen queries from the 100 image gallery setting, and (b) five randomly chosen queries on the PRW test data. Note, rank-2 and rank-3 in the "yellow T-shirt" example in (b) are false matches even though they look very similar. The bottom examples from both (a) and (b) show failure cases when the model failed to find a match in the top 8 ranks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .</head><label>7</label><figDesc>CMC curve of DLDP on Market-1501 compared to our baseline model and the BoW+KISSME baseline provided with the evaluation code. Results are for the single-query setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">DLDP re-id performance comparison against both super-</cell></row><row><cell cols="4">vised (KISSME, IDNet, Person Search) and unsupervised (Eu-</cell></row><row><cell cols="3">clidean, BoW) methods on the CUHK-SYSU dataset.</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">mAP Rank-1</cell></row><row><cell>DPM Inria</cell><cell>IDE [37] IDE det [37] BoW + XQDA [37] Baseline Model DLDP</cell><cell>13.7 18.8 12.1 12.9 15.9</cell><cell>38.0 47.7 36.2 36.5 45.4</cell></row><row><cell>SSD</cell><cell>BoW + XQDA (SSD VOC300) DLDP (SSD VOC300) DLDP (SSD VOC500)</cell><cell>6.8 10.1 11.8</cell><cell>26.6 35.3 37.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>DLDP re-id performance on the PRW dataset in comparison to state-of-the-art. All results are obtained by considering 5 bounding boxes per image. Note that all approaches except ours were trained (supervised) on the PRW dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Comparisons on the CUHK-SYSU occlusion and low resolution tests.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="8">ACF+BOW [33] Person Search [33] Person Search [33] rerun DLDP (SSD VOC500)</cell></row><row><cell></cell><cell>mAP</cell><cell>top-1</cell><cell>mAP</cell><cell>top-1</cell><cell>mAP</cell><cell>top-1</cell><cell>mAP</cell><cell>top-1</cell><cell>mAP</cell><cell>top-1</cell></row><row><cell>Whole</cell><cell>39.1</cell><cell>44.9</cell><cell>42.4</cell><cell>48.4</cell><cell>55.7</cell><cell>62.7</cell><cell>55.79</cell><cell>62.17</cell><cell>57.7</cell><cell>64.6</cell></row><row><cell cols="2">Occlusion 18.2</cell><cell>17.7</cell><cell>29.1</cell><cell>32.1</cell><cell>39.7</cell><cell>43.3</cell><cell>34.97</cell><cell>37.43</cell><cell>38.9</cell><cell>39.0</cell></row><row><cell>LowRes</cell><cell>43.8</cell><cell>42.8</cell><cell>44.9</cell><cell>53.8</cell><cell>35.7</cell><cell>42.1</cell><cell>35.21</cell><cell>40.00</cell><cell>41.9</cell><cell>49.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Results of DLDP on the occlusion and low resolution test sets using ground-truth detections.</figDesc><table><row><cell></cell><cell cols="5">mAP Rank-1 Rank-5 Rank-10 Rank-20</cell></row><row><cell>DLDP GT Whole</cell><cell>74.0</cell><cell>76.7</cell><cell>86.4</cell><cell>89.7</cell><cell>92.9</cell></row><row><cell cols="2">DLDP GT Occlusion 56.0</cell><cell>54.0</cell><cell>69.5</cell><cell>76.5</cell><cell>83.4</cell></row><row><cell>DLDP GT LowRes</cell><cell>72.0</cell><cell>74.1</cell><cell>86.9</cell><cell>89.7</cell><cell>93.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>DLDP's performance in context of many recent state-ofthe-art approaches for the single-query setting on the Market-1501 dataset.</figDesc><table><row><cell></cell><cell cols="2">mAP Rank-1</cell></row><row><cell>Gated S-CNN [7]</cell><cell>39.55</cell><cell>65.88</cell></row><row><cell>DNS [9]</cell><cell>35.68</cell><cell>61.02</cell></row><row><cell>SCSP [1]</cell><cell>26.35</cell><cell>51.90</cell></row><row><cell>DLDP</cell><cell>26.23</cell><cell>51.46</cell></row><row><cell cols="2">Multiregion Bilinear DML [6] 26.11</cell><cell>45.58</cell></row><row><cell>End-to-end CAN [3]</cell><cell>24.43</cell><cell>48.24</cell></row><row><cell>TMA LOMO [5]</cell><cell>22.31</cell><cell>47.92</cell></row><row><cell>WARCA-L [2]</cell><cell>-</cell><cell>45.16</cell></row><row><cell>MST-CNN [4]</cell><cell>-</cell><cell>45.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We choose 8 clusters as a tradeoff between the number of domains available to our model and the computational effort involved in training our domain-specific embeddings. Future work can further optimise it.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Evaluation on Market-1501</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3dpes: 3d people dataset for surveillance and forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Baltieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the joint ACM workshop on Human gesture and behavior understanding</title>
		<meeting>the joint ACM workshop on Human gesture and behavior understanding</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sarc3d: a new 3d body model for people tracking and re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Baltieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Processing</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Person re-identification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Custom pictorial structures for re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stoppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep feature learning with relative distance comparison for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The hda+ data set for research on fully automated re-identification systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Figueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taiana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nambiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bernardino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Person re-identification by descriptive and discriminative classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Scandinavian conference on Image analysis</title>
		<meeting>the Scandinavian conference on Image analysis</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cross dataset person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2288" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Domain transfer for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Layne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th</title>
		<meeting>the 4th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<title level="m">ACM/IEEE international workshop on Analysis and retrieval of tracked events and motion in imagery stream</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Locally aligned feature transforms across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02325</idno>
		<title level="m">Ssd: Single shot multibox detector</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Time-delayed correlation analysis for multi-camera activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cross-domain person reidentification using domain adaptation ranking svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Person re-identification over camera networks using multi-task distance metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dataaugmentation for reducing dataset bias in person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mclaughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Del Rincon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<meeting>the IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsupervised cross-dataset transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00567</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cross-scenario transfer person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Personnet: Person re-identification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.07255</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">An enhanced deep feature representation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-end deep learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Conference on Computer Vision (ECCV)</title>
		<meeting>the 14th European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Patter Recognition (ICPR)</title>
		<meeting>the International Conference on Patter Recognition (ICPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02531</idno>
		<title level="m">Person re-identification in the wild</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Associating groups of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Similarity learning with spatial constraints for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scalable metric learning via weighted approximate rank component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">End-to-end comparative attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04404</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-scale triplet cnn for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM Multimedia Conference (ACMMM)</title>
		<meeting>the 2016 ACM Multimedia Conference (ACMMM)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Temporal model adaptation for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Martinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Micheloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Multiregion bilinear convolutional neural networks for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.05300</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">End-to-end deep learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Conference on Computer Vision (ECCV)</title>
		<meeting>the 14th European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning a discriminative null space for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02531</idno>
		<title level="m">Person re-identification in the wild</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">The top 15 re-id matches by the DLDP model on the Market-1501 dataset. Correct matches are framed red. Matches labelled as &quot;junk&quot; in the dataset and not considered in the evaluation protocol are framed blue</title>
		<imprint/>
	</monogr>
	<note>Figure 8. The last row shows a failure case where only one. out</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
