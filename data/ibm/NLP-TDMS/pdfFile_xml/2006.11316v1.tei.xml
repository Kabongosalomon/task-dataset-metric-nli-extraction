<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SqueezeBERT: What can computer vision teach NLP about efficient neural networks?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-19">19 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
							<email>forresti@berkeley.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><forename type="middle">E</forename><surname>Shaw</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Krishna</surname></persName>
							<email>ravi.krishna@berkeley.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><forename type="middle">W</forename><surname>Keutzer</surname></persName>
							<email>keutzer@berkeley.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley EECS</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">UC Berkeley EECS</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SqueezeBERT: What can computer vision teach NLP about efficient neural networks?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-19">19 Jun 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets, large computing systems, and better neural network models, natural language processing (NLP) technology has made significant strides in understanding, proofreading, and organizing these messages. Thus, there is a significant opportunity to deploy NLP in myriad applications to help web users, social networks, and businesses. In particular, we consider smartphones and other mobile devices as crucial platforms for deploying NLP models at scale. However, today's highly-accurate NLP neural network models such as BERT and RoBERTa are extremely computationally expensive, with BERT-base taking 1.7 seconds to classify a text snippet on a Pixel 3 smartphone. In this work, we observe that methods such as grouped convolutions have yielded significant speedups for computer vision networks, but many of these techniques have not been adopted by NLP neural network designers. We demonstrate how to replace several operations in self-attention layers with grouped convolutions, and we use this technique in a novel network architecture called SqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test set. The SqueezeBERT code will be released.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Motivation</head><p>The human race writes over 300 billion messages per day <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">3,</ref><ref type="bibr">4]</ref>. Out of these, more than half of the world's emails are read on mobile devices, and nearly half of Facebook users exclusively access Facebook from a mobile device <ref type="bibr">[5,</ref><ref type="bibr">6]</ref>. Natural language processing (NLP) technology has the potential to aid these users and communities in several ways. When a person writes a message, NLP models can help with spelling and grammar checking as well as sentence completion. When content is added to a social network, NLP can facilitate content moderation before it appears in other users' news feeds. When a person consumes messages, NLP models can help classify messages into folders, composing news feeds, prioritizing messages, and identifying duplicate messages.</p><p>In recent years, the development and adoption of Attention Neural Networks has led to dramatic improvements in almost every area of NLP. In 2017, Vaswani et al. proposed the multi-head selfattention module, which demonstrated superior accuracy to recurrent neural networks on English-German machine language translation <ref type="bibr">[7]</ref>. 1 These modules have since been adopted by <ref type="bibr">GPT [8]</ref> and BERT <ref type="bibr" target="#b0">[9]</ref> for sentence classification, and by GPT-2 <ref type="bibr" target="#b1">[10]</ref> and CTRL <ref type="bibr" target="#b2">[11]</ref> for sentence completion and generation. Recent works such as ELECTRA <ref type="bibr" target="#b3">[12]</ref> and RoBERTa <ref type="bibr" target="#b4">[13]</ref> have shown that larger datasets and more sophisticated training regimes can further improve the accuracy of self-attention networks.</p><p>Considering the enormity of the textual data created by humans on mobile devices, a natural approach is to deploy NLP models themselves on the mobile devices themselves, embedding them in common apps that are used to read, write, and share text. Unfortunately, many of today's best state-of-the-art NLP models may are often rather computationally expensive, often making mobile deployment impractical. For example, we observe that running the BERT-base network on a Google Pixel 3 smartphone approximately 1.7 seconds to classify a single text data sample. <ref type="bibr">2</ref> Much of the research on efficient self-attention networks for NLP has just emerged in the past year. However, starting with SqueezeNet <ref type="bibr">[14]</ref>, the mobile computer vision (CV) community has spent the last four years optimizing neural networks for mobile devices. Intuitively, it seems like there must be opportunities to apply the lessons learned from the rich literature of mobile CV research to accelerate mobile NLP. In the following we review what has already been applied and propose two additional techniques from CV that we will leverage for accelerating NLP.</p><p>1.1 What has CV research already taught NLP research about efficient networks? In recent months, novel self-attention networks have been developed with the goal of achieving faster inference. At present, the MobileBERT network defines the state-of-the-art in low-latency text classification for mobile devices <ref type="bibr">[15]</ref>. MobileBERT takes approximately 0.6 seconds to classify a text sequence on a Google Pixel 3 smartphone. And, on the GLUE benchmark, which consists of 9 natural language understanding (NLU) datasets <ref type="bibr">[16]</ref>, MobileBERT achieves higher accuracy than other efficient networks such as DistilBERT <ref type="bibr" target="#b8">[17]</ref>, PKD <ref type="bibr" target="#b9">[18]</ref>, and several others <ref type="bibr" target="#b10">[19,</ref><ref type="bibr" target="#b11">20,</ref><ref type="bibr" target="#b12">21,</ref><ref type="bibr" target="#b13">22]</ref>. To achieve this, MobileBERT introduced two concepts into their NLP self-attention network that are already in widespread use in CV neural networks:</p><p>1. Bottleneck layers. In ResNet <ref type="bibr" target="#b14">[23]</ref>, the 3x3 convolutions are computationally expensive, so a 1x1 "bottleneck" convolution is employed to reduce the number of channels input to each 3x3 convolution layer. Similarly, MobileBERT adopts bottleneck layers that reduce the number of channels before each self-attention layer, and this reduces the computational cost of the self-attention layers. 2. High-information flow residual connections. In BERT-base, the residual connections serve as links between the low-channel-count (768 channels) layers. The high-channelcount (3072 channels) layers in BERT-base do not have residual connections. However, the ResNet and Residual-SqueezeNet <ref type="bibr">[14]</ref> CV networks connect the high-channel-count layers with residuals, which enables higher information flow through the network. Similar to these CV networks, MobileBERT adds residual connections between the high-channelcount layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">What else can CV research teach NLP research about efficient networks?</head><p>We are encouraged by the progress that MobileBERT has made in leveraging ideas that are popular in the CV literature to accelerate NLP.</p><p>However, we are aware of two other ideas from CV, which weren't used in MobileBERT, and that could be applied to accelerate NLP:</p><p>1. Convolutions. Since the 1980s, computer vision neural nets have relied heavily on convolutional layers <ref type="bibr" target="#b15">[24,</ref><ref type="bibr" target="#b16">25]</ref>. Convolutions are quite flexible and well-optimized in software, and they can implement things as simple as a 1D fully-connected layer, or as complex as a 3D dilated layer that performs upsampling or downsampling. 2. Grouped convolutions. A popular technique in modern mobile-optimized neural networks is grouped convolutions (see Section 3). Proposed by Krizhevsky et al. in the 2012 winning submission to the ImageNet image classification challenge <ref type="bibr" target="#b17">[26,</ref><ref type="bibr" target="#b18">27,</ref><ref type="bibr" target="#b19">28]</ref>, grouped convolutions disappeared from the literature from some years, then re-emerged as a key technique circa 2016 <ref type="bibr" target="#b20">[29,</ref><ref type="bibr" target="#b21">30]</ref> and today are extensively used in efficient CV networks such as Mo-bileNet <ref type="bibr" target="#b22">[31]</ref>, ShuffleNet <ref type="bibr" target="#b23">[32]</ref>, and EfficientNet <ref type="bibr" target="#b24">[33]</ref>. While common in efficient CV literature, we are not aware of work applying grouped convolutions to NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">SqueezeBERT: Applying lessons learned from CV to NLP</head><p>In this work, we describe how to apply convolutions and particularly grouped convolutions in the design of a novel self-attention network for NLP, which we call SqueezeBERT. Empirically, we find that SqueezeBERT runs at lower latency on a smartphone than BERT-base, MobileBERT, and several other efficient NLP models, while maintaining competitive accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Implementing self-attention with convolutions</head><p>In this section, first we review the basic structure of self-attention networks, next we identify that their biggest computational bottleneck is in their position-wise fully-connected (PFC) layers, and then we show that these PFC layers are equivalent to a 1D convolution with a kernel size of 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Self-attention networks</head><p>In most BERT-derived networks there are typically 3 stages: the embedding, the encoder, and the classifier <ref type="bibr" target="#b0">[9,</ref><ref type="bibr" target="#b4">13,</ref><ref type="bibr" target="#b3">12,</ref><ref type="bibr">15,</ref><ref type="bibr" target="#b10">19]</ref>. <ref type="bibr">3</ref> The embedding converts preprocessed words (represented as integervalued tokens) into learned feature-vectors of floating-point numbers; the encoder is comprised of a series of self-attention and other layers; and the classifier produces the network's final output. As we will see later in <ref type="table" target="#tab_0">Table 1</ref>, the embedding and the classifier account for less than 1% of the runtime of a self-attention network, so we focus our discussion on the encoder.</p><p>We now describe the encoder that is used in BERT-base <ref type="bibr" target="#b0">[9]</ref>. The encoder consists of a stack of blocks. Each block consists of a self-attention module followed by three position-wise fully-connected layers, known as feed-forward network (FFN) layers. Each self-attention module contains three seperate position-wise fully-connected (PFC) layers, which are used to generate the query (Q), key (K), and value (V ) activation vectors for each position in the feature embedding. Each of these PFC layers in self-attention applies the same operation to each position in the feature embedding independently. While neural networks traditionally multiply weights by activations, a distinguishing factor of attention neural networks is that they multiply activations by other activations, which enables dynamic weighting of tensor elements to adjust based on the input data. Further, attention networks allow modeling of arbitrary dependencies irregardless of their distance in the input or output [7]. The self-attention module proposed by Vaswani et al. <ref type="bibr">[7]</ref> (which is also used by GPT [8], BERT <ref type="bibr" target="#b0">[9]</ref>, RoBERTa <ref type="bibr" target="#b4">[13]</ref>, ELECTRA <ref type="bibr" target="#b3">[12]</ref> and others) multiplies the Q, K, and V activations together using the equation sof tmax </p><formula xml:id="formula_0">( QK T √ d k )V ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Benchmarking BERT for mobile inference</head><p>To identify the parts of BERT that are time-consuming to compute, we profile BERT on a smartphone. Specifically, we measure the neural network's latency using PyTorch <ref type="bibr" target="#b26">[34]</ref> and TorchScript on a Google Pixel 3 smartphone, with an input sequence length of 128 and a batch size of 1. In <ref type="table" target="#tab_0">Table 1</ref>, we show the breakdown of FLOPs and latency among the main components of the BERT network, and we observe that the sof tmax( QK T √ d k )V calculations in the self-attention modules account for only 11.3 percent of the total latency. However, the PFC layers in the self-attention modules account for 18.9 percent, and the PFC layers in the feed-forward network modules account for 69.4 percent, and in total the PFC layers account for 88.3 percent of the latency. Given that the PFC layers account for the overwhelming majority of the latency, we now turn our focus to reducing the latency of the PFC layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Replacing the position-wise fully connected layers (PFC) with convolutions</head><p>To address this, we intend to replace the PFC layers with grouped convolutions, which have been shown to produce significant speedups in computer vision networks. As a first step in this direction, we now show that the fully-connected layers used throughout the BERT encoder are a special case of non-grouped 1D convolution. In the following, f denotes the input feature vector, and w denotes the weights. Given an input feature vector of dimensions (P, C in ) with P positions and C in channels to generate an output of (P, C out ) features, the operation performed by the position-wise fullyconnected layer can be defined as follows:</p><formula xml:id="formula_1">P ositionwiseF ullyConnected p,cout (f, w) = Cin i w cout,i * f p,i (1)</formula><p>Then if we consider the definition of a 1D convolution with kernel size K with the same input and output dimensions</p><formula xml:id="formula_2">Convolution p,cout (f, w) = Cin i K k w cout,i,k * f (p− K−1 2 +k),i<label>(2)</label></formula><p>we observe that the position-wise fully-connected operation is equivalent to a convolution with a kernel size of k = 1. Thus, the PFC layers of Vaswani et al.</p><p>[7], GPT, BERT, and similar self-attention networks can be implemented using convolutions without changing the networks' numerical properties or behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Incorporating grouped convolutions into self-attention</head><p>Now that we have shown how to implement the expensive PFC layers in self-attention networks using convolutions, we can incorporate efficient grouped convolutions into a self-attention network. Grouped convolutions are defined as follows. Given an input feature vector of dimensions (P, C in ) with P positions and C in channels and outputting a vector with dimensions (P, C out ), a 1d convolution with kernel size K and G groups can be defined as follows.</p><formula xml:id="formula_3">GroupedConvolution p,cout (f, w) = C in G i K k w cout,i,k * f (p− K−1 2 +k),(i+⌊ (i)(G) C out ⌋ C in G )<label>(3)</label></formula><p>This is equivalent to splitting the the input vector into G separate vectors of size (P, Cin G ) along the P dimension and running G separate convolutions with independent weights each computing vectors of size (P, Cout G ). The grouped convolution, however, requires only 1 G as many floating point operations (FLOPs) and 1 G as many weights as an ordinary convolution, not counting the small (and unchanged) amount of operations needed for the channel-wise bias term that is often included in convolutional layers. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SqueezeBERT</head><p>Now, we describe our proposed neural architecture called SqueezeBERT, which uses grouped convolutions. SqueezeBERT is much like BERT-base, but with PFC layers implemented as convolutions, and grouped convolutions for many of the layers. Recall from Section 2 that each block in the BERT-base encoder has a self-attention module with 3 PFC layers, plus 3 more PFC layers called feed-forward network layers (FFN 1 , FFN 2 , and FFN 3 ). The FFN layers have the following dimensions: FFN 1 has C in = C out = 768, FFN 2 has C in = 768 and C out = 3072, and FFN 3 has C in = 3072 and C out = 768. In all PFC layers of the self-attention modules, and in the FFN 2 and FFN 3 layers, we use grouped convolutions with G = 4. To allow for mixing across channels of different groups, we use G = 1 in the less-expensive FFN 1 layers. Note that in BERT-base, FFN 2 and FFN 3 each have 4 times more arithmetic operations than FFN 1 . However, when we use G = 4 in FFN 2 and FFN 3 , now all FFN layers have the same number of arithmetic operations.</p><p>Finally, the embedding size (768), the number of blocks in the encoder <ref type="bibr" target="#b3">(12)</ref>, the number of heads per self-attention module <ref type="bibr" target="#b3">(12)</ref>, the tokenizer (WordPiece <ref type="bibr" target="#b27">[35,</ref><ref type="bibr" target="#b28">36]</ref>), and other aspects of SqueezeBERT are adopted from BERT-base. Aside from the convolution-based implementation and the adoption of grouped convolutions, the SqueezeBERT architecture is identical to BERT-base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Pretraining Data. For pretraining, we use a combination of Wikipedia and BooksCorpus <ref type="bibr" target="#b29">[37]</ref>, setting aside 3% of the combined dataset as a test set. Following the ALBERT paper, we use Masked Language Modeling (MLM) and Sentence Order Prediction (SOP) as pretraining tasks <ref type="bibr" target="#b10">[19]</ref>.</p><p>Finetuning Data. We finetune and evaluate SqueezeBERT (and other baselines) on the General Language Understanding Evaluation (GLUE) set of tasks. This benchmark consists of a diverse set of 9 NLU tasks; thanks to the structure and breadth of these tasks (see supplementary material for detailed task-level information), GLUE has become the standard evaluation benchmark for NLP research. A model's performance across the GLUE tasks likely provides a good approximation of that model's generalizability (esp. to other text classification tasks).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Methodology</head><p>Many of the recent papers on efficient NLP networks report results on models trained with bells and whistles such as distillation, adversarial training, and/or transfer learning across GLUE tasks. However, there is no standardization of these training schemes across different papers, making it difficult to distinguish the contribution of the model from the contribution of the training scheme to the final accuracy number. Therefore, we first train SqueezeBERT using a simple training scheme (described in Section 4.2.1, with results reported in Section 5.1), and then we train SqueezeBERT with distillation and other techniques (described in Section 4.2.2, with results reported in Section 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Training without bells and whistles</head><p>We pretrain SqueezeBERT from scratch (without distillation) using the LAMB optimizer, and we employ the hyperparameters recommended by the LAMB authors: a global batch size of 8192, a learning rate of 2.5e-3, and a warmup proportion of 0.28 <ref type="bibr" target="#b30">[38]</ref>. Following the LAMB paper's recommendations, we pretrain for 56k steps with a maximum sequence length of 128 and then for 6k steps with a maximum sequence length of 512.</p><p>For finetuning, we use the AdamW optimizer with a batch size of 16 without momentum or weight decay with β 1 = 0.9 and β 2 = 0.999 <ref type="bibr" target="#b31">[39]</ref>. As is common in the literature, during finetuning for each task, we perform hyperparameter tuning on the learning rate and dropout rate. We present more details on this in the supplementary material. In the interest of a fair comparison, we also train BERT-base using the aforementioned pretraining and finetuning protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Training with bells and whistles</head><p>We now review recent techniques for improving the training of NLP networks, and we describe the approaches that we will use for the training and evaluation of SqueezeBERT in Section 5.2.</p><p>Distillation approaches used in other efficient NLP networks. While the term "knowledge distillation" was coined by Hinton et al. to describe a specific method and equation <ref type="bibr" target="#b32">[40]</ref>, the term "distillation" is now used in reference to a diverse range of approaches where a "student" network is trained to replicate a "teacher" network. Some researchers distill only the final layer of the network <ref type="bibr" target="#b8">[17]</ref>, while others also distill the hidden layers <ref type="bibr">[15,</ref><ref type="bibr" target="#b9">18,</ref><ref type="bibr" target="#b13">22]</ref>. When distilling the hidden layers, some apply layer-by-layer distillation warmup, where each module of the student network is distilled <ref type="table">Table 2</ref>: Comparison of neural networks on the development set of the GLUE benchmark. denotes models trained by the authors of the present paper. Bells and whistles are: A = adversarial training; D = distillation of final layer; E = distillation of encoder layers; S = transfer learning across GLUE tasks (a.k.a. STILTs <ref type="bibr" target="#b33">[41]</ref>); W = per-layer warmup. In GLUE accuracy, a dash means that accuracy for this task is not provided in the literature. <ref type="bibr">GLUE</ref>  independently while downstream modules are frozen <ref type="bibr">[15]</ref>. Some distill during pretraining <ref type="bibr">[15,</ref><ref type="bibr" target="#b8">17]</ref>, some distill during finetuning <ref type="bibr" target="#b13">[22]</ref>, and some do both <ref type="bibr" target="#b9">[18,</ref><ref type="bibr" target="#b12">21]</ref>.</p><p>Bells and whistles used for training SqueezeBERT (for results in Section 5.2). Distillation is not a central focus of this paper, and there is a large design space of potential approaches to distillation, so we select a relatively simple form of distillation for use in SqueezeBERT training. We apply distillation only to the final layer, and only during finetuning. On the GLUE sentence classification tasks, we use soft cross entropy loss with respect to a weighted sum of the teacher's logits and a one-hot encoding of the ground-truth. Also note that GLUE has one regression task (STS-B text similarity), and for this task we replace the soft cross entropy loss with mean squared error. In addition to distillation, inspired by STILTS <ref type="bibr" target="#b33">[41]</ref> and ELECTRA <ref type="bibr" target="#b3">[12]</ref> we apply transfer learning from the MNLI GLUE task to other GLUE tasks as follows. The SqueezeBERT student model is pretrained using the approach described in Section 4.2.1, and then it is finetuned on the MNLI task. The weights from MNLI training are used as the initial student weights for other GLUE tasks except for CoLA. <ref type="bibr">6</ref> Similarly, the teacher model is a BERT-base model that is pretrained using the ELEC-TRA method and then finetuned on MNLI. Then, the teacher model is finetuned independently on each GLUE task, and these task-specific teacher weights are used for distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We now turn our attention to comparing SqueezeBERT to other efficient neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results without bells and whistles</head><p>In the upper portions of Tables 2 and 3, we compare our results to other efficient networks on the dev and test sets of the GLUE benchmark. Note that relatively few of the efficiency-optimized networks report results without bells and whistles, and most such results are reported on the development (not test) set of GLUE. Fortunately, the authors of MobileBERT -a network which we will find in the next section compares favorably to other efficient networks with bells and whistles enabled -do provide development-set results without distillation on 4 of the GLUE tasks. <ref type="bibr">7</ref> We observe in the upper portion of <ref type="table">Table 2</ref> that, when both networks are trained without distillation, SqueezeBERT achieves higher accuracy than MobileBERT on all of these tasks. This provides initial evidence that the techniques from computer vision that we have adopted can be applied to NLP, and reasonable accuracy can be obtained. Further, we observe that SqueezeBERT is 4.3x faster than BERT-base, while MobileBERT is 3.0x faster than BERT-base. 8</p><p>Due to the dearth of efficient neural network results on GLUE without bells and whistles, we also provide a comparison in <ref type="table">Table 2</ref> with the ALBERT-base network. ALBERT-base is a version of BERT-base that uses the same weights across multiple attention layers, and it has a smaller encoder than BERT. Due to these design choices, ALBERT-base has 9x fewer parameters than BERT-base. However, ALBERT-base and BERT-base have the same number of FLOPs, and we observe in our measurements in <ref type="table">Table 2</ref> that ALBERT-base does not offer a speedup over BERT-base on a smartphone. <ref type="bibr" target="#b0">9</ref> Further, on the two GLUE tasks where the ALBERT authors reported the accuracy of ALBERT-base, MobileBERT and SqueezeBERT both outperform the accuracy of ALBERT-base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results with bells and whistles</head><p>Now, we turn our attention to comparing SqueezeBERT to other models, all trained with bellsand-whistles. In the lower portion of <ref type="table" target="#tab_3">Table 3</ref>, we first observe that when trained with bells-andwhistles MobileBERT matches or outperforms the accuracy of the other efficient models (except SqueezeBERT) on 8 of the 9 GLUE tasks. Further, on 4 of the 9 tasks SqueezeBERT outperforms the accuracy of MobileBERT; on 4 of 9 tasks MobileBERT outperforms SqueezeBERT; and on 1 task (WNLI) all models predict the most frequently occurring category. <ref type="bibr" target="#b1">10</ref> Also, SqueezeBERT achieves 7 Note that some papers report results on only the development set or the test set, and some papers only report results on a subset of GLUE tasks. Our aim with this evaluation is to be as inclusive as possible, so we include papers with incomplete GLUE results in our results tables. <ref type="bibr">8</ref> In our measurements, we find MobileBERT takes 572ms to classify one length-128 sequence on a Pixel 3 phone. This is slightly faster than the 620ms reported by the MobileBERT authors in the same setting <ref type="bibr" target="#b34">[42]</ref>. We use the faster number in our comparisons. Further, all latencies in our results tables were benchmarked by us. <ref type="bibr" target="#b0">9</ref> However, reducing the number of parameters while retaining a high number of FLOPs can present other advantages, such as faster distributed training <ref type="bibr" target="#b10">[19,</ref><ref type="bibr" target="#b35">43]</ref> and superior energy-efficiency <ref type="bibr" target="#b36">[44]</ref>. <ref type="bibr" target="#b1">10</ref> Note that data augmentation approaches have been proposed to improve accuracy on WNLI; see <ref type="bibr" target="#b37">[45]</ref>. For fairness in comparing against our baselines, we choose not to use data augmentation to improve WNLI results.</p><p>an average score across all GLUE tasks that is within 0.4 percentage-points of MobileBERT. Given the speedup of SqueezeBERT over MobileBERT, we think it is reasonable to say that SqueezeBERT and MobileBERT each offer a compelling speed-accuracy tradeoff for NLP inference on mobile devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Quantization and Pruning. Quantization is a family of techniques which aims to reduce the number of bits required to store each parameter and/or activation in a neural network, while at the same time maintaining the accuracy of that network. This has been successfully applied to NLP in such works as <ref type="bibr" target="#b38">[46,</ref><ref type="bibr" target="#b39">47]</ref>. Pruning aims to directly eliminate certain parameters from the network while also maintaining accuracy, thereby reducing the storage and potentially computational cost of that network; for an application of this to NLP, please see <ref type="bibr" target="#b40">[48]</ref>. These methods could be applied to SqueezeBERT to yield further efficiency improvements, but quantization and pruning are not a focus of this paper.</p><p>Convolutions in self-attention networks for language-generation tasks. In this paper, our experiments focus on natural language understanding (NLU) tasks such as sentence classification. However, another widely-studied area is natural language generation (NLG), which includes the tasks of machine-translation (e.g., English-to-German) and language modeling (e.g., automated sentencecompletion). While we are not aware of work that adopts convolutions in self-attention networks for NLU, we are aware of such work in NLG. For instance, the Evolved Transformer and Lite Transformer architectures contain self-attention modules and convolutions in separate portions of the network <ref type="bibr" target="#b41">[49,</ref><ref type="bibr" target="#b42">50]</ref>. Additionally, LightConv shows that well-designed convolutional networks without self-attention produce comparable results to self-attention networks on certain NLG tasks <ref type="bibr" target="#b43">[51]</ref>. Also, Wang et al. sparsify the self-attention matrix multiplication using a pattern of nonzeros that is inspired by dilated convolutions <ref type="bibr" target="#b44">[52]</ref>. Finally, while not an attention network, Kim applied convolutional networks to NLU several years before the development of multi-head self-attention <ref type="bibr" target="#b45">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions &amp; Future Work</head><p>In this paper, we have studied how grouped convolutions, a popular technique in the design of efficient CV neural networks, can be applied to NLP. First, we showed that the position-wise fullyconnected layers of self-attention networks can be implemented with mathematically-equivalent 1D convolutions. Further, we proposed SqueezeBERT, an efficient NLP model which implements most of the layers of its self-attention encoder with 1D grouped convolutions. This model yields an appreciable &gt;4x latency decrease over BERT-base when benchmarked on a Pixel 3 phone. We also successfully applied distillation to improve our approach's accuracy to a level that is competitive with a distillation-trained MobileBERT and with the original version of BERT-base.</p><p>We now discuss some possibilities for future work in the directions outlined in this paper. There are several techniques in use in CV that could be applied to NLP which we have not covered in this paper. One very promising direction is downsampling strategies which decrease the sequence length of the activations in the self-attention network as the layers progress. Extensions of this idea, such as U-Nets <ref type="bibr" target="#b46">[54]</ref>, as well as modifying channel sizes (hidden size) instead of and in addition to sequence length would also be promising directions. On this path of techniques, applying ideas such as BiFPNs <ref type="bibr" target="#b47">[55]</ref>, striding, and dilation <ref type="bibr" target="#b48">[56]</ref> may also yield interesting results.</p><p>The addition of all of these potential techniques opens up a significantly broader search-space of neural architecture designs for NLP. This motivates the application of automated neural architecture search (NAS) approaches such as those described in <ref type="bibr" target="#b49">[57,</ref><ref type="bibr" target="#b50">58]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact Potential benefits of this work</head><p>We hope the techniques used in this paper will allow more efficient and practical deployment of selfattention based networks, particularly allowing more widespread use of these networks on mobile devices. Possible use cases include email sorting, chat analysis, spam detection, or hate-speech filtering. Facebook has begun using a self-attention based network to automatically detect and remove hate speech on their platform <ref type="bibr" target="#b51">[59,</ref><ref type="bibr" target="#b52">60]</ref>. Currently, companies usually run these models on the serverside, but some may be reticent to deploy them on a mass scale due to computation costs. Mobile inference wouldn't require expensive server infrastructure and allows use in cases where privacy and security may be a concern. Running on the edge devices may allow more real-time or offline use cases such as grammar checking, or sentence completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Potential malicious uses of this work</head><p>Given that we are releasing our model and training and inference code as free software, anyone can train our model on any dataset that they like. While we hope most practitioners will apply our work for altruistic or at least well-intentioned purposes, some may apply our work for ethically questionable or purely self-serving applications. For instance, low-cost mobile inference could allow "smart" key-loggers and eavesdropping viruses to be deployed, and for these devices to better avoid detection by computing locally and only uploading important information. And, while we hope that social networks and other text-centric portals will draw on our work to embed fair and just models into their mobile applications for content moderation, these companies could just as easily draw on our work to train and deploy models that censor content of political enemies or amplify only certain types of messages and voices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Potential effects of unintended bias in the neural network and its training data</head><p>Models for NLP commonly suffer from biases regarding race and gender <ref type="bibr" target="#b53">[61]</ref>. In our work, we have used standard datasets, and we are not aware of any experimental factors that would increase or decrease SqueezeBERT's propensity for bias, as compared to the approaches used in similar self-attention research such as BERT and ELECTRA. The gender-related biases of our pretraining corpora (Wikipedia and BooksCorpus) are investigated by Tan et al. <ref type="bibr" target="#b53">[61]</ref>, and the gender biases of BERT and GPT models finetuned on the GLUE tasks are investigated by Babaeianjelodar et al. <ref type="bibr" target="#b54">[62]</ref>.</p><p>Aside from gender bias, we are not aware of studies that investigate other patterns of bias in the datasets that we used in our work. However, Sheng et al. investigate biases regarding race, gender, and sexual orientation in a GPT-2 self-attention model trained on a language-modeling (textgeneration) task <ref type="bibr" target="#b55">[63]</ref>. Further, in the paper introducing the GPT-3 model, a study similar to that of Sheng et al. is performed <ref type="bibr" target="#b56">[64]</ref>.</p><p>According to the taxonomy described in <ref type="bibr" target="#b57">[65]</ref>, the harms caused by biased NLP technology include:</p><p>• Allocational harms: These arise when an "automated system allocates resources (e.g., credit) or opportunities (e.g., jobs) unfairly to different social group."</p><p>• Representational harms: These arise when "a system (e.g., a search engine) represents some social groups in a less favorable light than others, demeans them, or fails to recognize their existence altogether."</p><p>We direct the interested reader to work by Blodgett et al. <ref type="bibr" target="#b57">[65]</ref> and Sun et al. <ref type="bibr" target="#b58">[66]</ref> for more details and suggestions on how to minimize the bias in datasets and the neural networks that learn from them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>[1] D. Sayce, "The number of tweets per day in 2019," https://www.dsayce.com/social-media/tweets-day/, 2019.</p><p>[2] J. Schultz, "How much data is created on the internet each day?" https://blog.microfocus.com/how-much-data-is-created-on-the-internet-each-day/, 2019.</p><p>[3] A.</p><p>Al-Heeti, "WhatsApp: 65B messages sent each day, and more than 2B minutes of calls," CNET, 2018, https://www.cnet.com/news/whatsapp-65-billion-messages-sent-each-day-and-more-than-2-billion-minutes-of-calls/.</p><p>[4] Templatify, "How many emails are sent every day? top email statistics for business," https://info.templafy.com/blog/how-many-emails-are-sent-every-day-top-email-statistics-your-business-needs-to-know, 2017.</p><p>[5] Lovely Mobile News, "Mobile has largely displaced other channels for email," https://lovelymobile.news/mobile-has-largely-displaced-other-channels-for-email, 2017.</p><p>[6] G. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material for SqueezeBERT A Overview of GLUE tasks</head><p>In this section we provide an overview of each of the tasks within the GLUE benchmark, their evaluation metrics, and their potential applications outside of the benchmark. For further information on the benchmark, please see <ref type="bibr">[16]</ref>, where it was originally proposed.</p><p>Note that some tasks have one evaluation metric, and some have two. The possible evaluation metrics are Accuracy (abbreviated below as acc), F1 <ref type="bibr" target="#b59">[67]</ref>, Matthews Correlation Coefficient (MCC) <ref type="bibr" target="#b60">[68]</ref>, Pearson Correlation (pearson), and Spearman Correlation (spearman).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNLI:</head><p>Multi-genre Natural Language Inference <ref type="bibr" target="#b61">[69]</ref>. Given a pair of sentences (sentence 1 and sentence 2), the task is to predict whether sentence 2 entails sentence 1. Note that there are 2 test sets for this task: MNLI-matched (MNLI-m), and MNLI-mismatched (MNLI-mm). When computing an average score overall tasks, we follow the convention employed by the GLUE leaderboard, which is (acc(MNLI-m) + acc(MNLI-mm))/2. Therefore, while MNLI-m and MNLI-mm are two columns in our results table, they only count as one task in the average GLUE score. Potential applications include helping users improve their writing by checking for repetitive sections based on whether certain sentences are entailed by other previous ones. This task contains 392,703 training examples.</p><p>QQP: Quora Question Pairs <ref type="bibr" target="#b62">[70,</ref><ref type="bibr" target="#b63">71]</ref>. The goal of this task is to determine whether a pair of questions have the same meaning. In practical applications, such as organizing emails or organizing helpdesk tickets, this approach can be used to group similar questions together and only answer them once. There are 2 metrics for this task: acc and F1. In our results tables, we follow the approach of the GLUE leaderboard and compute QQP results as the average of these two metrics. We observe that, for most models, on the test set the F1 score is significantly lower than the accuracy score. For instance, previously reported BERT-base results are F1=71.2 and accuracy=89.2. The TinyBERT paper <ref type="bibr" target="#b12">[21]</ref> reports a TinyBERT QQP score of 71.3, relative to a BERT-base baseline of 71.2, so it appears that the TinyBERT paper only reports the QQP F1 (but not accuracy) score. Conversely, the ELECTRA <ref type="bibr" target="#b3">[12]</ref> paper reports an ELECTRA-small QQP score of 88.0, relative to a BERT-base QQP score of 89.2, so it appears that the ELECTRA paper only reports the QQP accuracy (but not F1) score. Out of fairness, we have omitted the TinyBERT and ELECTRA-small QQP test-set results from our results table, as it appears that they are each using a metric different from our metric of ((acc(QQP) + F1(QQP))/2). This task contains 363,871 training examples.</p><p>QNLI: Question-answering Natural Language Inference. This task is derived from the dataset published in <ref type="bibr" target="#b64">[72]</ref>. Specifically, it is reformulated as a two-class classification task, wherein the goal is to decide whether or not the given sentence does, or does not, contain the answer to a given question. In this way, QNLI bears some similarity to other entailment classification tasks such as MNLI. The evaluation metric used for QNLI is just acc(QNLI). In terms of applications, QNLI is useful for many of the same situations in which a standard question-answering dataset would be, such as in designing intelligent voice assistants which can respond to user questions. For instance, if an extractive or generative model is used to generate an answer to a question, a QNLI-trained model could be used as a sanity-check on whether the machine-generated response actually answers the question. Also, a QNLI-trained model could be used to check if an existing answer already answers a new question on a form or in a helpdesk database, thereby not requiring a new answer to be generated by a human. This task contains 104,744 training examples. <ref type="bibr" target="#b65">[73]</ref>. This is a sentiment classification dataset, with the goal being to predict whether a sentence has a positive or negative sentiment. The source data for SST-2 is hand-annotated movie review data. The metric used for this task is acc(SST-2). SST-2 applications include improving chatbot based systems which require responses that properly match the conversation context to be generated, and potentially triaging emails at helpdesks so that, for example, the ones with strong negative sentiments are prioritized. This task contains 67,350 training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SST-2: Stanford Sentiment Treebank</head><p>CoLA: Corpus of Linguistic Acceptability <ref type="bibr" target="#b66">[74]</ref>. This is a binary classification task, where the objective is to determine whether a given sentence meets the criteria for being a properly written English sentence. It is worth noting that CoLA is a binary classification task, and 69% of the data samples in the validation set are positive examples. The metric used for this task is different from the metrics used in other GLUE tasks: MCC(CoLA). A distinguishing feature of Matthews Correlation Coefficient (MCC) is that it effectively adjusts for class imbalance in the dataset. On the CoLA validation set, where 69% of the samples are positive, predicting the majority class would produce an accuracy of 69% but an MCC of 0.0. Also note that MCC can be anywhere between and including -1.0 and 1.0, as opposed to 0.0 (0%) and 1.0 (100%) for acc. Models trained on CoLA could be used to provide feedback on the grammatical correctness of user-generated text, analogous to a spell-checker, but potentially with more nuanced understanding of grammar than a rule-based grammar checker. CoLA-trained models could also be used to verify the grammatical correctness of text generated by other neural networks. This task contains 8,551 training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STS-B:</head><p>Semantic Textual Similarity Benchmark <ref type="bibr" target="#b67">[75]</ref>. This dataset is built from several sources including news headlines data as well as captions data. The goal of this task is to predict the level of semantic similarity between two input sentences, on a scale of 1 (minimum similarity) to 5 (maximum similarity). Notably, this is a regression task. The overall evaluation metric used on the GLUE leaderboard, which we report in our results tables, is (spearman(STS-B) + pearson(STS-B))/2. STS-B could have applications in several areas, such as plagiarism detection, as well as potentially other applications which are mentioned above in the description of QQP. This task contains 5,750 training examples. <ref type="bibr" target="#b68">[76]</ref>. This is classification task where the goal is to predict whether or not the two input sentences have the same semantic meaning. The data itself comes from online news articles and is hand-annotated. The metric used for this task is ((acc(QQP) + F1(QQP))/2), consistent with the GLUE test set leaderboard. MRPC would have applications similar to STS-B, such as plagiarism detection and others. This task contains 3,669 training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MRPC: Microsoft Research Paraphrase Corpus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RTE:</head><p>Recognizing Textual Entailment is a combination of several datasets from yearly challenges on entailment <ref type="bibr" target="#b69">[77,</ref><ref type="bibr" target="#b70">78,</ref><ref type="bibr" target="#b71">79,</ref><ref type="bibr" target="#b72">80]</ref>. RTE is a two-way classification task where the labels are entailment and not_entailment. The metric used for this task is acc(RTE). Potential applications of this dataset are similar to those of MNLI. This task contains 2,491 training examples.</p><p>WNLI: Winograd Schema Challenge, reformulated as Winograd Natural Language Inference <ref type="bibr" target="#b73">[81]</ref>. This goal of this task is for the model to predict the antecedent of a pronoun (out of a set of choices) given the input sentence containing that pronoun. The metric used for this task is acc(WNLI). As with other work in this area <ref type="bibr" target="#b0">[9]</ref>, we simply predict the majority class on this dataset, resulting in a test set accuracy of 65.1%. This task contains 636 training examples.</p><p>In Listings 1 and 2, we show how we calculate overall scores for the GLUE development set and test set. Other than omitting WNLI from the development-set score (following <ref type="bibr" target="#b0">[9]</ref>), these calculations adhere to the methodology used by the GLUE leaderboard. We do all pretraining and finetuning on an 8-GPU server without multi-server distributed training. Our server has 8 NVIDIA Titan RTX GPUs. The server also has an Intel Xeon Gold 6130 64-core CPU, and it has 256GB of RAM. Further, the data loading requirements for our NLP application are so small that we were able to use low-cost, low-bandwidth spinning-media (not flash) disks for storing the training data. We employed two of these servers for approximately 6 months to do various experiments, beginning with reproducing BERT-base, and then experimenting with various model architectures. No cloud computing resources or corporate computing resources were used for this research.</p><p>While we used our own computing resources for this work, we now consider what it would have cost to rent servers similar to ours from a cloud provider for the duration of our project. We can break this down into two types of costs, storage and computation:</p><p>• Storage. The datasets are small (under 100GB total), and we typically retain a few hundred gigabytes of model parameters from various training checkpoints, so we imagine the cloud storage costs would be negligible: As of this writing, Amazon Web Services charges $0.023/GB for general-purpose storage <ref type="bibr" target="#b74">[82]</ref>, so storing a terabyte of data and model checkpoints would cost just $23 per month, or $138 for 6 months. There may be additional fees for data transfers within the AWS ecosystem, but we have studied these costs in detail.</p><p>• Computation. As we mentioned earlier, to do the work in this project, including the initial work to reproduce baselines and the work to experiment with several potential neural network designs, we used 2 8-GPU servers for about 6 months. We estimate that renting this amount of computation from a cloud provider would cost around $100,000, calculated as (6 months) * (50% utilization) * (2 servers) * (30 days per month) * (24 hrs/day) * ($24/hr for an 8-GPU V100 machine from Amazon Web Services <ref type="bibr" target="#b75">[83]</ref>  <ref type="bibr" target="#b77">[85]</ref> repositories. We perform pretraining using 8 GPUs with 16-bit floating-point math, and we use the O2 optimization level in the NVIDIA Apex mixed-precision training primitives <ref type="bibr" target="#b78">[86]</ref>. We perform finetuning on a single GPU with 32-bit floating-point math, and we concurrently run multiple finetuning tasks across the 8-GPU machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Further details: distillation</head><p>In the distillation approach that we described in Section 4.2.2, we mentioned that we use teacher logits and one-hot ground-truth as the target output for our soft cross entropy loss. The weighting between the teacher logits and the ground-truth is controlled by a hyperparameter α. Let Ψ t represent the teacher logits and let Ψ g represent the one-hot encoding of the ground-truth. Formally, we write this weighted sum as: Ψ = (1 − α)Ψ t + αΨ g (4)</p><p>In the next section, we will tune α as part of our hyperparameter tuning scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Details of hyperparameter search during finetuning on GLUE tasks</head><p>We now present more details on the hyperparameter search approach that we used for training SqueezeBERT with bells and whistles. In <ref type="table" target="#tab_6">Table 4</ref>, we present the space of possible hyperparameters over which we performed a grid-search. Note that the time to finetune the model using one  set of hyperparameters varies significantly depending on the GLUE task, from 15 minutes for small datasets like RTE, to 14 hours for MNLI. For smaller datasets that require less training (e.g. RTE and MRPC), we use a broader search space and more epochs. And, for larger datasets (e.g. MNLI and QQP), we use a more narrow search space with fewer epochs. <ref type="table" target="#tab_7">Table 5</ref>, we present the best hyperparameters found in our search. We observe two interesting phenomena on this table. The first is regarding the use of distillation. Recall from Equation 4 that α is the hyperparameter that sets the weighting between the teacher logits and the ground-truth for distillation. When α = 1.0, the teacher logits are ignored, and thus distillation is disabled. So, it is interesting to note that on three of the eight GLUE tasks in <ref type="table" target="#tab_7">Table 5</ref> (MNLI, QNLI, and CoLA), distillation did not produce superior results over non-distillation finetuning. The second interesting phenomenon in this table is that maximum accuracy was not necessarily achieved on final epoch. For example, on QNLI, SqueezeBERT converged to its maximum development-set accuracy after just two epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now, in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Further details: Inference on a smartphone</head><p>To evaluate inference speed, we run the neural networks on a Google Pixel 3 smartphone. This phone contains the popular Qualcomm Snapdragon 845 processing chip, which is also used in popular smartphones from Samsung, Xiaomi, and Sony <ref type="bibr" target="#b79">[87]</ref>. The phone also has 4GB of LPDDR4x memory. To run a neural network on the phone, we do the following. First, we export the network to TorchScript using the torch.jit.trace() command, which yields a standalone neural net that does not require Python to run. Then, we copy the network to the phone, and we run it using a modified version of the speed_benchmark_torch.cc file that is built into PyTorch. We report the average latency of 40 runs, using the CPU cores of the phone. Following the protocol of MobileBERT, each run operates on a length-128 input sequence and a batch size of 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D More related work</head><p>Efficient convolutional networks for CV. Convolutional neural networks lead the state-of-the-art on computer vision (CV) tasks such as image classification, object detection, and semantic segmentation. In the last five years, the community has developed efficient convolutional neural networks for CV that run efficiently and in real-time on mobile devices. On the ImageNet [28] image classification task, from the year 2016 (ResNet-101 <ref type="bibr" target="#b14">[23]</ref>) to 2020 (FixEfficientNet-D0 <ref type="bibr" target="#b80">[88]</ref>), there is a 20x reduction in the number of floating-point operations (FLOPs) required, while accuracy has actually improved. <ref type="bibr">1112</ref> Further, on the COCO [89] object detection task, from 2016 (FPN <ref type="bibr" target="#b82">[90]</ref>) to</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Listing 1 : 10 )( 11 )B</head><label>11011</label><figDesc>Calculating average score on the GLUE development set. 1 dev_score = MEAN ( 2 ( acc ( MNLI -m ) + acc ( MNLI -mm ) ) /2 , 3 ( F1 ( QQP ) + acc ( QQP ) ) /2 , Listing 2: Calculating average score on the GLUE test set. 1 test_score = MEAN ( 2 ( acc ( MNLI -m ) + acc ( MNLI -mm ) ) /2 , pearson ( STS -B ) + spearman ( STS -B ) ) /2 , 8 ( F1 ( MRPC ) + acc ( MRPC ) ) /2 , 9 acc ( RTE ) , 10 acc ( WNLI ) More training details and results B.1 Training Hardware</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>How does BERT spend its time? This is a breakdown of computation (in floating-point operations, or FLOPs) and latency (on a Google Pixel 3 smartphone) in BERT-base, reported to three significant digits. The FC layers account for more than 97% of the FLOPs and over 88% of the latency.</figDesc><table><row><cell>Stage</cell><cell>Module type</cell><cell></cell><cell>FLOPs Latency</cell></row><row><cell>Embedding</cell><cell>Embedding</cell><cell></cell><cell>0.00% 0.26%</cell></row><row><cell>Encoder</cell><cell cols="2">FC in self-attention modules</cell><cell>24.3% 18.9%</cell></row><row><cell>Encoder</cell><cell>sof tmax( QK T √ d k</cell><cell cols="2">)V in self-attention modules 2.70% 11.3%</cell></row><row><cell>Encoder</cell><cell cols="2">FC in feed-forward network layers</cell><cell>73.0% 69.4%</cell></row><row><cell cols="3">Final Classifier FC layers in final classifier</cell><cell>0.00% 0.02%</cell></row><row><cell>Total</cell><cell></cell><cell></cell><cell>100% 100%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of neural networks on the test set of the GLUE benchmark. denotes models trained by the authors of the present paper. Bells and whistles are: A = adversarial training; D = distillation of final layer; E = distillation of encoder layers; S = transfer learning across GLUE tasks (a.k.a. STILTs<ref type="bibr" target="#b33">[41]</ref>); W = per-layer warmup.DESW  82.4 82.1 80.5 89.6 92.2 47.8 84.9 85.4 66.2 65.1 77.1 66 11.3 814 2.1x MobileBERT [15] DEW 84.3 83.4 79.4 91.6 92.6 51.1 85.5 86.7 70.4 65.1 78.5 25.3 5.36 572 3.</figDesc><table><row><cell>GLUE accuracy</cell><cell>efficiency</cell></row></table><note>0x SqueezeBERT DS 82.0 81.1 80.3 90.1 91.4 46.5 86.7 87.8 73.2 65.1 78.1 51.1 7.42 390 4.3x</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>).So, while we used our computing resources for this work, doing this entire research project from beginning to end in the cloud would have cost around $100,000. Now, we consider what it would cost to reproduce the SqueezeBERT model from scratch using cloud hardware. The SqueezeBERT model can be reproduced in approximately 5 days: 4 days for pretraining, and then under one day for finetuning on all GLUE tasks with the optimal hyperparameters discovered by our hyperparameter search (see Section B.4). In total, this would cost approximately $2880, calculated as (5 days) * (24 hrs/day) * ($24/hr for one 8-V100 AWS machine).</figDesc><table><row><cell>B.2 Training Software</cell></row><row><cell>Our PyTorch-based training and inference code draws heavily on the HuggingFace Transform-</cell></row><row><cell>ers [84] and NVIDIA Deep Learning Examples</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Our hyperparameter search space. Hyperparameter MNLI, QQP, QNLI, STS-2 STS-B, MRPC, RTE CoLA α [0.8, 0.9, 1.0] [0.8, 0.9, 1.0] [0.8, 0.9, 1.0] Learning Rate [1e-05, 2e-05, 3e-05, 4e-05] [1e-05, 2e-05, 3e-05, 4e-05, 5e-05] [1e-05, 2e-05, 3e-05, 4e-05, 5e-05]</figDesc><table><row><cell cols="2">Encoder Dropout [0.0, 0.1]</cell><cell>[0.0, 0.1]</cell><cell>[0.0, 0.1]</cell></row><row><cell>Final Dropout</cell><cell>[0.1, 0.2]</cell><cell>[0.1, 0.2]</cell><cell>[0.0, 0.1]</cell></row><row><cell>Epochs</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>Batch Size</cell><cell>16</cell><cell>16</cell><cell>[16, 32, 48]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameters selected by our hyperparameter search when training SqueezeBERT with bells-and-whistles.</figDesc><table><row><cell>Hyperparameter</cell><cell cols="9">MNLI-m MNLI-mm QQP QNLI STS-2 CoLA STS-B MRPC RTE</cell></row><row><cell>α</cell><cell>1.0</cell><cell>1.0</cell><cell>0.8</cell><cell>1.0</cell><cell>0.8</cell><cell>1.0</cell><cell>0.8</cell><cell>0.9</cell><cell>0.8</cell></row><row><cell>Learning Rate</cell><cell>3e-05</cell><cell>3e-05</cell><cell>4e-05</cell><cell>3e-05</cell><cell>3e-05</cell><cell>2e-05</cell><cell>4e-05</cell><cell>3e-05</cell><cell>3e-05</cell></row><row><cell cols="2">Encoder Dropout 0.0</cell><cell>0.0</cell><cell>0.1</cell><cell>0.0</cell><cell>0.1</cell><cell>0.0</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Final Dropout</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Epochs</cell><cell>4</cell><cell>4</cell><cell>5</cell><cell>2</cell><cell>5</cell><cell>5</cell><cell>10</cell><cell>9</cell><cell>3</cell></row><row><cell>Batch Size</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>16</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Neural networks that use the self-attention modules of Vaswani et al. are sometimes called "Transformers," but in the interest of clarity we call them "self-attention networks."</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that BERT-base<ref type="bibr" target="#b0">[9]</ref>, RoBERTa-base<ref type="bibr" target="#b4">[13]</ref>, and ELECTRA-base<ref type="bibr" target="#b3">[12]</ref> all use the same self-attention encoder architecture, and therefore these networks incur approximately the same latency on a smartphone.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Some self-attention networks such as [7, 8] also have "decoder" stage. The decoder typically uses a similar neural architecture as the encoder, but is auto-regressive.4  For example, in BERT-base, the self-attention module has 768 channels and 12 heads, so d k = 768 12 = 64.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Note that the grouped convolution with G = 1 is identical to an ordinary convolution.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">For CoLA, the student weights are pretrained (per Section 4.2.1) but not finetuned on MNLI prior to taskspecific training.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">In our terminology throughout the paper, a multiply-add operation is two FLOPs.<ref type="bibr" target="#b3">12</ref> These results hold without expanding the training data beyond the ImageNet training set.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pd" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Ctrl: A conditional transformer language model for controllable generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ELECTRA: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">SqueezeNet: Alexnet-level accuracy with 50x fewer parameters and &lt;0.5mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MobileBERT: a compact task-agnostic BERT for resource-limited devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02984</idno>
		<ptr target="https://arxiv.org/abs/2004.02984" />
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">GLUE: A multitask benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Patient knowledge distillation for BERT model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">ALBERT: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Well-read students learn better: On the importance of pre-training compact models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Turc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08962</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10351</idno>
		<title level="m">TinyBERT: Distilling bert for natural language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02925</idno>
		<title level="m">Bert-of-theseus: Compressing bert by progressive module replacing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">cuda-convnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<ptr target="https://code.google.com/archive/p/cuda-convnet" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1610.02357" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">MobileNets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ShuffleNet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<ptr target="http://proceedings.mlr.press/v97/tan19a.html" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Japanese and korean voice search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Riesa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rudnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Large batch optimization for deep learning: Training bert in 76 minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Févry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01088</idno>
		<title level="m">Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">MobileBERT: Task-agnostic compression of BERT by progressive knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJxjVaNKwB" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>OpenReview submission</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">FireCaffe: near-linear acceleration of deep neural network training on compute clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Small neural nets are beautiful: Enabling embedded systems with small deep-neural-network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A surprisingly robust trick for winograd schema challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kocijan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-M</forename><surname>Cretu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-M</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yordanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Q-BERT: Hessian based ultra low precision quantization of bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zafrir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boudoukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izsak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wasserblat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06188</idno>
		<title level="m">Q8BERT: Quantized 8bit bert</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Movement pruning: Adaptive sparsity by fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07683</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The evolved transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Lite transformer with long short term attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06170</idno>
		<title level="m">Transformer on a diet</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D14-1181" />
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computerassisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">EfficientDet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1911.09070" />
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">SqueezeNAS: Fast neural architecture search for faster semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sidhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Neural Architects Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">FBNet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">AI advances to better detect hate speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dansby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ozertem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moghbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Facebook AI Blog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Facebook is using more AI to detect hate speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Johnson</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>VentureBeat, 2020. [Online</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Assessing social and intersectional biases in contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Celis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Quantifying gender bias in different corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeianjelodar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Freitag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>WWW Companion</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The woman worked as a babysitter: On biases in language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peng</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D19-1339" />
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Language (technology) is power: A critical survey of &quot;bias&quot; in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Blodgett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14050</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Mitigating gender bias in natural language processing: Literature review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elsherief</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Belding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P19-1159" />
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
		<title level="m">Information Retrieval</title>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
	<note>Butterworth-Heinemann</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Comparison of the predicted and observed secondary structure of t4 phage lysozyme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biochimica et Biophysica Acta (BBA)-Protein Structure</title>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Quora question pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quora</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Quora question pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="http://static.hongbozhang.me/doc/STAT_441_Report.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Neural network acceptability judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh International Workshop on Semantic Evaluations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Paraphrasing</title>
		<meeting>the International Workshop on Paraphrasing</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">The second pascal recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Haim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Szpektor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment</title>
		<meeting>the Second PASCAL Challenges Workshop on Recognising Textual Entailment</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">The third pascal recognizing textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing</title>
		<meeting>the ACL-PASCAL workshop on textual entailment and paraphrasing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">The fifth pascal recognizing textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning</title>
		<meeting>the Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Amazon s3 pricing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Services</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Amazon ec2 p3 instance product details</title>
		<ptr target="https://aws.amazon.com/ec2/instance-types/p3" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brew</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Deep learning examples for tensor cores</title>
		<ptr target="https://github.com/NVIDIA/DeepLearningExamples" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
		<respStmt>
			<orgName>NVIDIA</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">APEX -A PyTorch Extension: Tools for easy mixed precision and distributed training in pytorch</title>
		<ptr target="https://github.com/NVIDIA/apex" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">List of smartphones powered by qualcomm snapdragon 845 processor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Do</surname></persName>
		</author>
		<ptr target="https://www.techwalls.com/qualcomm-snapdragon-845-smartphones" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Techwalls</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Fixing the train-test resolution discrepancy: FixEfficientNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Multi-task deep neural networks for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">there is a 334x reduction in the number of FLOPs, and an improvement in accuracy. 14 Given that the datasets were, for the most part, held constant, to what do we owe these improvements? One factor that has helped is the design of new neural network architectures with superior FLOP-accuracy tradeoffs. There have been several innovations to neural architecture for CV, including dilated convolutions [56], creative approaches for multiscale recognition [90, 55], and long-range aggregation of skip-connections [94]. However, above all else, there is one influential design element that has been adopted by nearly all efficient convolutional neural network designs developed in the last three years: grouped convolutions. Grouped convolutions, which are form of structured sparsity in the channel dimension, have a hyperparameter G, which reduces the number of parameters and the number of computations by 1 G Grouped convolutions were proposed by</title>
		<idno>FCN-8s [92]) to 2020 (SqueezeNAS-MAC-Small [57</idno>
	</analytic>
	<monogr>
		<title level="m">there is a 38x reduction in the number of FLOPs and an improvement in accuracy. 13 Finally, on the Cityscapes [91] semantic segmentation task</title>
		<editor>Krizhevsky et al.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>EfficientDet-D1 [55. in 2012 [26, 27. but they were largely forgotten, and they were not used by subsequent state-of-the-art networks such as VGG [95], Inception [96], and ResNet [23]. Grouped convolutions began to reemerge in the literature in 2016 with Xception [29] and ResNext [30</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">MobileNet showed in 2017 that in certain cases grouped convolutions can preserve most of the accuracy (compared to a non-grouped baseline) while producing extreme reductions in FLOPs, particularly when setting G = C (where C is the number of channels) in certain layers [31]. 15 The efficient FixEfficientNet</title>
		<imprint/>
	</monogr>
	<note>For ImageNet image classification. SqueezeNAS networks that we covered above all make extensive use of grouped convolutions</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Improved training regimes. A number of techniques have been developed to train a given selfattention neural network to higher accuracy on sentence classification tasks. GPT and BERT proposed methods for self-supervised pretraining of attention networks on large corpora such as Wikipedia to improve sentence classification accuracy</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
		</imprint>
	</monogr>
	<note>STILTS showed that, after performing the BERT pretraining scheme, accuracy can be further improved by applying transfer learning across multiple sentence classification datasets</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">MT-DNN showed that training BERT to simultaneously perform multiple sentence classification tasks can yield higher accuracy on some tasks [97]. 16 In addition, RoBERTa showed that pretraining BERT for more iterations on a larger dataset yields higher accuracy on NLU tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Further</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">ELECTRA trained BERT using an adversarial generator-discriminator method, and it achieved superior accuracy to RoBERTa without changing the design of the BERT encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Further</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">The RoBERTa and ELECTRA training regimes yield significant accuracy improvements when applied to both BERT-base and BERT-large, which suggests that these regimes improve the accuracy of larger (higher latency) as well as smaller (lower latency) network architectures. Finally, we believe it may be possible to further improve the accuracy of SqueezeBERT by pretraining on more data and for more iterations (similar to RoBERTa) and by pretraining with an adversarial method such as ELECTRA</title>
	</analytic>
	<monogr>
		<title level="m">13 FPN and EfficientDet-D1 were both pretrained on COCO and finetuned on ImageNet</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
				<title level="m">FCN-8s was pretrained on ImageNet and finetuned on Cityscapes. SqueezeNAS-MAC-Small was pretrained on ImageNet and COCO and finetuned on Cityscapes. Chen et al. found that pretraining on COCO improved Cityscapes accuracy by approximately 2 percentage-points</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">The special case of grouped convolution with G = C is known as a depthwise convolution</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">When running sentence classification tasks (e.g sentiment analysis and linguistic correctness checking), the MT-DNN [97] approach amoritizes much of the neural network computation across multiple tasks. We are interested to explore this angle for potential further speedups in future work</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
