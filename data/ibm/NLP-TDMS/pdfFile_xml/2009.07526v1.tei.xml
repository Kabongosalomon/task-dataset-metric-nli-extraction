<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CogTree: Cognition Tree Loss for Unbiased Scene Graph Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Chai</surname></persName>
							<email>chaiyuan@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of ASEE</orgName>
								<orgName type="laboratory">Intelligent Computing and Machine Learning Lab</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
							<email>huyue@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CogTree: Cognition Tree Loss for Unbiased Scene Graph Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T14:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scene graphs are semantic abstraction of images that encourage visual understanding and reasoning. However, the performance of Scene Graph Generation (SGG) is unsatisfactory when faced with biased data in real-world scenarios. Conventional debiasing research mainly studies from the view of data representation, e.g. balancing data distribution or learning unbiased models and representations, ignoring the mechanism that how humans accomplish this task. Inspired by the role of the prefrontal cortex (PFC) in hierarchical reasoning, we analyze this problem from a novel cognition perspective: learning a hierarchical cognitive structure of the highly-biased relationships and navigating that hierarchy to locate the classes, making the tail classes receive more attention in a coarse-tofine mode. To this end, we propose a novel Cognition Tree (CogTree) loss for unbiased SGG. We first build a cognitive structure CogTree to organize the relationships based on the prediction of a biased SGG model. The CogTree distinguishes remarkably different relationships at first and then focuses on a small portion of easily confused ones. Then, we propose a hierarchical loss specially for this cognitive structure, which supports coarse-to-fine distinction for the correct relationships while progressively eliminating the interference of irrelevant ones. The loss is model-independent and can be applied to various SGG models without extra supervision. The proposed CogTree loss consistently boosts the performance of several state-of-the-art models on the Visual Genome benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Making abstraction from an image into high-level semantics is one of the most remarkable capabilities of humans. Scene Graph Generation (SGG) <ref type="bibr" target="#b8">(Krishna et al. 2017</ref>) − a task of extracting objects and their semantic relationships in an image to form a graphical representation − aims to achieve the abstraction capability and bridge the gap between vision and language. SGG has greatly benefited the down-stream tasks in the domain of question answering (Norcliffe-Brown, Vafeias, and Parisot 2018; <ref type="bibr" target="#b35">Zhu et al. 2020)</ref> and visual understanding <ref type="bibr" target="#b24">(Shi, Zhang, and Li 2019;</ref><ref type="bibr" target="#b7">Jiang et al. 2020</ref>). Thereinto, some works <ref type="bibr" target="#b35">(Zhu et al. 2020;</ref><ref type="bibr" target="#b7">Jiang et al. 2020</ref>) feed the scene graphs into graph neural networks for relation-aware object representation, and some others <ref type="bibr" target="#b6">(Hudson and Manning 2019)</ref> perform sequential reasoning by traversing the relational graphs. Compared with independent objects, the rich relationships in SGG play the predominant role, which improve the performance of down-stream tasks and enable explainable reasoning.</p><p>Although much effort has been made in SGG with high accuracy in object detection, most of the detected relationships are far from satisfaction due to the long-tailed data distribution. Only a small portion of the collected relationship classes have abundant samples (head) while most classes contain just a few (tail). This heavily biased training data causes biased relationship prediction. Fine-grained relationships (tail) will be mostly predicted into head classes, which are not accurate or discriminative enough for high-level reasoning tasks, such as falsely predicting on instead of looking at, and coarsely predicting on instead of walking on. Besides, some fine-grained relationships, like standing on, sitting on and lying on can be even harder to distinguish from each other due to their visual similarity and scarce training data.</p><p>To tackle this problem, most research focuses on learning unbiased models by re-weighting losses <ref type="bibr" target="#b32">(Zareian, Karaman, and Chang 2020)</ref> or disentangling unbiased representations from the biased <ref type="bibr" target="#b25">(Tang et al. 2020</ref>). However, humans can effectively infer the correct relationships even when some relationships appear more frequently than others. The essential difference between human and AI systems that has been ignored lies not in the learning strategy or feature representation, but in the way that the concepts are organized. To illustrate this discrepancy, we show an example trunk parked on street in <ref type="figure" target="#fig_0">Figure 1</ref>. Existing models treat all the relationships independently for flat classification <ref type="figure" target="#fig_0">(Figure 1(a)</ref>). Because of the data bias, the head relationships, e.g. on and near, obtain high predicted probability. In contrast, the recent proposed cognition theory <ref type="bibr" target="#b23">(Sarafyazd and Jazayeri 2019)</ref> supports that humans process information hierarchically due to the role of the prefrontal cortex (PFC). In the SGG task, we generally start from making a rough distinction between remarkably different relationships. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b), relationships belonging to the concept "on", e.g. on and parked on, will firstly be distinguished from the ones about "near" concept, e.g. near and behind; then we move to distinguish the slight discrepancy among some easily confused ones in one concept, e.g. parked on, standing on and walking on, regardless of most irrelevant relationships.</p><p>Inspired by the hierarchical reasoning mechanism in PFC, we propose a novel loss function, Cognition Tree (CogTree) loss, for unbiased scene graph generation. We first propose to build a hierarchy of the relationships, imitating the knowledge structure built from the independent relationships in human mind. The CogTree is derived from the prediction of a biased SGG model that satisfies the aforementioned thinking principles: distinguishing remarkably different relationships at first and then focusing on a small portion of easily confused ones. Then we design a CogTreebased loss to train the SGG network from scratch. This loss enables the network to surpass the noises from inter-concept relationships and then intra-concept relationships progressively. In this way, the SGG models are no longer required to distinguish each detailed discrepancy among all the relationships as flat thinking, resulting in more accurate prediction due to the coarse-to-fine thinking strategy.</p><p>The main contributions are summarized as follows: (1) We exploit the possibility of cognition in SGG by building the cognitive structure of the relationships, which reveals the hierarchy that the relationships are organized after preliminary learning; (2) We propose a hierarchical loss specially for the above cognitive structure, which supports coarse-tofine distinction for the correct relationships while progressively eliminating the interference of irrelevant ones. It is model-independent and can be applied to various SGG models without extra supervision; (3) We perform extensive eval-uation on state-of-the-art models and a stronger transformer baseline. Results show that the CogTree loss consistently boosts their performance with remarkable improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Scene Graph Generation. SGG <ref type="bibr" target="#b30">(Xu et al. 2017)</ref> products graphical abstraction of an image and encourages visual relational reasoning and understanding in various downstream tasks <ref type="bibr" target="#b35">(Zhu et al. 2020;</ref><ref type="bibr" target="#b7">Jiang et al. 2020)</ref>. Early works focus on object detection and relationship detection via independent networks <ref type="bibr" target="#b17">(Lu et al. 2016;</ref><ref type="bibr" target="#b34">Zhang et al. 2017)</ref>, ignoring the rich contextual information. To incorporate the global visual context, recent works leverage message passing mechanism <ref type="bibr" target="#b30">(Xu et al. 2017;</ref><ref type="bibr" target="#b11">Li et al. 2017;</ref><ref type="bibr" target="#b31">Yang et al. 2018;</ref><ref type="bibr" target="#b21">Qi et al. 2019;</ref><ref type="bibr" target="#b2">Chen et al. 2019a</ref>) and recurrent sequential architectures <ref type="bibr" target="#b33">(Zellers et al. 2018;</ref><ref type="bibr" target="#b29">Woo et al. 2018;</ref><ref type="bibr" target="#b26">Tang et al. 2019)</ref> for more discriminative object and relationship representations. Although the accuracy is high in object detection, the relationship detection is far from satisfaction due to the heavily biased data. <ref type="bibr" target="#b3">(Chen et al. 2019b;</ref><ref type="bibr" target="#b26">Tang et al. 2019)</ref> consider the biased SGG problem and propose mean Recall as the unbiased metric without corresponding debiasing solutions. The recent work <ref type="bibr">(Liang et al. 2019)</ref> prunes the predominant spacial relationships and keeps the tail but informative ones in the dataset. <ref type="bibr" target="#b25">(Tang et al. 2020)</ref> proposes the first solution for unbiased SGG by counterfactual surgeries on causal graphs. We rethink SGG task from the cognition perspective and novelly solve the bias problem based on the coarse-to-fine structure of the relationships, imitating human's hierarchical thinking mechanism. Biased Classification. Classification on highly-biased training data has been extensively studied in previous work, which can be divided into three categories: (1) balancing data distribution by data augmentation or re-sampling <ref type="bibr" target="#b0">(Burnaev, Erofeev, and Papanov 2015;</ref><ref type="bibr" target="#b9">Li, Li, and Vasconcelos 2018;</ref><ref type="bibr" target="#b12">Li and Vasconcelos 2019)</ref>; (2) debiasing learning strategies by re-weighting losses or training curriculums <ref type="bibr" target="#b18">(Mikolov et al. 2013;</ref><ref type="bibr" target="#b5">Huang et al. 2016;</ref><ref type="bibr" target="#b16">Lin et al. 2017b;</ref><ref type="bibr" target="#b4">Cui et al. 2019)</ref>; (3) separating biased representations from the unbiased for prediction <ref type="bibr" target="#b19">(Misra et al. 2016;</ref><ref type="bibr" target="#b1">Cadene et al. 2019;</ref><ref type="bibr" target="#b25">Tang et al. 2020)</ref>. Our CogTree loss belongs to the second category but differs from existing methods in that we first leverage the hierarchical structure inherent in the relationships, which enables more discriminative representation learning by a coarse-to-fine mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>The central goal of our CogTree loss is to enable the existing SGG models to generate unbiased scene graphs with highly biased data. Since the CogTree loss is model-independent, in this section, we start by summarizing the framework of conventional biased SGG models. Based on this framework, we propose a novel transformer-based SGG network (SG-Transformer) for better considering contextual information as a stronger baseline. Then we introduce the CogTree building process to adaptively construct the coarse-to-fine structure among the independent relationships from the biased SGG models. A CogTree loss is then proposed to train  the network by suppressing inter-concept and intra-concept noises in a hierarchical way, resulting in unbiased scene graphs without requiring extra supervision. The framework is illustrated in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Framework of Biased SGG Models</head><p>We summarize the SGG framework from three typical models, i.e. the state-of-the-art MOTIFS <ref type="formula">(</ref> Object classification aims to detect the object locations and object classes in an image. Typically, a pre-trained Faster R-CNN <ref type="bibr" target="#b22">(Ren et al. 2015</ref>) is applied to extract K objects O = {o i } K and describe object o i by the visual feature v i , the initial object class probabilities c i , and the spatial feature b i . The above three kinds of contextual features are concatenated, linear transformed and fed into Encoder o to obtain the object embedding m i , which is then decoded by Decoder o to predict its fine-tuned object label l o i . It is a beneficial prior that some objects always cooccur in different scenes, which is an informative clue to enrich object semantics. In MOTIFS and VCTree, LSTMs/TreeLSTMs are used in Encoder o and Decoder o to capture the co-occurrence among objects. In this work, we propose a transformer-based Encoder o to adaptively gather multi-view contextual information for a certain object without the limitation of sequential inputs. As shown in the bottom of <ref type="figure" target="#fig_1">Figure 3</ref>, the encoder consists of N object-toobject (O2O) blocks and each block contains a multi-head self-attention layer followed by a fully-connected layer, both succeeded with residual connection and layer normalization <ref type="bibr" target="#b27">(Vaswani et al. 2017)</ref>. Our Decoder o is a fully connected layer followed by a softmax layer. Relationship classification predicts the relationship l r ij between objects o i and o j by typically taking three inputs: object embeddings v i and v j , word embeddings of the object predictions s i and s j , and the visual features of the union region v ij . MOTIFS and VCTree utilize sum fusion as Encoder r and a fully connected layer as Decoder r .</p><p>Unlike previous works that only encode subject and object features,we observe that the global objects are also beneficial for relationship representations. For example, wave, beach and their spatial information will help to distinguish whether the relationship is man-riding-surfboard or man-carrying-surfboard. Therefore, we propose a transformer-based Encoder r to adaptively capture such contextual semantics by M relation-to-object (R2O) blocks. R2O differs from O2O only in that the self-attention layer is replaced by a cross-attention layer to gather relevant information from global objects {m i } K . The input relationship embedding r ij is computed by a linear transformation of the concatenated feature</p><formula xml:id="formula_0">[v ij , s i , s j , b i , b j ].</formula><p>The output of the last R2O block will be concatenated with subject m i and object m j orderly, and fed into the Decoder r , a fully connected layer followed by a softmax layer, to predict l r ij . Training loss is the softmax cross-entropy loss <ref type="bibr" target="#b33">(Zellers et al. 2018)</ref> for both object and relationship classification. It can be regarded as flat thinking that treats all the classes independently and makes prediction at one time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bias-Adaptive Cognition Tree Building</head><p>Once the above SGG models have been trained on the imbalanced data, the biased models are most likely to predict finegrained relationships (tail) into inaccurate but reasonable head ones. The models can make rough distinction between remarkably different kinds of relationships. Once predicted into the same class, different relationships mostly share similar properties on either visual appearance (e.g. walking on and standing on) or high-level semantics (e.g. has and with). We term each distinct kind of relationships sharing similar properties as a concept. Next, we induce concepts from all the relationships and represent their hierarchical structure by a cognition tree. As shown on the top of <ref type="figure">Figure 2</ref>, this process includes the following three steps (see Appendix 1 for the pseudo code):</p><p>Step 1: Bias-Adaptive Concept Induction. We first induct each relationship into a certain concept based on the biased predictions. Specifically, for all the samples with the same ground-truth class d i , we predict their relationships via a biased model and calculate the distribution of predicted label frequency, denoted as P i . The most frequently predicted class for the ground-truth class d i is regarded as its concept relationship. As shown in <ref type="figure">Figure 2</ref>, on is the concept relationship of itself and standing on. The above operation induces all the relationships into C concepts with corresponding concept relationships {c i } C .</p><p>Step 2: Concept-Centered Subtree Building. We represent the containment structure of relationships in each concept by a Concept-Centered Subtree. For the i th subtree, the root is c i while the leaves are the relationships induced in this concept. Let the edges pointing from the root to the leaves, indicating the containment relations. Note that, if a subtree only contains the root c j without leaves, e.g. parked on in <ref type="figure">Figure 2</ref>, we directly link it to the concept relationship c k , which has the second highest frequency in P j and has leaves in its subtree. This operation supports to induce some tail but isolated relationships into the most approximate concepts. This process outputs T subtrees.</p><p>Step 3: Cognition-Based Subtree Aggregation. At last, we aggregate all the subtrees into one CogTree by a coarse-tofine approach: we construct 4 layers, including root layer y 0 , concept layer y 1 , coarse-fine layer y 2 and fine-grained layer y 3 , one at a time progressively. The goal of each layer is to induce relationships into coarser groups than the layer that comes after it. As illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>, y 1 contains T virtual nodes, each representing a subtree induced in Step 2. The following y 2 layer distinguishes the coarse and finegrained relationships in that subtree by splitting each node in y 1 into two nodes: one leaf node indicating the concept relationship while the other virtual node representing the cluster of fine-grained relationships. It is worth mentioning that y 2 only focuses on distinguishing whether the input can be described by a coarse or fine-grained relationship, without the burden of discriminating slight discrepancy among the finegrained ones. The virtual node in y 2 links to its fine-grained relationships of this concept in y 3 , which only contains easily confused ones, e.g. standing on and walking on, regardless of most irrelevant relationships. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning with Cognition Tree Loss</head><p>Even though the SGG models are encouraged to separate relationships by "flat" cross-entropy loss, it is non-trivial to clearly separate both coarse and fine-grained relationships at one time, especially when similarities existed among them.</p><p>In fact, it is not a one-shot deal in human mind. Throughout this section, we make unbiased predictions by training the SGG models from scratch based on the above induced CogTree, imitating humans' hierarchical thinking. The basic idea is illustrated in the bottom right of <ref type="figure">Figure 2</ref>. We add loss terms that further encourage the models to separate relationships from coarse to fine progressively. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, we define the loss terms as follows:</p><p>Ground-Truth Labels. Given the ground-truth label of a sample, we track the path from the root to the ground-truth leaf node in CogTree and denote the path as S 0 → S 1 → ... → S K , where the k th node S k in the path denotes the ground-truth node at layer k. K=2 or 3 in our CogTree. Predicted Probability. Given a sample, the predicted probabilities over all the classes D r from a biased model is denoted as P pred ={p 1 , ..., p Dr }. The probability of each leaf node in CogTree corresponds to the value in P pred with the same class. The probability of each internal node is the average value of its children. For the internal node i, we denote z i as its own probability and Z(i) as the set of probabilities of its children.</p><p>Class-Balanced Weight. Since the success of re-weighting strategy for debiasing, we adopt a well performed weighting factor w i = (1−β)/(1−β ni ) <ref type="bibr" target="#b4">(Cui et al. 2019)</ref> as the balance weight of a leaf node, with the hyper-parameter β ∈ [0, 1) and the sample number n i of the leaf class. For the weight of each internal node, we compute the average value of its children in the same way as probability computation. We denote w i as the weight of node i. Cognition Tree Loss. It contains two parts: the classbalanced (CB) softmax cross-entropy loss for "flat" thinking among independent relationships and the tree-based classbalanced (TCB) softmax cross-entropy loss for "coarse-tofine" thinking along the ground-truth path in CogTree. Given a sample with the ground-truth label d i , the CB loss is computed between predicted probabilities P pred and label d i as:</p><formula xml:id="formula_1">L CB = −w di log( exp(p di ) pj ∈P pred exp(p j ) )<label>(1)</label></formula><p>For this sample, if the ground-truth path in CogTree has K + 1 nodes, except the leaf node, we would have K different terms in our TCB loss. For the root and each internal node in the ground-truth path, we compute the classbalanced softmax cross-entropy across the children and average over all the terms to obtain the TCB loss as:</p><formula xml:id="formula_2">L T CB = 1 K K k=1 −w S k log( exp(z S k ) zj ∈Z(S k−1 ) exp(z j ) )<label>(2)</label></formula><p>TCB loss enables the network to surpass the noises from inter-concept relationships to learn concept-independent representations first, and then surpass the noises from intraconcept relationships to refine relationship-independent representations, resulting in more accurate and discriminative representations. The CogTree loss is defined as a weighted sum of L CB and L T CB to leverage both of their advantages:</p><formula xml:id="formula_3">L = L CB + λL T CB (3)</formula><p>where λ is a hyper-parameter. The SGG model is trained from scratch by the CogTree loss without any extra supervision. In the test stage, the models predict via original decoders without any modification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Dataset: We evaluate the CogTree loss on the Visual Genome dataset <ref type="bibr" target="#b8">(Krishna et al. 2017)</ref>, which contains 108k images, 75k object classes and 37k predicate (i.e. relationship) classes. Since 92% predicate classes have less than 10 samples, we adopt the widely used VG split <ref type="bibr" target="#b30">(Xu et al. 2017;</ref><ref type="bibr" target="#b33">Zellers et al. 2018;</ref><ref type="bibr" target="#b26">Tang et al. 2019;</ref><ref type="bibr" target="#b2">Chen et al. 2019a;</ref><ref type="bibr" target="#b25">Tang et al. 2020)</ref>, with the 150 most frequent object classes and 50 predicate classes. The VG split only contains training set and test set and we follow previous work <ref type="bibr" target="#b33">(Zellers et al. 2018)</ref> to sample a 5K validation set from the training set. Tasks and Evaluation: Following previous work <ref type="bibr" target="#b33">(Zellers et al. 2018)</ref>, the SGG task can be divided into three sub-tasks: (1) Predicate Classification (PredCls) takes the ground-truth object labels and bounding boxes for relationship prediction;</p><p>(2) Scene Graph Classification (SGCls) takes ground-truth bounding boxes for object label prediction;</p><p>(3) Scene Graph Detection (SGDet) predict SGs from scratch. To evaluate the unbiased SGG, we follow <ref type="bibr" target="#b3">(Chen et al. 2019b;</ref><ref type="bibr" target="#b26">Tang et al. 2019)</ref> to use the unbiased metric mean Recall@K (mR@K), which calculates R@K for each class separately and average R@K for all the classes. Implementation: We use a pre-trained Faster R-CNN <ref type="bibr" target="#b22">(Ren et al. 2015)</ref> with ResNeXt-101-FPN <ref type="bibr" target="#b15">(Lin et al. 2017a</ref>) as the object detector. SG-transformer contains 3 O2O blocks, 2 R2O blocks and 12 attention heads. The balanced weight λ in CogTee loss is set to 1. β in the re-weighting factor is set to 0.999. Our models are trained by SGD optimizer with 5 epochs, where the mini-batch size is 12 and the learning rate is 1.2 × 10 −3 . All the experiments are implemented with PyTorch and conducted with NVIDIA Tesla V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">State-of-the-Art Comparison</head><p>We evaluate the CogTree loss on three baseline models: MOTIFS, VCTree, and SG-Transformer, and compare the performance with the state-of-the-art debiasing approach TDE <ref type="bibr" target="#b25">(Tang et al. 2020</ref>). All the above models share the same pre-trained Faster R-CNN detector and sum fusion decoder (except SG-Transformer). We also compare the performance with existing biased models, including IMP+ <ref type="bibr" target="#b30">(Xu et al. 2017)</ref>, FREQ <ref type="bibr" target="#b33">(Zellers et al. 2018)</ref>, <ref type="bibr">KERN (Chen et al. 2019b</ref>), MOTIFS <ref type="bibr" target="#b33">(Zellers et al. 2018)</ref>, and VCTree <ref type="bibr" target="#b26">(Tang et al. 2019</ref><ref type="bibr" target="#b25">(Tang et al. , 2020</ref>.</p><p>In <ref type="table" target="#tab_1">Table 1</ref>, we have the following observations: (1) CogTree loss is a stable method that remarkably improves all the baselines on all the metrics. SGCls and PredCls achieve more significant improvements compared with SGDet. (2) CogTree loss consistently outperforms conventional debiasing methods, including focal, reweight and resample, which achieve limited performance increase on MOTIFS.</p><p>(3) Compared with the state-of-the-art debiasing method TDE, CogTree loss has obvious advantages on all the base-  lines. Moreover, the results of R@K in Appendix 2.1 show a performance increase from TDE to CogTree, which indicates that our method generates better unbiased SGs while keeping more correct head predicates compared with TDE.</p><p>(4) SG-Transformer consistently outperforms all the existing SGG models and baselines, which verifies that the contextual information gathered by transformer structure is of great benefit for discriminative object and relationship representations. Notably, SG-Transformer achieves new state-ofthe-art performance on SGG tasks among the biased models. We also report the R@100 performance of each class in <ref type="figure">Figure 5</ref>. SG-Transformer+CogTree obviously improves from SG-Transformer at most tail classes but drops from SG-Transformer at a few head classes, further indicating that the increase on mR@K is mainly due to the improvement of the tail classes instead of the head. All the improvement should owe to the discriminative relationship representations leaned by the CogTree loss. As the t-SNE visualiza-tion shown in <ref type="figure">Figure 6</ref>, samples of different relationships, including both the concept relationships (A, D) and the finegrained ones (B, C, E, F) are separated more obviously while samples of each relationship are clustered more densely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>In <ref type="table" target="#tab_2">Table 2</ref>, we show ablation results to verify the contribution of each component in our CogTree loss on the SG-Transformer baseline. In models '1-4, we assess the effectiveness of each part in the loss. Compared with the full model, the performance of both '1 and '3 decreases when removing the other loss term, which indicates that both hierarchical and flat losses are beneficial to provide complementary information for SGG tasks. Thereinto, the hierarchical loss has greater influence than the flat one. When removing the class-balanced weight from both '1 and '3, '2' and '4' result in a further decrease. It proves the benefits of reweighting in model debiasing, though it is obviously less influential on the hierarchical loss. Model '4' utilizes the softmax cross-entropy loss and obtains the worst performance.</p><p>In models '5-7', we evaluate the influence of the tree structure on the performance (see Appendix 3 for tree structure visualization). The CogTree is built based on two principles: a) Relationships belonging to the same concept are organized in one subtree; b) Relationships in one subtree are organized in different layers from coarse to fine. We manually adjust the induced tree that violates the above principles and evaluate the performance with the same loss L. In '5', we violate the second principle and place the fine-grained relationships in the same coarse-fine layer as the concept relationship. In '6', we disregard the first principle and mix relationships belonging to different concepts in one subtree. Specifically, we simply link all the concept relationships flatly to the root while placing all the fine-grained relationships flatly in a subtree linked to the root. We can observe a significant decrease in performance when the tree structure does not satisfy either of the two principles. In '7', we build the tree structure by the conventional hierarchical clustering on relationship representations extracted from the last fully-connected layer weights <ref type="bibr" target="#b28">(Wan et al. 2020</ref>). This tree is indicative of the visual similarity of relationships, which results in an obvious decrease compared to the full model. It indicates that the relationship hierarchy is better based on semantic correlation instead of visual similarity. In models '8' and '9', we test the influence of different functions: MAX and SUM, for calculating the predicted probabilities and the class-balanced weights of internal nodes. Compared with the full model using AVER-AGE, both '8' and '9' have an obvious performance drop. Further analysis implies that both MAX and SUM increase the predicted probabilities of the internal node in the coarsefine layer, thus decreasing the incorrect prediction penalty of fine-grained relationships in Eq. 2. In comparison, the increased values of balancing weights by MAX and SUM have less influence on the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Analysis</head><p>We visualize several PredCls samples that generated by SG-Transformer (blue) and SG-Transformer+CogTree (green) in <ref type="figure" target="#fig_5">Figure 7</ref>. Even though SG-Transformer is the strongest baseline among the three, it still achieves remarkable improvement when equipped with the CogTree loss: (1) CogTree loss encourages the model to predict more accurate and discriminative relationships compared with the baseline. As shown in the first two rows, the baseline prefers to predict the reasonable but trivial head classes on and near due to the biased training, while our model precisely predicts the fine-grained and informative relationships like parked on and in front of. It mainly because that the CogTree loss effectively separates the concept relationships apart from the fine-grained ones in the coarsefine layer. (2) CogTree loss enables the model to distinguish visually and semantically similar relationships, which is quite difficult for the baseline as shown in the last row. The baseline falsely predicts walking on as standing on since it can not capture the detailed difference between the two actions. Our model succeeds in such prediction by surpassing noises from all the irrelevant relationships and just focusing on semantically similar ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Parameter Analysis</head><p>In <ref type="table" target="#tab_3">Table 3</ref>, we assess the influence of different values of λ on the performance. λ = 1 achieves the best performance on most metrics. The performance drops slightly in the range of [0.7, 1.3]. We also vary the number of O2O and R2O blocks from 2 to 4 in SG-Transformer (see Appendix 2.2) and obtain the highest mR@K with 3 O2O blocks and 2 R2O blocks. We use the above settings in our full model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a CogTree loss to generate unbiased scene graphs with highly biased data, which focuses on hierarchical relationship distinction from the cognition perspective. We novelly leverage the biased prediction from SGG models to organize the independent relationships by a tree structure, which contains multiple layers corresponding to the relationships from coarse to fine. We propose a CogTree loss specially for the above tree structure that supports hierarchical distinction for the correct relationships while progressively eliminating the irrelevant ones. The loss is model-independent and consistently boosting the performance of various SGG models with remarkable improvement. How to incorporate commonsense knowledge to optimize the CogTree structure will be our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) SGG model with conventional flat loss. (b) SGG model with our proposed cognition tree loss. The word size in the top-left box is proportional to the word frequency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The encoder structure in SG-Transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Zellers et al. 2018), VCTree (Tang et al. 2019) and our proposed Transformerbased SGG network (SG-Transformer). As shown in Figure 2, it mainly contains two processes: object classification (bottom) and relationship classification (top).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of calculating the TCB loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6</head><label>56</label><figDesc>R@100 of the most frequent 35 classes on PredCls. : t-SNE visualization of relationship embeddings. A-C are from SG-Transformer while D-F are from SG-Transformer+CogTree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of scene graphs generated by SG-Transformer (blue) and SG-Transformer+CogTree (green). Compared with the ground-truth, the quality of predicted relationships are marked in three colors: red (false), blue (correct), purple (better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Input Image Learning with Cognition Tree Loss On Parked on Standing on Walking on Near In front of Behind A B</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Joint Objects</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Relation Embeddings</cell><cell>Flat Loss</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>…</cell><cell></cell><cell cols="3">Relation Encoder</cell><cell></cell><cell>…</cell><cell cols="2">Relation Decoder</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Object and Label Embeddings</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Objects</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Object Embeddings</cell><cell>Flat Loss</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>…</cell><cell></cell><cell cols="3">Object Encoder</cell><cell></cell><cell>…</cell><cell cols="2">Object Decoder</cell></row><row><cell cols="14">Figure 2: The overview of CogTree loss applied to SGG models. It contains three parts: Scene Graph Generation Network</cell></row><row><cell cols="14">summarizes the framework of biased SGG models; Bias-Adaptive Cognition Tree Building organizes relationships by a coarse-</cell></row><row><cell>Joint Objects …</cell><cell>Attention</cell><cell>Relation-Object</cell><cell>Connection</cell><cell cols="4">Layer Norm Relation Encoder Feed Forward Residual</cell><cell>Connection</cell><cell>Residual</cell><cell>Layer Norm</cell><cell>× M</cell><cell>Relation Embeddings …</cell><cell>…</cell></row><row><cell>Objects</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Object Encoder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Object Embeddings</cell></row><row><cell>…</cell><cell>Attention</cell><cell>Object-Object</cell><cell>Connection</cell><cell>Residual</cell><cell>Layer Norm</cell><cell>Forward</cell><cell>Feed</cell><cell>Connection</cell><cell>Residual</cell><cell>Layer Norm</cell><cell></cell><cell>…</cell><cell>…</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>× N</cell><cell></cell></row></table><note>to-fine tree based on biased prediction; Learning with CogTree Loss supports network to distinguish relationships hierarchically.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>State-of-the-art comparison on Visual Genome dataset.</figDesc><table><row><cell></cell><cell cols="3">Scene Graph Detection</cell><cell cols="3">Scene Graph Classification</cell><cell cols="3">Predicate Classification</cell></row><row><cell>Model</cell><cell cols="9">mR@20 mR@50 mR@100 mR@20 mR@50 mR@100 mR@20 mR@50 mR@100</cell></row><row><cell>IMP+</cell><cell>-</cell><cell>3.8</cell><cell>4.8</cell><cell>-</cell><cell>5.8</cell><cell>6.0</cell><cell>-</cell><cell>9.8</cell><cell>10.5</cell></row><row><cell>FREQ</cell><cell>4.5</cell><cell>6.1</cell><cell>7.1</cell><cell>5.1</cell><cell>7.2</cell><cell>8.5</cell><cell>8.3</cell><cell>13.0</cell><cell>16.0</cell></row><row><cell>KERN</cell><cell>-</cell><cell>6.4</cell><cell>7.3</cell><cell>-</cell><cell>9.4</cell><cell>10.0</cell><cell>-</cell><cell>17.7</cell><cell>19.2</cell></row><row><cell>MOTIFS</cell><cell>4.2</cell><cell>5.7</cell><cell>6.6</cell><cell>6.3</cell><cell>7.7</cell><cell>8.2</cell><cell>10.8</cell><cell>14.0</cell><cell>15.3</cell></row><row><cell>VCTree</cell><cell>5.2</cell><cell>6.9</cell><cell>8.0</cell><cell>8.2</cell><cell>10.1</cell><cell>10.8</cell><cell>14.0</cell><cell>17.9</cell><cell>19.4</cell></row><row><cell>MOTIFS (baseline)</cell><cell>4.1</cell><cell>5.5</cell><cell>6.8</cell><cell>6.5</cell><cell>8.0</cell><cell>8.5</cell><cell>11.5</cell><cell>14.6</cell><cell>15.8</cell></row><row><cell>MOTIFS + Focal</cell><cell>3.9</cell><cell>5.3</cell><cell>6.6</cell><cell>6.3</cell><cell>8.0</cell><cell>8.5</cell><cell>11.5</cell><cell>14.6</cell><cell>15.8</cell></row><row><cell>MOTIFS + Reweight</cell><cell>6.5</cell><cell>8.4</cell><cell>9.8</cell><cell>8.4</cell><cell>10.1</cell><cell>10.9</cell><cell>16.0</cell><cell>20.0</cell><cell>21.9</cell></row><row><cell>MOTIFS + Resample</cell><cell>5.9</cell><cell>8.2</cell><cell>9.7</cell><cell>9.1</cell><cell>11.0</cell><cell>11.8</cell><cell>14.7</cell><cell>18.5</cell><cell>20.0</cell></row><row><cell>MOTIFS + TDE</cell><cell>5.8</cell><cell>8.2</cell><cell>9.8</cell><cell>9.8</cell><cell>13.1</cell><cell>14.9</cell><cell>18.5</cell><cell>25.5</cell><cell>29.1</cell></row><row><cell>MOTIFS + CogTree</cell><cell>7.9</cell><cell>10.4</cell><cell>11.8</cell><cell>12.1</cell><cell>14.9</cell><cell>16.1</cell><cell>20.9</cell><cell>26.4</cell><cell>29.0</cell></row><row><cell>VCTree (baseline)</cell><cell>4.2</cell><cell>5.7</cell><cell>6.9</cell><cell>6.2</cell><cell>7.5</cell><cell>7.9</cell><cell>11.7</cell><cell>14.9</cell><cell>16.1</cell></row><row><cell>VCTree + TDE</cell><cell>6.9</cell><cell>9.3</cell><cell>11.1</cell><cell>8.9</cell><cell>12.2</cell><cell>14.0</cell><cell>18.4</cell><cell>25.4</cell><cell>28.7</cell></row><row><cell>VCTree + CogTree</cell><cell>7.8</cell><cell>10.4</cell><cell>12.1</cell><cell>15.4</cell><cell>18.8</cell><cell>19.9</cell><cell>22.0</cell><cell>27.6</cell><cell>29.7</cell></row><row><cell>SG-transformer (baseline)</cell><cell>5.6</cell><cell>7.7</cell><cell>9.0</cell><cell>8.6</cell><cell>11.5</cell><cell>12.3</cell><cell>14.4</cell><cell>18.5</cell><cell>20.2</cell></row><row><cell>SG-transformer + CogTree</cell><cell>7.9</cell><cell>11.1</cell><cell>12.7</cell><cell>13.0</cell><cell>15.7</cell><cell>16.7</cell><cell>22.9</cell><cell>28.4</cell><cell>31.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of key components in our CogTree loss.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Scene Graph Detection</cell><cell></cell><cell>Scene Graph Classification</cell><cell>Predicate Classification</cell></row><row><cell cols="5">Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">mR@20 mR@50 mR@100 mR@20 mR@50 mR@100 mR@20 mR@50 mR@100</cell></row><row><cell cols="11">CogTree + L (full model)</cell><cell></cell><cell></cell><cell cols="3">7.92</cell><cell cols="2">11.05</cell><cell></cell><cell></cell><cell cols="3">12.70</cell><cell>12.96</cell><cell>15.68</cell><cell>16.72</cell><cell>22.89</cell><cell>28.38</cell><cell>30.97</cell></row><row><cell cols="11">1 CogTree + L T CB</cell><cell></cell><cell></cell><cell cols="3">7.70</cell><cell cols="2">10.39</cell><cell></cell><cell></cell><cell cols="3">12.07</cell><cell>12.15</cell><cell>15.07</cell><cell>16.15</cell><cell>21.08</cell><cell>27.08</cell><cell>29.41</cell></row><row><cell cols="11">2 CogTree + L T CE</cell><cell></cell><cell></cell><cell cols="3">7.57</cell><cell cols="2">10.53</cell><cell></cell><cell></cell><cell cols="3">11.86</cell><cell>12.14</cell><cell>14.42</cell><cell>15.29</cell><cell>21.16</cell><cell>26.14</cell><cell>28.32</cell></row><row><cell cols="6">3 L CB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">6.74</cell><cell cols="2">9.56</cell><cell></cell><cell></cell><cell cols="3">11.29</cell><cell>10.76</cell><cell>13.13</cell><cell>13.88</cell><cell>18.02</cell><cell>23.40</cell><cell>25.25</cell></row><row><cell cols="6">4 L CE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">5.55</cell><cell cols="2">7.74</cell><cell></cell><cell></cell><cell cols="2">8.98</cell><cell>8.57</cell><cell>11.46</cell><cell>12.27</cell><cell>14.35</cell><cell>18.48</cell><cell>20.21</cell></row><row><cell cols="10">5 Fuse-layer + L</cell><cell></cell><cell></cell><cell></cell><cell cols="3">5.86</cell><cell cols="2">8.02</cell><cell></cell><cell></cell><cell cols="2">9.05</cell><cell>8.17</cell><cell>10.39</cell><cell>11.32</cell><cell>13.77</cell><cell>18.87</cell><cell>20.77</cell></row><row><cell cols="11">6 Fuse-subtree + L</cell><cell></cell><cell></cell><cell cols="3">5.36</cell><cell cols="2">7.19</cell><cell></cell><cell></cell><cell cols="2">8.28</cell><cell>8.71</cell><cell>10.66</cell><cell>11.61</cell><cell>16.20</cell><cell>20.17</cell><cell>22.12</cell></row><row><cell cols="10">7 Cluster-tree + L</cell><cell></cell><cell></cell><cell></cell><cell cols="3">5.84</cell><cell cols="2">8.10</cell><cell></cell><cell></cell><cell cols="2">9.12</cell><cell>8.86</cell><cell>10.88</cell><cell>11.52</cell><cell>15.12</cell><cell>19.20</cell><cell>20.81</cell></row><row><cell cols="11">8 CogTree + L(MAX)</cell><cell></cell><cell></cell><cell cols="3">5.38</cell><cell cols="2">7.16</cell><cell></cell><cell></cell><cell cols="2">8.16</cell><cell>8.97</cell><cell>10.85</cell><cell>11.83</cell><cell>15.48</cell><cell>19.93</cell><cell>21.87</cell></row><row><cell cols="11">9 CogTree + L(SUM)</cell><cell></cell><cell></cell><cell cols="3">1.86</cell><cell cols="2">3.09</cell><cell></cell><cell></cell><cell cols="2">3.68</cell><cell>6.58</cell><cell>8.82</cell><cell>9.86</cell><cell>11.31</cell><cell>15.67</cell><cell>17.98</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>on</cell><cell>has</cell><cell>we arin g</cell><cell>of</cell><cell>in</cell><cell>nea r</cell><cell>wit h</cell><cell>hol din g</cell><cell>beh ind abo ve</cell><cell>sitt ing on ridi ng</cell><cell>und er in fro nt of sta ndi ng on</cell><cell>at</cell><cell cols="2">car ryin g att ach ed to wa lkin g on</cell><cell>for</cell><cell>ove r</cell><cell>lay ing on loo kin g at han gin g fro m bel ong ing to par ked on</cell><cell>eat ing usi ng</cell><cell>and</cell><cell>cov erin g cov ere d in</cell><cell>bet we en alo ng</cell><cell>lyin g on</cell><cell>wa tch ing</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SG-Transformer</cell><cell cols="4">SG-Transformer (CogTree)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance with different balancing weights.</figDesc><table><row><cell></cell><cell cols="3">Scene Graph Detection</cell><cell cols="3">Scene Graph Classification</cell><cell cols="3">Predicate Classification</cell></row><row><cell>λ</cell><cell cols="9">mR@20 mR@50 mR@100 mR@20 mR@50 mR@100 mR@20 mR@50 mR@100</cell></row><row><cell>0.4</cell><cell>7.22</cell><cell>9.72</cell><cell>11.30</cell><cell>12,09</cell><cell>15.03</cell><cell>16.12</cell><cell>21.17</cell><cell>27.20</cell><cell>29.59</cell></row><row><cell>0.7</cell><cell>8.62</cell><cell>11.30</cell><cell>12.70</cell><cell>12.22</cell><cell>14.91</cell><cell>15.88</cell><cell>21.28</cell><cell>26.39</cell><cell>28.69</cell></row><row><cell>1</cell><cell>7.92</cell><cell>11.05</cell><cell>12.70</cell><cell>12.96</cell><cell>15.68</cell><cell>16.72</cell><cell>22.89</cell><cell>28.38</cell><cell>30.97</cell></row><row><cell>1.3</cell><cell>7.84</cell><cell>10.48</cell><cell>12.19</cell><cell>12.35</cell><cell>15.04</cell><cell>15.87</cell><cell>22.06</cell><cell>27.11</cell><cell>29.21</cell></row><row><cell>1.6</cell><cell>6.78</cell><cell>9.04</cell><cell>10.22</cell><cell>12.20</cell><cell>14.70</cell><cell>15.82</cell><cell>21.34</cell><cell>26.80</cell><cell>29.18</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Influence of resampling on accuracy of imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burnaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Erofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="987521" to="987521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rubi: reducing unimodal biases for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dancette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="841" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Counterfactual critic multi-agent training for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4613" to="4623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowledgeembedded routing network for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6163" to="6171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5375" to="5384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning by abstraction: the neural state machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5903" to="5916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DualVD: an adaptive dual encoding model for deep visual understanding in visual dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11125" to="11132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual genome: connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">RESOUND: towards action recognition without representation bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="513" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Factorizable net: an efficient subgraph-based framework for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="335" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scene graph generation from objects, phrases and region captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1261" to="1270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Repair: removing representation bias by dataset resampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9572" to="9581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">VrR-VG: refocusing visually-relevant relationships</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page" from="10403" to="10412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="852" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Seeing through the human reporting bias: visual classifiers from noisy human-centric labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2930" to="2939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning conditioned graph structures for interpretable visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Norcliffe-Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vafeias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parisot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8334" to="8343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attentive relational networks for mapping images to scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3957" to="3966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical reasoning by neural circuits in the frontal cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarafyazd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jazayeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">364</biblScope>
			<biblScope unit="issue">6441</biblScope>
			<biblScope unit="page">8911</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Explainable and explicit visual reasoning over scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8376" to="8384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unbiased scene graph generation from biased training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3716" to="3725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to compose dynamic tree structures for visual contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6619" to="6628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dunlap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petryk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00221</idno>
		<title level="m">NBDT: neuralbacked decision trees</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Linknet: Relational embedding for scene graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="560" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5410" to="5419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph r-cnn for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="670" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bridging knowledge graphs to generate scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural Motifs: scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5831" to="5840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Relationship proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5678" to="5686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mucko: multi-Layer cross-modal knowledge reasoning for fact-based visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1097" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
