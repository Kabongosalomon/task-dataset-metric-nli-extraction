<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">iPerceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video Captioning and Video Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aman</forename><surname>Chadha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gurneet</forename><surname>Arora</surname></persName>
							<email>gkarora@uwaterloo.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navpreet</forename><surname>Kaloty</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">iPerceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video Captioning and Video Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most prior art in visual understanding relies solely on analyzing the "what" (e.g., event recognition) and "where" (e.g., event localization), which in some cases, fails to describe correct contextual relationships between events or leads to incorrect underlying visual attention. Part of what defines us as human and fundamentally different from machines is our instinct to seek causality behind any association, say an event Y that happened as a direct result of event X. To this end, we propose iPerceive, a framework capable of understanding the "why" between events in a video by building a common-sense knowledge base using contextual cues to infer causal relationships between objects in the video. We demonstrate the effectiveness of our technique using the dense video captioning (DVC) and video question answering (VideoQA) tasks. Furthermore, while most prior work in DVC and VideoQA relies solely on visual information, other modalities such as audio and speech are vital for a human observer's perception of an environment. We formulate DVC and VideoQA tasks as machine translation problems that utilize multiple modalities. By evaluating the performance of iPerceive DVC and iPerceive VideoQA on the ActivityNet Captions and TVQA datasets respectively, we show that our approach furthers the state-of-the-art. Code and samples are available at:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Today's computer vision systems are good at telling us the "what" (e.g., classification <ref type="bibr" target="#b20">[20]</ref>, segmentation <ref type="bibr" target="#b10">[10]</ref>) and "where" (e.g., detection <ref type="bibr" target="#b31">[31]</ref>, localization <ref type="bibr" target="#b52">[52]</ref>, tracking <ref type="bibr" target="#b63">[63]</ref>). Common-sense reasoning <ref type="bibr" target="#b43">[43]</ref>, which leads to the interesting question of "why", is a thinking gap in today's pattern learning-based systems which rely on the likelihood of observing object Y given object X, P (Y |X).</p><p>Failing to factor in causality leads to the incorrect conclusion that the co-existence of objects X and Y might be <ref type="bibr">Figure 1</ref>. Top: An example of a cognitive error in DVC. While the girl tries to block the boy's dunking attempt, him jumping (event X) eventually leads to him dunking the basketball through the hoop (event Y ). Bottom: An example of incorrect attention where conventional DVC approaches correlate a chef and steak to the activity of cooking without even attending to the nearby oven. We used <ref type="bibr" target="#b18">[18]</ref> as our DVC baseline as it is the current state-of-the-art. attributed to spurious observational bias <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b37">37]</ref>. For e.g., if a keyboard and mouse are often observed on a table, the model learns to develop an "association" between the two. The underlying common-sense that the keyboard and mouse are parts of a computer would not be inferred, and in fact the duo would be wrongly associated as being part of a table.</p><p>In the event that a keyboard and mouse are observed outside of a tabular setting, the model can commit a cognitive error.</p><p>Prior work <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b37">37]</ref> has unraveled the spurious observational bias that models fall prey to. To alleviate this, we propose iPerceive -a framework that utilizes contextual cues to establish a common-sense knowledge base from one of the most common ways humans acquire information: videos.</p><p>Given the prevalence of visual information, video understanding is particularly important. To that end, the task of dense video captioning (DVC) <ref type="bibr" target="#b27">[27]</ref> aims to temporally localize events from an untrimmed video and describe them using natural language. On the other hand, video question answering (VideoQA) is another challenging task in computer vision which requires significant expressive power from the model to distill visual events and their relations using linguistic concepts.</p><p>The vast majority of research in the field of DVC <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b57">57]</ref> and VideoQA <ref type="bibr" target="#b62">[62,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36]</ref> generates captions purely based on visual information. However, given the fact that auditory feedback is an essential aspect of human communication, unsurprisingly, almost all videos include an audio track and some also include a speech track, both of which could provide vital cues for understanding the context of the event. Inspired by Iashin et al. <ref type="bibr" target="#b18">[18]</ref>, our DVC model consumes the video, audio and speech modality for the caption generation process. Similarly, our VideoQA implementation utilizes video and text (in the form of dense captions, subtitles and QA).</p><p>The task of DVC can be decomposed into two parts: event detection and event description. Existing methods tackle this using a module for each of these sub-tasks, and either train the two modules independently <ref type="bibr" target="#b18">[18]</ref> or in alternation <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b5">5]</ref>. This restricts model "wiggle", i.e., since events in a video sequence and the generated language are closely related, the language information should ideally be able to help localize events in the video. We address this by performing end-to-end training of our DVC model. While <ref type="bibr" target="#b18">[18]</ref> present a detailed study of the merits of using multiple modalities for DVC, they do not implement an endto-end trainable system and train the captioning module on ground-truth event proposals. To this end, we utilize an endto-end trainable model similar to <ref type="bibr" target="#b64">[64]</ref> -this fosters consistency between the content in the proposed video segment and the semantic information in the language description. Note that Zhou et al. <ref type="bibr" target="#b64">[64]</ref> rely solely on the visual modality and hence, do not implement multi-modal DVC. Our approach blends multi-modal DVC with end-to-end learning.</p><p>We present iPerceive, a framework that generates common-sense features by inferring the causal relationships between events in videos using contextual losses as selfsupervised training mechanisms. This enables the model to seek intrinsic causal relationships between objects within events in a video sequence. Furthermore, we offer handson evaluation of iPerceive using the tasks of DVC and VideoQA as case-studies. iPerceive DVC is a system that utilizes common-sense features and offers an end-to-end trainable multi-modal architecture that enables coherent dense video captions. Next, we propose an enhanced multimodal architecture called iPerceive VideoQA that utilizes common-sense feature generation using iPerceive and dense captions using iPerceive DVC as its building blocks.</p><p>Our key contributions are centered around commonsense reasoning for videos, which we envision as a step towards human-level causal learning. Wang et al. <ref type="bibr" target="#b58">[58]</ref> tackle the issue of observational bias in the context of images using common-sense generation, but applying a similar set of ideas to videos comes with its own set of challenges distinct from the image case. One observation is that events in videos can range across multiple time scales and can even overlap. Also, events can have causal relationships between themselves that humans subconsciously perceive without any visible acknowledgment/feedback. Humans naturally learn common sense in an unsupervised fashion by exploring the physical world, and until machines imitate this learning path, there will be a "gap" between man and machine. This requires us to build a knowledge base and acquire contextual information from temporal events in a video sequence to determine inherent causal relationships. These "context-aware" features can improve both the accuracy of contextual relationships as well as steer attention to the appropriate entities. Furthermore, videos are generally challenging to process compared to images owing to the sheer amount of data they contain.</p><p>We employ several techniques to tackle the aforementioned nuances specific to videos (cf. Section 3.3 for details). Furthermore, we offer a two-pronged evaluation of our proposed model by applying it to the challenging domains of DVC and VideoQA. A noteworthy point is that since our common-sense features can be generated in a selfsupervised manner, they have a certain universality and are not limited to the realizations of DVC and VideoQA discussed in this work. As such, they can be easily adapted for other video-based vision tasks such as scene understanding <ref type="bibr" target="#b16">[16]</ref>, panoptic segmentation <ref type="bibr" target="#b21">[21]</ref>, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Visual Common-Sense</head><p>Current research in the field of building a common-sense knowledge base mainly falls into two categories: (i) learning from images <ref type="bibr" target="#b61">[61,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b66">66]</ref> and (ii) learning actions from videos <ref type="bibr" target="#b9">[9]</ref>. While the former limits learning to humanannotated knowledge which restricts its effectiveness and outreach, the latter is essentially learning from correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Causality in Vision</head><p>There has been a recent surge of interest in coupling the complementary strengths of computer vision and causal reasoning <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b42">42]</ref>. The union of these fields has been explored in several contexts, including image classification <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b4">4]</ref>, reinforcement learning <ref type="bibr" target="#b39">[39]</ref>, adversarial learning <ref type="bibr" target="#b26">[26]</ref>, visual dialog <ref type="bibr" target="#b44">[44]</ref>, image captioning <ref type="bibr" target="#b65">[65]</ref> and scene/knowledge graph generation <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b40">40]</ref>. While these methods offer limited task-specific causal inference, <ref type="bibr" target="#b58">[58]</ref> offers a generic feature extractor for images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Video Captioning</head><p>With the success of neural models in translation systems <ref type="bibr" target="#b50">[50]</ref>, similar methods became widely popular in video captioning <ref type="bibr" target="#b60">[60,</ref><ref type="bibr" target="#b56">56]</ref>. The core rationale behind this approach is to train two recurrent neural networks (RNNs) in an encoder-decoder fashion. Specifically, an encoder inputs a set of video features and accumulates its hidden state, which is passed on to a decoder for captioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Dense Video Captioning</head><p>A significant milestone in the domain of video understanding was reached when Krishna et al. <ref type="bibr" target="#b27">[27]</ref>, inspired by the idea of the dense image captioning task <ref type="bibr" target="#b19">[19]</ref>, introduced the problem of DVC and the idea of contextawareness to utilize both past and future context. They also released a new dataset called ActivityNet Captions which has propelled research in the field <ref type="bibr" target="#b64">[64,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b32">32]</ref>. Furthermore, <ref type="bibr" target="#b57">[57]</ref> proposed attentive fusion to differentiate captions from highly overlapped events. With the recent success of Transformers <ref type="bibr" target="#b54">[54]</ref> in the machine translation task, it was inevitable for them to enter the similarly-complex task of video understanding. Zhou et al. <ref type="bibr" target="#b64">[64]</ref> adopted Transformers for DVC to alleviate the limitations of RNNs when modeling long-term dependencies in videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Multi-Modal Dense Video Captioning</head><p>Several attempts have been made to incorporate additional cues like audio and speech <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b14">14]</ref> for the dense video captioning task. Rahman et al. <ref type="bibr" target="#b45">[45]</ref> utilized the idea of cycle-consistency to build a model with visual and audio inputs. Hessel et al. <ref type="bibr" target="#b14">[14]</ref> and Shi et al. <ref type="bibr" target="#b48">[48]</ref> employ a Transformer architecture to encode both video frames and speech segments to generate captions for instructional cooking videos where speech and the captions are usually wellaligned with the visual content <ref type="bibr" target="#b38">[38]</ref>. While they achieve stellar results, their model fails to generalize to real-world videos where speech and captions can have a "gap" with the visual inputs. Iashin et al. <ref type="bibr" target="#b18">[18]</ref> tackle the problem of multi-modal DVC using a Transformer-based architecture and renders great results, but do not incorporate the concept of end-to-end training introduced in [64].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Visual/Video Question Answering</head><p>The allied tasks of Visual Question Answering (VQA) and VideoQA involve the important ability of understand-ing visual information conditioned on language. While QA based on a single image, i.e., VQA, has been well explored <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36]</ref>, the field of VideoQA is now picking up intense momentum <ref type="bibr" target="#b62">[62,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b28">28]</ref>. While VQA relies on spatial information, VideoQA requires an understanding of both spatial and temporal information, making the task of VideoQA inherently much more complex compared to VQA. Within VideoQA, Zeng et al. <ref type="bibr" target="#b62">[62]</ref> have explored using non-dense image captions. However, there exists limited research that utilizes dense captions to help improve the temporal localization of videos. Kim et al. <ref type="bibr" target="#b22">[22]</ref> tackle the task of VideoQA using DIC, while we take it a step further by utilizing DVC with common-sense features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">iPerceive DVC: Proposed Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Top Level View</head><p>Fig. 2 outlines the goals of iPerceive DVC: (i) temporally localize a set of events in a video, (ii) build a knowledge base for common-sense reasoning and, (iii) produce a textual description using audio, visual, and speech cues for each event. To this end, we apply a three-stage approach. In this paper, we limit our discussion of implementational details to the blocks that we adapt to enable the commonsense reasoning aspect of iPerceive DVC. For further details on the building blocks of our architecture, we refer the curious reader to our baseline for the DVC task <ref type="bibr" target="#b18">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Event Proposal Module</head><p>We localize the temporal events in the video using the bidirectional single-stream (Bi-SST) network <ref type="bibr" target="#b57">[57]</ref>. Bi-SST applies 3D convolutions (C3D) <ref type="bibr" target="#b53">[53]</ref> to video frames and passes on the extracted features to a Bi-directional LSTM <ref type="bibr" target="#b15">[15]</ref> network. The LSTM accumulates visual cues from past and future context over time and predicts the endpoints of each event in the video along with its confidence scores. The output of the LSTM feeds the common-sense reasoning module to convey the set of events in the input video to extract common-sense features for.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Common-Sense Reasoning: Videos vs. Images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Computational Complexity</head><p>It is common knowledge that compared to images, tasks involving videos need an exponentially larger set of resources, both in terms of compute and time. To make the task of common-sense feature generation for videos computationally tractable, we only generate features for a frame when we detect a change in the environmental "setting" going from one frame to the next, within a particular localized event. Specifically, we check for changes in the set of object labels in a scene and only generate common-sense features if a change is detected; if not, we re-use the common-sense features from the last frame. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">On-the-fly RoI Generation</head><p>Note that the architecture proposed in <ref type="bibr" target="#b58">[58]</ref> essentially serves as an improved visual region encoder given region(s) of interest (RoIs) in an image. As such, it assumes that an RoI exists and is available at train time. This greatly limits its usability to models that inherently extract RoIs (such as Mask R-CNN <ref type="bibr" target="#b11">[11]</ref>, Faster R-CNN <ref type="bibr" target="#b46">[46]</ref>, etc.), and thus reduces its effectiveness with use-cases beyond object detection, such as DVC and VideoQA. As such, we extend their work by utilizing a pre-trained Mask R-CNN <ref type="bibr" target="#b11">[11]</ref> model to generate RoIs for frames within each event that has been localized by the event proposal module before passing them onto the common-sense reasoning module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Common-Sense Reasoning Module</head><p>Common-sense reasoning via self-supervised representation learning serves as an improved visual region encoder for subsequent processing. In the context of developing a common-sense knowledge base, one of the biggest challenges involved is determining causal context: how do you figure out the cause and effect relationship between objects and thus re-align the model's context? Common-sense reasoning is based on causality which relies on P (Y |do(X)) <ref type="bibr" target="#b43">[43]</ref>. This is fundamentally different from what prior work in the domain of DVC or VideoQA, or video understanding in general, relies on: the conventional likelihood, P (Y |X).</p><p>Building upon the approach in <ref type="bibr" target="#b58">[58]</ref>, we carry out the following deliberate "borrow-put" experiment for a given frame within an event localized by the proposal module: (1) "borrow" non-local context, say an object Z from another event, (2) "put" Z in the context of object X and object Y and, (3) test if object X still causes the existence of object Y given Z. This experiment helps determine if the chance of Z is independent on X or Y . Thus, by using P (Y |do(X)) as the learning objective instead of P (Y |X), the observational bias from the "apparent" context can be alleviated. As such, the training objective of the commonsense reasoning module is the proxy task of predicting the contextual objects of an event.</p><p>Our visual world contains several confounding agents z ∈ Z that add spurious observational bias around objects X and Y and hinder common-sense development. This limits the model's learning using the traditional likelihood P (Y |X), which can be defined <ref type="bibr" target="#b58">[58]</ref> using Bayes' rule as:</p><formula xml:id="formula_0">P (Y |X) = z P (Y |X, z)P (z|X)<label>(1)</label></formula><p>where, the confounder z introduces spurious bias via P (z|X).</p><p>Since we can hardly identify all confounders in the real world, we approximate the set of confounder objects Z to a fixed confounder dictionary in the shape of a N × d matrix for practical use, where N is the number of classes in the dataset (e.g., 80 in MS-COCO <ref type="bibr" target="#b33">[33]</ref>) and d is the feature dimension of each RoI. Each entry z ∈ Z is the averaged RoI feature, obtained using Faster R-CNN <ref type="bibr" target="#b46">[46]</ref> for the samples in the dataset that belong to same class as z.</p><p>We similarly define the "do" operation by disrupting the causal link between z and X (and thus de-biasing X) as,</p><formula xml:id="formula_1">P (Y |do (X)) = z P (Y |X, z)P (z)<label>(2)</label></formula><p>Each RoI X is then fed into two sibling branches: (i) a self-predictor to predict the class of the "center" object x ∈ X and, (ii) a context-predictor to predict the "center" object's context labels, y i ∈ Y . The self-predictor outputs a probability distribution p over N categories. On the other hand, the context-predictor outputs a probability distribution for a pair of RoIs (x, y i ). The last layer of the network uses a Softmax classifier for label prediction:</p><formula xml:id="formula_2">P (Y |do (X)) = E z (Sof tmax(f (x, z)))<label>(3)</label></formula><p>where, f (·) calculates the logits for N categories and E z is obtained by sampling z over the set of confounders Z.</p><p>We utilize the normalized weighted geometric mean (NWGM) to approximate the above expectation. A detailed discourse for NWGM has been provided in <ref type="bibr" target="#b58">[58]</ref>. Furthermore, for an in-depth visual treatment on how the features learned by correlation P (Y |X) and causal intervention P (Y |do(X)) differ, along with examples on how this leads to building up of common-sense, we direct the avid reader to <ref type="bibr" target="#b58">[58]</ref>. These common-sense features are then paired with the corresponding visual features for each localized event and sent downstream for captioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Captioning Module</head><p>Given an event proposal and its common-sense vectors, the captioning module generates a caption using audio, visual and speech modalities. We formulate the captioning task as a machine translation problem and adapt the Transformer-based architecture from <ref type="bibr" target="#b18">[18]</ref>. We use inflated 3D convolutions (I3D) <ref type="bibr" target="#b3">[3]</ref> to process visual modalities and the VGGish network <ref type="bibr" target="#b13">[13]</ref> for audio modalities. We deploy an automatic speech recognition (ASR) system <ref type="bibr" target="#b8">[8]</ref> to extract temporally-aligned speech transcriptions. These are juxtaposed alongside the video frames and the corresponding audio track, and fed in as input to our model. Features from each modality are then fed to individual Transformer models along with the generated caption so far. The output of each Transformer is fused and a probability distribution is obtained over the vocabulary to sample the next word until a special end token is obtained.</p><p>A notable point is that the self-attentive operation inherently has an indiscriminate correlation against our new learning objective P (Y |do(X)) <ref type="bibr" target="#b58">[58]</ref>. Put differently, self-attention implicitly applies conventional likelihood P (Y |X), which contradicts causal reasoning P (Y |do(X)). Furthermore, given that the computation of self-attention is expensive, especially in the case of multiple heads, early concatenation of common-sense features significantly slows down training. We thus omit the self-multi-headed attention component in the encoder of <ref type="bibr" target="#b18">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Loss Functions</head><p>iPerceive DVC uses a four-fold training loss: (i) cross entropy M CE as proposal loss L p to balance positive and negative proposals, (ii) multi-task common-sense reasoning loss L cs , (iii) binary cross entropy BCE as mask prediction loss L m and, (iv) cross entropy M CE across all words in every sentence as captioning loss L c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1">Proposal Loss</head><formula xml:id="formula_3">L p = M CE(c, t, X, y)<label>(4)</label></formula><p>where, c is the prediction score at time t, X is the input video and y is the ground truth label with an acceptable intersection over union (IoU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.2">Common-Sense Reasoning Losses</head><p>The common-sense reasoning losses are a set of auxiliary self-supervised losses that help capture knowledge about the co-occurence of objects within events in a video scene. Consider a video frame which consists of a "center" object and a "context" object. For a "center" object x ∈ X in the video frame at time t, the self-predictor loss L self can be defined using negative log likelihood as,</p><formula xml:id="formula_4">L self (p, x c , t) = −log(p[x c ])<label>(5)</label></formula><p>where, p is the probability distribution output of the selfpredictor over N categories for X; x c is the ground-truth class of RoI X. Similarly, for a "context" object y i ∈ Y in the video frame at time t, the context-predictor loss L cxt is defined for a pair of RoI feature vectors (x, y i ) using negative log likelihood as,</p><formula xml:id="formula_5">L cxt (p i , y c i , t) = −log(p i [y c i ])<label>(6)</label></formula><p>where, y c i is the ground-truth label for y i ; p i is calculated using p i = P (Y i |do(X)) in Eq. 2 and p i = (p i <ref type="bibr" target="#b0">[1]</ref>, ..., p i [N ]) is the probability over N categories. The overall multi-task loss for each RoI X is, </p><formula xml:id="formula_6">L cs = L self + 1 K i L cxt (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.3">Mask Prediction Loss</head><formula xml:id="formula_7">L m = BCE(Bin(S a , E a , t), f M (S a , E p , S a , E a , t)) (8) where, Bin(.) is '1' if t ∈ [S a ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.4">Captioning Loss</head><formula xml:id="formula_8">L c = M CE(w t , w t )<label>(9)</label></formula><p>where w t is the ground truth word at time t. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.5">Overall Loss Formulation</head><p>The final loss L is a combination of the individual losses,</p><formula xml:id="formula_9">L = λ 1 L p + λ 2 L cs + λ 3 L m + λ 4 L c<label>(10)</label></formula><p>where λ 1−4 weigh the individual loss components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">iPerceive VideoQA: Proposed Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Top Level View</head><p>Building upon the architecture proposed by <ref type="bibr" target="#b22">[22]</ref>, we propose iPerceive VideoQA, a model that uses common-sense knowledge to perform VideoQA. We utilize dense captions using iPerceive DVC to offer the model additional telemetry to correlate objects identified from video frames and their salient actions expressed through dense captions. Similar to iPerceive DVC, we limit our discussion of implementational details in this study to the blocks that we adapt to enable the common-sense reasoning aspect of iPerceive VideoQA. We refer the curious reader to our baseline for the VideoQA task <ref type="bibr" target="#b22">[22]</ref> for details on the building blocks of our architecture. <ref type="figure" target="#fig_2">Fig. 3</ref> outlines the goals of iPerceive VideoQA: (i) build a common-sense knowledge base, (ii) extract features from multiple modalities: video and text (in the form of dense captions, subtitles and QA) and, (iii) implement the relevant-frames selection problem as a multi-label classification task. As such, we apply a two-stage approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Feature Fusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Feature Generation Module</head><p>Leveraging the approach in <ref type="bibr" target="#b22">[22]</ref>, we extract features from multiple modalities viz. video, dense captions, question-answer (QA) pairs and subtitles. We encode the visual features using a convolutional encoder. To generate dense captions, we utilize iPerceive DVC and operate it at a framelevel to derive dense captions for the current frame. We create five hypotheses by concatenating a question feature with each of five answer features, and we pair each visual frame feature with temporally neighboring subtitles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Common-Sense Reasoning Module</head><p>We utilize the common-sense generation module proposed in Section 3.4 to generate common-sense vectors corresponding to each frame of the input video. iPerceive VideoQA concatenates common-sense features with the features extracted from the convolutional encoder and sends the output downstream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Dual-Layer Attention</head><p>Word/Object-Level Attention: The visual features for each frame are combined with the textual features (QA and subtitles) using word/object-level attention, following the approach in <ref type="bibr" target="#b22">[22]</ref>. Separately, we also combine the DVC features with the textual features in a similar manner. To this end, we calculate similarity matrices <ref type="bibr" target="#b47">[47]</ref> from (i) QA/subtitle and QA/visual features and, (ii) QA/subtitle and QA/DVC features, respectively. Attended subtitle features are obtained from the similarity matrices.</p><p>Frame-Level Attention: The fused features from word/object-level attention are integrated frame-wise via frame-level attention.</p><p>Similar to the idea behind word/object-level attention, a similarity matrix is calculated which is used to compute the attended frame-level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Video-DVC Integration Module</head><p>[22] implements self-cross attention to amalgamate information from the dual-layer attended visual and dense caption features, both of which have been fused with QA and subtitles. As discussed in Section 3.4, due to the challenges associated with generating common-sense features using a model that implements self-attention, we carry out concatenation of video and dense caption features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Frame Selection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Selection Gates</head><p>Similar to <ref type="bibr" target="#b22">[22]</ref>, we utilize gates to selectively control the flow of information and ensure only relevant information propagates through to the classifier. As such, we use a fullyconnected layer to get frame-relevance scores that indicate the appropriateness quotient of each frame for answering a particular question. From the logits for the five candidate answers, we choose the highest value as our prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Loss Functions</head><p>iPerceive VideoQA uses a four-fold training loss: (i) multi-task common-sense reasoning loss L cs , (ii) softmax cross-entropy loss as answer selection loss L ans , (iii) balanced binary cross entropy as frame-selection loss L f s and, (iv) in-and-out frame score margin L io . We adopt similar multi-task common-sense reasoning losses L cs as in Section 3.6.2. The other loss components are similar to <ref type="bibr" target="#b22">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Overall Loss Formulation</head><p>The final loss L is a combination of the individual losses,</p><formula xml:id="formula_10">L = λ 1 L cs + λ 2 L ans + λ 3 L f s + λ 4 L io<label>(11)</label></formula><p>where λ 1−4 weigh the individual loss components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">iPerceive DVC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Dataset</head><p>We train and assess iPerceive DVC using the Activi-tyNet Captions <ref type="bibr" target="#b27">[27]</ref> dataset, using a train/val/test split of 50%/25%/25%. ActivityNet Captions contains 20k videos from YouTube. On an average, each video has 3.65 events that are 2 minutes long and are annotated by two different annotators using 13.65 words. We report all results using the validation set (since no ground truth is available for the test set).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Metrics</head><p>We use BLEU@N <ref type="bibr" target="#b41">[41]</ref> and METEOR <ref type="bibr" target="#b6">[6]</ref>, which are common DVC evaluation metrics. We use the official evaluation script provided in <ref type="bibr" target="#b27">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Comparison with Baseline Methods</head><p>Tab. 1 compares iPerceive DVC with the state-of-the-art. Algorithms were split into the ones which "saw" all training videos and others which trained on partially available data (since some YouTube videos which were part of the Ac-tivityNet Captions dataset are no longer available). <ref type="figure" target="#fig_3">Fig. 4</ref> offers a qualitative comparison between iPerceive DVC and <ref type="bibr" target="#b64">[64]</ref> and <ref type="bibr" target="#b58">[58]</ref>, which were the best performing baselines for captioning and event localization, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Ablation Analysis</head><p>Tab. 2 shows ablation studies for iPerceive DVC to assess the impact of common-sense reasoning and end-to-end training as design decisions.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Comparison with Baseline Methods</head><p>Tab. 3 shows a comparison of iPerceive VideoQA with the state-of-the-art. <ref type="figure" target="#fig_4">Fig. 5</ref> offers a qualitative comparison between iPerceive VideoQA and <ref type="bibr" target="#b22">[22]</ref> which was the best performing baseline at the time of writing.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Ablation Analysis</head><p>Tab. 4 shows ablation studies for iPerceive VideoQA to assess the impact of common-sense reasoning and iPerceive DVC as design decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We proposed iPerceive, a portable framework that enables common-sense learning for videos by building a  knowledge base using contextual cues. We demonstrated the effectiveness of iPerceive on the tasks of DVC and VideoQA using the ActivityNet Captions and TVQA datasets, respectively. Furthermore, iPerceive DVC blends multi-modal DVC with end-to-end Transformerbased learning. Using ablation studies, we showed that these common-sense features help the model better perceive relationships between events in videos, leading to improved performance on challenging video tasks that need cognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Broader Impact</head><p>Machines perceive their immediate world by analyzing videos of their environment,à la humans being cognizant of their surroundings. Video understanding is thus a critical area to conquer the perception gap between man and machine. Our work propels the idea of causal reasoning for machines and bring us a step closer to the ultimate goal of visual-linguistic causal reasoning, a distinct quality that makes us human. Since our work is easily portable, we hope that the promising results in our work would encourage researchers to further explore the idea of common-sense reasoning and apply it to new applications in the field of video and language understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Supplementary Material</head><p>The supplementary material consists of two sections. In Section 8.1, we provide qualitative results of iPerceive DVC on another example video from the ActivityNet Captions validation set. In Section 8.2, we provide qualitative results of iPerceive VideoQA on additional samples from the TVQA test set. <ref type="figure" target="#fig_10">Figure 6</ref> shows a qualitative analysis of DVC using a sample from the ActivityNet Captions validation set which features p 1−8 as its event proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Qualitative sampling of iPerceive DVC</head><p>For each event proposal p n , we compare the output of <ref type="bibr" target="#b64">[64,</ref><ref type="bibr" target="#b18">18]</ref>, iPerceive DVC and the ground truth. The video (YouTube video id: EGrXaq213Oc) lasts two minutes and contains 12 human annotations. The video is an advertisement for snowboarding lessons for children. It shows examples of children successfully riding a snowboard on a hill and supportive adults that help them to learn. A lady narrates the video and appears in the shot a couple of times.</p><p>From the captions of the event proposals, we can identify numerous instances where the fact that there exists causal relationships between events is helpful for the model. For instance, the fact that the children are being "instructed" because they are "attending snowboarding school" helps the model render a better output especially in event 5. A similar pattern can be observed in event 4 where "children are getting up" because they "fell down". Iashin et al. <ref type="bibr" target="#b18">[18]</ref> offer an in-depth treatment into the importance of additional modalities for dense video captioning, namely, speech and audio and how they enhance the articulation quotient of the output. For each sample, video frames with the corresponding subtitles and question-answers are included. The correct answers are highlighted for each sample. Note that the samples in <ref type="figure" target="#fig_5">Figures 7 and 8</ref> show the attended visual area that the model uses to make its prediction. Similarly, the samples in <ref type="figure" target="#fig_8">Figures 9 and 10</ref> highlight the subtitles that are relevant to the question at hand. Similar to what we saw above with iPerceive DVC, common-sense reasoning helps correct the underlying visual attention while deriving causal relationships both within and between events. This manifests as improved accuracy for the VideoQA task.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Qualitative sampling of iPerceive VideoQA</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Architectural overview of iPerceive DVC. iPerceive DVC generates common-sense vectors from the temporal events that the event proposal module localizes (left). Features from all modalities are sent to the corresponding encoder-decoder Transformers (middle). Upon fusing the processed features we finally output the next word in the caption using the distribution over the vocabulary (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>E a ] and '0' otherwise; f M is a differentiable mask for time t<ref type="bibr" target="#b64">[64]</ref>; (S p , E p ) are the start and end times of the event; (S a , E a ) are the start and end positions of the anchor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Architectural overview of iPerceive VideoQA. Our model consists of two main components: feature fusion and frame selection. For feature fusion, we encode features using a convolutional encoder, generate common-sense vectors from the input video sequence, and extract dense captions using iPerceive DVC (left). Features from all modalities (video, dense captions, QA and subtitles) are then fed to dual-layer attention: word/object and frame-level (middle). Upon fusing the attended features, we calculate frame-relevance scores (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative sampling of iPerceive DVC: Captioning results for a sample video from the ActivityNet Captions validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative sampling of iPerceive VideoQA: Questionanswering for a sample from the TVQA validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figures 7 ,</head><label>7</label><figDesc>8, 9 and 10  show qualitative analyses of VideoQA using the TVQA test set featuring a Castle, Friends, The Big Bang Theory and House episode respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>VideoQA for a sample from the TVQA test set featuring a Castle episode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>VideoQA for a sample from the TVQA test set featuring a Friends episode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>VideoQA for a sample from the TVQA test set featuring an episode from The Big Bang Theory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>VideoQA for a sample from the TVQA test set featuring a House episode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 .</head><label>6</label><figDesc>DVC results for a sample video from the ActivityNet Captions validation set with a multitude of overlapping and individual event proposals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Evaluation of iPerceive DVC on the ActivityNet Captions validation set using BLEU@N (B@N) and METEOR (M).</figDesc><table><row><cell>Method</cell><cell cols="3">GT Proposals B@3 B@4</cell><cell>M</cell><cell cols="2">Learned Proposals B@3 B@4 M</cell></row><row><cell>Seen full dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Krishna et al. [27]</cell><cell>4.09</cell><cell>1.60</cell><cell cols="2">8.88</cell><cell>1.90</cell><cell>0.71 5.69</cell></row><row><cell>Wang et al. [57]</cell><cell>-</cell><cell>-</cell><cell cols="3">10.89 2.55</cell><cell>1.31 5.86</cell></row><row><cell>Zhou et al. [64]</cell><cell>5.76</cell><cell cols="4">2.71 11.16 2.42</cell><cell>1.15 4.98</cell></row><row><cell>Li et al. [32]</cell><cell>4.55</cell><cell cols="4">1.62 10.33 2.27</cell><cell>0.73 6.93</cell></row><row><cell>Seen part of the dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rahman et al. [45]</cell><cell>3.04</cell><cell>1.46</cell><cell cols="2">7.23</cell><cell>1.85</cell><cell>0.90 4.93</cell></row><row><cell>Iashin et al. [18]</cell><cell>4.12</cell><cell cols="4">1.81 10.09 2.31</cell><cell>0.92 6.80</cell></row><row><cell>iPerceive DVC</cell><cell>5.23</cell><cell cols="4">2.34 11.77 2.59</cell><cell>1.07 7.29</cell></row><row><cell>Iashin et al. (all modalities)</cell><cell>5.83</cell><cell cols="4">2.86 11.72 2.60</cell><cell>1.07 7.31</cell></row><row><cell cols="2">iPerceive DVC (all modalities) 6.13</cell><cell cols="4">2.98 12.27 2.93</cell><cell>1.29 7.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation analysis for iPerceive DVC.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Evaluation of iPerceive VideoQA on the TVQA dataset.</figDesc><table><row><cell>Method</cell><cell cols="7">Test-Public (%) Mean BBT Friends HIMYM Grey House Castle</cell><cell>Val (%)</cell></row><row><cell>Lei et al. [29]</cell><cell cols="2">66.46 70.25</cell><cell>65.78</cell><cell>64.02</cell><cell cols="3">67.20 66.84 63.96</cell><cell>65.85</cell></row><row><cell>J. Kim et al. [24]</cell><cell>66.77</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>J. Kim et al. [23]</cell><cell>67.05</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>J. Kim et al. [25]</cell><cell>71.13</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Yang et al. [59]</cell><cell>73.15</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>H. Kim et al. [22]</cell><cell cols="2">74.09 74.04</cell><cell>73.03</cell><cell>74.34</cell><cell cols="3">73.44 74.68 74.86</cell><cell>74.20</cell></row><row><cell cols="3">iPerceive VideoQA 75.15 75.32</cell><cell>74.22</cell><cell>75.14</cell><cell cols="3">74.42 75.22 75.77</cell><cell>76.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Ablation analysis for iPerceive VideoQA.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Chalupka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Eberhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.2309</idno>
		<title level="m">Visual causal feature learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yida</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08854</idno>
		<title level="m">Ruc+ cmu: System report for dense captioning events in videos</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth workshop on statistical machine translation</title>
		<meeting>the ninth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Heterogeneous memory enhanced multimodal attention model for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyou</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wensheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">YouTube Data API Video Captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<ptr target="https://developers.google.com/youtube/v3/docs/captions" />
		<imprint>
			<date type="published" when="2008-05-21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 ieee computer society conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2141" to="2148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Women also snowboard: Overcoming bias in captioning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaylee</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="793" to="811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cnn architectures for large-scale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seybold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 ieee international conference on acoustics, speech and signal processing (icassp)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A case study on combining asr and visual features for generating instructional video captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.02930</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Probabilistic future prediction for video scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergal</forename><surname>Cotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corina</forename><surname>Gurau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06409</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-modal dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Iashin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Densecap: fully convolutional localization networks for dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07571</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Video panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9859" to="9868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Densecaption matching and frame-selection gating for temporal localization in videoqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyounghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zineng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.06409</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gaining extra supervision via multitask learning for multi-modal video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyeong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minuk</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Progressive attention memory network for movie story question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyeong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minuk</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8337" to="8346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modality shifting attention network for multi-modal video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyeong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minuk</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10106" to="10115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murat</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alexandros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Causalgan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02023</idno>
		<title level="m">Learning causal implicit generative models with adversarial training</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="706" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Hierarchical conditional relation networks for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Thao Minh Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10698</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01696</idno>
		<title level="m">Tvqa: Localized, compositional video question answering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Tvqa+: Spatio-temporal grounding for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11574</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Foreground object detection from videos containing complex background</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weimin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Irene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM international conference on Multimedia</title>
		<meeting>the eleventh ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="2" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Jointly localizing and describing events for dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7492" to="7500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discovering causal signals in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6979" to="6987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Explicit bias discovery in visual question answering models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nirat</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9562" to="9571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Causal induction from visual observations for goal directed tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01751</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Spatio-temporal graph for video captioning with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoye</forename><surname>Boxiao Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Hui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13942</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Interpretation and identification of causal mediation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological methods</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">459</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Causal inference in statistics: A primer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madelyn</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas P</forename><surname>Jewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Two causal principles for improving visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10496</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Watch, listen and tell: Multi-modal weakly supervised dense event captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanzila</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bicheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8908" to="8917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dense procedure captioning in narrated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaobo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6382" to="6391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Towards vqa models that can read</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meet</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8317" to="8326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Unbiased scene graph generation from biased training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11949</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Audio-visual event localization in unconstrained videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bochen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="247" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning common sense through visual abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2542" to="2550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4534" to="4542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Bidirectional attentive fusion with context gating for dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7190" to="7198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Visual commonsense r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Bert representations for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noa</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhui</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayu</forename><surname>Otani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haruo</forename><surname>Takemura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1556" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4507" to="4515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Stating the obvious: Extracting visual common sense knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="193" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Leveraging video descriptions to learn video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuo-Hao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tseng-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Hong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Maei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08936</idno>
		<title level="m">Deep reinforcement learning for visual object tracking in videos</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with masked transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8739" to="8748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">More grounded image captioning by distilling image-text matching model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00390</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Reasoning about object affordances in a knowledge base representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="408" to="424" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
