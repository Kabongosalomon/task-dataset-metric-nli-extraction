<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Stored-program Memory</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-12-26">26 Dec 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Applied AI Institute</orgName>
								<orgName type="institution">Deakin University</orgName>
								<address>
									<settlement>Geelong</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Tran</surname></persName>
							<email>truyen.tran@deakin.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Applied AI Institute</orgName>
								<orgName type="institution">Deakin University</orgName>
								<address>
									<settlement>Geelong</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
							<email>svetha.venkatesh@deakin.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Applied AI Institute</orgName>
								<orgName type="institution">Deakin University</orgName>
								<address>
									<settlement>Geelong</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Stored-program Memory</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-12-26">26 Dec 2019</date>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural networks powered with external memory simulate computer behaviors. These models, which use the memory to store data for a neural controller, can learn algorithms and other complex tasks. In this paper, we introduce a new memory to store weights for the controller, analogous to the stored-program memory in modern computer architectures. The proposed model, dubbed Neural Stored-program Memory, augments current memory-augmented neural networks, creating differentiable machines that can switch programs through time, adapt to variable contexts and thus resemble the Universal Turing Machine. A wide range of experiments demonstrate that the resulting machines not only excel in classical algorithmic problems, but also have potential for compositional, continual, few-shot learning and question-answering tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recurrent Neural Networks (RNNs) are <ref type="bibr">Turing-complete (Siegelmann &amp; Sontag, 1995)</ref>. However, in practice RNNs struggle to learn simple procedures as they lack explicit memory <ref type="bibr" target="#b11">(Graves et al., 2014;</ref><ref type="bibr" target="#b21">Mozer &amp; Das, 1993)</ref>. These findings have sparked a new research direction called Memory Augmented Neural Networks (MANNs) that emulate modern computer behavior by detaching memorization from computation via memory and controller network, respectively. MANNs have demonstrated significant improvements over memory-less RNNs in various sequential learning tasks <ref type="bibr" target="#b17">Le et al., 2018a;</ref><ref type="bibr" target="#b28">Sukhbaatar et al., 2015)</ref>. Nonetheless, MANNs have barely simulated general-purpose computers.</p><p>Current MANNs miss a key concept in computer design: stored-program memory. The concept has emerged from the idea of Universal Turing Machine (UTM) <ref type="bibr" target="#b32">(Turing, 1936)</ref> and further developed in Harvard Architecture <ref type="bibr" target="#b5">(Broesch, 2009)</ref>, Von Neumann Architecture <ref type="bibr" target="#b34">(von Neumann, 1993)</ref>. In UTM, both data and programs that manipulate the data are stored in memory. A control unit then reads the programs from the memory and executes them with the data. This mechanism allows flexibility to perform universal computations. Unfortunately, current MANNs such as Neural Turing Machine (NTM) <ref type="bibr" target="#b11">(Graves et al., 2014)</ref>, Differentiable Neural Computer (DNC)  and Least Recently Used Access (LRUA) <ref type="bibr" target="#b23">(Santoro et al., 2016)</ref> only support memory for data and embed a single program into the controller network, which goes against the stored-program memory principle.</p><p>Our goal is to advance a step further towards UTM by coupling a MANN with an external program memory. The program memory co-exists with the data memory in the MANN, providing more flexibility, reuseability and modularity in learning complicated tasks. The program memory stores the weights of the MANN's controller network, which are retrieved quickly via a key-value attention mechanism across timesteps yet updated slowly via backpropagation. By introducing a meta network to moderate the operations of the program memory, our model, henceforth referred to as Neural Stored-program Memory (NSM), can learn to switch the programs/weights in the controller network appropriately, adapting to different functionalities aligning with different parts of a sequential task, or different tasks in continual and few-shot learning.</p><p>To validate our proposal, the NTM armed with NSM, namely Neural Universal Turing Machine (NUTM), is tested on a variety of synthetic tasks including algorithmic tasks from <ref type="bibr" target="#b11">Graves et al. (2014)</ref>, composition of algorithmic tasks and continual procedure learning. For these algorithmic problems, we demonstrate clear improvements of NUTM over NTM. Further, we investigate NUTM in few-shot learning by using LRUA as the MANN and achieve notably better results. Finally, we expand NUTM application to linguistic problems by equipping NUTM with DNC core and achieve competitive performances against stateof-the-arts in the bAbI task .</p><p>Taken together, our study advances neural network simulation of Turing Machines to neural architecture for Universal Turing Machines. This develops a new class of MANNs that can store and query both the weights and data of their own controllers, thereby following the stored-program principle. A set of five diverse experiments demonstrate the computational universality of the approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section, we briefly review MANN and its relations to Turing Machines. A MANN consists of a controller network and an external memory M ∈ R N ×M , which is a collection of N M -dimensional vectors. The controller network is responsible for accessing the memory, updating its state and optionally producing output at each timestep. The first two functions are executed by an interface network and a state network 1 , respectively. Usually, the interface network is a Feedforward neural network whose input is c t -the output of the state network implemented as RNNs. Let W c denote the weight of the interface network, then the state update and memory control are as follows,</p><formula xml:id="formula_0">h t , c t = RN N ([x t , r t−1 ] , h t−1 ) (1) ξ t = c t W c<label>(2)</label></formula><p>where x t and r t−1 are data from current input and the previous memory read, respectively. The interface vector ξ t then is used to read from and write to the memory M. We use a generic notation memory (ξ t , M) to represent these memory operations that either update or retrieve read value r t from the memory. To support multiple memory accesses per step, the interface network may produce multiple interfaces, also known as control heads. Readers are referred to App. F and <ref type="bibr" target="#b11">Graves et al. (2014;</ref>; <ref type="bibr" target="#b23">Santoro et al. (2016)</ref> for details of memory read/write examples. A deterministic one-tape Turing Machine can be defined by 4-tuple (Q, Γ, δ, q 0 ), in which Q is finite set of states, q 0 ∈ Q is an initial state, Γ is finite set of symbol stored in the tape (the data) and δ is the transition function (the program), δ : Q × Γ → Γ × {−1, 1} × Q. At each step, the machine performs the transition function, which takes the current state and the read value from the tape as inputs and outputs actions including writing new values, moving tape head to new location (left/right) and jumping to another state. Roughly mapping to current MANNs, Q, Γ and δ map to the set of the controller states, the read values and the controller network, respectively. Further, the function δ can be factorized into two sub functions: Q × Γ → Γ × {−1, 1} and Q × Γ → Q, which correspond to the interface and state networks, respectively.</p><p>By encoding a Turing Machine into the tape, one can build a UTM that simulates the encoded machine <ref type="bibr" target="#b32">(Turing, 1936)</ref>. The transition function δ u of the UTM queries the encoded Turing Machine that solves the considering task. Amongst 4 tuples, δ is the most important and hence uses most of the encoding bits. In other words, if we assume that the space of Q, Γ and q 0 are shared amongst Turing Machines, we can simulate any Turing Machine by encoding only its transition function δ. Translating to neural language, if we can store the controller network into a queriable memory and make use of it, we can build a Neural Universal Turing Machine. Using NSM is a simple way to achieve this goal, which we introduce in the subsequent section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neural Stored-program Memory</head><p>A Neural Stored-program Memory (NSM) is a key-value memory M p ∈ R P ×(K+S) , whose values are the basis weights of another neural network−the programs. P , K, and S are the number of programs, the key space dimension and the program size, respectively. This concept is a hybrid between the traditional slow-weight and fast-weight <ref type="bibr" target="#b14">(Hinton &amp; Plaut, 1987)</ref>. Like slow-weight, the keys and values in NSM are updated gradually by backpropagation. However, the values are dynamically interpolated to produce the working weight on-the-fly during the processing of a sequence, which resembles fast-weight computation. Let us denote M p (i) .k and M p (i) .v as the key and the program of the i-th memory slot. At timestep t, given a query key k p t , the working program is retrieved as follows,</p><formula xml:id="formula_1">D (k p t , M p (i).k) = k p t · M p (i).k ||k p t || · ||M p (i).k)|| (3) w p t (i) = softmax (β p t D (k p t , M p (i).k)) (4) p t = P i=1 w p t (i) M p (i) .v<label>(5)</label></formula><p>where D (·) is cosine similarity and β p t is the scalar program strength parameter. The vector working program p t is then reshaped to its matrix form and ready to be used as the weight of other neural networks.</p><p>The key-value design is essential for convenient memory access as the size of the program stored in M p can be millions of dimensions and thus, direct content-based addressing as in <ref type="bibr" target="#b11">Graves et al. (2014;</ref>; <ref type="bibr" target="#b23">Santoro et al. (2016)</ref> is infeasible. More importantly, we can inject external control on the behavior of the memory by imposing constraints on the key space. For example, program collapse will happen when the keys stored in the memory stay close to each other. When this happens, p t is a balanced mixture of all programs regardless of the query key and thus having multiple programs is useless. We can avoid this phenomenon by minimizing a regularization loss defined as the following,</p><formula xml:id="formula_2">l p = P i=1 P j=i+1 D (M p (i).k, M p (j).k)<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Neural Universal Turing Machine</head><p>It turns out that the combination of MANN and NSM approximates a Universal Turing Machine (Sec. 2). At each timestep, the controller in MANN reads its state and memory to generate control signal to the memory via the interface network W c , then updates its state using the state network RN N . Since the parameters of RN N and W c represent the encoding of δ, we should store both into NSM to completely encode an MANN. For simplicity, in this paper, we only use NSM to store W c , which is equivalent to the Universal Turing Machine that can simulate any one-state Turing Machine.</p><p>In traditional MANN, W c is constant across timesteps and only updated slowly during training, typically through backpropagation. In our design, we compute W c t from NSM for every timestep and thus, we need a program interface network−the meta network P I −that generates an interface vector for the program memory:</p><formula xml:id="formula_3">ξ p t = P I (c t ), where ξ p t = [k p t , β p t ]</formula><p>. Together with the RN N , P I simulates δ u of the UTM and is implemented as a Feedforward neural network. The procedure for computing W c t is executed by following Eqs.</p><p>(3)-(5), hereafter referred to as N SM (ξ p t , M p ). <ref type="figure" target="#fig_0">Figure 1</ref> depicts the integration of NSM into MANN. In this implementation, key-value NSM offers a more flexible learning scheme than direct attention, in which the meta-network can generate the weight w p t directly without matching </p><formula xml:id="formula_4">k p t with M p (i) .k.</formula><p>That is, only the meta-network learns the mapping from context c t to program. When it falls into some local-minima (generating suboptimal w p t ), the metanetwork struggles to escape. In our proposal, together with the meta-network, the memory keys are learnable. When the memory keys are slowly updated, the meta-network will shift its query key generation to match the new memory keys and possibly escape from the local-minima.</p><p>For the case of multi-head NTM, we implement one NSM per control head and name this model Neural Universal Turing Machine (NUTM). One NSM per head is to ensure programs for one head do not interfere with other heads and thus, encourage functionality separation amongst heads. Each control head will read from (for read head) or write to (for write head) the data memory M via memory (ξ t , M) as described in <ref type="bibr" target="#b11">Graves et al. (2014)</ref> . It should be noted that using multiple heads is unlike using multiple controllers per head. The former increases the number of accesses to the data memory at each timestep and employs a fixed controller to compute multiple heads, which may improve capacity yet does not enable adaptability. On the contrary, the latter varies the property of each memory access across timesteps by switching the controllers and thus potential for adaptation.</p><p>Other MANNs such as DNC  and LRUA <ref type="bibr" target="#b23">(Santoro et al., 2016)</ref> can be armed with NSM in this manner. We also employ the regularization loss l p to prevent the programs from collapsing, resulting in a final loss as follows,</p><formula xml:id="formula_5">Loss = Loss pred + η t l p (7)</formula><p>where Loss pred is the prediction loss and η t is annealing factor, reducing as the training step increases. The details of NUTM operations are presented in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">On the Benefit of NSM to MANN: An Explanation from Multilevel Modeling</head><p>Learning to access memory is a multi-dimensional regression problem. Given the input c t , which is derived from the state h t of the controller, the aim is to generate a correct interface vector ξ t via optimizing the interface network. Instead of searching for one transformation that maps the whole space of c t to the optimal space of ξ t , NSM first partitions the space of c t into subspaces, then finds multiple transformations, each of which covers subspace of </p><formula xml:id="formula_6">corresponding to R control heads 1: Initilize h 0 , r 0 2: for t = 1, T do 3: h t , c t = RN N ([x t , r t−1 ], h t−1 )</formula><p>⊲ RN N can be replaced by GRU/LSTM 4:</p><p>for n = 1, R do 5:</p><p>Compute the program interface ξ p t,n ← P I,n (c t )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Compute the program W c t,n ← N SM ξ p t,n , M p,n 7:</p><p>Compute the data interface ξ t,n ← c t W c t,n 8:</p><p>Read r t,n from memory M (if read head) or update memory M (if write head) using memory n (ξ t,n , M) 9: end for 10: c t . The program interface network P I is a meta learner that routes c t to the appropriate transformation, which then maps c t to the ξ t space. This is analogous to multilevel regression in statistics <ref type="bibr" target="#b2">(Andrew Gelman, 2006)</ref>. Practical studies have shown that multilevel regression is better than ordinary regression if the input is clustered <ref type="bibr" target="#b7">(Cohen et al., 2014;</ref><ref type="bibr" target="#b15">Huang, 2018)</ref>.</p><formula xml:id="formula_7">r t ← [r t,1 , ..., r t,R ] 11: end for</formula><p>RNNs have the capacity to learn to perform finite state computations <ref type="bibr" target="#b6">(Casey, 1996;</ref><ref type="bibr" target="#b31">Tiňo et al., 1998)</ref>. The states of a RNN must be grouped into partitions representing the states of the generating automaton. As Turing Machines are finite state automata augmented with an external memory tape, we expect MANN, if learnt well, will organize its state space clustered in a way to reflect the states of the emulated Turing Machine. That is, h t as well as c t should be clustered. We realize that NSM helps NTM learn better clusterization over this space (see App. A), thereby improving NTM's performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">NTM Single Tasks</head><p>In this section, we investigate the performance of NUTM on algorithmic tasks introduced in Graves et al. <ref type="formula" target="#formula_0">(2014)</ref>   <ref type="table">Table 1</ref>: Generalization performance of best models measured in average bit error per sequence (lower is better). For each task, we pick 1,000 longer sequences as test data.</p><p>doubles the length of training sequences in the Copy task. In these tasks, the model will be fed a sequence of input items and is required to infer a sequence of output items. Each item is represented by a binary vector.</p><p>In the experiment, we compare two models: NTM 2 and NUTM with two programs. Although the tasks are atomic, we argue that there should be at least two memory manipulation schemes across timesteps, one for encoding the inputs to the memory and another for decoding the output from the memory. The two models are trained with cross-entropy objective function under the same setting as in <ref type="bibr" target="#b11">Graves et al. (2014)</ref> . For fair comparison, the controller hidden dimension of NUTM is set smaller to make the total number of parameters of NUTM equivalent to that of NTM. The number of memory heads for both models are always equal and set to the same value as in the original paper (details in App. C).</p><p>We run each experiments five times and report the mean with error bars of training losses for NTM tasks in <ref type="figure" target="#fig_1">Fig. 2 (a)</ref>. Except for the Copy task, which is too simple, other tasks observe convergence speed improvement of NUTM over that of NTM, thereby validating the benefit of using two programs across timesteps even for the single task setting. NUTM requires fewer training samples to converge and it generalizes better to unseen sequences that are longer than training sequences. <ref type="table">Table 1</ref> reports the test results of the best models chosen after five runs and confirms the outperformance of NUTM over NTM for generalization.</p><p>To illustrate the program usage, we plot NUTM's program distributions across timesteps for Repeat Copy and Priority Sort in <ref type="figure" target="#fig_2">Fig. 3 (a)</ref> and (b), respectively. Examining the read head for Repeat Copy, we observe two program usage patterns corresponding to the encoding and decoding phases. As there is no reading in encoding, NUTM assigns the "no-read" strategy mainly to the "orange program". In decoding, the sequential reading is mostly done by the "blue program" with some contributions from the "orange program" when resetting reading head. Similar behaviors can be found in the write head for Priority Sort. While the encoding "fitting writing" (see <ref type="bibr" target="#b11">Graves et al. (2014)</ref> for explanation on the strategy) is often executed by the "blue program", the decoding writing is completely taken by the "orange" program (more visualizations in App. B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation study on Associative Recall</head><p>In this section, we conduct an ablation study on Associative Recall (AR) to validate the benefit of proposed components that constitute NSM. We run the task with three additional baselines: NUTM using direct attention (DA), NUTM using key-value without regularization (KV), NUTM using fixed, uniform program distribution (UP) and a vanilla NTM with 2 memory heads (h = 2). The meta-network P I in DA generates the attention weight w p t directly. The KV employs key-value attention yet excludes the regularization loss presented in Eq. (6). The training curves over 5 runs are plotted in <ref type="figure" target="#fig_1">Fig. 2 (b)</ref>. The results demonstrate that DA exhibits fast yet shallow convergence. It tends to fall into local minima, which finally fails to reach zero loss. Key-value attention helps NUTM converge completely with fewer iterations. The performance is further improved with the proposed regularization loss. UP underperforms NUTM as it lacks dynamic programs. The NTM with 2 heads shows slightly better convergence compared to the NTM, yet obviously underperforms NUTM (p = 2) with 1 head and fewer parameters. This validates our argument on the difference between using multiple heads and multiple programs (Sec. 3.2). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">NTM Sequencing Tasks</head><p>In neuroscience, sequencing tasks test the ability to remember a series of tasks and switch tasks alternatively <ref type="bibr" target="#b4">(Blumenfeld, 2010)</ref>. A dysfunctional brain may have difficulty in changing from one task to the next and get stuck in its preferred task (perseveration phenomenon). To analyze this problem in NTM, we propose a new set of experiments in which a task is generated by sequencing a list of subtasks. The set of subtasks is chosen from the NTM single tasks (excluding Dynamic N-grams for format discrepancy) and the order of subtasks in the sequence is dictated by an indicator vector put at the beginning of the sequence. Amongst possible combinations of subtasks, we choose {Copy, Repeat Copy}(C+RC), {Copy, Associative Recall} (C+AR), {Copy, Priority Sort} (C+PS) and all (C+RC+AC+PS) 3 . The learner observes the order indicator followed by a sequence of subtasks' input items and is requested to consecutively produce the output items of each subtasks.</p><p>As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, some tasks such as Copy and Associative Recall, which are easy to solve if trained separately, become unsolvable by NTM when sequenced together. One reason is NTM fails to change the memory access behavior (perseveration). For examples, NTM keeps following repeat copy reading strategy for all timesteps in C+RC task ( <ref type="figure" target="#fig_2">Fig. 3 (d)</ref>). Meanwhile, NUTM can learn to change program distribution when a new subtask appears in the sequence and thus ensure different accessing strategy per subtask <ref type="figure" target="#fig_2">(Fig. 3 (c)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Continual Procedure Learning</head><p>In continual learning, catastrophic forgetting happens when a neural network quickly forgets previously acquired skills upon learning new skills <ref type="bibr" target="#b10">(French, 1999)</ref>. In this section, we prove the versatility of NSM by showing that a naive application of NSM without much modification can help NTM to mitigate catastrophic forgetting. We design an experiment similar to the Split MNIST <ref type="bibr" target="#b36">(Zenke et al., 2017)</ref> to investigate whether NSM can improve NTM's performance. In our experiment, we let the models see the training data from the  while freezing others, we force "hard" attention over the programs by replacing the softmax function in Eq. 5 with the Gumbel-softmax <ref type="bibr" target="#b16">(Jang et al., 2016)</ref>. Also, to ignore catastrophic forgetting in the state network, we use Feedforward controllers in the two baselines.</p><p>After finishing one task, we evaluate the bit accuracy −measured by 1−(bit error per sequence/total bits per sequence) over 4 tasks. As shown in in <ref type="figure" target="#fig_4">Fig. 5</ref>, NUTM outperforms NTM by a moderate margin (10-40% per task). Although NUTM also experiences catastrophic forgetting, it somehow preserves some memories of previous tasks. Especially, NUTM keeps performing perfectly on Copy even after it learns Repeat Copy. For other dissimilar task transitions, the performance drops significantly, which requires more effort to bring NSM to continual learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Few-shot Learning</head><p>Few-shot learning or meta learning tests the ability to rapidly adapt within a task while gradually capturing the way the task structure varies <ref type="bibr" target="#b30">(Thrun, 1998)</ref>. By storing sampleclass bindings, MANNs are capable of classifying new data after seeing only few samples <ref type="bibr" target="#b23">(Santoro et al., 2016)</ref>. As NSM gives flexible memory controls, it makes MANN more adaptive to changes and thus perform better in this setting. To verify that, we apply NSM to the LRUA memory and follow the experiments introduced in <ref type="bibr" target="#b23">Santoro et al. (2016)</ref> , using the Omniglot dataset to measure few-shot classification accuracy. The dataset includes images of 1623 characters, with 20 examples of each character. During training, a sequence (episode) of images are randomly selected from C classes of characters in the training set (1200 characters), where C = 5, 10 corresponding to sequence length of 50, 75, respectively. Each class is assigned a random label which shuffles between episodes and is revealed to the models after each prediction. After 100,000 episodes of training, the models are tested with unseen images from the testing set (423 characters). The two baselines are MANN and NUTM (both use LRUA core). For NUTM, we only tune p and pick the best values: p = 2 and p = 3 for 5 classes and 10 classes, respectively.   <ref type="bibr" target="#b23">(Santoro et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Error DNC  16.7 ± 7.6 SDNC <ref type="bibr" target="#b22">(Rae et al., 2016)</ref> 6.4 ± 2.5 ADNC <ref type="bibr" target="#b9">(Franke et al., 2018)</ref> 6.3 ± 2.7 DNC-MD <ref type="bibr" target="#b8">(Csordas &amp; Schmidhuber, 2019)</ref> 9.5 ± 1.6 NUTM (DNC core, p=1) 9.7 ± 3.5 NUTM (DNC core, p=2) 7.5 ± 1.6 NUTM (DNC core, p=4) 5.6 ± 1.9 persistent memory mode, which demands fast forgetting old experiences in previous episodes, NUTM outperforms MANN significantly (10-20%) 4 . Readers are referred to App. D for more details on learning curves and more results of the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Text Question Answering</head><p>Reading comprehension typically involves an iterative process of multiple actions such as reading the story, reading the question, outputting the answers and other implicit reasoning steps . We apply NUTM to the question answering domain by replacing the NTM core with DNC . Compared to NTM's sequential addressing, dynamic memory addressing in DNC is more powerful and thus suitable for NSM integration to solve non-algorithmic problems such as question answering. Following previous works of DNC, we use bAbI dataset  to measure the performance of the NUTM with DNC core (three variants p = 1, p = 2 and p = 4). In the dataset, each story is followed by a series of questions and the network reads all word by word, then predicts the answers. Although synthetically generated, bAbI is a good benchmark that tests 20 aspects of natural language reasoning including complex skills such as induction and counting,</p><p>We found that increasing number of programs helps NUTM improve performance. In particular, NUTM with 4 programs, after 50 epochs jointly trained on all 20 question types, can achieve a mean test error rate of 3.3% and manages to solve 19/20 tasks (a task is considered solved if its error &lt;5%). The mean and s.d. across 10 runs are also compared with other results reported by recent works (see <ref type="table" target="#tab_4">Table 3</ref>). Excluding baselines under different setups, our result is the best reported mean result on bAbI that we are aware of. More details are described in App. E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Previous investigations into MANNs mostly revolve around memory access mechanisms. The works in <ref type="bibr" target="#b11">Graves et al. (2014;</ref> introduce content-based, location-based and dynamic memory reading/writing. Further, <ref type="bibr" target="#b22">Rae et al. (2016)</ref> scales to bigger memory by sparse access; <ref type="bibr" target="#b19">Le et al. (2019)</ref> optimizes memory operations with uniform writing; and MANNs with extra memory have been proposed <ref type="bibr" target="#b18">(Le et al., 2018b)</ref>. However, these works keep using memory for storing data rather than the weights of the network and thus parallel to our approach. Other DNC modifications <ref type="bibr" target="#b8">(Csordas &amp; Schmidhuber, 2019;</ref><ref type="bibr" target="#b9">Franke et al., 2018)</ref> are also orthogonal to our work.</p><p>Another line of related work involves modularization of neural networks, which is designed for visual question answering. In module networks <ref type="bibr" target="#b1">(Andreas et al., 2016b;</ref><ref type="bibr">a)</ref>, the modules are manually aligned with predefined concepts and the order of execution is decided by the question. Although the module in these works resembles the program in NSM, our model is more generic and flexible with soft-attention over programs and thus fully differentiable. Further, the motivation of NSM does not limit to a specific application. Rather, NSM aims to help MANN reach general-purpose computability.</p><p>If we view NSM network as a dynamic weight generator, the program in NSM can be linked to fast weight <ref type="bibr" target="#b33">(von der Malsburg, 1981;</ref><ref type="bibr" target="#b14">Hinton &amp; Plaut, 1987;</ref><ref type="bibr" target="#b26">Schmidhuber, 1993b)</ref>. These papers share the idea of using different weights across timesteps to enable dynamic adaptation. Using outer-product is a common way to implement fast-weight <ref type="bibr" target="#b25">(Schmidhuber, 1993a;</ref><ref type="bibr" target="#b24">Schlag &amp; Schmidhuber, 2017)</ref>. These fast weights are directly generated and thus different from our programs, which are interpolated from a set of slow weights.</p><p>Tensor/Multiplicative RNN <ref type="bibr" target="#b29">(Sutskever et al., 2011)</ref> and Hypernetwork <ref type="bibr" target="#b13">(Ha et al., 2016)</ref> are also relevant related works. These methods attempt to make the working weight of RNNs dependent on the input to enable quick adaption through time. Nevertheless, they do not support modularity. In particular, Hypernetwork generates scaling factors for the single weight of the main RNN. It does not aim to use multiple slow-weights (programs) and thus, different from our approach. Tensor RNN is closer to our idea when the authors propose to store M slow-weights, where M is the number of input dimension, which is acknowledged impractical. Unlike our approach, they do not use a meta-network to generate convex combinations amongst weights. Instead, they propose Multiplicative RNN that factorizes the working weight to product of three matrices, which looses modularity. On the contrary, we explicitly model the working weight as an interpolation of multiple programs and use a meta-network to generate the coefficients. This design facilitates modularity because each program is trained towards some functionality and can be switched or combined with each other to perform the current task. Last but not least, while the related works focus on improving RNN with fast-weight, we aim to reach a neural simulation of Universal Turing Machine, in which fast-weight is a way to implement stored-program principle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper introduces the Neural Stored-program Memory (NSM), a new type of external memory for neural networks. The memory, which takes inspirations from the stored-program memory in computer architecture, gives memory-augmented neural networks (MANNs) flexibility to change their control programs through time while maintaining differentiability. The mechanism simulates modern computer behavior, potential making MANNs truly neural computers. Our experiments demonstrated that when coupled with our model, the Neural Turing Machine learns algorithms better and adapts faster to new tasks at both sequence and sample levels. When used in few-shot learning, our method helps MANN as well. We also applied the NSM to the Differentiable Neural Computer and observed a significant improvement, reaching the state-of-the-arts in the bAbI task. Although this paper limits to MANN integration, other neural networks can also reap benefits from our proposed model, which will be explored in future works.                 Testing accuracy through time is listed below,    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Details on bAbI Task</head><p>We train the models using RMSprop optimizer with fixed learning rate of 10 −4 and momentum of 0.9. The batch size is 32 and we adopt layer normalization (Lei  to DNC's layers. Following <ref type="bibr" target="#b9">Franke et al. (2018)</ref> practice, we also remove temporal linkage for faster training. The details of hyper-parameters are listed in <ref type="table" target="#tab_3">Table 12</ref>. Full NUTM (p = 4) results are reported in <ref type="table" target="#tab_4">Table 13</ref>.   3.3 5.6 ± 1.9 Failed (Err. &gt;5%) 1 3 ± 1.2 <ref type="table" target="#tab_4">Table 13</ref>: NUTM (p = 4) bAbI best and mean errors (%). 9 When p = 1, the model converges to layer-normed DNC <ref type="figure" target="#fig_1">Figure 23</ref>: Learning curves on Associative Recall (AR) ablation study.</p><p>For all tasks, η t is fixed to 0.1, reducing with decay rate of 0.9.</p><p>Ablation study's learning losses with mean and error bar are plotted in <ref type="figure" target="#fig_1">Fig. 23.</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Introducing NSM into MANN. At each timestep, the program interface network (P I ) receives input from the state network and queries the program memory M p , acquiring the working weight for the interface network (W c t ). The interface network then operates on the data memory M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Learning curves on NTM tasks (a) and Associative Recall (AR) ablation study (b). Only mean is plotted in (b) for better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a,b,c) visualizes NUTM's executions in synthetic tasks: the upper rows are memory read (left)/write (right) locations; the lower rows are program distributions over timesteps. The green line indicates the start of the decoding phase. (d) visualizes perseveration in NTM: the upper row are input, output, predicted output with errors (orange bits); the lower row is reading location.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Learning curves on sequencing NTM tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Mean bit accuracy with error bars for the continual algorithmic tasks. Each of the first four panels show bit accuracy on four tasks after finishing a task. The rightmost shows the average accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>B. 1 Figure 7 :</head><label>17</label><figDesc>Visualization on program distribution across timesteps (single tasks) Copy (p=2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Repeat Copy (p=2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Associative Recall (p=2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Dynamic N-grams (p=2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Priority Sort (p=2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :Figure 13 :</head><label>1213</label><figDesc>Long Copy (p=2). B.2 Visualization on program distribution across timesteps (sequencing tasks) Copy+Repeat Copy (p=3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 :</head><label>14</label><figDesc>Copy+Associative Recall (p=3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 :</head><label>15</label><figDesc>Copy+Priority Sort (p=3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 16 :</head><label>16</label><figDesc>Copy+Repeat Copy+Associative Recall+Priority Sort (p=4). B.3 Perseveration phenomenon in NTM (sequencing tasks) Figure 17: Copy+Repeat Copy perseveration (only Repeat Copy).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 18 :</head><label>18</label><figDesc>Copy+Associative Recall perseveration (only Copy).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 19 :</head><label>19</label><figDesc>Copy+Priority Sort perseveration (only Copy).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 20 :</head><label>20</label><figDesc>Copy+Repeat Copy+Associative Recall+Priority Sort perseveration (only Repeat Copy).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 21 :</head><label>21</label><figDesc>Testing accuracy during training (five random classes/episode, one-hot vector labels, of length 50).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 22 :</head><label>22</label><figDesc>Testing accuracy during training (ten random classes/episode, one-hot vector labels, of length 75).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Neural Universal Turing MachineRequire: a sequence x = {x t } T t=1 , a data memory M and R program memories {M p,n }</figDesc><table><row><cell>R</cell></row><row><cell>n=1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>: Copy, Repeat Copy, Associative Recall, Dynamic N-Grams and Priority Sort. Besides these five NTM tasks, we add another task named Long Copy whichTaskCopy R. Copy A. Recall D. N-grams P. Sort L.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Copy</cell></row><row><cell>NTM</cell><cell>0.00</cell><cell>405.10</cell><cell>7.66</cell><cell>132.59</cell><cell>24.41</cell><cell>16.04</cell></row><row><cell cols="2">NUTM (p=2) 0.00</cell><cell>366.69</cell><cell>1.35</cell><cell>127.68</cell><cell>20.00</cell><cell>0.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 11</head><label>11</label><figDesc>reports the classification accuracy when the models see characters for the second, third and fifth time. NUTM generally achieves better results than MANN, especially when the number of classes increases, demanding more adaptation within an episode. For the</figDesc><table><row><cell>Model</cell><cell>Persistent memory 5</cell><cell>2 nd</cell><cell>5 classes 3 rd</cell><cell>5 th</cell><cell>2 nd</cell><cell>10 classes 3 rd</cell><cell>5 th</cell></row><row><cell>MANN (LRUA)*</cell><cell>No</cell><cell cols="3">82.8 91.0 94.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MANN (LRUA)</cell><cell>No</cell><cell cols="6">82.3 88.7 92.3 52.7 60.6 64.7</cell></row><row><cell>NUTM (LRUA)</cell><cell>No</cell><cell cols="6">85.7 91.3 95.5 68.0 78.1 82.8</cell></row><row><cell>MANN (LRUA)</cell><cell>Yes</cell><cell cols="6">66.2 73.4 81.0 51.3 59.2 63.3</cell></row><row><cell>NUTM (LRUA)</cell><cell>Yes</cell><cell cols="6">77.8 85.8 89.8 69.0 77.9 82.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Test-set classification accuracy (%) on the Omniglot dataset after 100,000 episodes of training. * denotes available results from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Mean and s.d. for bAbI error (%).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Task settings (sequencing tasks).</figDesc><table><row><cell cols="5">C.3 Continual procedure learning tasks</cell><cell></cell><cell></cell></row><row><cell cols="4">#Read/Write Head Controller Size</cell><cell cols="2">Memory Size</cell><cell cols="2">#Parameters</cell></row><row><cell>NTM</cell><cell>NUTM</cell><cell cols="4">NTM NUTM NTM NUTM</cell><cell>NTM</cell><cell>NUTM</cell></row><row><cell>1</cell><cell>1</cell><cell>200</cell><cell>150</cell><cell>128</cell><cell>128</cell><cell cols="2">206,444 196,590</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Model hyper-parameters (continual procedure learning tasks). NUTM uses 6 programs per head.</figDesc><table><row><cell>Tasks</cell><cell>Training</cell><cell>Testing</cell></row><row><cell>Copy</cell><cell cols="2">Sequence length range: [1, 10] Sequence length range: [1, 10]</cell></row><row><cell>Repeat Copy</cell><cell>Sequence length range: [1, 5] #Repeat range: [1, 5]</cell><cell>Sequence length range: [1, 5] #Repeat range: [1, 5]</cell></row><row><cell></cell><cell>Sequence length: 3</cell><cell>Sequence length: 3</cell></row><row><cell>Associative Recall</cell><cell>#Item range: [2, 3]</cell><cell>#Item range: [2, 3]</cell></row><row><cell></cell><cell>Item length: 3</cell><cell>Item length: 3</cell></row><row><cell>Priority Sort</cell><cell>#Item: 10 #Sorted Item: 8</cell><cell>#Item: 10 #Sorted Item: 8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Task settings (continual procedure learning tasks).</figDesc><table><row><cell cols="4">D Details on Few-shot Learning Task</cell><cell></cell><cell></cell></row><row><cell cols="7">We use similar hyper-parameters as in Santoro et al. (2016) , which are reported in Tab.</cell></row><row><cell>10.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="5">p #Read Head #Write Head Controller Size N</cell><cell>M M p .K Size</cell></row><row><cell cols="2">MANN (LRUA) 1</cell><cell>4</cell><cell>1</cell><cell>200</cell><cell cols="2">128 40</cell><cell>0</cell></row><row><cell cols="2">NUTM (LRUA) 2</cell><cell>4</cell><cell>1</cell><cell>180</cell><cell cols="2">128 40</cell><cell>2</cell></row><row><cell cols="2">NUTM (LRUA) 3</cell><cell>4</cell><cell>1</cell><cell>150</cell><cell cols="2">128 40</cell><cell>3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Hyper-parameters for few-shot learning. All models use RMSprop optimizer with learning rate 10 −4 .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Test-set classification accuracy (%) on the Omniglot dataset after 100,000 episodes of training. * denotes available results from Santoro et al. (2016) (some are estimated from plotted figures).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>NUTM hyper-parameters for bAbI.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Some MANNs (e.g., NTM with Feedforward Controller) neglect the state network, only implementing the interface network and thus analogous to one-state Turing Machine.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For algorithmic tasks, we choose NTM as the only baseline as NTM is known to perform and generalize well on these tasks. If NSM can help NTM in these tasks, it will probably help other MANNs as well.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">tasks: Copy (C), Repeat Copy (RC), Associative Recall (AR) and Priority Sort (PS), consecutively in this order. Each task is trained in 20,000 iterations with batch size 16 (see App. C for task details). To encourage NUTM to spend exactly one program per task3  We focus on the combinations that contain Copy as Copy is the only task where NTM reach NUTM's performance. If NTM fails in these combinations, it will most likely fail in others.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">It should be noted that our goal was not to achieve state of the art performance on this dataset. It was to exhibit the benefit of NSM to MANN. Compared to current methods, the MANN and NUTM used in our experiments do not use CNN to extract visual features, thus achieve lower accuracy than recent state-of-the-arts.5 If the memory is not artificially erased between episodes, it is called persistent. This mode is hard for the case of 5 classes as shown in<ref type="bibr" target="#b23">(Santoro et al., 2016)</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Normally, black is bit 0, white is bit 1 in vector data. Orange is prediction error. In tasks including priority sort, because data vectors not only include value 0-1, but also other float values (e.g., priority score), the color scale is automatically changed. Basically, error bit is given darker color than 0 and lighter color than 1. For example, in priority sort task, yellow is prediction error, and orange is bit 1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">In NTM, the number of read and write heads are equal.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">If the memory is not artificially erased between episodes, it is called persistent. This mode is hard for the case of 5 classes as shown in<ref type="bibr" target="#b23">Santoro et al. (2016)</ref> </note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Clustering on The Latent Space</head><p>As previously mentioned in Sec. 3.3, MANN should let its states form clusters to wellsimulate Turing Machine. <ref type="figure">Fig. 6 (a)</ref> and (c) show NTM actually organizes its c t space into clusters corresponding to processing states (e.g, encoding and decoding). NUTM, which explicitly partitions this space, clearly learn better clusters of c t (see <ref type="bibr">Fig. 6 (b)</ref> and <ref type="formula">(d)</ref>). This contributes to NUTM's outperformance over NTM. <ref type="figure">Figure 6</ref>: Visualization of the first two principal components of c t space in NTM (a,c) and NUTM (b,d) for Copy (red) and Repeat Copy (blue). Fader color denotes lower timestep in a sequence. Both can learn clusters of hidden states yet NUTM exhibits clearer partition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Program Usage Visualizations</head><p>B.1 and B.2 visualize the best inferences of NUTM on test data from single and sequencing tasks. Each plot starts with the input sequence and the predicted output sequence with error bits in the first row 6 . The second and fourth rows depict the read and write locations on data memory, respectively. The third and fifth rows depict the program distribution of the read head and write head, respectively. B.3 visualizes random failed predictions of NTM on sequencing tasks. The plots follow previous pattern except for the program distribution rows.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details on Synthetic Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Example of memory operation function in NTM</head><p>In NTM, ξ t = {β t , k t , g t , s t , e t , v t }. The memory addressing weight is initially computed by content-based attention,</p><p>Here, w c t ∈ R N is the content-based weight, β t is a strength scalar, and m is implemented as cosine similarity</p><p>In addition, NTM supports location-based addressing started with an interpolation between content-based weight and the previous weight</p><p>10) where g t is the interpolation gate that determines to use (or ignore) content-based addressing. Then, NTM can shift the address to other rows by performing convolution shift modulo R,w</p><p>where s t is the shift weighting. To prevent the shifted weight from blurring, sharpening is applied</p><p>Then, the memory is updated as follows,</p><p>where e t ∈ R D and v t ∈ R D are erase vector and update vector, respectively. The read value is computed using the same address weight as follows,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Others</head><p>If we deliberately set the key dimension equal to the number of programs, we can even place an orthogonal basis constraint on the key space of NSM by minimizing the following loss, l p2 = M p .KM p .K T − I (16) where M p .K and I denote the key part in NSM and the identity matrix, respectively.</p><p>Direct attention is one special case of key-value attention when the memory keys form orthogonal basis. When this happens, the generated key k p t plays a direct role as the attention weight w p t . Thus, using key-value attention is more generic.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1181</idno>
		<ptr target="https://www.aclweb.org/anthology/N16-1181" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1545" to="1554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Data Analysis Using Regression and Multilevel/Hierarchical Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gelman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using fast weights to attend to the recent past</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4331" to="4339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Neuroanatomy through Clinical Cases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Blumenfeld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Chapter 8 -digital signal processors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">D</forename><surname>Broesch</surname></persName>
		</author>
		<idno type="DOI">10.1016/B978-0-7506-8976-2.00008-0.URLhttp:/www.sciencedirect.com/science/article/pii/B9780750689762000080</idno>
		<idno>978-0-7506-8976-2</idno>
		<ptr target="https://doi.org/10.1016/B978-0-7506-8976-2.00008-0.URLhttp://www.sciencedirect.com/science/article/pii/B9780750689762000080" />
	</analytic>
	<monogr>
		<title level="m">Digital Signal Processing</title>
		<editor>James D. Broesch</editor>
		<meeting><address><addrLine>Newnes, Burlington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
	<note>Instant Access</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The dynamics of discrete-time computation, with application to recurrent neural networks and finite state machine extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Casey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1135" to="1178" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Applied multiple regression/correlation analysis for the behavioral sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leona</forename><forename type="middle">S</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aiken</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving differentiable neural computers through memory masking, de-allocation, and link distribution sharpness control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Csordas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HyGEM3C9KQ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust and scalable differentiable neural computer for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W18-2606" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Machine Reading for Question Answering</title>
		<meeting>the Workshop on Machine Reading for Question Answering</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="47" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwińska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09106</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using fast weights to deblur old memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David C</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plaut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth annual conference of the Cognitive Science Society</title>
		<meeting>the ninth annual conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multilevel modeling and ordinary least squares regression: how comparable are they?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Experimental Education</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="265" to="281" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbelsoftmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Variational memory encoderdecoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thin</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1515" to="1525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dual memory neural computer for asynchronous two-view sequential learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/3219819.3219981</idno>
		<ptr target="http://doi.acm.org/10.1145/3219819.3219981" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery; Data Mining, KDD &apos;18</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery; Data Mining, KDD &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to remember more with less memorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1xlvi0qYm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A connectionist symbol manipulator that discovers the structure of context-free languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sreerupa</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="863" to="870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scaling memory-augmented neural networks with sparse reads and writes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3621" to="3629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gated fast weights for on-the-fly neural program generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imanol</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Metalearning Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reducing the ratio between learning complexity and number of time varying variables in fully recurrent nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="460" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A self-referential weight matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="446" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the computational power of neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">D</forename><surname>Siegelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer and system sciences</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="132" to="150" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lifelong learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="181" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Finite state machines and recurrent neural networks-automata and dynamical systems approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Tiňo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Horne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><forename type="middle">C</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collingwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks and pattern recognition</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="171" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On computable numbers, with an application to the entscheidungsproblem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Turing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<publisher>London Mathematical Society</publisher>
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The correlation theory of brain function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Von Der Malsburg</surname></persName>
		</author>
		<ptr target="http://cogprints.org/1380/" />
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">First draft of a report on the edvac</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John Von Neumann</surname></persName>
		</author>
		<idno type="DOI">10.1109/85.238389</idno>
		<ptr target="https://doi.org/10.1109/85.238389" />
	</analytic>
	<monogr>
		<title level="j">IEEE Ann. Hist. Comput</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="27" to="75" />
			<date type="published" when="1993-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Towards ai-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Continual learning through synaptic intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedemann</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3987" to="3995" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
