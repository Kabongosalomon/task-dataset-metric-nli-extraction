<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2018 NOISY NETWORKS FOR EXPLORATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Gheshlaghi Azar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Menick</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Osband</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Legg</surname></persName>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2018 NOISY NETWORKS FOR EXPLORATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>DeepMind {meirefortunato</term>
					<term>mazar</term>
					<term>piot</term>
					<term>jmenick</term>
					<term>mtthss</term>
					<term>iosband</term>
					<term>gravesa</term>
					<term>vmnih</term>
					<term>munos</term>
					<term>dhcontact</term>
					<term>pietquin</term>
					<term>cblundell</term>
					<term>legg}@googlecom</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent's policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and Dueling agents (entropy reward and -greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance. * Equal contribution.</p><p>Published as a conference paper at ICLR 2018 can apply to any type of parametric policies (including neural networks), they are usually not data efficient and require a simulator to allow many policy evaluations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Despite the wealth of research into efficient methods for exploration in Reinforcement Learning (RL) <ref type="bibr" target="#b20">(Kearns &amp; Singh, 2002;</ref><ref type="bibr" target="#b19">Jaksch et al., 2010)</ref>, most exploration heuristics rely on random perturbations of the agent's policy, such as -greedy <ref type="bibr" target="#b39">(Sutton &amp; Barto, 1998)</ref> or entropy regularisation <ref type="bibr" target="#b44">(Williams, 1992)</ref>, to induce novel behaviours. However such local 'dithering' perturbations are unlikely to lead to the large-scale behavioural patterns needed for efficient exploration in many environments .</p><p>Optimism in the face of uncertainty is a common exploration heuristic in reinforcement learning. Various forms of this heuristic often come with theoretical guarantees on agent performance <ref type="bibr" target="#b1">(Azar et al., 2017;</ref><ref type="bibr" target="#b21">Lattimore et al., 2013;</ref><ref type="bibr" target="#b19">Jaksch et al., 2010;</ref><ref type="bibr" target="#b0">Auer &amp; Ortner, 2007;</ref><ref type="bibr" target="#b20">Kearns &amp; Singh, 2002)</ref>. However, these methods are often limited to small state-action spaces or to linear function approximations and are not easily applied with more complicated function approximators such as neural networks (except from work by <ref type="bibr" target="#b12">(Geist &amp; Pietquin, 2010a;</ref><ref type="bibr">b)</ref> but it doesn't come with convergence guarantees). A more structured approach to exploration is to augment the environment's reward signal with an additional intrinsic motivation term <ref type="bibr" target="#b38">(Singh et al., 2004</ref>) that explicitly rewards novel discoveries. Many such terms have been proposed, including learning progress <ref type="bibr" target="#b32">(Oudeyer &amp; Kaplan, 2007)</ref>, compression progress <ref type="bibr" target="#b36">(Schmidhuber, 2010)</ref>, variational information maximisation <ref type="bibr" target="#b18">(Houthooft et al., 2016)</ref> and prediction gain <ref type="bibr" target="#b3">(Bellemare et al., 2016)</ref>. One problem is that these methods separate the mechanism of generalisation from that of exploration; the metric for intrinsic reward, and-importantly-its weighting relative to the environment reward, must be chosen by the experimenter, rather than learned from interaction with the environment. Without due care, the optimal policy can be altered or even completely obscured by the intrinsic rewards; furthermore, dithering perturbations are usually needed as well as intrinsic reward to ensure robust exploration <ref type="bibr" target="#b31">(Ostrovski et al., 2017)</ref>. Exploration in the policy space itself, for example, with evolutionary or black box algorithms <ref type="bibr" target="#b27">(Moriarty et al., 1999;</ref><ref type="bibr" target="#b9">Fix &amp; Geist, 2012;</ref><ref type="bibr" target="#b35">Salimans et al., 2017)</ref>, usually requires many prolonged interactions with the environment. Although these algorithms are quite generic and</p><p>We propose a simple alternative approach, called NoisyNet, where learned perturbations of the network weights are used to drive exploration. The key insight is that a single change to the weight vector can induce a consistent, and potentially very complex, state-dependent change in policy over multiple time steps -unlike dithering approaches where decorrelated (and, in the case of -greedy, state-independent) noise is added to the policy at every step. The perturbations are sampled from a noise distribution. The variance of the perturbation is a parameter that can be considered as the energy of the injected noise. These variance parameters are learned using gradients from the reinforcement learning loss function, along side the other parameters of the agent. The approach differs from parameter compression schemes such as variational inference <ref type="bibr" target="#b16">(Hinton &amp; Van Camp, 1993;</ref><ref type="bibr" target="#b7">Bishop, 1995;</ref><ref type="bibr" target="#b14">Graves, 2011;</ref><ref type="bibr" target="#b8">Blundell et al., 2015;</ref><ref type="bibr" target="#b11">Gal &amp; Ghahramani, 2016)</ref> and flat minima search <ref type="bibr" target="#b17">(Hochreiter &amp; Schmidhuber, 1997</ref>) since we do not maintain an explicit distribution over weights during training but simply inject noise in the parameters and tune its intensity automatically. Consequently, it also differs from Thompson sampling <ref type="bibr" target="#b41">(Thompson, 1933;</ref><ref type="bibr" target="#b23">Lipton et al., 2016)</ref> as the distribution on the parameters of our agents does not necessarily converge to an approximation of a posterior distribution.</p><p>At a high level our algorithm is a randomised value function, where the functional form is a neural network. Randomised value functions provide a provably efficient means of exploration <ref type="bibr" target="#b28">(Osband et al., 2014)</ref>. Previous attempts to extend this approach to deep neural networks required many duplicates of sections of the network <ref type="bibr" target="#b29">(Osband et al., 2016)</ref>. By contrast in our NoisyNet approach while the number of parameters in the linear layers of the network is doubled, as the weights are a simple affine transform of the noise, the computational complexity is typically still dominated by the weight by activation multiplications, rather than the cost of generating the weights. Additionally, it also applies to policy gradient methods such as A3C out of the box <ref type="bibr" target="#b25">(Mnih et al., 2016)</ref>. Most recently (and independently of our work) <ref type="bibr" target="#b33">Plappert et al. (2017)</ref> presented a similar technique where constant Gaussian noise is added to the parameters of the network. Our method thus differs by the ability of the network to adapt the noise injection with time and it is not restricted to Gaussian noise distributions. We need to emphasise that the idea of injecting noise to improve the optimisation process has been thoroughly studied in the literature of supervised learning and optimisation under different names (e.g., Neural diffusion process <ref type="bibr" target="#b26">(Mobahi, 2016)</ref> and graduated optimisation <ref type="bibr" target="#b15">(Hazan et al., 2016)</ref>). These methods often rely on a noise of vanishing size that is non-trainable, as opposed to NoisyNet which tunes the amount of noise by gradient descent.</p><p>NoisyNet can also be adapted to any deep RL algorithm and we demonstrate this versatility by providing NoisyNet versions of DQN <ref type="bibr" target="#b24">(Mnih et al., 2015)</ref>, Dueling <ref type="bibr" target="#b43">(Wang et al., 2016)</ref> and A3C <ref type="bibr" target="#b25">(Mnih et al., 2016)</ref> algorithms. Experiments on 57 Atari games show that NoisyNet-DQN and NoisyNet-Dueling achieve striking gains when compared to the baseline algorithms without significant extra computational cost, and with less hyper parameters to tune. Also the noisy version of A3C provides some improvement over the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>This section provides mathematical background for Markov Decision Processes (MDPs) and deep RL with Q-learning, dueling and actor-critic methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">MARKOV DECISION PROCESSES AND REINFORCEMENT LEARNING</head><p>MDPs model stochastic, discrete-time and finite action space control problems <ref type="bibr" target="#b5">(Bellman &amp; Kalaba, 1965;</ref><ref type="bibr" target="#b6">Bertsekas, 1995;</ref><ref type="bibr" target="#b34">Puterman, 1994</ref>). An MDP is a tuple M = (X , A, R, P, γ) where X is the state space, A the action space, R the reward function, γ ∈]0, 1[ the discount factor and P a stochastic kernel modelling the one-step Markovian dynamics (P (y|x, a) is the probability of transitioning to state y by choosing action a in state x). A stochastic policy π maps each state to a distribution over actions π(·|x) and gives the probability π(a|x) of choosing action a in state x. The quality of a policy π is assessed by the action-value function Q π defined as:</p><formula xml:id="formula_0">Q π (x, a) = E π +∞ t=0 γ t R(x t , a t ) ,<label>(1)</label></formula><p>where E π is the expectation over the distribution of the admissible trajectories (x 0 , a 0 , x 1 , a 1 , . . . ) obtained by executing the policy π starting from x 0 = x and a 0 = a. Therefore, the quantity Q π (x, a) represents the expected γ-discounted cumulative reward collected by executing the policy π starting from x and a. A policy is optimal if no other policy yields a higher return. The action-value function of the optimal policy is Q (x, a) = arg max π Q π (x, a).</p><p>The value function V π for a policy is defined as V π (x) = E a ∼π(·|x) [Q π (x, a)], and represents the expected γ-discounted return collected by executing the policy π starting from state x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DEEP REINFORCEMENT LEARNING</head><p>Deep Reinforcement Learning uses deep neural networks as function approximators for RL methods. Deep Q-Networks (DQN) <ref type="bibr" target="#b24">(Mnih et al., 2015)</ref>, Dueling architecture <ref type="bibr" target="#b43">(Wang et al., 2016)</ref>, Asynchronous Advantage Actor-Critic (A3C) <ref type="bibr" target="#b25">(Mnih et al., 2016)</ref>, Trust Region Policy Optimisation <ref type="bibr" target="#b37">(Schulman et al., 2015)</ref>, Deep Deterministic Policy Gradient <ref type="bibr" target="#b22">(Lillicrap et al., 2015)</ref> and distributional RL (C51)  are examples of such algorithms. They frame the RL problem as the minimisation of a loss function L(θ), where θ represents the parameters of the network. In our experiments we shall consider the DQN, Dueling and A3C algorithms.</p><p>DQN <ref type="bibr" target="#b24">(Mnih et al., 2015)</ref> uses a neural network as an approximator for the action-value function of the optimal policy Q (x, a). DQN's estimate of the optimal action-value function, Q(x, a), is found by minimising the following loss with respect to the neural network parameters θ:</p><formula xml:id="formula_1">L(θ) = E (x,a,r,y)∼D r + γ max b∈A Q(y, b; θ − ) − Q(x, a; θ) 2 ,<label>(2)</label></formula><p>where D is a distribution over transitions e = (x, a, r = R(x, a), y ∼ P (·|x, a)) drawn from a replay buffer of previously observed transitions. Here θ − represents the parameters of a fixed and separate target network which is updated (θ − ← θ) regularly to stabilise the learning. An -greedy policy is used to pick actions greedily according to the action-value function Q or, with probability , a random action is taken.</p><p>The Dueling DQN <ref type="bibr" target="#b43">(Wang et al., 2016)</ref> is an extension of the DQN architecture. The main difference is in using Dueling network architecture as opposed to the Q network in DQN. Dueling network estimates the action-value function using two parallel sub-networks, the value and advantage subnetwork, sharing a convolutional layer. Let θ conv , θ V , and θ A be, respectively, the parameters of the convolutional encoder f , of the value network V , and of the advantage network A; and θ = {θ conv , θ V , θ A } is their concatenation. The output of these two networks are combined as follows for every (x, a) ∈ X × A:</p><formula xml:id="formula_2">Q(x, a; θ) = V (f (x; θ conv ), θ V ) + A(f (x; θ conv ), a; θ A ) − b A(f (x; θ conv ), b; θ A ) N actions .<label>(3)</label></formula><p>The Dueling algorithm then makes use of the double-DQN update rule  to optimise θ:</p><formula xml:id="formula_3">L(θ) = E (x,a,r,y)∼D r + γQ(y, b * (y); θ − ) − Q(x, a; θ) 2 ,<label>(4)</label></formula><formula xml:id="formula_4">s.t. b * (y) = arg max b∈A Q(y, b; θ),<label>(5)</label></formula><p>where the definition distribution D and the target network parameter set θ − is identical to DQN.</p><p>In contrast to DQN and Dueling, A3C <ref type="bibr" target="#b25">(Mnih et al., 2016</ref>) is a policy gradient algorithm. A3C's network directly learns a policy π and a value function V of its policy. The gradient of the loss on the A3C policy at step t for the roll-out (x t+i , a t+i ∼ π(·|x t+i ; θ), r t+i ) k i=0 is:</p><formula xml:id="formula_5">∇ θ L π (θ) = −E π k i=0 ∇ θ log(π(a t+i |x t+i ; θ))A(x t+i , a t+i ; θ) + β k i=0</formula><p>∇ θ H(π(·|x t+i ; θ)) .</p><p>(6) H[π(·|x t ; θ)] denotes the entropy of the policy π and β is a hyper parameter that trades off between optimising the advantage function and the entropy of the policy. The advantage function A(x t+i , a t+i ; θ) is the difference between observed returns and estimates of the return produced by A3C's value network:</p><formula xml:id="formula_6">A(x t+i , a t+i ; θ) = k−1 j=i γ j−i r t+j + γ k−i V (x t+k ; θ) − V (x t+i ; θ)</formula><p>, r t+j being the reward at step t + j and V (x; θ) being the agent's estimate of value function of state x.</p><p>The parameters of the value function are found to match on-policy returns; namely we have</p><formula xml:id="formula_7">L V (θ) = k i=0 E π (Q i − V (x t+i ; θ)) 2 | x t+i (7)</formula><p>where Q i is the return obtained by executing policy π starting in state x t+i . In practice, and as in <ref type="bibr" target="#b25">Mnih et al. (2016)</ref>, we estimate</p><formula xml:id="formula_8">Q i asQ i = k−1 j=i γ j−i r t+j + γ k−i V (x t+k ; θ) where {r t+j } k−1 j=i</formula><p>are rewards observed by the agent, and x t+k is the kth state observed when starting from observed state x t . The overall A3C loss is then L(θ) = L π (θ) + λL V (θ) where λ balances optimising the policy loss relative to the baseline value function loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NOISYNETS FOR REINFORCEMENT LEARNING</head><p>NoisyNets are neural networks whose weights and biases are perturbed by a parametric function of the noise. These parameters are adapted with gradient descent. More precisely, let y = f θ (x) be a neural network parameterised by the vector of noisy parameters θ which takes the input x and outputs y. We represent the noisy parameters θ as θ</p><formula xml:id="formula_9">def = µ + Σ ε, where ζ def = (µ, Σ)</formula><p>is a set of vectors of learnable parameters, ε is a vector of zero-mean noise with fixed statistics and represents element-wise multiplication. The usual loss of the neural network is wrapped by expectation over the noise ε:</p><formula xml:id="formula_10">L(ζ) def = E [L(θ)]</formula><p>. Optimisation now occurs with respect to the set of parameters ζ.</p><p>Consider a linear layer of a neural network with p inputs and q outputs, represented by</p><formula xml:id="formula_11">y = wx + b,<label>(8)</label></formula><p>where x ∈ R p is the layer input, w ∈ R q×p the weight matrix, and b ∈ R q the bias. The corresponding noisy linear layer is defined as:</p><formula xml:id="formula_12">y def = (µ w + σ w ε w )x + µ b + σ b ε b ,<label>(9)</label></formula><p>where</p><formula xml:id="formula_13">µ w + σ w ε w and µ b + σ b ε b replace w and b in Eq. (8), respectively. The parameters µ w ∈ R q×p , µ b ∈ R q , σ w ∈ R q×p and σ b ∈ R q are learnable whereas ε w ∈ R q×p and ε b ∈ R q are</formula><p>noise random variables (the specific choices of this distribution are described below). We provide a graphical representation of a noisy linear layer in <ref type="figure">Fig. 4</ref> (see Appendix B).</p><p>We now turn to explicit instances of the noise distributions for linear layers in a noisy network. We explore two options: Independent Gaussian noise, which uses an independent Gaussian noise entry per weight and Factorised Gaussian noise, which uses an independent noise per each output and another independent noise per each input. The main reason to use factorised Gaussian noise is to reduce the compute time of random number generation in our algorithms. This computational overhead is especially prohibitive in the case of single-thread agents such as DQN and Duelling. For this reason we use factorised noise for DQN and Duelling and independent noise for the distributed A3C, for which the compute time is not a major concern.</p><p>(a) Independent Gaussian noise: the noise applied to each weight and bias is independent, where each entry ε w i,j (respectively each entry ε b j ) of the random matrix ε w (respectively of the random vector ε b ) is drawn from a unit Gaussian distribution. This means that for each noisy linear layer, there are pq + q noise variables (for p inputs to the layer and q outputs).</p><p>(b) Factorised Gaussian noise: by factorising ε w i,j , we can use p unit Gaussian variables ε i for noise of the inputs and and q unit Gaussian variables ε j for noise of the outputs (thus p + q unit Gaussian variables in total). Each ε w i,j and ε b j can then be written as:</p><formula xml:id="formula_14">ε w i,j = f (ε i )f (ε j ),<label>(10)</label></formula><formula xml:id="formula_15">ε b j = f (ε j ),<label>(11)</label></formula><p>where f is a real-valued function. In our experiments we used f (x) = sgn(x) |x|. Note that for the bias Eq. <ref type="formula" target="#formula_0">(11)</ref> we could have set f (x) = x, but we decided to keep the same output noise for weights and biases.</p><p>Since the loss of a noisy network,L(ζ) = E [L(θ)], is an expectation over the noise, the gradients are straightforward to obtain:</p><formula xml:id="formula_16">∇L(ζ) = ∇E [L(θ)] = E [∇ µ,Σ L(µ + Σ ε)] .<label>(12)</label></formula><p>We use a Monte Carlo approximation to the above gradients, taking a single sample ξ at each step of optimisation:</p><formula xml:id="formula_17">∇L(ζ) ≈ ∇ µ,Σ L(µ + Σ ξ).<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DEEP REINFORCEMENT LEARNING WITH NOISYNETS</head><p>We now turn to our application of noisy networks to exploration in deep reinforcement learning. Noise drives exploration in many methods for reinforcement learning, providing a source of stochasticity external to the agent and the RL task at hand. Either the scale of this noise is manually tuned across a wide range of tasks (as is the practice in general purpose agents such as DQN or A3C) or it can be manually scaled per task. Here we propose automatically tuning the level of noise added to an agent for exploration, using the noisy networks training to drive down (or up) the level of noise injected into the parameters of a neural network, as needed.</p><p>A noisy network agent samples a new set of parameters after every step of optimisation. Between optimisation steps, the agent acts according to a fixed set of parameters (weights and biases). This ensures that the agent always acts according to parameters that are drawn from the current noise distribution.</p><p>Deep Q-Networks (DQN) and Dueling. We apply the following modifications to both DQN and Dueling: first, ε-greedy is no longer used, but instead the policy greedily optimises the (randomised) action-value function. Secondly, the fully connected layers of the value network are parameterised as a noisy network, where the parameters are drawn from the noisy network parameter distribution after every replay step. We used factorised Gaussian noise as explained in (b) from Sec. 3. For replay, the current noisy network parameter sample is held fixed across the batch. Since DQN and Dueling take one step of optimisation for every action step, the noisy network parameters are re-sampled before every action. We call the new adaptations of DQN and Dueling, NoisyNet-DQN and NoisyNet-Dueling, respectively.</p><p>We now provide the details of the loss function that our variant of DQN is minimising. When replacing the linear layers by noisy layers in the network (respectively in the target network), the parameterised action-value function Q(x, a, ε; ζ) (respectively Q(x, a, ε ; ζ − )) can be seen as a random variable and the DQN loss becomes the NoisyNet-DQN loss:</p><formula xml:id="formula_18">L(ζ) = E E (x,a,r,y)∼D [r + γ max b∈A Q(y, b, ε ; ζ − ) − Q(x, a, ε; ζ)] 2 ,<label>(14)</label></formula><p>where the outer expectation is with respect to distribution of the noise variables ε for the noisy value function Q(x, a, ε; ζ) and the noise variable ε for the noisy target value function Q(y, b, ε ; ζ − ).</p><p>Computing an unbiased estimate of the loss is straightforward as we only need to compute, for each transition in the replay buffer, one instance of the target network and one instance of the online network. We generate these independent noises to avoid bias due to the correlation between the noise in the target network and the online network. Concerning the action choice, we generate another independent sample ε for the online network and we act greedily with respect to the corresponding output action-value function.</p><p>Similarly the loss function for NoisyNet-Dueling is defined as:</p><formula xml:id="formula_19">L(ζ) = E E (x,a,r,y)∼D [r + γQ(y, b * (y), ε ; ζ − ) − Q(x, a, ε; ζ)] 2 (15) s.t. b * (y) = arg max b∈A Q(y, b(y), ε ; ζ).<label>(16)</label></formula><p>Both algorithms are provided in Appendix C.1.</p><p>Asynchronous Advantage Actor Critic (A3C). A3C is modified in a similar fashion to DQN: firstly, the entropy bonus of the policy loss is removed. Secondly, the fully connected layers of the policy network are parameterised as a noisy network. We used independent Gaussian noise as explained in (a) from Sec. 3. In A3C, there is no explicit exploratory action selection scheme (such as -greedy); and the chosen action is always drawn from the current policy. For this reason, an entropy bonus of the policy loss is often added to discourage updates leading to deterministic policies. However, when adding noisy weights to the network, sampling these parameters corresponds to choosing a different current policy which naturally favours exploration. As a consequence of direct exploration in the policy space, the artificial entropy loss on the policy can thus be omitted. New parameters of the policy network are sampled after each step of optimisation, and since A3C uses n step returns, optimisation occurs every n steps. We call this modification of A3C, NoisyNet-A3C.</p><p>Indeed, when replacing the linear layers by noisy linear layers (the parameters of the noisy network are now noted ζ), we obtain the following estimation of the return via a roll-out of size k:</p><formula xml:id="formula_20">Q i = k−1 j=i γ j−i r t+j + γ k−i V (x t+k ; ζ, ε i ).<label>(17)</label></formula><p>As A3C is an on-policy algorithm the gradients are unbiased when noise of the network is consistent for the whole roll-out. Consistency among action value functionsQ i is ensured by letting letting the noise be the same throughout each rollout, i.e., ∀i, ε i = ε. Additional details are provided in the Appendix A and the algorithm is given in Appendix C.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">INITIALISATION OF NOISY NETWORKS</head><p>In the case of an unfactorised noisy networks, the parameters µ and σ are initialised as follows. Each element µ i,j is sampled from independent uniform distributions U[− 3 p , + 3 p ], where p is the number of inputs to the corresponding linear layer, and each element σ i,j is simply set to 0.017 for all parameters. This particular initialisation was chosen because similar values worked well for the supervised learning tasks described in <ref type="bibr" target="#b10">Fortunato et al. (2017)</ref>, where the initialisation of the variances of the posteriors and the variances of the prior are related. We have not tuned for this parameter, but we believe different values on the same scale should provide similar results.</p><p>For factorised noisy networks, each element µ i,j was initialised by a sample from an independent uniform distributions U[− 1 √ p , + 1 √ p ] and each element σ i,j was initialised to a constant σ0 √ p . The hyperparameter σ 0 is set to 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head><p>We evaluated the performance of noisy network agents on 57 Atari games  and compared to baselines that, without noisy networks, rely upon the original exploration methods (ε-greedy and entropy bonus).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">TRAINING DETAILS AND PERFORMANCE</head><p>We used the random start no-ops scheme for training and evaluation as described the original DQN paper <ref type="bibr" target="#b24">(Mnih et al., 2015)</ref>. The mode of evaluation is identical to those of <ref type="bibr" target="#b25">Mnih et al. (2016)</ref> where randomised restarts of the games are used for evaluation after training has happened. The raw average scores of the agents are evaluated during training, every 1M frames in the environment, by suspending (a) Improvement in percentage of NoisyNet-DQN over DQN <ref type="bibr" target="#b24">(Mnih et al., 2015)</ref> (b) Improvement in percentage of NoisyNet-Dueling over Dueling <ref type="bibr" target="#b43">(Wang et al., 2016)</ref> (c) Improvement in percentage of NoisyNet-A3C over A3C <ref type="bibr" target="#b25">(Mnih et al., 2016)</ref> Figure 1: Comparison of NoisyNet agent versus the baseline according to <ref type="bibr">Eq. (19)</ref>. The maximum score is truncated at 250%. learning and evaluating the latest agent for 500K frames. Episodes are truncated at 108K frames (or 30 minutes of simulated play) .</p><p>We consider three baseline agents: DQN <ref type="bibr" target="#b24">(Mnih et al., 2015)</ref>, duel clip variant of Dueling algorithm <ref type="bibr" target="#b43">(Wang et al., 2016)</ref> and A3C <ref type="bibr" target="#b25">(Mnih et al., 2016)</ref>. The DQN and A3C agents were training for 200M and 320M frames, respectively. In each case, we used the neural network architecture from the corresponding original papers for both the baseline and NoisyNet variant. For the NoisyNet variants we used the same hyper parameters as in the respective original paper for the baseline.</p><p>We compared absolute performance of agents using the human normalised score:</p><formula xml:id="formula_21">100 × Score agent − Score Random Score Human − Score Random ,<label>(18)</label></formula><p>where human and random scores are the same as those in <ref type="bibr" target="#b43">Wang et al. (2016)</ref>. Note that the human normalised score is zero for a random agent and 100 for human level performance. Per-game maximum scores are computed by taking the maximum raw scores of the agent and then averaging over three seeds. However, for computing the human normalised scores in <ref type="figure" target="#fig_0">Figure 2</ref>, the raw scores are evaluated every 1M frames and averaged over three seeds. The overall agent performance is measured by both mean and median of the human normalised score across all 57 Atari games.</p><p>The aggregated results across all 57 Atari games are reported in <ref type="table" target="#tab_0">Table 1</ref>, while the individual scores for each game are in <ref type="table">Table 3</ref>   <ref type="bibr">(18)</ref>. We report on the last column the percentage improvement on the baseline in terms of median human-normalised score.</p><p>without noisy networks:</p><formula xml:id="formula_22">100 × Score NoisyNet − Score Baseline max(Score Human , Score Baseline ) − Score Random .<label>(19)</label></formula><p>As before, the per-game score is computed by taking the maximum performance for each game and then averaging over three seeds. The relative human normalised scores are shown in <ref type="figure">Figure 1</ref>. As can be seen, the performance of NoisyNet agents (DQN, Dueling and A3C) is better for the majority of games relative to the corresponding baseline, and in some cases by a considerable margin. Also as it is evident from the learning curves of <ref type="figure" target="#fig_0">Fig. 2</ref> NoisyNet agents produce superior performance compared to their corresponding baselines throughout the learning process. This improvement is especially significant in the case of NoisyNet-DQN and NoisyNet-Dueling. Also in some games, NoisyNet agents provide an order of magnitude improvement on the performance of the vanilla agent; as can be seen in <ref type="table">Table 3</ref> in the Appendix E with detailed breakdown of individual game scores and the learning curves plots from Figs 6, 7 and 8, for DQN, Dueling and A3C, respectively. We also ran some experiments evaluating the performance of NoisyNet-A3C with factorised noise. We report the corresponding learning curves and the scores in <ref type="figure">Fig. ??</ref> and <ref type="table">Table 2</ref>, respectively (see Appendix D). This result shows that using factorised noise does not lead to any significant decrease in the performance of A3C. On the contrary it seems that it has positive effects in terms of improving the median score as well as speeding up the learning process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ANALYSIS OF LEARNING IN NOISY LAYERS</head><p>In this subsection, we try to provide some insight on how noisy networks affect the learning process and the exploratory behaviour of the agent. In particular, we focus on analysing the evolution of the noise weights σ w and σ b throughout the learning process. We first note that, as L(ζ) is a positive and continuous function of ζ, there always exists a deterministic optimiser for the loss L(ζ) (defined in Eq. <ref type="formula" target="#formula_0">(14)</ref>). Therefore, one may expect that, to obtain the deterministic optimal solution, the neural network may learn to discard the noise entries by eventually pushing σ w s and σ b towards 0.</p><p>To test this hypothesis we track the changes in σ w s throughout the learning process. Let σ w i denote the i th weight of a noisy layer. We then defineΣ, the mean-absolute of the σ w i s of a noisy layer, as</p><formula xml:id="formula_23">Σ = 1 N weights i |σ w i |.<label>(20)</label></formula><p>Intuitively speakingΣ provides some measure of the stochasticity of the Noisy layers. We report the learning curves of the average ofΣ across 3 seeds in <ref type="figure" target="#fig_1">Fig. 3</ref> for a selection of Atari games in NoisyNet-DQN agent. We observe thatΣ of the last layer of the network decreases as the learning proceeds in all cases, whereas in the case of the penultimate layer this only happens for 2 games out of 5 (Pong and Beam rider) and in the remaining 3 gamesΣ in fact increases. This shows that in the case of NoisyNet-DQN the agent does not necessarily evolve towards a deterministic solution as one might have expected. Another interesting observation is that the wayΣ evolves significantly differs from one game to another and in some cases from one seed to another seed, as it is evident from the error bars. This suggests that NoisyNet produces a problem-specific exploration strategy as opposed to fixed exploration strategy used in standard DQN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We have presented a general method for exploration in deep reinforcement learning that shows significant performance improvements across many Atari games in three different agent architectures. In particular, we observe that in games such as Beam rider, Asteroids and Freeway that the standard DQN, Dueling and A3C perform poorly compared with the human player, NoisyNet-DQN, NoisyNet-Dueling and NoisyNet-A3C achieve super human performance, respectively. Although the improvements in performance might also come from the optimisation aspect since the cost functions are modified, the uncertainty in the parameters of the networks introduced by NoisyNet is the only exploration mechanism of the method. Having weights with greater uncertainty introduces more variability into the decisions made by the policy, which has potential for exploratory actions, but further analysis needs to be done in order to disentangle the exploration and optimisation effects.</p><p>Another advantage of NoisyNet is that the amount of noise injected in the network is tuned automatically by the RL algorithm. This alleviates the need for any hyper parameter tuning (required with standard entropy bonus and -greedy types of exploration). This is also in contrast to many other methods that add intrinsic motivation signals that may destabilise learning or change the optimal policy. Another interesting feature of the NoisyNet approach is that the degree of exploration is contextual and varies from state to state based upon per-weight variances. While more gradients are needed, the gradients on the mean and variance parameters are related to one another by a computationally efficient affine function, thus the computational overhead is marginal. Automatic differentiation makes implementation of our method a straightforward adaptation of many existing methods. A similar randomisation technique can also be applied to LSTM units <ref type="bibr" target="#b10">(Fortunato et al., 2017)</ref> and is easily extended to reinforcement learning, we leave this as future work.</p><p>Note NoisyNet exploration strategy is not restricted to the baselines considered in this paper. In fact, this idea can be applied to any deep RL algorithms that can be trained with gradient descent, including <ref type="bibr">DDPG (Lillicrap et al., 2015)</ref>, TRPO <ref type="bibr" target="#b37">(Schulman et al., 2015)</ref> or distributional RL (C51) . As such we believe this work is a step towards the goal of developing a universal exploration strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A NOISYNET-A3C IMPLEMENTATION DETAILS</head><p>In contrast with value-based algorithms, policy-based methods such as A3C <ref type="bibr" target="#b25">(Mnih et al., 2016)</ref> parameterise the policy π(a|x; θ π ) directly and update the parameters θ π by performing a gradient ascent on the mean value-function E x∼D [V π(·|·;θπ) (x)] (also called the expected return) <ref type="bibr" target="#b40">(Sutton et al., 1999)</ref>. A3C uses a deep neural network with weights θ = θ π ∪θ V to parameterise the policy π and the value V . The network has one softmax output for the policy-head π(·|·; θ π ) and one linear output for the value-head V (·; θ V ), with all non-output layers shared. The parameters θ π (resp. θ V ) are relative to the shared layers and the policy head (resp. the value head). A3C is an asynchronous and online algorithm that uses roll-outs of size k + 1 of the current policy to perform a policy improvement step.</p><p>For simplicity, here we present the A3C version with only one thread. For a multi-thread implementation, refer to the pseudo-code C.2 or to the original A3C paper <ref type="bibr" target="#b25">(Mnih et al., 2016)</ref>. In order to train the policy-head, an approximation of the policy-gradient is computed for each state of the roll-out (x t+i , a t+i ∼ π(·|x t+i ; θ π ), r t+i ) k i=0 :</p><formula xml:id="formula_24">∇ θπ log(π(a t+i |x t+i ; θ π ))[Q i − V (x t+i ; θ V )],<label>(21)</label></formula><p>whereQ i is an estimation of the returnQ i =</p><formula xml:id="formula_25">k−1 j=i γ j−i r t+j + γ k−i V (x t+k ; θ V ).</formula><p>The gradients are then added to obtain the cumulative gradient of the roll-out:</p><formula xml:id="formula_26">k i=0 ∇ θπ log(π(a t+i |x t+i ; θ π ))[Q i − V (x t+i ; θ V )].<label>(22)</label></formula><p>A3C trains the value-head by minimising the error between the estimated return and the value k i=0 (Q i − V (x t+i ; θ V )) 2 . Therefore, the network parameters (θ π , θ V ) are updated after each roll-out as follows:</p><formula xml:id="formula_27">θ π ← θ π + α π k i=0 ∇ θπ log(π(a t+i |x t+i ; θ π ))[Q i − V (x t+i ; θ V )],<label>(23)</label></formula><formula xml:id="formula_28">θ V ← θ V − α V k i=0 ∇ θ V [Q i − V (x t+i ; θ V )] 2 ,<label>(24)</label></formula><p>where (α π , α V ) are hyper-parameters. As mentioned previously, in the original A3C algorithm, it is recommended to add an entropy term β k i=0 ∇ θπ H(π(·|x t+i ; θ π )) to the policy update, where H(π(·|x t+i ; θ π )) = −β a∈A π(a|x t+i ; θ π ) log(π(a|x t+i ; θ π )). Indeed, this term encourages exploration as it favours policies which are uniform over actions. When replacing the linear layers in the value and policy heads by noisy layers (the parameters of the noisy network are now ζ π and ζ V ), we obtain the following estimation of the return via a roll-out of size k:</p><formula xml:id="formula_29">Q i = k−1 j=i γ j−i r t+j + γ k−i V (x t+k ; ζ V , ε i ).<label>(25)</label></formula><p>We would likeQ i to be a consistent estimate of the return of the current policy. To do so, we should force ∀i, ε i = ε. As A3C is an on-policy algorithm, this involves fixing the noise of the network for the whole roll-out so that the policy produced by the network is also fixed. Hence, each update of the parameters (ζ π , ζ V ) is done after each roll-out with the noise of the whole network held fixed for the duration of the roll-out:</p><formula xml:id="formula_30">ζ π ← ζ π + α π k i=0 ∇ ζπ log(π(a t+i |x t+i ; ζ π , ε))[Q i − V (x t+i ; ζ V , ε)],<label>(26)</label></formula><formula xml:id="formula_31">ζ V ← ζ V − α V k i=0 ∇ ζ V [Q i − V (x t+i ; ζ V , ε)] 2 .<label>(27)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B NOISY LINEAR LAYER</head><p>In this Appendix we provide a graphical representation of noisy layer.   <ref type="table">Table 2</ref>: Comparison between the baseline DQN, Dueling and A3C and their NoisyNet version in terms of median and mean human-normalised scores defined in Eq. (18). In the case of A3C we inculde both factorised and non-factorised variant of the algorithm. We report on the last column the percentage improvement on the baseline in terms of median human-normalised score.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of the learning curves of NoisyNet agent versus the baseline according to the median human normalised score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of the learning curves of the average noise parameterΣ across five Atari games in NoisyNet-DQN. The results are averaged across 3 seeds and error bars (+/-standard deviation) are plotted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 : 4 Set x ← x 0 5QFigure 5 :</head><label>4405</label><figDesc>Graphical representation of a noisy linear layer. The parameters µ w , µ b , σ w and σ b are the learnables of the network whereas ε w and ε b are noise variables which can be chosen in factorised or non-factorised fashion. The noisy layer functions similarly to the standard fully connected linear layer. The main difference is that in the noisy layer both the weights vector and the bias is perturbed by some parametric zero-mean noise, that is, the noisy weights and the noisy bias can be expressed as w = µ w + σ w ε w and b = µ b + σ b ε b , respectively. The output of the noisy layer is then simply obtained as y = wx + b.C ALGORITHMS C.1 NOISYNET-DQN AND NOISYNET-DUELINGAlgorithm 1: NoisyNet-DQN / NoisyNet-Dueling Input :Env Environment; ε set of random variables of the network Input :DUELING Boolean; "true" for NoisyNet-Dueling and "false" for NoisyNet-DQN Input :B empty replay buffer; ζ initial network parameters; ζ − initial target network parameters Input :N B replay buffer size; N T training batch size; N − target network replacement frequency Output :Q(·, ε; ζ) action-value function1 for episode e ∈ {1, . . . , M } do 2 Initialise state sequence x 0 ∼ Env 3 for t ∈ {1, . . . } do / * l[−1] is the last element of the list l * / Sample a noisy network ξ ∼ ε 6 Select an action a ← argmax b∈A Q(x, b, ξ; ζ) 7 Sample next state y ∼ P (·|x, a), receive reward r ← R(x, a) and set x 0 ← y 8 Add transition (x, a, r, y) to the replay buffer B[−1] ← (x, a, r, y) 9 if |B| &gt; N B then 10 Delete oldest transition from B 11 end / * D is a distribution over the replay, it can be uniform or implementing prioritised replay * / 12 Sample a minibatch of N T transitions ((x j , a j , r j , y j ) ∼ D) variable for the online network ξ ∼ ε 14 Sample the noisy variables for the target network ξ ∼ ε 15 if DUELING then 16 Sample the noisy variables for the action selection network ξ ∼ ε 17 for j ∈ {1, . . . , N T } do 18 if y j is a terminal state then 19 Q ← r j 20 if DUELING then 21 b * (y j ) = arg max b∈A Q(y j , b, ξ ; ζ) 22Q ← r j + γQ(y j , b * (y j ), ξ ; ζ − ) ← r j + γ max b∈A Q(y j , b, ξ ; ζ − ) 25Do a gradient step with loss ( Q − Q(x j , a j , ξ; ζ)) Comparison of the learning curves of factorised and non-factorised NoisyNet-A3C versus the baseline according to the median human normalised score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Training curves for all Atari games comparing Duelling and NoisyNet-Dueling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Training curves for all Atari games comparing A3C and NoisyNet-A3C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>from the Appendix E. The median human normalised score is improved in all agents by using NoisyNet, adding at least 18 (in the case of A3C) and at most 48 (in the case of DQN) percentage points to the median human normalised score. The mean human normalised score is also significantly improved for all agents. Interestingly the Dueling case, which relies on multiple modifications of DQN, demonstrates that NoisyNet is orthogonal to several other improvements made to DQN. We also compared relative performance of NoisyNet agents to the respective baseline agent Comparison between the baseline DQN, Dueling and A3C and their NoisyNet version in terms of median and mean human-normalised scores defined in Eq.</figDesc><table><row><cell></cell><cell cols="2">Baseline</cell><cell cols="2">NoisyNet</cell><cell>Improvement</cell></row><row><cell></cell><cell cols="5">Mean Median Mean Median (On median)</cell></row><row><cell>DQN</cell><cell>319</cell><cell>83</cell><cell>379</cell><cell>123</cell><cell>48%</cell></row><row><cell>Dueling</cell><cell>524</cell><cell>132</cell><cell>633</cell><cell>172</cell><cell>30%</cell></row><row><cell>A3C</cell><cell>293</cell><cell>80</cell><cell>347</cell><cell>94</cell><cell>18%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Koray Kavukcuoglu, Oriol Vinyals, Daan Wierstra, Georg Ostrovski, Joseph Modayil, Simon Osindero, Chris Apps, Stephen Gaffney and many others at DeepMind for insightful discussions, comments and feedback on this work.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Algorithm 2: NoisyNet-A3C for each actor-learner thread Input :Environment Env, Global shared parameters (ζ π , ζ V ), global shared counter T and maximal time T max . Input :Thread-specific parameters (ζ π , ζ V ), Set of random variables ε, thread-specific counter t and roll-out size t max . Output :π(·; ζ π , ε) the policy and V (·; ζ V , ε) the value.</p><p>1 Initial thread counter t ← 1 2 repeat 3 Reset cumulative gradients: dζ π ← 0 and dζ V ← 0. 4 Synchronise thread-specific parameters: ζ π ← ζ π and ζ V ← ζ V . 5 counter ← 0. Accumulate policy-gradient:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>26</head><p>Accumulate value-gradient:</p><p>Perform asynchronous update of ζ π : ζ π ← ζ π + α π dζ π 29 Perform asynchronous update of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E LEARNING CURVES AND RAW SCORES</head><p>Here we directly compare the performance of DQN, Dueling DQN and A3C and their NoisyNet counterpart by presenting the maximal score in each of the 57 Atari games (  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Logarithmic online regret bounds for undiscounted reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Ortner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">49</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Mohammad Gheshlaghi Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Munos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05449</idno>
		<title level="m">Minimax regret bounds for reinforcement learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The arcade learning environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yavar</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unifying count-based exploration and intrinsic motivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1471" to="1479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A distributional perspective on reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Marc G Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Dynamic programming and modern control theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bellman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Kalaba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965" />
			<publisher>Academic Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic programming and optimal control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Training with noise is equivalent to Tikhonov regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="108" to="116" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1613" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Monte-Carlo swarm policy search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Fix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Geist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Swarm and Evolutionary Computation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="75" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02798</idno>
		<title level="m">Bayesian recurrent neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v48/gal16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<editor>Maria Florina Balcan and Kilian Q. Weinberger</editor>
		<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="20" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Kalman temporal differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="483" to="532" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Managing uncertainty within value function approximation in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Active Learning and Experimental Design workshop (collocated with AISTATS 2010)</title>
		<meeting><address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Practical variational inference for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2348" to="2356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On graduated optimization for stochastic non-convex problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Kfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1833" to="1841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Keeping the neural networks simple by minimizing the description length of the weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Camp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth annual conference on Computational learning theory</title>
		<meeting>the sixth annual conference on Computational learning theory</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="5" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Flat minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">VIME: Variational information maximizing exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><forename type="middle">De</forename><surname>Turck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1109" to="1117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Near-optimal regret bounds for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Jaksch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Ortner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1563" to="1600" />
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Near-optimal reinforcement learning in polynomial time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="209" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The sample-complexity of general reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tor</forename><surname>Lattimore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sunehag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 30th International Conference on Machine Learning</title>
		<meeting>The 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Timothy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Efficient exploration for dialogue policy learning with BBQ networks &amp; replay buffer spiking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zachary C Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05081</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><forename type="middle">Puigdomenech</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Training recurrent neural networks by diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.04114</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evolutionary algorithms for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Moriarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">J</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="241" to="276" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Generalization and exploration via randomized value functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Benjamin Van Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.0635</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep exploration via bootstrapped DQN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4026" to="4034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Roy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07608</idno>
		<title level="m">Deep exploration via randomized value functions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Count-based exploration with neural density models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Munos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01310</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">What is intrinsic motivation? A typology of computational approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Oudeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Kaplan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neurorobotics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szymon</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01905</idno>
		<title level="m">Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Markov decision processes: discrete stochastic dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Formal theory of creativity, fun, and intrinsic motivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Autonomous Mental Development</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="230" to="247" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Intrinsically motivated reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuttapong</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chentanez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1281" to="1288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Cambridge Univ Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the likelihood that one unknown probability exceeds another in view of the evidence of two samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William R Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="285" to="294" />
			<date type="published" when="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with double qlearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2094" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1995" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
