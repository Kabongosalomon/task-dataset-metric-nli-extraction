<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MLSL: Multi-Level Self-Supervised Learning for Domain Adaptation with Spatially Independent and Semantically Consistent Labeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javed</forename><surname>Iqbal</surname></persName>
							<email>javed.iqbal@itu.edu.pk</email>
							<affiliation key="aff0">
								<orgName type="institution">Information Technology University of Punjab</orgName>
								<address>
									<settlement>Lahore</settlement>
									<country key="PK">Pakistan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Ali</surname></persName>
							<email>mohsen.ali@itu.edu.pk</email>
							<affiliation key="aff0">
								<orgName type="institution">Information Technology University of Punjab</orgName>
								<address>
									<settlement>Lahore</settlement>
									<country key="PK">Pakistan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MLSL: Multi-Level Self-Supervised Learning for Domain Adaptation with Spatially Independent and Semantically Consistent Labeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most of the recent Deep Semantic Segmentation algorithms suffer from large generalization errors, even when powerful hierarchical representation models based on convolutional neural networks have been employed. This could be attributed to limited training data and large distribution gap in train and test domain datasets. In this paper, we propose a multi-level self-supervised learning model for domain adaptation of semantic segmentation. Exploiting the idea that an object (and most of the stuff given context) should be labeled consistently regardless of its location, we generate spatially independent and semantically consistent (SISC) pseudo-labels by segmenting multiple sub-images using base model and designing an aggregation strategy. Image level pseudo weak-labels, PWL, are computed to guide domain adaptation by capturing global context similarity in source and domain at latent space level. Thus helping latent space learn the representation even when there are very few pixels belonging to the domain category (small object for example) compared to rest of the image. Our multi-level Self-supervised learning (MLSL) outperforms existing state-of-art (self or adversarial learning) algorithms. Specifically, keeping all setting similar and employing MLSL we obtain an mIoU gain of 5.1% on GTA-V to Cityscapes adaptation and 4.3% on SYNTHIA to Cityscapes adaptation compared to existing state-of-art method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the evolution of deep learning methods during the last decade and the availability of densely labeled datasets <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>, a considerable attention has been devoted to improving the performance of semantic segmentation <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. Significant reliance of real-time applications like autonomous vehicles <ref type="bibr" target="#b10">[11]</ref>, bio-medical imaging <ref type="bibr" target="#b11">[12]</ref>, etc. over robust and accurate semantic segmentation step has also helped it gain prominence in current research. However, with the limited datasets for such a complex task (pixel-wise annotation), the state-of-the-art models have been reported to produce large generalization errors <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. This occurs naturally, because the train data may vary from test data (domain shift) in many aspects like illumination, visual appearance, camera quality, etc. It is time consuming and labor-intensive to densely label high resolution images covering all the domain variations. Modern computer graphics makes it easier to train deep models using synthetic images with computer generated dense labels <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. However, these simulated-scene datasets are significantly different in visual appearance and object structures compared to real-life roadscene datasets, limiting the model performance. To overcome these domain shift issues, many techniques have been proposed to adapt the target data distribution <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>. Here our focus is to adapt the target domain dataset without labels in an unsupervised manner using Self-supervised learning.</p><p>Due to large real-world applications, unsupervised domain adaptation (UDA) is a well-studied field in the current decade and aims to generalize to unseen data using only the labeled data of source domain. In UDA, most of the algorithms try to match the source and target data distribution using adversarial loss <ref type="bibr" target="#b17">[18]</ref> either at structured output level <ref type="bibr" target="#b16">[17]</ref> or latent space features level <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref> respectively. Similarly, UDA based on adversarial learning augmented with other methods have recently produced good results on adaptation of semantic segmentation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref>. However, Zou et al. in <ref type="bibr" target="#b12">[13]</ref> showed that a comparative performance can be achieved using an alternative method contrary to adversarial learning with less computational resources required compared to these complex methods. They introduced a class balanced self-supervised training method by generating pseudo-labels using the source-data trained model and tried to minimize a single loss function. However, they failed to capture the global context of the image referenced to categories and also the generated pseudolabels had high uncertainty.</p><p>In this work, we propose a novel Multi-level Self-Supervised learning (MLSL) approach for UDA of semantic segmentation. The proposed approach consists of two complementary strategies. First, we propose spatially independent and semantically consistent (SISC) pseudo-labels generation process. We make reasonable assumption that an object should be segmented with similar label regardless of the location of the object. Same could be said about the stuff representing grass, road, sky, etc., given a reasonable context in surrounding. Using base model, multiple subimages (extracted from an image) are segmented independently and output probability volume is aggregated. This not only generates better pseudo-labels than single instance (SI) based ones, the assumption is more general than the spatial consistency assumption used by <ref type="bibr" target="#b12">[13]</ref>.</p><p>Secondly, we enforce the global context and small object information preservation while adaptation by attaching a category based image classification module at latent space level. For each target image, Image level labels, called pseudo weak-labels (PWL) are generated using SISC pseudo-labels and size statistics collected from source domain. In summary, our main contributions are : 1. A Multi-level self learning strategy for UDA of semantic segmentation by generating pseudo-labels at finegrain pixel-level and image level, helping identify domain invariant features at both latent and output level. 2. Designing a strategy, based on a reasonable assumption that for most categories labels should be location invariant given enough context, to generate spatially independent and semantically consistent pixelwise pseudo-labels 3. Using category wise size statistics to help build PWL and train latent space. 4. State-of-the-art performance on benchmark datasets by further augmenting the class spatial and category distribution priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Due to the evolution of deep learning methods, most of the computer vision tasks including but not limited to object detection, semantic segmentation, etc., are shifted to deep neural networks based methods <ref type="bibr" target="#b22">[23]</ref>. In <ref type="bibr" target="#b3">[4]</ref>, the authors proposed a fully convolutional network for pixel-level dense classification for the first time. Following them, many researchers proposed state-of-the-art methods for semantic segmentation taking the performance to an acceptable level for many computer vision tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>Domain adaptation is a widely studied area in computer vision for segmentation, detection, and classification tasks. With the emergence of semantic segmentation algorithms <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>, availability of datasets <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> and modern applications demanding real-time constraints, e.g., selfdriving cars, domain adaptation for semantic segmentation is in the spotlight. Many approaches exploited an appealing direction in semantic segmentation using domain adaptation from synthetic dataset to real-life datasets <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21]</ref>. The underlying idea of UDA include matching target and source features using discrepancy minimization <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref>, self-supervised learning with pseudo-labels <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24]</ref> and reweighting source domain to look like target domain <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref>. This work thoroughly investigates the unsupervised domain adaptation for semantic segmentation with focus on selfsupervised learning approach.</p><p>Adversarial learning is the most explored method for UDA of semantic segmentation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref>. Adversarial loss-based training is exploited for feature matching, structured output matching, and re-weighting processes frequently in UDA. The authors in <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b24">[25]</ref> exploited latent space representations and used an adversarial loss to match the latent space features of source and target domains. Similarly, Chen et al. <ref type="bibr" target="#b18">[19]</ref> used the adversarial loss for UDA of semantic segmentation augmented with classspecific adversaries to enhance the adaptation performance. The authors in <ref type="bibr" target="#b21">[22]</ref> also proposed the latent space domain matching based on adversarial loss augmented with appearance adaptation network at the input. They tried to combine the latent space representation adaptation and re-weighting process and observed a significant gain in performance. In <ref type="bibr" target="#b15">[16]</ref> the authors adapted similar approach to first transform the fully labeled source images to target images, train the segmentation model using the labeled source data, and then adapt further to target data. Rui et al. <ref type="bibr" target="#b25">[26]</ref> devised a domain flow approach to transfer source images to new domains using adversarial learning at intermediate levels. In <ref type="bibr" target="#b26">[27]</ref>, the authors leveraged the spatial structure of source and target domain dataset, and working in latent space, proposed domain independent structure and domain specific texture based composite architecture for UDA. However, due to high dimensional feature representation at latent space, it is hard to adapt to new data distributions using adversarial loss because of the instability of the adversarial learning process.</p><p>In <ref type="bibr" target="#b16">[17]</ref>, the authors proposed a structured output domain adaptation based on adversarial learning. Their proposed method does not suffer from high dimensional representation of latent space and performs well due to a defined structure of road scene imagery at the output. They proposed state-of-the-art results in comparison with previous methods and also provided a baseline solution for other methods. Zou et al. <ref type="bibr" target="#b12">[13]</ref> proposed a comparative performance method based on iterative learning. They proposed a class balanced self-training mechanism and obtained state-of-the-art performance using spatial priors in the pseudo-labels generation process. A tri-branch UDA model for semantic segmentation is proposed in <ref type="bibr" target="#b23">[24]</ref>, where they generate pseudolabels from two branches and train the third branch on that pseudo-labels alternatively. The authors in <ref type="bibr" target="#b13">[14]</ref> stated that, only adversarial learning at latent space or output space is not enough to learn the target distribution. They used a direct entropy minimization algorithm augmented with an entropy-based adversarial loss for UDA of semantic segmentation.</p><p>In summary, the existing solutions are suffering due to various problems e.g. latent space adaptation suffers from high dimensional feature representation, output space adaptation struggles with small and thin objects, re-weighting independently is not enough to achieve the goal. Similarly, the existing iterative methods are not capable to generate good pseudo-labels and cannot capture the global image context. In this work we propose category-based image classification using PWL and SISC based self-supervised learning for domain adaptation of semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this section, we present the proposed self-supervised and weakly-supervised learning approaches based on SISC pseudo-labels and PWL for domain adaptation of semantic segmentation. We start with existing state-of-the-art networks in semantic segmentation <ref type="bibr" target="#b27">[28]</ref> and self-training for domain adaptation <ref type="bibr" target="#b12">[13]</ref> as baseline methods and plugin additional modules for proposed approaches. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates, iterative self-supervised learning technique for UDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Let I s ∈ R H×W ×3 and Y s ∈ R H×W ×C where, I s corresponds to RGB images of source dataset with resolution H × W and Y s are ground truth labels as C-classes onehot vectors with same spatial resolution as I s . Let G be a fully convolutional network which predicts softmax outputs G(I) = G(I H×W ×3 ) = P H×W ×C I = P I for an input image I. One needs to learn the parameters w g of G by minimizing the cross-entropy loss given in Eq. 1 on source domain images.</p><formula xml:id="formula_0">Lseg(Is, Ys) = − H,W,C Y H×W ×C s log(P H×W ×C Is ) (1)</formula><p>If ground truth labels for target dataset are available, the most direct strategy would be to use Eq. 1 and fine-tune the source trained model to target dataset. However, labels for target dataset are not available most of the time especially in real-time applications, e.g., self-driving cars. Therefore, an alternate way for unsupervised domain adaptation is to finetune the source trained model on the most confident outputs called "pseudo-labels", which the model produces on target domain images. The pseudo-labels have exactly the same dimensions as Y s . The loss function for the target domain images is formulated as follows:</p><formula xml:id="formula_1">Lseg(It,Ŷt) = − H,W,C d H×WŶ H×W ×C t log(P H×W ×C I t ) (2)</formula><p>whereL seg (I t ,Ŷ t ) in Eq. 2 is self-training loss withŶ t as the pseudo-labels one-hot vectors with C classes, and d H×W is a binary map, obtained from pseudo-labelsŶ t e.g., d ij = 1 if any pseudo-label is there atŶ tij , and d ij = 0 if there is no pseudo-label assigned atŶ tij , where i = 1, ..., H and j = 1, ..., W . d allows to back propagate loss for those pixel locations only, which are assigned pseudo-labels. We name the training method as "self-supervised learning" or "self-training".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Semantically consistent pseudo-labels</head><p>Training a network using single inference (SI) generated pseudo-labels only misleads the training process as there is no guarantee over the quality of pseudo-labels. An initial optimal strategy is to jointly train the segmentation network using the ground truth labels of source images and the generated pseudo-labels of target images. The joint loss function is given by Eq. 3. where, L seg (I s , Y s ) is the loss of source images and L seg (I t ,Ŷ t ) is the loss of target images given in Eq. 1 and Eq. 2 respectively. To minimize the loss in Eq. 3, we follow the two stage alternative process given below:  1. Generate pseudo-labels by fixing the model parameters w g . 2. Minimize the loss in Eq. 3 with respect to w g by fixing the pseudo-labelsŶ t generated in the previous step.</p><p>In this work, Step 1 and Step 2 are executed alternatively and repeated for multiple iterations. A work-flow of the proposed algorithm is shown in <ref type="figure" target="#fig_0">Fig. 1.</ref> Step 1 tries to generate pseudo-labels using the output softmax probabilities of the target images based on the more confident examples. Once the pseudo-labels are generated, Step 2 updates the model parameters w g using stochastic gradient descent (SGD) by minimizing the loss function given in Eq. 3.</p><p>Spatially independent and semantically consistent pseudo-labels: Instead of generating pseudo-labels using SI, (e.g., segmenting the whole image simultaneously), we generate "spatially independent and semantically consistent (SISC)" pseudo-labels. We leverage the spatial independence of our baseline semantic segmentation model to generate spatially independent and semantically consistent predictions. To quantitatively show the contribution of semantic consistency, we evaluate the softmax predictions based on different spatial context and select the most consistent ones. For each target image I t , we select K partially overlapping patches [p 1 , p 2 , ..., p K ] of size h × w each. Each patch p i is passed through the segmentation algorithm to as-sign pixel-wise confidence vectors using softmax outputs. The output softmax probabilities for each patch are added to an empty matrix P Ic ∈ R H×W ×C in specific locations where each patch belongs, and generate the composite output. Each pixel in P Ic has an associated count based on the number of occurrences in different patches during inference. We normalize P Ic with associated counts to obtain a normalized probability map and forward it to pseudo-label selection step which chooses the most confident outputs as pseudo-labels. The whole process of patch-based and single inference based pseudo-label generation is shown in <ref type="figure" target="#fig_3">Fig. 2</ref>.</p><p>Unlike simple pseudo-labels generation methods which suffer from category distribution imbalance problem, we use the category-balanced pseudo-label selection similar to the method used in <ref type="bibr" target="#b12">[13]</ref>. Using the obtained normalized probability map, we further normalize the category-wise probabilities and select the pixels having high probability within a specific category. For example, we select all pixels locations which are assigned to be "road", normalize probabilities on that locations and then select the most confident ones. This process balances the inter-category pseudolabels ratio and avoids the training process to adapt simple examples only. The obtained pseudo-labels belong to the more consistent pixels inferred without the global view. The loss function given in Eq. 3 is minimized using the original labels for source domain and SISC pseudo-labels for the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pseudo weak-labels guided domain adaptation</head><p>The cross-entropy loss for an input image/label pair defined in Eq. 1 calculates the sum of independent pixel-wise entropies, dealing with each pixel and label at the location independently. Thus ignoring any spatially global information, prone to effected by sparse erroneous pseudo-labels. Due to unbalanced pixels per category distribution, minimizing the summation of independent pixels entropies ignores the global data distribution. Even balancing the labels <ref type="bibr" target="#b12">[13]</ref>, the low-density classes fades (for target domain) as self-training proceeds.</p><p>We employ the pseudo weak-labels (PWL), guided multi-task weakly-supervised learning to regularize the pixel wise cross-entropy loss. The PWL based category level cross-entropy loss is attached at the encoder level while adapting. This forces the latent space to learn to represent target categories, even for the small objects whose latent space representation might be faded if only pixel-wise cross-entropy loss is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">PWL Filtering</head><p>The pixel-wise pseudo-labels are too noisy to generate the image level pseudo-labels. Assuming that source and target have similar objects and their instances, we build a naive model for the category's size relationship with the image. From the source dataset we calculate h s = {m 1 , m 2 , . . . , m c }, to represent mean size of each class, where</p><formula xml:id="formula_2">m i = 1 ( N j=1 1 j i ) × H × W N j=1 1 j i { H x=1 W y=1 Y j s (x, y, i)} (4)</formula><p>N stands for total images, and indicator function 1 j i is 1 if j th image has class i, otherwise zero. For each target image I t , we compute SISC pseudo-labelsŶ t and use it to compute array h t . PWL vector for image I t is an indicator vector c pwl , s.t.</p><formula xml:id="formula_3">c pwl i = 1 if h t (i) &gt; ηh s (i) otherwise zero.</formula><p>η is a small value chosen by the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">PWL Loss</head><p>Given any image I, an image classification module F cl is designed to input the latent space representation (in this case of ResNet-38), and predict labels <ref type="figure" target="#fig_3">(Fig. 2(c)</ref>). Instead of softmax, we use sigmoid so that it can predict multiple labels for the image and use binary cross-entropy loss function given in Eq.5 .</p><formula xml:id="formula_4">L F cl (I, c) = − 1 C C i=1 (c i )log(F cl (I)) + (1 − c i )log(1 − F cl (I))<label>(5)</label></formula><p>For the source images I s , indicator vector c represents image level label crated from ground truth segmentation la-bels. For the images in target domain I t , image level weaklabels c pwl are created as detailed in Sec. 3.3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Final Loss Function</head><p>The overall loss function for segmentation network and category-based image classification network for source domain is the composition of both 1 and 5, and is given by</p><formula xml:id="formula_5">Lcmp(I, Y, c) = Lseg(I, Y ) + λF cl LF cl (I, c)<label>(6)</label></formula><p>where λ F cl is the scaling factor and c is image level label. The combined loss function for self-supervised and weaklysupervised learning is given by;</p><p>LST W L(Is, Ys, It,Ŷt, c) = Lcmp(Is, Ys, c) +Lcmp(It,Ŷt, c)</p><p>Eq. 7, is minimized using criteria described in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we present experimental details and discuss the main results of our proposed UDA methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup 4.1.1 Datasets</head><p>We follow the synthetic-to-real setup for unsupervised domain adaptation. We use GTA-V <ref type="bibr" target="#b2">[3]</ref> and SYNTHIA <ref type="bibr" target="#b1">[2]</ref> as our source domain synthetic datasets and Cityscapes <ref type="bibr" target="#b0">[1]</ref> as real-world target domain dataset. GTA-V consist of 24966 synthetic frames of spatial resolution 1052 × 1914 extracted from a video game. All the 24966 frames have pixel level labels available for 33 categories, but we used 19 categories compatible with real-world Cityscapes dataset. Similarly, we use SYNTHIA-RAND-CITYSCAPES set having 9400 synthetic frames of size 760×1280 from SYNTHIA dataset. We train and evaluate our baseline and proposed models with 16 common classes in SYNTHIA and Cityscapes. We also report the 13 classes evaluation as described in <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b12">[13]</ref>.</p><p>In both the experiments, we use the Cityscapes training set without labels for unsupervised domain adaptation and evaluate the adapted models on Cityscapes separate validation set having 500 images. We use standard mean Intersection-over-Union (mIoU) as our evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Model architecture</head><p>We use ResNet-38 <ref type="bibr" target="#b27">[28]</ref> as our baseline semantic segmentation model. The pre-trained ResNet-38 (trained on Ima-geNet <ref type="bibr" target="#b30">[31]</ref>) is trained for semantic segmentation on GTA-V and SYNTHIA datasets. The architecture of ResNet-38 contains 7-blocks are there followed by two segmentation layers and an upsampling layer. We also call the ResNet-38 as encoder for segmentation network and refer its output as latent space representation. The two convolution layers <ref type="table">Table 1</ref>. Semantic segmentation performance when the model trained on GTA-V dataset is adapted to Cityscapes dataset. We present the results of our proposed SISC pseudo-labels based self-supervised learning and PWL augmented self-training. We use the competitive baseline model and show a thorough comparison with existing state-of-the-art methods. The abbreviations "ST" and "Adv" indicates the self-training (self-supervised learning) and adversarial learning respectively. <ref type="bibr">GTA</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Implementation and training details</head><p>We use MxNet <ref type="bibr" target="#b31">[32]</ref> deep learning framework and a single Core-i5 machine with 32GB RAM and a GTX 1080 GPU with 8GB of memory to implement the proposed methods for domain adaptation of semantic segmentation. Our proposed model uses SGD optimizer for training with an initial learning rate of 1 × 10 −4 . To generate SISC pseudo-labels, K = 50 is chosen (e.g. 50 sub-images of a target image are selected randomly). For SISC pseudo-labels based selfsupervised learning, a batch size of 2 is chosen while the weakly-supervised setup described in section 3.3 processes a single image only. To optimize the joint loss function given in Eq. refeqn:7, the value of λ F cl is investigated thoroughly (as shown in Section 4.3) and chosen as 0.025 to limit the image classification loss to back propagate large gradients. λ F cl also controls the speed of adaptation with trade-off to segmentation performance, so the mentioned nominal value is used for all followed experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental results</head><p>The experimental results of our proposed approaches compared to baseline ResNet-38 and existing state-of-theart UDA methods are presented in this section. Our proposed approaches perform superior to other methods for domain adaptation and produce state-of-the-art results on two benchmark datasets. We also describe in detail, the behaviour of proposed approaches when exploited with different settings and different source datasets.</p><p>GTA-V to Cityscapes: <ref type="table">Table 1</ref> details the experimental results of 19 categories when adapted from GTA-V to Cityscapes. We use standard mIoU as semantic segmentation performance measure and report results on Cityscapes validation set. Our proposed approach of self-supervised learning with SISC pseudo-labels, shows state-of-the-art performance with ResNet-38 segmentation model. The SISC approach outperforms the latest approaches for UDA of semantic segmentation. Compared to MinEnt <ref type="bibr" target="#b13">[14]</ref> which tries to minimize the self-entropy using direct entropy minimization, our SISC approach shows 13.1% improvement in overall mIoU. Similarly, compared to the self-training approach presented in <ref type="bibr" target="#b12">[13]</ref>, the proposed SISC method outperforms it with a margin of 5.1% in mIoU.</p><p>Our weak-labels guided UDA approach tries to capture the global image context by category (object/stuff) based image classification. This model helps improving the overall performance, and especially boost the performance for small and less occurring objects as shown in <ref type="table">Table 1</ref>. The consistency and accuracy of pseudo weak-labels for image classification enable this approach to help the segmentation model for better performance. With ResNet-38 baseline, pseudo weak-labels when combined with CBST <ref type="bibr" target="#b12">[13]</ref> provides 2.3% boost in mIoU compared to simple CBST. Similarly, when SISC is augmented with PWL based image classification, the mIoU performance increases by 5.7% from existing stat-of-the-art CBST-SP <ref type="bibr" target="#b12">[13]</ref> as shown in <ref type="table">Table 1</ref>. The ensemble of the two proposed approaches for UDA achieve 49.0 mIoU on Cityscapes validation set, which sets a new benchmark. The high boost in performance shows that both the approaches are capable to extract domain independent representations and produce better segmentation results comparatively.</p><p>For a more fair comparison with other UDA methods, in <ref type="table" target="#tab_2">Table 3</ref>, we show the mIoU gain with respect to specific baselines methods used. Compared to more complex models with very deep backbones, our approaches produces a higher gain of +13.6 points to source model surpassing the existing methods by a minimum margin of 20%. <ref type="figure">Fig.  3</ref> shows some examples of semantic segmentation before and after domain adaptation. As illustrated in the figure, the segmentation results improves significantly with SISC and SISC+PWL based approaches compared to source and CBST-SP methods. SYNTHIA to Cityscapes: SYNTHIA is a more diverse dataset with multiple viewpoints and different spatial constraints compared to GTA-V and Cityscapes. In Table 2, we present the unsupervised adaptation results on Cityscapes validation set when adapted from SYNTHIA. The categories in SYNTHIA and Cityscapes do not fully overlap, so we have selected the common 16 classes as done in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33]</ref> for evaluation. We have also reported the performance (mIoU*) over the 13 common classes as used in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">30]</ref>. With ResNet-38 as baseline netwrok, our proposed SISC based sef-supervised learning method performs superior to existing state-of-the-art methods as shown in <ref type="table" target="#tab_1">Table 2</ref>. Compared to MinEnt <ref type="bibr" target="#b13">[14]</ref> which uses similar entropy minimization technique, our SISC based UDA approach achieves 14.2% gain in mIoU and 13.3% gain in mIoU*. Similarly, compared to CBST presented in <ref type="bibr" target="#b12">[13]</ref>, our SISC based approach gains 4.3% and 4.7% points in mIoU and mIoU* respectively. Our proposed PWL guided UDA approach combined with SISC based self-supervised learning provides 6.0% and 5.1% boost in mIoU and mIoU* respec-tively when compared with CBST. Compared to an ensemble method (adversarial training and self-training) <ref type="bibr" target="#b13">[14]</ref>, our composite UDA method achieves 9.8% and 7.1% gain in mIoU and mIoU* respectively.</p><p>To make a more fair comparison with existing methods, <ref type="table" target="#tab_2">Table 3</ref> shows the baseline, after adaptation, and gain in terms of mIoU*. It is fair to say, that our proposed methods outperforms the existing state-of-the-art methods achieving the gain over baseline with a minimum margin of 16.3%. In <ref type="figure">Fig. 4</ref>, some examples of semantic segmentation before and after UDA are shown. As illustrated, the segmentation results improves significantly with SISC and SISC+PWL based approaches compared to source and CBST methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation experiments</head><p>Relative frequency based pseudo-labels: Besides the adapted methodology in Section 3.2, we also generated pixel classification relative frequency based pseudo-labels. The randomly selected patches like SISC are segmented and recombined in the large output map. A count is made for each pixel with respect to assigned category in each patch, and then relative frequency is calculated. This relative frequency is used as prediction probability and incorporated in pseudo-labels generation. Due to hard decision, the pseudolabels generated were not effective and lead to a decline in the performance. Patch size selection: Our base models for semantic segmentation in both cases are trained on 500 × 500 random patches selected from the whole image randomly. Following that nominal size, we have chosen 512 × 512 as our patch size for pseudo-label generation. We also tried with 256 × 256 patch size but on high resolution Cityscapes images, these small image patches were not contributing. For patch size greater than 512 × 512 there were GPU memory limitations. Similarly, we selected 25, 50 and 100 patches per image randomly for SISC pseudo-labels generation. 25 patches were not enough to capture the high Target Image</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gound Truth</head><p>ResNet-38 <ref type="bibr" target="#b27">[28]</ref> CBST-SP <ref type="bibr" target="#b12">[13]</ref> Ours (SISC) Ours (SISC+PWL) <ref type="figure">Figure 3</ref>. Segmentation results on Cityscapes validation set when adapted from GTA to Cityscapes.</p><p>Target Image Gound Truth ResNet-38 <ref type="bibr" target="#b27">[28]</ref> CBST-SP <ref type="bibr" target="#b12">[13]</ref> Ours (SISC) Ours (SISC+PWL) <ref type="figure">Figure 4</ref>. Segmentation results on Cityscapes validation set when adapted from SYNTHIA to Cityscapes.</p><p>resolution Cityscapes images and 100 patches were taking the process very slow with negligible gain over 50 patches. Therefore, for all experiments, we have chosen 50 random patches per image. Category based image classification loss weight: Since image classification is added as a supporting module to segmentation network, the loss contribution by this module should also be limited. We tried multiple weight factors, and selected λ F cl = 0.025 <ref type="table" target="#tab_3">(Table 4)</ref>. Pseudo-weak-label generation: For category based image classification loss, the PWL are generated from segmentation pseudo-labels. Since it is difficult to set a minimum number of pixels limit for a category to be labeled as present in an image. Therefore, we exploited the category distribution of source datasets and assigned pseudo weak-labels to present categories based on source data distribution. For GTA-V to Cityscapes, we select a category to be labeled as present in an image if, it has more pixels compared to the 5% of mean category pixels of the same category in the source dataset. A detailed comparison along with respective mIoU is shown in <ref type="table" target="#tab_3">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we have proposed, Multi-level self learning strategy (MLSL) for UDA of semantic segmentation by generating pseudo-labels at fine-grain pixel-level and image level, helping identify domain invariant features at both latent and output space. Using a reasonable assumption that labels of objects and stuff should be same regardless of their location, we generate Spatially independent but Semantically Consistent Labels. Image level labels, called pseudo weak-label (PWL) are generated by learning the pixel-wise object size distribution in the source domain images and using it as consistency check over SISC pseudo-labels. Binary cross-entropy loss using PWL enforces latent space to preserve the information about the objects, helping domain adapt for small objects. This multi-level pseudo-label generation for self-supervised learning, allows the network to learn domain-invariant features at different hierarchical levels. The rigorous experimentation demonstrates that the proposed SISC based self-supervised method alone outperforms the existing state-of-the-art algorithms on benchmark datasets: mIoU* improves from 46.2 to 48.7 and 48.4 to 50.8 on GTA-V &amp; SYNTHIA to Cityscapes. This includes both, ones using self-supervision or adversarial learning. Augmented with a PWL based image classification module, our proposed method further improves the performance, especially in the small objects. Effectiveness of SISC and PWL is highlighted by the substantial improvement of mean IOU over the base model, which is significantly more than previous state-of-methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>An illustration of the alternating self-supervised learning method for UDA of semantic segmentation. (a) shows pseudo-label generation and (b) shows segmentation network training on source and target images. (a) and (b) are repeated iteratively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>min wg LST (Is, Ys, It,Ŷt) = Lseg(Is, Ys) +Lseg(It,Ŷt) (3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>− category histogram of target label ℎ : per − category histogram of source data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>(a) Single-inference pseudo-label generation, (b) SISC pseudo-labels generation where, from left to right: patches are extracted randomly, segmented, recombined, normalized and pseudo-labels are generated. (c) shows the semantic segmentation and category-based image classification model, and (d) describes the PWL generation process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>comprises of 3 × 3</head><label>33</label><figDesc>filters with depth of 512 and C (number of classes to segment). At the end the upsampling layer up-scales the output using bi-linear interpolation.Similarly, the image classification part discussed in Section 3.3 is a category (object/stuff) based image classification module augmented with ResNet-38. The image classification module consist of two convolution layers with filters [1 × 1, 3 × 3] with depth 2048 each. A global average pooling (GAP) layer is applied to capture the global nature of the feature map channels. The output of GAP is passed through two fully connected layers of depth 512 and C respectively. Relu activation function is applied except the last layer where sigmoid is used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Semantic segmentation performance of Cityscapes validation set when adapted from SYNTHIA dataset. We present mIoU and mIoU* (13-categories) comparison with existing state-of-the-art methods for Cityscapes validation set.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SYNTHIA → Cityscapes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Appr.</cell><cell>Road</cell><cell>Sidewalk</cell><cell>Building</cell><cell>Wall</cell><cell>Fence</cell><cell>Pole</cell><cell>T. Light</cell><cell>T. Sign</cell><cell>Veg.</cell><cell>Sky</cell><cell>Person</cell><cell>Rider</cell><cell>Car</cell><cell>Bus</cell><cell>M.cycle</cell><cell>Bicycle</cell><cell>mIoU</cell><cell>mIoU*</cell></row><row><cell>ResNet-38 [28]</cell><cell>-</cell><cell cols="18">32.6 21.5 46.5 4.81 0.03 26.5 14.8 13.1 70.8 60.3 56.6 3.5 74.1 20.4 8.9 13.1 29.2 33.6</cell></row><row><cell>Road [21]</cell><cell cols="5">Adv 77.7 30.0 77.5 9.6</cell><cell cols="14">0.3 25.8 10.3 15.6 77.6 79.8 44.5 16.6 67.8 14.5 7.0 23.8 36.2 41.8</cell></row><row><cell>AdaptSetNet [17]</cell><cell cols="8">Adv 81.7 39.1 78.4 11.1 0.3 25.8 6.8</cell><cell cols="11">9.0 79.1 80.8 54.8 21.0 66.8 34.7 13.8 29.9 39.6 45.8</cell></row><row><cell>MinEnt [14]</cell><cell>ST</cell><cell cols="4">73.5 29.2 77.1 7.7</cell><cell cols="14">0.2 27.0 7.1 11.4 76.7 82.1 57.2 21.3 69.4 29.2 12.9 27.9 38.1 44.2</cell></row><row><cell>CLAN [30]</cell><cell cols="4">Adv 81.3 37.0 80.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="10">16.1 13.7 78.2 81.5 53.4 21.2 73.0 32.9 22.6 30.7</cell><cell>-</cell><cell>47.8</cell></row><row><cell>All Structure [27]</cell><cell cols="5">Adv 91.7 53.5 77.1 2.5</cell><cell cols="3">0.2 27.1 6.2</cell><cell cols="11">7.6 78.4 81.2 55.8 19.2 82.3 30.3 17.1 34.3 41.5 48.7</cell></row><row><cell>CBST [13]</cell><cell>ST</cell><cell cols="18">53.6 23.7 75.0 12.5 0.3 36.4 23.5 26.3 84.8 74.7 67.2 17.5 84.5 28.4 15.2 55.8 42.5 48.4</cell></row><row><cell>Ours (SISC)</cell><cell>ST</cell><cell cols="18">73.7 34.4 78.7 13.7 2.9 36.6 28.2 22.3 86.1 76.8 65.3 20.5 81.7 31.4 13.9 47.3 44.4 50.8</cell></row><row><cell>Ours (SISC+PWL)</cell><cell>ST</cell><cell cols="18">59.2 30.2 68.5 22.9 1.0 36.2 32.7 28.3 86.2 75.4 68.6 27.7 82.7 26.3 24.3 52.7 45.2 51.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Performance (mIoU, mIoU*) gain comparison between the GTA-V and SYNTHIA trained source models and the respective adapted models from GTA-V and SYNTHIA to Cityscapes.</figDesc><table><row><cell>Dataset</cell><cell cols="3">GTA → Cityscapes</cell><cell cols="3">SYN → Cityscapes</cell></row><row><cell>Methods</cell><cell>Source</cell><cell>UDA</cell><cell>mIoU</cell><cell>Source</cell><cell>UDA</cell><cell>mIoU*</cell></row><row><cell></cell><cell>only</cell><cell>Algo.</cell><cell>gain</cell><cell>only</cell><cell>Algo.</cell><cell>gain</cell></row><row><cell>FCN in the wild [15]</cell><cell>21.2</cell><cell>27.1</cell><cell>5.9</cell><cell>23.6</cell><cell>25.4</cell><cell>1.8</cell></row><row><cell>Curriculam DA [33]</cell><cell>22.3</cell><cell>28.9</cell><cell>6.6</cell><cell>28.4</cell><cell cols="2">34.82 6.42</cell></row><row><cell>AdaptSetNet [17]</cell><cell>36.6</cell><cell>42.4</cell><cell>5.8</cell><cell>38.6</cell><cell>46.7</cell><cell>8.1</cell></row><row><cell>MinEnt [14]</cell><cell>36.6</cell><cell>42.3</cell><cell>5.7</cell><cell>38.6</cell><cell>44.2</cell><cell>5.6</cell></row><row><cell>CLAN [30]</cell><cell>36.6</cell><cell>43.2</cell><cell>6.6</cell><cell>38.6</cell><cell>47.8</cell><cell>9.2</cell></row><row><cell>All Structure [27]</cell><cell>36.6</cell><cell>45.4</cell><cell>8.8</cell><cell>38.6</cell><cell>48.7</cell><cell>10.1</cell></row><row><cell>CBST [13]</cell><cell>35.4</cell><cell>46.2</cell><cell>10.8</cell><cell>33.6</cell><cell>48.4</cell><cell>14.8</cell></row><row><cell>Ours (SISC)</cell><cell>35.4</cell><cell>48.7</cell><cell>13.3</cell><cell>33.6</cell><cell>50.8</cell><cell>17.2</cell></row><row><cell>Ours (SISC+PWL)</cell><cell>35.4</cell><cell>49</cell><cell>13.6</cell><cell>33.6</cell><cell>51.0</cell><cell>17.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Influence of λF cl and η on overall performance.</figDesc><table><row><cell></cell><cell cols="3">GTA-V → Cityscapes</cell><cell></cell></row><row><cell>λ F cl</cell><cell>0.1</cell><cell>0.05</cell><cell>0.025</cell><cell>0.001</cell></row><row><cell>SISC+PWL</cell><cell>46.0</cell><cell>48.1</cell><cell>49.0</cell><cell>48.24</cell></row><row><cell>η</cell><cell>0.0</cell><cell>0.1</cell><cell>0.05</cell><cell>0.025</cell></row><row><cell>SISC+PWL</cell><cell>45.5</cell><cell>46.0</cell><cell>49.0</cell><cell>47.33</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple high performance approach to semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename><surname>Cycada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmässan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">No more discrimination: Cross city adaptation of road scene segmenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Cheng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2011" to="2020" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Boosting domain adaptation by discovering latent domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Road: Reality oriented adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional adaptation networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6810" to="6818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Domain adaptive faster r-cnn for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A fully convolutional tri-branch network (fctn) for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-C Jay</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3001" to="3005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning from synthetic data: Addressing domain shift for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dlow: Domain flow for adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">All about structure: Adapting structural information across domains for boosting semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui-Po</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Hsiao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Effective use of synthetic data for urban scene semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fatemeh Sadat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Sadegh Aliakbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose M</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="86" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Categorylevel adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1512.01274</idno>
	</analytic>
	<monogr>
		<title level="j">Learn-ingSys Workshop</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
