<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Soft Anchor-Point Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
							<email>chenchez@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Chen</surname></persName>
							<email>fangyic@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
							<email>zhiqians@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
							<email>marioss@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Soft Anchor-Point Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Object detection</term>
					<term>Anchor-point detector</term>
					<term>Soft-weighted an- chor points</term>
					<term>Soft-selected pyramid levels</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, anchor-free detection methods have been through great progress. The major two families, anchor-point detection and keypoint detection, are at opposite edges of the speed-accuracy trade-off, with anchor-point detectors having the speed advantage. In this work, we boost the performance of the anchor-point detector over the keypoint counterparts while maintaining the speed advantage. To achieve this, we formulate the detection problem from the anchor point's perspective and identify ineffective training as the main problem. Our key insight is that anchor points should be optimized jointly as a group both within and across feature pyramid levels. We propose a simple yet effective training strategy with soft-weighted anchor points and soft-selected pyramid levels to address the false attention issue within each pyramid level and the feature selection issue across all the pyramid levels, respectively. To evaluate the effectiveness, we train a single-stage anchor-free detector called Soft Anchor-Point Detector (SAPD). Experiments show that our concise SAPD pushes the envelope of speed/accuracy tradeoff to a new level, outperforming recent state-of-the-art anchor-free and anchor-based detectors. Without bells and whistles, our best model can achieve a single-model single-scale AP of 47.4% on COCO.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, anchor-free object detectors <ref type="bibr" target="#b11">[12,</ref><ref type="bibr">37,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr">36,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b28">29]</ref> have drawn a lot of attention. They don't rely on anchor boxes. Predictions are generated in a point(s)-to-box style. Compared to conventional anchor-based approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b8">9]</ref>, anchor-free detectors have a few advantages in general: 1) no manual tuning of hyperparameters for the anchor configuration; 2) usually simpler architecture of detection head; 3) less training memory cost.</p><p>The anchor-free detectors can be roughly divided into two categories, i.e. anchor-point detection and key-point detection. Anchor-point detectors, such as <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr">37,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29]</ref>, encode and decode object bounding boxes as anchor points with corresponding point-to-boundary distances, where the anchor points are the pixels on the pyramidal feature maps and they are associated with the features at their locations just like the anchor boxes. Key-point detectors, such as <ref type="bibr" target="#b11">[12,</ref><ref type="bibr">36,</ref><ref type="bibr" target="#b5">6]</ref>, predict the locations of key points of the bounding box, e.g. corners, center, or extreme points, using a high-resolution feature map and repeated bottom-up top-down inference <ref type="bibr" target="#b19">[20]</ref>, and group those key points to form a box. Compared to key-point detectors, anchor-point detectors have several advantages: 1) simpler network architecture; 2) faster training and inference speed; 3) potential to benefit from augmentations on feature pyramids <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26]</ref>; 4) flexible feature level selection. However, they cannot be as accurate as key-pointbased methods under the same image scale of testing.</p><p>A natural question to ask is: what hinders a simple anchor-point detector from achieving similar accuracy as key-point detectors? In this work, we push the envelope further: we present Soft Anchor-Point Detector (SAPD), a concise single-stage anchor-point detector with both faster speed and higher accuracy. To achieve this, we formulate the detection problem from the anchor point's perspective and identify ineffective training as the major obstacle impeding anchorpoint detector from exploring more potentials of network power both within and across the feature pyramid levels. Specifically, the conventional training strategy has two overlooked issues, i.e. false attention within each pyramid level and feature selection across all pyramid levels. For anchor points on the same pyramid level, those receiving false attention in training will generate detections with unnecessarily high confidence scores but poor localization during inference, suppressing some anchor points with accurate localization but lower score. This can confuse the post-processing step since high-score detections usually have priority to be kept over the low-score ones in non-maximum suppression, resulting in low AP scores at strict IoU thresholds. For anchor points at the same spatial location across different pyramid levels, their associated features are similar but how much they contribute to the network loss is decided without careful consideration. Current methods make the selection based on ad-hoc heuristics like instance scale and usually limited to a single level per instance. This causes a waste of unselected features.</p><p>These issues motivate us to propose a novel training strategy with two softened optimization techniques, i.e. soft-weighted anchor points and soft-selected pyramid levels. For anchor points on the same pyramid level, we reduce the false attention by reweighting their contributions to the network loss according to their geometrical relation with the instance box. We argue that the more close to the instance boundaries, the harder for anchor points to localize objects precisely due to feature misalignment, the less they should contribute to the network loss. Additionally, we further reweight an anchor point by the instance-dependent "participation" degree of its pyramid level. We implement a light-weight feature selection network to learn the per-level "participation" degrees given the object instances. The feature selection network is jointly optimized with the detector and not involved in detector inference.</p><p>Comprehensive experiments show that the proposed training strategy consistently improves the baseline FSAF [37] module by a large margin without inference slowdown, e.g. 2.1% AP increase on COCO <ref type="bibr" target="#b15">[16]</ref> detection benchmark with ResNet-50 <ref type="bibr" target="#b7">[8]</ref>. The improvements are robust and insensitive to specific hyperparameters and implementations, including advanced feature pyramid designs. With Balanced Feature Pyramid <ref type="bibr" target="#b20">[21]</ref>, our complete detector achieves the best speed-accuracy balance among recent state-of-the-art anchor-free detectors, see <ref type="figure">Figure 1</ref>. We report single-model single-scale speed/accuracy of SAPD with different backbones, and with or without DCN <ref type="bibr" target="#b3">[4]</ref>. The fast variant without DCN outperforms the best key-point detector, CenterNet <ref type="bibr" target="#b5">[6]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Anchor-free detectors Despite the dominance of anchor-based methods, anchorfree detectors are continuously under development. Earlier works like DenseBox <ref type="bibr" target="#b9">[10]</ref> and UnitBox <ref type="bibr" target="#b29">[30]</ref> explore an alternate direction of region proposal. And it has been used in tasks such as scene text detection <ref type="bibr" target="#b33">[34]</ref> and pedestrian detection <ref type="bibr" target="#b18">[19]</ref>. Recent efforts have pushed the anchor-free detection outperforming the anchor-based counterparts. Most of them are single-stage detectors. For instance, CornerNet <ref type="bibr" target="#b11">[12]</ref>, ExtremeNet [36] and CenterNet <ref type="bibr" target="#b5">[6]</ref> reformulate the detection problem as locating several key points of the bounding boxes. FSAF [37], Guided Anchoring <ref type="bibr" target="#b27">[28]</ref>, FCOS <ref type="bibr" target="#b26">[27]</ref> and FoveaBox <ref type="bibr" target="#b10">[11]</ref> encode and decode the bounding boxes as anchor points and point-to-boundary distances. Anchorfree methods can also be in the form of two-stage detectors, such as Guided Anchoring <ref type="bibr" target="#b27">[28]</ref> and RPDet <ref type="bibr" target="#b28">[29]</ref>. Feature selection in detection Modern object detectors often construct the feature pyramid to alleviate the scale variation problem. With multiple levels in the feature pyramid, selecting the suitable feature level for each instance is a crucial problem. Anchor-based methods make the implicit selection by the anchor matching mechanism, which is based on ad-hoc heuristics like scales and aspect ratios. Similarly, most anchor-free approaches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29]</ref> assign the instances according to scale. The FSAF module [37], on the other hand, makes the assignment by choosing the pyramid level with the minimal instance-dependent loss in a dynamic style during training but limited to one level per instance. In two-stage detectors, some methods consider feature selection in the second stage by feature fusion. PANet <ref type="bibr" target="#b16">[17]</ref> proposed the adaptive feature pooling with the element-wise maximize operation. But this requires the input of region proposals for both training and testing, which is not compatible with single-stage methods. Our soft feature selection is designed for single-stage anchor-free methods and can dynamically choose multiple pyramid levels with differentiation. Soft weighting in detection FCOS <ref type="bibr" target="#b26">[27]</ref> predicts the "center-ness" masks and multiplies the confidence scores of anchor points with the masks. But this is in the inference stage so the false attention problem still affects the network training and the extra "center-ness" branches complicate the network architecture. We show that our simple soft-weighting scheme during training is more effective than the "center-ness" masks in the appendix. Previous works doing soft weighting in the training stage include Focal Loss <ref type="bibr" target="#b14">[15]</ref> and Consistent Loss <ref type="bibr" target="#b24">[25]</ref>. They reshape the classification loss but treat all samples independently. Our training strategy is more direct and comprehensive since we reshape the combination of classification and regression loss and consider jointly weighting a group of anchor points spreading both within and across feature pyramid levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Soft Anchor-Point Detector</head><p>In this section, we present our Soft Anchor-Point Detector (SAPD). First, we formulate the detection problem from the anchor point's perspective in the setting of a vanilla anchor-point detector with a simple head architecture (3.1). Then we introduce our novel training strategy ( <ref type="figure" target="#fig_3">Figure 3</ref>) including soft-weighted anchor points (3.2) and soft-selected pyramid levels (3.3) to address the false attention within pyramid level and feature selection across pyramid levels respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Detection Formulation with Anchor Points</head><p>The first anchor-point detector can be traced back to DenseBox <ref type="bibr" target="#b9">[10]</ref>. The recent modern anchor-point detectors are more or less attaching the detection head of DenseBox with additional convolution layers to multiple levels in the feature pyramids. Here we introduce the general concept of a representative in terms of network architecture, supervision targets, and loss functions. Network architecture As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the network consists of a backbone, a feature pyramid, and one detection head per pyramid level, in a fully convolutional style. A pyramid level is denoted as P l where l indicates the level number and it has 1/s l resolution of the input image size W × H. s l is the feature stride and s l = 2 l . A typical range of l is 3 to 7. A detection head has two task-specific subnets, i.e. classification subnet and localization subnet. They both have five 3 × 3 conv layers. The classification subnet predicts the probability of objects at each anchor point location for each of the K object classes. The localization subnet predicts the 4-dimensional class-agnostic distance from each anchor point to the boundaries of a nearby instance if the anchor point is positive (defined next). Supervision targets We first define the concept of anchor points. An anchor point p lij is a pixel on the pyramid level P l located at (i, j) with i = 0, 1, . . . , W/s l − 1 and j = 0, 1, . . . , H/s l − 1. Each p lij has a corresponding im- </p><formula xml:id="formula_0">age space location (X lij , Y lij ) where X lij = s l (i + 0.5) and Y lij = s l (j + 0.5). Next we define the valid box B v of a ground-truth instance box B = (c, x, y, w, h) where c is the class id, (x,</formula><formula xml:id="formula_1">d l = 1 zs l [X lij − (x − w/2)] d t = 1 zs l [Y lij − (y − h/2)] d r = 1 zs l [(x + w/2) − X lij ] d b = 1 zs l [(y + h/2) − Y lij ]<label>(1)</label></formula><p>where z is the normalization scalar. For negative anchor points, their classification targets are background (c = 0), and localization targets are set to null because they don't need to be learned. To this end, we have a classification target c lij and a localization target d lij for all of each anchor point p lij . A visualization of the classification targets and the localization targets of one feature level is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. Loss functions Given the architecture and the definition of anchor points, the network generates a K-dimensional classification outputĉ lij and a 4-dimensional localization outputd lij per anchor point p lij . Focal loss <ref type="bibr" target="#b14">[15]</ref> (l F L ) is adopted for the training of classification subnets to overcome the extreme class imbalance between positive and negative anchor points. IoU loss <ref type="bibr" target="#b29">[30]</ref> (l IoU ) is used for the training of localization subnets. Therefore, the per anchor point loss L lij is calculated as Eq. <ref type="formula" target="#formula_2">(2)</ref>.</p><formula xml:id="formula_2">L lij = l F L (ĉ lij , c lij ) + l IoU (d lij , d lij ), p lij ∈ p + l F L (ĉ lij , c lij ), p lij ∈ p −<label>(2)</label></formula><p>where p + and p − are the sets of positive and negative anchor points respectively. The loss for the whole network is the summation of all anchor point losses divided by the number of positive anchor points (3).  The key insight is the joint optimization of anchor points as a group both within and across feature pyramid levels.</p><formula xml:id="formula_3">L = 1 N p + l ij L lij<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Soft-Weighted Anchor Points</head><p>False attention Under the conventional training strategy, we observe that during inference some anchor points generate detection boxes with poor localization but high confidence score, which suppress the boxes with more precise localization but lower score. As a result, the non-maximum suppression (NMS) tends to keep the poorly localized detections, leading to low AP at a strict IoU threshold. We visualize an example of this observation in <ref type="figure" target="#fig_5">Figure 4</ref> (a). We plot the detection boxes before NMS with confidence scores indicated by the color. The box with more precise localization of the person is suppressed by other boxes not so accurate but having high scores. Then the final detection (bold box) after NMS doesn't have high IoU with the ground-truth. So why this is the case? The conventional training strategy treats anchor points independently in Eq. (3), i.e. they receive equal attention. For a group of anchor points inside B v , their spatial locations and associated features are different. So their abilities to localize B are also different. We argue that anchor points located close to instance boundaries don't have features well aligned with the instance. Their features tend to be hurt by content outside the instance because their receptive fields include too much information from the background, resulting in less representation power for precise localization. Thus, forcing these anchor points to perform as well as those with powerful feature representation is misleading the network. Less attention should be paid to anchor points close to instance boundaries than those surrounding the center in training. In other words, the network should focus more on optimizing the anchor points with powerful feature representation and reduce the false attention to others. Our solution To address the false attention issue, we propose a simple and effective soft-weighting scheme. The basic idea is to assign an attention weight w lij for each anchor point's loss L lij . For each positive anchor point, the weight depends on the distance between its image space location and the corresponding instance boundaries. The closer to a boundary, the more down-weighted the anchor point gets. Thus, anchor points close to boundaries are receiving less attention and the network focuses more on those surrounding the center. For negative anchor points, they are kept unchanged since they are not involved in localization, i.e. their weights are all set to 1. Mathematically, w lij is defined in Eq. <ref type="formula" target="#formula_4">(4)</ref>:</p><formula xml:id="formula_4">w lij = f (p lij , B), ∃B, p lij ∈ B v 1, otherwise<label>(4)</label></formula><p>where f is a function reflecting how close p lij is to the boundaries of B. Closer distance yields less attention weight. We instantiate f using a generalized version of centerness function <ref type="bibr" target="#b26">[27]</ref></p><formula xml:id="formula_5">, i.e. f (p lij , B) = [ min(d l lij ,d r lij ) min(d t lij ,d b lij ) max(d l lij ,d r lij ) max(d t lij ,d b lij ) ] η ,</formula><p>where η controls the decreasing steepness. An illustration of the soft-weighted anchor points is shown in <ref type="figure" target="#fig_3">Figure 3</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Soft-Selected Pyramid Levels</head><p>Feature selection Unlike anchor-based detectors, anchor-free methods don't have constraints from anchor matching to select feature levels for instances from the feature pyramid. In other words, we can assign each instance to arbitrary feature level(s) in anchor-free methods during training. And selecting the right feature levels can make a big difference <ref type="bibr">[37]</ref>. We approach the issue of feature selection by looking into the properties of the feature pyramid. Indeed, feature maps from different pyramid levels are somewhat similar to each other, especially the adjacent levels. We visualize the response of all pyramid levels in <ref type="figure" target="#fig_6">Figure 5</ref>. It turns out that if one level of feature is activated in a certain region, the same regions of adjacent levels may also be activated in a similar style. But the similarity fades as the levels are farther apart. This means that features from more than one pyramid level can participate together in the detection of a particular instance, but the degrees of participation from different levels should be somewhat different.</p><p>Inspired by the above analysis, we argue there should be two principles for proper pyramid level selection. Firstly, the selection should be related to the pattern of feature response, rather than some ad-hoc heuristics. And the instancedependent loss can be a good reflection of whether a pyramid level is suitable for detecting some instances. This principle is also supported by <ref type="bibr">[37]</ref>. Secondly, we should allow features from multiple levels involved in the training and testing for each instance, and each level should make distinct contributions. FoveaBox <ref type="bibr" target="#b10">[11]</ref> has shown that assigning instances to multiple feature levels can improve the performance to some extent. But assigning to too many levels may instead hurt the performance severely. We believe this limitation is caused by the hard selection of pyramid levels. For each instance, the pyramid levels in FoveaBox are either selected or discarded. The selected levels are treated equally no matter how different their feature responses are.</p><p>Therefore, the solution lies in reweighting the pyramid levels for each instance. In other words, a weight is assigned to each pyramid level according to the feature response, making the selection soft. This can also be viewed as assigning a proportion of the instance to a level.   Our solution So how to decide the weight of each pyramid level per instance?</p><p>We propose to train a feature selection network to predict the weights for soft feature selection. The input to the network is instance-dependent feature responses extracted from all the pyramid levels. This is realized by applying the RoIAlign layer <ref type="bibr" target="#b6">[7]</ref> to each pyramid feature followed by concatenation, where the RoI is the instance ground-truth box. Then the extracted feature goes through a feature selection network to output a vector of the probability distribution, as shown in <ref type="figure" target="#fig_7">Figure 6</ref>. We use the probabilities as the weights of soft feature selection.</p><p>There are multiple architecture designs for the feature selection network. For simplicity, we present a light-weight instantiation. It consists of three 3 × 3 conv layers with no padding, each followed by the ReLU function, and a fullyconnected layer with softmax. <ref type="table" target="#tab_3">Table 1</ref> details the architecture. The feature selection network is jointly trained with the detector. Cross entropy loss is used for optimization and the ground-truth is a one-hot vector indicating which pyramid level has minimal loss as defined in the FSAF module <ref type="bibr">[37]</ref>.</p><p>So far, each instance B is associated with a per level weight w B l via the feature selection network. Together with the soft-weighting scheme in Section 3.2, the anchor point loss L lij is down-weighed further if B is assigned to P l and p lij is inside B v . We assign each instance B to topk feature levels with k minimal instance-dependent losses during training. Thus, Eq. (4) is augmented into Eq. <ref type="bibr" target="#b4">(5)</ref>.</p><formula xml:id="formula_6">w lij = w B l f (p lij , B), ∃B, p lij ∈ B v 1, otherwise<label>(5)</label></formula><p>The total loss of the whole model is the weighted sum of anchor point losses plus the classification loss (L select-net ) from the feature selection network, as in Eq. <ref type="bibr" target="#b5">(6)</ref>. <ref type="figure" target="#fig_3">Figure 3</ref> shows the effect of applying soft-selection weights.</p><formula xml:id="formula_7">L = 1 p lij ∈p + w lij lij w lij L lij + λL select-net<label>(6)</label></formula><p>where λ is the hyperparameter that controls the proportion of classification loss L select-net for feature selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>Initialization We follow [37] for the initialization of the detection network. Specifically, the backbone networks are pre-trained on ImageNet1k <ref type="bibr" target="#b4">[5]</ref>. The classification layers in the detection head are initialized with bias − log((1 − π)/π) where π = 0.01, and a Gaussian weight. The localization layers in the detection head are initialized with bias 0.1, and also a Gaussian weight. For the newly added feature selection network, we initialize all layers in it using a Gaussian weight. All the Gaussian weights are filled with σ = 0.01. Optimization The entire detection network and the feature selection network are jointly trained with stochastic gradient descent on 8 GPUs with 2 images per GPU using the COCO train2017 set <ref type="bibr" target="#b15">[16]</ref>. Unless otherwise noted, all models are trained for 12 epochs (∼90k iterations) with an initial learning rate of 0.01, which is divided by 10 at the 9th and the 11th epoch. Horizontal image flipping is the only data augmentation unless otherwise specified. For the first 6 epochs, we don't use the output from the feature selection network. The detection network is trained with the same online feature selection strategy as in the FSAF module [37], i.e. each instance is assigned to only one feature level yielding the minimal loss. We plug in the soft selection weights and choose the topk levels for the second 6 epochs. This is to stabilize the feature selection network first and to make the learning smoother in practice. We use the same training hyperparameters for the shrunk factor = 0.2 and the normalization scalar z = 4.0 as [37]. We set λ = 0.1 although results are robust to the exact value.</p><p>Inference At the time of inference, the network architecture is as simple as in <ref type="figure" target="#fig_1">Figure 2</ref>. The feature selection network is not involved in the inference so the runtime speed is not affected. An image is forwarded through the network in a fully convolutional style. Then classification predictionĉ lij and localization predictiond lij are generated for all each anchor point p lij . Bounding boxes can be decoded using the reverse of Eq. (1). We only decode box predictions from at most 1k top-scoring anchor points in each pyramid level, after thresholding the confidence scores by 0.05. These top predictions from all feature levels are merged, followed by non-maximum suppression with a threshold of 0.5, yielding the final detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct experiments on the COCO <ref type="bibr" target="#b15">[16]</ref> detection track using the MMDetection <ref type="bibr" target="#b1">[2]</ref> codebase. All models are trained on the train2017 split including around 115k images. We analyze our method by ablation studies on the val2017 split containing 5k images. When comparing to the state-of-the-art detectors, we report the Average Precision (AP) scores on the test-dev split.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation Studies</head><p>All results in ablation studies are based on models trained and tested with an image scale of 800 pixels. We study the contribution of each proposed component by gradually applying these components to the baseline FSAF [37] module. For the soft-weighted anchor points and soft-selected pyramid levels, we first study the effect of varying hyperparameters on them and then apply each component with the best hyperparameter to the baseline. We also report more ablative experiments in Appendix A.   Soft-weighted anchor points improve the localization. We first apply the soft-weighting scheme (Eq. (4)) to the training of the baseline FSAF module. Results are reported in <ref type="table" target="#tab_5">Table 2</ref> and 3. Soft-weighted anchor points offer a significant improvement (up to 1.1% AP) over the baseline, being insensitive to various hyperparameters. More importantly, AP 75 is increased by 1.6%, indicating better localization accuracy at a strict IoU threshold. We also visualize the effect of our soft-weighting scheme in <ref type="figure" target="#fig_5">Figure 4(b)</ref>. The precise box (marked as bold) is kept while the other poorly localized boxes are suppressed, reducing the false attention issue effectively.</p><p>Soft-selected pyramid levels utilize the feature power better. Next, we further apply the soft feature selection on top of the soft-weighting scheme, so that each anchor point is down-weighted as in Eq. <ref type="bibr" target="#b4">(5)</ref>. <ref type="table" target="#tab_5">Table 2</ref> and 4 reports the ablative results. We find that as long as each instance is assigned to more than one pyramid level, we can observe robust ∼1.0% absolute AP improvements over the FSAF module plus soft-weighted anchor points. This indicates that allowing instances to optimize multiple pyramid levels is essential to utilize the feature power as much as possible. Empirically, we assign each instance to the top 3 feature levels with the minimal instance-dependent losses according to <ref type="table" target="#tab_7">Table 4</ref>. To understand how does the feature selection network assign instances, we visualize the predicted soft selection weights in <ref type="figure">Figure 7</ref>. It turns out that larger instances tend to be assigned high weights for higher pyramid levels. The majority of instances can be learned with no more than two levels. Very rare instances need to be modeled by more than two levels, e.g. the sofa in the top right sub-figure of <ref type="figure">Figure 7</ref>. This is consistent with the results in <ref type="table" target="#tab_7">Table 4</ref>. <ref type="figure">Fig. 7</ref>: Visualization of the soft feature selection weights from the feature selection network. Weights (the top-left red bars) ranging from 0 to 1 of five pyramid levels (P 3 to P 7 ) are predicted for each instance (blue box). The more filled a red bar is, the higher the weight. Best viewed in color when zoomed in.</p><p>Joint training of the feature selection network has a negligible effect on performance. As shown in <ref type="figure" target="#fig_7">Figure 6</ref>, the feature selection network takes in feature extracted from the shared feature pyramid and is jointly trained with the detector. One may argue that the performance improvement by the soft-selected pyramid levels is due to the multi-task learning of the feature selection network and the detector. We prove this is not the case. We conduct an experiment in which the feature selection network is jointly trained with the detector but its predicted soft selection weights are not used. In other words, the weights for anchor points are still following Eq. (4) and the feature selection strategy is the same as the baseline FSAF module. It turns out the final AP is 37.1%, only 0.1% higher than the 2nd entry and 0.9% lower than the 3rd entry in <ref type="table" target="#tab_5">Table 2</ref>. This means that the major contribution of the soft-selected pyramid levels is actually from softly selecting multiple levels rather than the multi-task learning effect from the feature selection network.</p><p>Our training strategy works well with augmented feature pyramids. Different from key-point detectors that use a single high-resolution feature map, the SAPD can enjoy the merits brought by the advanced designs of feature pyramids. Here we adopt the Balanced Feature Pyramid (BFP) <ref type="bibr" target="#b20">[21]</ref> and achieve further improvement. The BFP pushes our model with ResNet-50 to a 38.8% AP, which is 2.9% higher than the baseline FSAF module. More importantly, our proposed training strategy can robustly work with advanced feature pyramids, offering a steady 2% AP gain (see 4th and 5th entries in <ref type="table" target="#tab_5">Table 2</ref>).</p><p>SAPD is robust and efficient. Our SAPD can consistently provide robust performance using deeper and better backbone networks, while at the same time keeping the detection head as simple as possible. We report the head-to-head  <ref type="table">Table 5</ref>:</p><p>Head-to-head comparisons of anchorbased RetinaNet, anchorbased plus FSAF module, and our purely anchorfree SAPD with different backbone networks on the COCO val2017 set. AB: Anchor-based branches. R: ResNet. X: ResNeXt.</p><p>comparisons with anchor-based RetinaNet and the more complex anchor-based plus FSAF detector in terms of detection accuracy and speed in <ref type="table">Table 5</ref>. Except for the head architectures, all other settings are the same. All detectors run on a single GTX 1080Ti GPU with CUDA 10 using a batch size of 1. It turns out that our SAPD gets both sides of two worlds. Our SAPD with purely anchorfree heads can not only run faster than the anchor-based counterparts due to simpler head architecture, but also outperform the combination of anchor-based and anchor-free heads by significant margins, i.e. 1.6%, 1.7%, and 1.5% absolute AP increases on ResNet-50, ResNet-101, and ResNeXt-101-64x4d backbones respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison to State of the Art</head><p>We evaluate our complete SAPD on the COCO test-dev set to compare with recent state-of-the-art anchor-free and anchor-based detectors. All of our models are trained using scale jitter by randomly scaling the shorter side of images in the range from 640 to 800 and for 2× number of epochs as the models in Section 4.1 with the learning rate change points scaled proportionally. Other settings are the same as Section 4.1.</p><p>For a fair comparison, we report the results of single-model single-scale testing for all methods, as well as their corresponding inference speeds in <ref type="table" target="#tab_10">Table 6</ref>. A visualization of the accuracy-speed trade-off is shown in <ref type="figure">Figure 1</ref>. The inference speeds are measured by Frames-per-Second (FPS) on the same machine with a GTX 1080Ti GPU using a batch size of 1 whenever possible. A "n/a" indicates the case that the method doesn't provide trained models nor self-timing results from the original paper.</p><p>Our proposed SAPD pushes the envelope of accuracy-speed boundary to a new level. We report the results of two series of the backbone models, one without DCN and the other with DCN. Without DCN, our fastest SAPD version based on ResNet-50 can reach a 14.9 FPS while maintaining a 41.7% AP, outperforming some of the methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b20">21]</ref> using ResNet-101. With DCN, our SAPD forms an upper envelope of state-of-the-art anchor-free detectors and some recent anchor-based detectors. The closest competitor, RPDet <ref type="bibr" target="#b28">[29]</ref>, is 1.0%  AP worse and 15ms slower than ours. Compared to key-point anchor-free detectors <ref type="bibr" target="#b5">[6,</ref><ref type="bibr">36,</ref><ref type="bibr" target="#b11">12]</ref> using Hourglass, our SAPD enjoys significantly faster inference speed (up to 5× times) and a 2.5% AP improvement (47.4% vs. 44.9%) over the best key-point detector, CenterNet <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This work studied the anchor-point object detection and discovered the key insight lies in the joint optimization of a group of anchor points both within and across the feature pyramid levels. We proposed a novel training strategy addressing two underexplored issues of anchor-point detection approaches, i.e. the false attention issue within each pyramid level and the feature selection issue across all pyramid levels. Applying our training strategy to a simple anchor-point detector leads to a new upper envelope of the speed-accuracy trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Discussion</head><p>Besides the ablation studies of the main paper, we ask more questions and conduct additional experiments to further understand our proposed training strategy for SAPD. We follow the same experimental setting as in the ablation studies in Section 4.1. All models are using the ResNet-50 [8] backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Soft-Weighting during Training or Testing?</head><p>Previous work like FCOS <ref type="bibr" target="#b26">[27]</ref> applied soft-weighting in testing. Specifically, FCOS predicts the "center-ness" masks from extra network branches and the final score is computed by multiplying the predicted center-ness with the corresponding classification score. Differently, our soft-weighting scheme is applied during the training phase to down-weight anchor points' contribution to the network loss. In other words, FCOS is trained to predict the "center-ness" function but we are using the function to directly reweight the loss of anchor points.  <ref type="table">Table 7</ref>: Performance comparison between soft-weighting the loss during training by soft-weighted anchor points and down-weighting the confidence score during testing by predicted "center-ness". SW: soft-weighted anchor points, CN: center-ness, "on cls.": center-ness branch on the classification branch, "on reg.": center-ness branch on the regression branch.</p><p>For comparison between soft-weighting in training vs. in testing, we implement the "center-ness" mask branches attached to our baseline FSAF module [37] using the official code and optimize them the same way as <ref type="bibr" target="#b26">[27]</ref>. Performances are reported in <ref type="table">Table 7</ref>. Our soft-weighting scheme in training is more effective than the various versions of center-ness weighting in testing. The best version of center-ness improves the AP by 0.6% while our soft-weighting scheme achieves a 1.1% AP gain. We think the reason is that soft-weighting during training is directly addressing the false attention issue, in which anchor points with poorly aligned features for precise localization are down-weighted. But in center-ness weighting, the anchor points are still contributing equally to the network loss, which is forcing all anchor points to perform equally well no matter how good are their feature representations. So the soft-weighting during testing is not fully resolving the false attention issue. This is further verified by the fact that if our soft-weighting scheme is applied on top of the center-ness weighting we can observe another improvement (see 4th and 5th entries in <ref type="table">Table 7</ref>). However, if we compare between 2nd and 5th entries, applying center-ness weighting on top of our soft-weighting scheme is not improving the performance, which indicates that our soft-weighting scheme alone can work well to suppress the poorly localized detections. Therefore, we believe reweighting the anchor loss is more close to the essence of suppressing poorly localized detections than reshaping the confidence score during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Which Loss to Reweight?</head><p>In Eq. (2) of the main paper, the anchor point loss is the summation of the classification focal loss and the localization IoU loss for positive samples. By default, our soft-weighting scheme is applied to both classification and localization losses in the soft-weighted anchor points. In this section, we study the effect of applying our soft-weighting scheme to only the classification (cls) loss or the localization (loc) loss. Results are reported in <ref type="table">Table 8</ref>. If we only reweight the single cls loss or loc loss, the performance becomes even worse than the baseline. The possible reason is that down-weighting a loss causes the network to focus on optimizing the other unweighted loss and the network is biased to be good at a single task. But the detection problem requires the network to be balanced for both proper classification and localization abilities.  <ref type="table">Table 8</ref>: The effect of applying our soft-weighting scheme to only the classification (cls) loss, or only the localization (loc) loss, or the summation of classification and regression loss (cls+loc) for the soft-weighted anchor points. SW: soft-weighed anchor points.</p><p>Therefore, compared to previous detection methods that reweighting only the classification loss <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25]</ref>, our soft-weighting scheme is more comprehensive and balanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Visualization of Feature Selection Network</head><p>We visualize more examples of the soft-selected pyramid levels from the feature selection network in <ref type="figure">Figure 8</ref>. The feature selection network predicts the perlevel "participation" degree for each instance to fully explore the power of feature pyramid and it is agnostic to the instance class, being general for a variety of objects including animals, human, food, vehicle, furniture, etc. Using features from multiple pyramid levels for detection is better than the online feature selection strategy in the FSAF module <ref type="bibr">[37]</ref>, which only chooses a single level to assign the instance when training the network. <ref type="figure">Fig. 8</ref>: More visualization of the soft-selection weights from the feature selection network. Weights (the top-left red bars) ranging from 0 to 1 of five pyramid levels (P 3 to P 7 ) are predicted for each instance (blue box). The more filled a red bar is, the higher the weight is. Best viewed in digital version and zoomed in.</p><p>36. Zhou, X., Zhuo, J., Krahenbuhl, P.: Bottom-up object detection by grouping extreme and center points. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 850-859 (2019) 37. Zhu, C., He, Y., Savvides, M.: Feature selective anchor-free module for singleshot object detection. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2019)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The network architecture of a vanilla anchor-point detector with a simple detection head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>y) is the box center, and w, h are box width and height respectively. B v is a central shrunk box of B, i.e. B v = (c, x, y, w, h), where is the shrunk factor. An anchor point p lij is positive if and only if some instance B is assigned to P l and the image space location (X lij , Y lij ) of p lij is inside B v , otherwise it is a negative anchor point. For a positive anchor point, its classification target is c and localization targets are calculated as the normalized distances d = (d l , d t , d r , d b ) from the anchor point to the left, top, right, bottom boundaries of B respectively (1),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Illustrative overview of our training strategy with soft-weighted anchor points and soft-selected pyramid levels. The black bars indicate the assigned weights of positive anchor points' contribution to the network loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) w/o soft weights (b) w/ soft weights</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>(a) Poorly localized detection boxes with high scores are generated by anchor points receiving false attention. (b) Our soft-weighting scheme effectively improves localization. Box score is indicated by the color bar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Feature responses from P 3 to P 7 . They look similar but the details gradually vanish as the resolution becomes smaller. Selecting a single level per instance causes the waste of network power.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>The weights prediction for softselected pyramid levels. "C" indicates the concatenation operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>η</head><label></label><figDesc>AP AP50 AP75 APS APM APL 0.10 36.8 56.2 39.0 20.6 40.3 48.3 0.50 36.9 55.8 39.4 20.2 40.1 48.7 1.0 37.0 55.8 39.5 20.5 40.1 48.5 2.0 36.6 55.1 39.1 19.8 40.3 47.8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>(45.4% vs. 44.9%) while running about 2× times faster. The accurate variant with DCN forms an upper bound of speed/accuracy trade-offs for recent single-stage and multi-stage detectors, surpassing the accurate TridentNet [13] (47.4% vs. 46.8%) and being more than 3× faster.</figDesc><table><row><cell></cell><cell>48</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fig. 1: Single-model single-</cell></row><row><cell></cell><cell>46</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>scale speed (ms) vs. accuracy</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(AP) on COCO test-dev.</cell></row><row><cell>Overall AP (%)</cell><cell>36 38 40 42 44</cell><cell></cell><cell></cell><cell></cell><cell>Backbones R-50 R-101 X-101-32 X-101-64 R-50-DCN R-101-DCN X-101-32-DCN X-101-64-DCN HG-104</cell><cell></cell><cell></cell><cell cols="2">Methods SAPD (Ours) FreeAnchor CenterNet RPDet FCOS FCOS w/ imprv TridentNet AB+FSAF GA-Faster-RCNN GA-RetinaNet Libra R-CNN Cascade R-CNN ExtremeNet CornerNet FoveaBox RetinaNet Faster R-CNN w/ FPN</cell><cell>We show variants of our SAPD with and without DCN [4]. Without DCN, our fastest version can run up to 5× faster than other meth-ods with comparable accu-racy. With DCN, our SAPD forms an upper envelop of all</cell></row><row><cell></cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250 GPU Time (ms/img) 300</cell><cell>350</cell><cell>400</cell><cell>450</cell><cell>500</cell><cell>recent detectors.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Architecture of the feature selection network. The conv layers have no padding.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Ablative experiments for the SAPD on the COCO val2017. ResNet-50 is the backbone network for all experiments in this table. We study the effect of SW: soft-weighted anchor points, SS: soft-selected pyramid levels, and BFP<ref type="bibr" target="#b20">[21]</ref>: augmented feature pyramids.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Varying η for the generalized centerness function in soft-weighted anchor points.</figDesc><table><row><cell>topk AP AP50 AP75 APS APM APL</cell></row><row><cell>2 37.9 56.9 40.5 21.1 41.0 50.1</cell></row><row><cell>3 38.0 56.9 40.5 21.2 41.2 50.2</cell></row><row><cell>4 37.9 56.9 40.3 21.2 41.1 50.2</cell></row><row><cell>5 37.9 56.8 40.5 21.0 41.1 50.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Varying k for number of se- lected levels in soft-selected pyramid level with η = 1.0.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Single-model and single-scale accuracy and inference speed of SAPD vs. recent state-of-the-art detectors on the COCO test-dev set. FPS is measured on the same machine with a single GTX 1080Ti GPU using the official source code whenever possible. "n/a" means that both trained models and timing results from original papers are not available. R: ResNet. X: ResNeXt. HG: Hourglass. For a visualized comparison, please refer toFigure 1.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Centernet: Object detection with keypoint triplets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bounding box regression with uncertainty for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03797</idno>
		<title level="m">Foveabox: Beyond anchor-based object detector</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scale-aware trident networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">High-level semantic feature detection: A new perspective for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Libra r-cnn: Towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="821" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dsod: Learning deeply supervised object detectors from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1919" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid reconfiguration with consistent loss for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5041" to="5051" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Region proposal by guided anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2965" to="2974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unitbox: An advanced object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM international conference on Multimedia</title>
		<meeting>the 24th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="516" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4203" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Freeanchor: Learning to match anchors for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">M2det: A single-shot object detector based on multi-level feature pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9259" to="9266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">An anchor-free region proposal network for faster r-cnn based text detection approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09003</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
