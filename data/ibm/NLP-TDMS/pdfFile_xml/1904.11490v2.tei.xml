<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RepPoints: Point Set Representation for Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
							<email>yangze@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
							<email>hanhu@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
							<email>wanglw@cis.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RepPoints: Point Set Representation for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern object detectors rely heavily on rectangular bounding boxes, such as anchors, proposals and the final predictions, to represent objects at various recognition stages. The bounding box is convenient to use but provides only a coarse localization of objects and leads to a correspondingly coarse extraction of object features. In this paper, we present RepPoints (representative points), a new finer representation of objects as a set of sample points useful for both localization and recognition. Given ground truth localization and recognition targets for training, RepPoints learn to automatically arrange themselves in a manner that bounds the spatial extent of an object and indicates semantically significant local areas. They furthermore do not require the use of anchors to sample a space of bounding boxes. We show that an anchor-free object detector based on RepPoints can be as effective as the state-of-the-art anchor-based detection methods, with 46.5 AP and 67.4 AP 50 on the COCO test-dev detection benchmark, using ResNet-101 model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection aims to localize objects in an image and provide their class labels. As one of the most fundamental tasks in computer vision, it serves as a key component for many vision applications, including instance segmentation <ref type="bibr" target="#b29">[30]</ref>, human pose analysis <ref type="bibr" target="#b36">[37]</ref>, and visual reasoning <ref type="bibr" target="#b40">[41]</ref>. The significance of the object detection problem together with the rapid development of deep neural networks has led to substantial progress in recent years <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>In the object detection pipeline, bounding boxes, which encompass rectangular areas of an image, serve as the basic element for processing. They describe target locations recognition supervision convert feature extraction + classification localization supervision <ref type="figure">Figure 1</ref>. RepPoints is a new representation for object detection that consists of a set of points which indicate the spatial extent of an object and semantically significant local areas. This representation is learned via weak localization supervision from rectangular ground-truth boxes and implicit recognition feedback. Based on the richer RepPoints representation, we develop an anchor-free object detector that yields improved performance compared to using bounding boxes.</p><p>of objects throughout the stages of an object detector, from anchors and proposals to final predictions. Based on these bounding boxes, features are extracted and used for purposes such as object classification and location refinement. The prevalence of the bounding box representation can partly be attributed to common metrics for object detection performance, which account for the overlap between estimated and ground truth bounding boxes of objects. Another reason lies in its convenience for feature extraction in deep networks, because of its regular shape and the ease of subdividing a rectangular window into a matrix of pooled cells. Though bounding boxes facilitate computation, they provide only a coarse localization of objects that does not conform to an object's shape and pose. Features extracted from the regular cells of a bounding box may thus be heavily influenced by background content or uninformative foreground areas that contain little semantic information. This may result in lower feature quality that degrades classification performance in object detection.</p><p>In this paper, we propose a new representation, called</p><p>RepPoints, that provides more fine-grained localization and facilitates classification. Illustrated in <ref type="figure">Fig. 1</ref>, RepPoints is a set of points that learns to adaptively position themselves over an object in a manner that circumscribes the object's spatial extent and indicates semantically significant local areas. The training of RepPoints is driven jointly by object localization and recognition targets, such that the Rep-Points are tightly bound by the ground-truth bounding box and guide the detector toward correct object classification. This adaptive and differentiable representation can be coherently used across the different stages of a modern object detector, and does not require the use of anchors to sample over a space of bounding boxes. RepPoints differs from existing non-rectangular representations for object detection, which are all built in a bottom-up manner <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b47">48]</ref>. These bottom-up representations identify individual points (e.g., bounding box corners or object extremities) and rely on handcrafted clustering to group them into object models. Their representations furthermore either are still axis-aligned like bounding boxes <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b20">21]</ref> or require ground truth object masks as additional supervision <ref type="bibr" target="#b47">[48]</ref>. In contrast, RepPoints are learned in a top-down fashion from the input image / object features, allowing for end-to-end training and producing fine-grained localization without additional supervision.</p><p>With RepPoints replacing all the conventional bounding box representations in a two-stage object detector, including anchors, proposals and final localization targets, we develop a clean and effective anchor-free object detector, which achieves 42.8 AP and 65.0 AP 50 on the COCO benchmark <ref type="bibr" target="#b25">[26]</ref> without multi-scale training and testing, and achieves 46.5 AP and 67.4 AP 50 with multi-scale training and testing using ResNet-101 model. The proposed object detector not only surpasses all existing anchor-free detectors but also performing on-par with state-of-the-art anchor-based baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Bounding boxes for the object detection problem. The bounding box has long been the dominant form of object representation in the field of object detection. One reason for its prevalence is that a bounding box is convenient to annotate with little ambiguity, while providing sufficiently accurate localization for the subsequent recognition process. This may explain why the major benchmarks all utilize annotations and evaluations based on bounding boxes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b19">20]</ref>. In turn, these benchmarks motivate object detection methods to use the bounding box as their basic representation in order to align with the evaluation protocols.</p><p>Another reason for the dominance of bounding boxes is that almost all image feature extractors, both before <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b4">5]</ref> and during the deep learning era <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b14">15]</ref>, are based on an input patch with a regular grid form. It is thus con-venient to use the bounding box representation to facilitate feature extraction <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Although the proposed RepPoints has an irregular form, we show that it can be amenable to convenient feature extraction. Our system utilizes RepPoints in conjunction with deformable convolution <ref type="bibr" target="#b3">[4]</ref>, which naturally aligns with RepPoints in that it aggregates information from input features at several sample points. Besides, a rectangular pseudo box can be readily generated from RepPoints (see Section 3.2), allowing the new representation to be used with object detection benchmarks.</p><p>Bounding boxes in modern object detectors. The bestperforming object detectors to date generally follow a multistage recognition paradigm <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b26">27]</ref>, and the bounding box representation appears in almost all stages: 1) as pre-defined <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b24">25]</ref> or learnt <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref> anchors that serve as hypotheses over the bounding box space; 2) as refined object proposals connecting successive recognition stages <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b1">2]</ref>; and 3) as the final localization targets.</p><p>The RepPoints can be used to replace the bounding box representations in all stages of model object detectors, resulting in a more effective new object detector. Specifically, the anchors are replaced by center points, which is a special configuration of RepPoints. The bounding box proposals and final localization targets are replaced by the RepPoints proposals and final targets. Note the resulting object detector is anchor-free, due to the use of center points for initial object representation. It is thus even more convenient in use than the bounding box based methods.</p><p>Other representations for object detection. To address limitations of rectangular bounding boxes, there have been some attempts to develop more flexible object representations. These include an elliptic representation for pedestrian detection <ref type="bibr" target="#b21">[22]</ref> and a rotated bounding box to better handle rotational variations <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>Other works aim to represent an object in a bottom-up manner. Early bottom-up representations include DPM <ref type="bibr" target="#b8">[9]</ref> and Poselet <ref type="bibr" target="#b0">[1]</ref>. Recently, bottom-up approaches to object detection have been explored with deep networks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b47">48]</ref>. CornerNet <ref type="bibr" target="#b20">[21]</ref> first predicts top-left and bottom-right corners and then employs a specialized grouping method <ref type="bibr" target="#b27">[28]</ref> to obtain the bounding boxes of objects. However, the two opposing corner points still essentially model a rectangular bounding box. ExtremeNet <ref type="bibr" target="#b47">[48]</ref> is proposed to locate the extreme points of objects in the x-and y-directions <ref type="bibr" target="#b28">[29]</ref> with supervision from ground-truth mask annotations. In general, bottom-up detectors benefit from a smaller hypothesis space (for example, CornerNet and ExtremeNet both detect 2-d points instead of directly detecting a 4-d bounding box) and potentially finer-grained localization. However, they have limitations such as relying on handcrafted clustering or post-processing steps to compose whole ob-jects from the detected points.</p><p>Similar to these bottom-up works, RepPoints is also a flexible object representation. However, the representation is constructed in a top-down manner, without the need for handcrafted clustering steps. RepPoints can automatically learn extreme points and key semantic points without supervision beyond ground-truth bounding boxes, unlike ExtremeNet <ref type="bibr" target="#b47">[48]</ref> where additional mask supervision is required.</p><p>Deformation modeling in object recognition. One of the most fundamental challenges for visual recognition is to recognize objects with various geometric variations. To effectively model such variations, a possible solution is to make use of bottom-up composition of low-level components. Representative detectors along this direction include DPM <ref type="bibr" target="#b8">[9]</ref> and Poselet <ref type="bibr" target="#b0">[1]</ref>. An alternative is to implicitly model the transformations in a top-down manner, where a lightweight neural network block is applied on input features, either globally <ref type="bibr" target="#b17">[18]</ref> or locally <ref type="bibr" target="#b3">[4]</ref>.</p><p>RepPoints is inspired by these works, especially the topdown deformation modeling approach <ref type="bibr" target="#b3">[4]</ref>. The main difference is that we aim at developing a flexible object representation for accurate geometric localization in addition to semantic feature extraction. In contrast, both the deformable convolution and deformable RoI pooling methods are designed to improve feature extraction only. The inability of deformable RoI pooling to learn accurate geometric localization is examined in Section 4 and appendix. In this sense, we expand the usage of adaptive sample points in previous geometric modeling methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b3">4]</ref> to include finer localization of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The RepPoints Representation</head><p>We first review the bounding box representation and its use within multi-stage object detectors. This is followed by a description of RepPoints and its differences from bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Bounding Box Representation</head><p>The bounding box is a 4-d representation encoding the spatial location of an object, B = (x, y, w, h), with x, y denoting the center points and w, h denoting the width and height. Due to its simplicity and convenience in use, modern object detectors heavily rely on bounding boxes for representing objects at various stages of the detection pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Review of Multi-Stage Object Detectors</head><p>The best performing object detectors usually follow a multi-stage recognition paradigm, where object localization is refined stage by stage. The role of the object representation through the steps of this pipeline is as follows: bbox anchors bbox reg.</p><formula xml:id="formula_0">− −−−− → bbox proposals (S1) bbox reg. − −−−− → bbox proposals (S2) ... bbox reg. − −−−− → bbox object targets (1)</formula><p>At the beginning, multiple anchors are hypothesized to cover a range of bounding box scales and aspect ratios. In general, high coverage is obtained through dense anchors over the large 4-d hypothesis space. For instance, 45 anchors per location are utilized in RetinaNet <ref type="bibr" target="#b24">[25]</ref>.</p><p>For an anchor, the image feature at its center point is adopted as the object feature, which is then used to produce a confidence score about whether the anchor is an object or not, as well as the refined bounding box by a bounding box regression process. The refined bounding box is denoted as "bbox proposals (S1)".</p><p>In the second stage, a refined object feature is extracted from the refined bounding box proposal, usually by RoIpooling <ref type="bibr" target="#b9">[10]</ref> or RoI-Align <ref type="bibr" target="#b12">[13]</ref>. For the two-stage framework <ref type="bibr" target="#b32">[33]</ref>, the refined feature will produce the final bounding box target by bounding box regression. For the multistage approach <ref type="bibr" target="#b1">[2]</ref>, the refined feature is used to generate intermediate refined bounding box proposals (S2), also by bounding box regression. This step can be iterated multiple times before producing the final bounding box target.</p><p>In this framework, bounding box regression plays a central role in progressively refining object localization and object features. We formulate the process of bounding box regression in the following paragraph.</p><p>Bounding Box Regression Conventionally, a 4-d regression vector (∆x p , ∆y p , ∆w p , ∆h p ) is predicted to map the current bounding box proposal B p = (x p , y p , w p , h p ) into a refined bounding box B r , where</p><formula xml:id="formula_1">B r = (x p + w p ∆x p , y p + h p ∆y p , w p e ∆wp , h p e ∆hp ). (2)</formula><p>Given the ground truth bounding box of an object B t = (x t , y t , w t , h t ), the goal of bounding box regression is to have B r and B t as close as possible. Specifically, in the training of an object detector, we use the distance between the predicted 4-d regression vector and the expected 4-d regression vectorF(B p , B t ) as the learning target, using a smooth l 1 loss:</p><formula xml:id="formula_2">F(B p , B t ) = ( x t − x p w p , y t − y p h p , log w t w p , log h t h p ). (3)</formula><p>This bounding box regression process is widely used in existing object detection methods. It performs well in practice when the required refinement is small, but it tends to perform poorly when there is large distance between the initial representation and the target. Another issue lies in the scale difference between ∆x, ∆y and ∆w, ∆h, which requires tuning of their loss weights for optimal performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">RepPoints</head><p>As previously discussed, the 4-d bounding box is a coarse representation of object location. The bounding box representation considers only the rectangular spatial scope of an object, and does not account for shape and pose and the positions of semantically important local areas, which could be used toward finer localization and better object feature extraction.</p><p>To overcome the above limitations, RepPoints instead models a set of adaptive sample points:</p><formula xml:id="formula_3">R = {(x k , y k )} n k=1 ,<label>(4)</label></formula><p>where n is the total number of sample points used in the representation. In our work, n is set to 9 by default.</p><p>RepPoints refinement Progressively refining the bounding box localization and feature extraction is important for the success of multi-stage object detection methods. For RepPoints, the refinement can be expressed simply as</p><formula xml:id="formula_4">R r = {(x k + ∆x k , y k + ∆y k )} n k=1 ,<label>(5)</label></formula><p>where {(∆x k , ∆y k )} n k=1 are the predicted offsets of the new sample points with respect to the old ones. We note that this refinement does not face the problem of scale differences among the bounding box regression parameters, since the offsets are at the same scale in the refinement process of RepPoints.</p><p>Converting RepPoints to bounding box To take advantage of bounding box annotations in the training of Rep-Points, as well as to evaluate RepPoint-based object detectors, a method is needed for converting RepPoints into a bounding box. We perform this conversion using a predefined converting function T : R P → B P , where R P denotes the RepPoints for object P and T (R P ) represents a pseudo box.</p><p>Three converting functions are considered for this purpose: These functions are all differentiable, enabling end-to-end learning when inserted into an object detection system. In our experiments, we found them to work comparably well.</p><formula xml:id="formula_5">• T = T 1 : Min-max</formula><p>Learning RepPoints The learning of RepPoints is driven by both an object localization loss and an object recognition loss. To compute the object localization loss, we first convert RepPoints into a pseudo box using the previously discussed transformation function T . Then, the difference between the converted pseudo box and the groundtruth bounding box is computed. In our system, we use the smooth l 1 distance between the top-left and bottom-right points to represent the localization loss. This smooth l 1 distance does not require the tuning of different loss weights as done in computing the distance between bounding box regression vectors (i.e., for ∆x, ∆y and ∆w, ∆h). <ref type="figure" target="#fig_3">Figure 4</ref> indicates that when the training is driven by this combination of object localization and object recognition losses, the extreme points and semantic key points of objects are automatically learned (T 1 is used in transforming RepPoints to pseudo box).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RPDet: an Anchor Free Detector</head><p>We design an anchor-free object detector that utilizes RepPoints in place of bounding boxes as its basic representation. Within a multi-stage pipeline, the object representation evolves as follows: object centers RP refine − −−−− → RepPoints proposals (S1) RP refine − −−−− → RepPoints proposals (S2) ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RP refine</head><p>− −−−− → RepPoints object targets (6) Our RepPoints Detector (RPDet) is constructed with two recognition stages based on deformable convolution, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. Deformable convolution pairs nicely with RepPoints, as its convolutions are computed on an irregularly distributed set of sample points and conversely its recognition feedback can guide training for the positioning of these points. In this section, we present the design of RPDet and discuss its relationship to and differences from existing object detectors.</p><p>Center point based initial object representation. While predefined anchors dominate the representation of objects in the initial stage of object detection, we follow YOLO <ref type="bibr" target="#b30">[31]</ref> and DenseBox <ref type="bibr" target="#b16">[17]</ref> by using center points as the initial representation of objects, which leads to an anchor-free object detector.</p><p>An important benefit of the center point representation lies in its much tighter hypothesis space compared to the anchor based counterparts. While anchor based approaches usually rely on a large number of multi-ratio and multi-scale anchors to ensure dense coverage of the large 4-d bounding box hypothesis space, a center point based approach can more easily cover its 2-d space. In fact, all objects will have center points located within the image. However, the center point based method also faces the problem of recognition target ambiguity, caused by two different objects locating at the same position in a feature map, which limits its prevalence in modern object detectors. In previous methods <ref type="bibr" target="#b30">[31]</ref>, this is mainly addressed by producing multiple targets at each position, which faces another issue of vesting ambiguity 1 . In RPDet, we show that this issue can be greatly alleviated by using the FPN structure <ref type="bibr" target="#b23">[24]</ref> for the following reasons: first, objects of different scales will be assigned to different image feature levels, which addresses objects of different scales and the same center points locations; second, FPN has a high-resolution feature map for small objects, which also reduces the chance of two objects having centers located at the same feature position. In fact, we observe that only 1.1% of objects in the COCO datasets <ref type="bibr" target="#b25">[26]</ref> suffer from the issue of center points located at the same position when FPN is used.</p><p>It is worth noting that the center point representation can be viewed as a special RepPoints configuration, where only a single fixed sample point is used, thus maintaining a coherent representation throughout the proposed detection framework. <ref type="figure" target="#fig_1">Figure 2</ref>, Rep-Points serve as the basic object representation throughout our detection system. Starting from the center points, the first set of RepPoints is obtained via regressing offsets over the center points. The learning of these RepPoints is driven by two objectives: 1) the top-left and bottom-right points distance loss between the induced pseudo box and the ground-truth bounding box; 2) the object recognition loss of the subsequent stage. As illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>, extreme <ref type="bibr" target="#b0">1</ref> If the center points of multiple ground truth objects are located at a same feature map position, only one randomly chosen ground truth object is assigned to be the target of this position.  and key points are automatically learned. The second set of RepPoints represents the final object localization, which is refined from the first set of RepPoints by Eq. (5). Driven by the points distance loss alone, this second set of RepPoints aims to learn finer object localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Utilization of RepPoints. As shown in</head><p>Relation to deformable RoI pooling <ref type="bibr" target="#b3">[4]</ref>. As mentioned in Section 2, deformable RoI pooling plays a different role in object detection compared to the proposed RepPoints. Basically, RepPoints is a geometric representation of objects, reflecting more accurate semantic localization, while deformable RoI pooling is geared towards learning stronger appearance features of objects. In fact, deformable RoI pooling cannot learn sample points representing accurate localization of objects (please see the appendix for a proof). We also note that deformable RoI pooling can be complementary to RepPoints, as indicated in <ref type="table" target="#tab_4">Table 6</ref>.</p><p>Backbone and head architectures Our FPN backbone follows <ref type="bibr" target="#b24">[25]</ref>, which produces 5 feature pyramid levels from stage 3 (downsampling ratio of 8) to stage 7 (downsampling ratio of 128).</p><p>The head architecture is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. There are two non-shared subnets, aiming at localization (RepPoints generation) and classification, respectively. The localization subnet first apply three 256-d 3 × 3 conv layers, fol-lowed by two successive small networks to compute offsets for the two sets of RepPoints. The classification subnetwork also apply three 256-d 3 × 3 conv layers, followed by a 256-d 3 × 3 deformable conv layer with its input offset field shared with that of the first deformable conv layer in the localization subnetwork. The group normalization layer is applied after each of the first three 256-d 3×3 conv layers in both subnets.</p><p>Note although our approach utilizes two stages of localization, it is even more efficient than the one stage RetinaNet <ref type="bibr" target="#b24">[25]</ref> detector (210.9 vs. 234.5 GFLOPS using ResNet-50). The additional localization stage introduces little overhead due to layer sharing. The anchor-free design reduces the burden of the final classification layer which leads to a slight reduction in computation.</p><p>Localization/class target assignment. There are two localization stages: generating the first set of RepPoints by refining from the object center point hypothesis (feature map bins); generating the second set of RepPoints by refining from the first RepPoints set. For both stages, only positive object hypothesis are assigned with localization (Rep-Points) targets in training. For the first localization stage, a feature map bin is positive if 1) the pyramidal level of this feature map bin equals the log scale of a ground-truth object s(B) = log 2 ( √ w B h B /4) ; 2) the projection of this ground-truth object's center point locate within this feature map bin. For the second localization stage, the first Rep-Points is positive if its induced pseudo box has sufficient overlap with a ground-truth object that their intersectionover-union is larger than 0.5.</p><p>Classification is conducted on the first set of RepPoints only. The classification assignment criterion follows <ref type="bibr" target="#b24">[25]</ref> that IoU (between the induced pseudo box and ground-truth bounding box) larger than 0.5 indicates positive, smaller than 0.4 indicates background, and otherwise ignored. Focal loss is employed for classification training <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Settings</head><p>We present experimental results of our proposed RPDet framework on the MS-COCO <ref type="bibr" target="#b25">[26]</ref> detection benchmark, which contains 118k images for training, 5k images for validation (minival) and 20k images for testing (test-dev). All the ablation studies are conducted on minival with ResNet-50 <ref type="bibr" target="#b14">[15]</ref>, if not otherwise specified. The state-of-the-art comparison is reported on test-dev in <ref type="table">Table 7</ref>.</p><p>Our detector is trained with synchronized stochastic gradient descent (SGD) over 4 GPUs with a total of 8 images per minibatch (2 images per GPU). The ImageNet <ref type="bibr" target="#b5">[6]</ref> pretrained model was used for initialization. Our learning rate schedule follows the '1x' setting <ref type="bibr" target="#b11">[12]</ref>. Random horizon- tal image flipping is adopted in training. In inference, NMS (σ = 0.5) is employed to post-process the results, following <ref type="bibr" target="#b24">[25]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Study</head><p>RepPoints vs. bounding box. To demonstrate the effectiveness of the proposed RepPoints, we compare the proposed RPDet to a baseline detector where RepPoints are all replaced by the regular bounding box representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline detector based on bounding box representations.</head><p>A single anchor with scale of 4 and aspect ratio of 1 : 1 is adopted for the initial object representation, where the anchor box is positive if the IoU with a ground-truth object is larger than 0.5. The two sets of RepPoints are replaced by bounding box representation, where the geometric refinement is achieved by the standard bounding box regression method, and the feature extraction is replaced by the RoIAlign <ref type="bibr" target="#b12">[13]</ref> method using 3 × 3 grid points 2 . All other settings are the same as in the proposed RPDet method. <ref type="table">Table 1</ref> shows the comparison of two detectors. While the bounding box based method achieves 36.2 mAP, indicating a strong baseline detector, the change of object representation from bounding box to RepPoints brings a +2.1 mAP improvement using ResNet-50 <ref type="bibr" target="#b14">[15]</ref> and a +2.0 mAP improvement using a ResNet-101 <ref type="bibr" target="#b14">[15]</ref> backbone, demonstrating the advantage of the RepPoints representation over bounding boxes for object detection.</p><p>Supervision source for RepPoints learning. RPDet uses both an object localization loss and an object recognition loss (gradient from later recognition) to drive the learning of <ref type="bibr" target="#b1">2</ref> It can be also implemented by a deformable convolution operator with an unlearnable input offset field induced by the 3 × 3 grid points.  the first set of RepPoints, which represents the object proposals of the first stage. <ref type="table">Table 2</ref> ablates the use of these two supervision sources in the learning of the object representations. As mentioned before, describing the geometric localization of objects is an important duty of a representation method. Without the object localization loss, it is hard for a representation method to accomplish this duty, as it results in significant performance degradation of the object detectors. For RepPoints, we observe a huge drop of 4.5 mAP by removing the object localization supervision, showing the importance of describing the geometric localization for an object representation method. <ref type="table">Table 2</ref> also demonstrates the benefit of inluding the object recognition loss in learning RepPoints (+0.7 mAP). The use of the object recognition loss can drive the RepPoints to locate themselves at semantically meaningful positions on an object, which leads to fine-grained localization and improves object feature extraction for the following recognition stage. Note that the object recognition feedback cannot benefit object detection with the bounding box representation (see the first block in <ref type="table">Table 2</ref>), further demonstrating the advantage of RepPoints in flexible object representation.</p><p>Anchor-free vs. anchor-based. We first compare the center point based method (a special RepPoints configuration) and the prevalent anchor based method in representing initial object hypotheses, in <ref type="table">Table 3</ref>. For both detectors us-Backbone Anchor-Free AP AP 50 AP 75 AP S AP M AP L Faster R-CNN w. FPN <ref type="bibr" target="#b23">[24]</ref> ResNet-101 36.  <ref type="table">Table 7</ref>. Comparison of the proposed RPDet with the previous state-of-the-art detectors on COCO <ref type="bibr" target="#b25">[26]</ref> test-dev. The proposed RPDet can achieve 46.5 AP, significantly surpasses all other detectors. Also note the AP50 of RPDet is relatively high, which is believed as a better metric in many applications <ref type="bibr" target="#b31">[32]</ref>. "ms" indicates multi-scale.</p><p>ing bounding boxes and RepPoints, the center point based method surpass the anchor based method by +1.1 mAP and +1.4 mAP, respectively, likely because of its better coverage of ground-truth objects.</p><p>We also compare the proposed anchor-free detector based on RepPoints to RetinaNet <ref type="bibr" target="#b24">[25]</ref> (a popular one-stage anchor-based method), FPN <ref type="bibr" target="#b23">[24]</ref> with RoIAlign (a popular two-stage anchor-based method) <ref type="bibr" target="#b12">[13]</ref>, and a YOLOlike detector which is adapted from the anchor-free method of YOLOv1 <ref type="bibr" target="#b30">[31]</ref>, in <ref type="table">Table 4</ref>. The proposed method outperforms both RetinaNet <ref type="bibr" target="#b24">[25]</ref> and the FPN <ref type="bibr" target="#b23">[24]</ref> methods, which utilize multiple anchors per scale and sophisticated anchor configurations (FPN). The proposed method also significantly surpasses another anchor-free method (the YOLO-like detector) by +4.4 mAP and +4.1 mAP, respectively, probably due to the flexible RepPoints representation and its effective refinement.</p><p>Converting RepPoints to pseudo box. <ref type="table" target="#tab_2">Table 5</ref> shows that different instantiations of the transformation functions T presented in Section 3.2 work comparably well.</p><p>RepPoints act complementary to deformable RoI pooling <ref type="bibr" target="#b3">[4]</ref>. <ref type="table" target="#tab_4">Table 6</ref> shows the effect of applying the deformable RoI pooling layer <ref type="bibr" target="#b3">[4]</ref> to both bounding box proposals and RepPoints proposals. While applying the deformable RoI pooling layer to bounding box proposals brings a +0.7 mAP gain, applying it to RepPoints proposals also brings a +0.8 mAP gain, implying that the roles of deformable RoI pooling and the proposed RepPoints are complementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">RepPoints Visualization</head><p>In <ref type="figure" target="#fig_3">Figure 4</ref>, we visualize the learned RepPoints and the corresponding detection results on several examples from the COCO <ref type="bibr" target="#b25">[26]</ref> minival set. It can be observed that Rep-Points tend to be located at extreme points or key semantic points of objects. These point distributed over objects are automatically learned without explicit supervision. The visualized results also indicate that the proposed RPDet, implemented here with the min-max transformation function, can effectively detect tiny objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">State-of-the-art Comparison</head><p>We compare RPDet with state-of-the-art detectors on test-dev. <ref type="table">Table 7</ref> shows the results. Without any bells and whistles, RPDet achieves 42.8 AP on COCO benchmark <ref type="bibr" target="#b25">[26]</ref>, which is on-par with 4-stage anchor-based Cascade R-CNN <ref type="bibr" target="#b1">[2]</ref> and outperforms all existing anchor-free approaches. By adopting multi-scale training and testing (see Appendix for details), RPDet can achieve 46.5 AP, significantly surpassing all other detectors. Also note the AP 50 of RPDet is relatively high, which is believed as a better metric in many applications <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose RepPoints, a representation for object detection that models fine-grained localization information and identifies local areas significant for object classification. Based on RepPoints, we develop an object detector called RPDet that achieves competitive object detection performance without the need of anchors. Learning richer and more natural object representations like RepPoints is a direction that holds much promise for object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1. Relationship between RepPoints and Deformable RoI pooling</head><p>In this section, we explain the differences between our method and deformable RoI pooling <ref type="bibr" target="#b3">[4]</ref> in greater detail. We first describe the translation sensitivity of the regression step in the object detection pipeline. Then, we discuss how deformable RoI pooling <ref type="bibr" target="#b3">[4]</ref> works and why it does not provide a geometric representation of objects, unlike the proposed RepPoints representation.</p><p>Translation Sensitivity We explain the translation sensitivity of the regression step in the context of bounding boxes. Denote a rectangular bounding box proposal before regression as B P and the ground-truth bounding box as B GT . The target for bounding box regression can then be expressed as</p><formula xml:id="formula_6">T P = F(B P , B GT ),<label>(7)</label></formula><p>where F is a function for transforming B P to B GT . This transformation is conventionally learned as a regression function R B :</p><formula xml:id="formula_7">R B (P B (I, B P )) = T P = F(B P , B GT ),<label>(8)</label></formula><p>where I is the input image and P B is a pooling function defined over the rectangular proposal, e.g., direct cropping of the image <ref type="bibr" target="#b10">[11]</ref>, RoIPooling <ref type="bibr" target="#b32">[33]</ref>, or RoIAlign <ref type="bibr" target="#b12">[13]</ref>. This formulation aims to predict the relative displacement to the ground truth box based on features within the area of B P . Shifts in B P should change the target accordingly:</p><formula xml:id="formula_8">R B (P B (I, B P + ∆B)) = F(B P + ∆B, B GT ).<label>(9)</label></formula><p>Thus, the pooled feature P B (I, B P ) should be sensitive to the box proposal B P . Specifically, for any pair of proposals B 1 = B 2 , we should have P B (I, B 1 ) = P B (I, B 2 ). Most existing feature extractors P B satisfy this property. Note that the improvement of RoIAlign <ref type="bibr" target="#b12">[13]</ref> over RoIPooling <ref type="bibr" target="#b32">[33]</ref> is partly due to this guaranteed translation sensitivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of Deformable RoI Pooling. For deformable</head><p>RoI pooling <ref type="bibr" target="#b3">[4]</ref>, the system generates a pointwise deformation of samples on a regular grid <ref type="bibr" target="#b12">[13]</ref> to produce a set of sample points S P for each proposal. This can be formulated as</p><formula xml:id="formula_9">S P = D(I, B P ),<label>(10)</label></formula><p>where D is the function for generating the sample points. Then, bounding box regression aims to learn a regression function R S which utilizes the sampled features via S P to predict the target T P as follows: <ref type="figure">Figure 5</ref>. Illustration that deformable RoI pooling <ref type="bibr" target="#b3">[4]</ref> is unable to serve as a geometric object representation, as discussed in Section 4 in the main paper. We consider two bounding box regressions based on different proposals. Assume that deformable RoI pooling <ref type="bibr" target="#b3">[4]</ref> can learn a similar geometric object representation where the two sets of sample points lie at similar locations over the object of interest. For that to happen, the sampled features would need to be similar, such that the two proposals cannot be differentiated. However, deformable RoI pooling <ref type="bibr" target="#b3">[4]</ref> can indeed differentiate nearby object proposals, leading to a contradiction. Thus, it is concluded that deformable RoI pooling <ref type="bibr" target="#b3">[4]</ref> cannot learn the geometric representation of objects. <ref type="figure">Figure 6</ref>. Visualization of the learned sample points of 3×3 deformable RoI pooling <ref type="bibr" target="#b3">[4]</ref>. It is shown that the scale of sample points changes as the scale of the proposal changes, indicating that the sample points do not adapt to form a geometric object representation.</p><formula xml:id="formula_10">R S (P S (I, S P )) = T P = F(B P , B GT )<label>(11)</label></formula><p>where P S is the pooling function with respect to the sample points S P . From the translation sensitivity property, we have P S (I, D(I, B 1 )) = P S (I, D(I, B 2 )), ∀B 1 = B 2 . Because the pooled feature P S <ref type="figure">(I, D(I, B)</ref>) is determined by the locations of sample points D(I, B), we have D(I, B 1 ) = D(I, B 2 ), ∀B 1 = B 2 . This means that for two different proposals B 1 and B 2 of the same object, the sample points of these two proposals by deformable RoI pooling should be different. Hence, the sample points of different proposals cannot correspond to the geometry of the same object. They represent a property of the proposals rather than the geometry of the object. <ref type="figure">Figure 5</ref> illustrates the contradiction that arises if deformable RoI pooling were a representation of object ge-  <ref type="table">Table 8</ref>. Benchmark results of RPDet on MS-COCO <ref type="bibr" target="#b25">[26]</ref> validation set (minival). All the models here are trained with FPN <ref type="bibr" target="#b23">[24]</ref> under the '2x' setting <ref type="bibr" target="#b11">[12]</ref>. For the backbone notation, 'R-50' and 'R-101' denotes ResNet-50 and ResNet-101 <ref type="bibr" target="#b14">[15]</ref> respectively. 'R-101-DCN' denotes ResNet-101 with all convolution layers substituted with deformable convolution layers <ref type="bibr" target="#b3">[4]</ref>. 'X' denotes the ResNeXt-101 <ref type="bibr" target="#b42">[43]</ref> backbone. "ms" indicates multi-scale.</p><p>ometry. Moreover, <ref type="figure">Figure 6</ref> illustrates that, for the learned sample points of two proposals for the same object by deformable RoI pooling, the sample points represent a property of the proposals instead of the geometry of the object.</p><p>RepPoints In contrast to deformable RoI pooling where the pooled features represent the original bounding box proposals, the features extracted from RepPoints localize the object. As it is not restricted by translation sensitivity requirements, RepPoints can learn a geometric representation of objects when localization supervision on the corresponding pseudo box is provided (see <ref type="figure" target="#fig_3">Figure 4</ref> in the main paper). While object localization supervision is not applied on the sample points of deformable RoI pooling, we show in Table 2 in the main paper that such supervision is crucial for RepPoints. It is worth noting that deformable RoI pooling <ref type="bibr" target="#b3">[4]</ref> is shown to be complementary to the RepPoints representation (see <ref type="table" target="#tab_4">Table 6</ref> in the main paper), further indicating their different functionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2. More Benchmark Results for RPDet</head><p>We present more benchmark results of our proposed detector RPDet in <ref type="table">Table 8</ref>. Our PyTorch implementation is available at https://github.com/microsoft/RepPoints. All models were tested on MS-COCO <ref type="bibr" target="#b25">[26]</ref> validation set (minival). Multi-scale training and test settings. In multi-scale training, for each mini-batch, the shorter side is randomly selected from a range of <ref type="bibr">[480,</ref><ref type="bibr">960]</ref>. In multi-scale test-ing, we first resize each image to a shorter side of {400, 600, 800, 1000, 1200, 1400}. Then the detection results (before NMS) from all scales are merged, followed by a NMS step to produce the final detection results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>function. Min-max operation over both axes are performed over the RepPoints to determine B p , equivalent to the bounding box over the sample points. • T = T 2 : Partial min-max function. Min-max operation over a subset of the sample points is performed over both axes to obtain the rectangular box B p . • T = T 3 : Moment-based function. The mean value and the standard deviation of the RepPoints is used to compute the center point and scale of the rectangular box B p , where the scale is multiplied by globallyshared learnable multipliers λ x and λ y .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the proposed RPDet (RepPoints detector). While feature pyramidal networks (FPN)<ref type="bibr" target="#b23">[24]</ref> are adopted as the backbone, we only draw the afterwards pipeline of one scale of FPN feature maps for clear illustration. Note all scales of FPN feature maps share the same afterwards network architecture and the same model weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>+Figure 3 .</head><label>3</label><figDesc>The head architecture of RPDet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of the learnt RepPoints and the induced bounding boxes on several examples from the COCO [26] minival set (using the pseudo box converting function T1). In general, the learned RepPoints are located on extreme or semantic keypoints of objects. pseudo box converting function AP AP 50 AP 75 T = T 1 : min-max 38.2 59.7 40.7 T = T 2 : partial min-max 38.1 59.6 40.5 T = T 3 : moment-based 38.3 60.0 41.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .Table 3 .</head><label>123</label><figDesc>RepresentationBackbone AP AP 50 AP 75 Comparison of the RepPoints and bounding box representations in object detection. The network structures are the same except for processing the given object representation. Ablation of the supervision sources, for both bounding box and RepPoints based object detection. "loc." indicates the object localization loss. "rec." indicates the object recognition loss from the next detection stage. Comparison of using a single anchor and a center point as the initial object representation ("init repr."). "other repr." indicates representation method for object proposals and final targets.</figDesc><table><row><cell>Bounding box</cell><cell></cell><cell cols="3">ResNet-50 36.2 57.3</cell><cell>39.8</cell></row><row><cell cols="5">RepPoints (ours) ResNet-50 38.3 60.0</cell><cell>41.1</cell></row><row><cell>Bounding box</cell><cell></cell><cell cols="3">ResNet-101 38.4 59.9</cell><cell>42.4</cell></row><row><cell cols="5">RepPoints (ours) ResNet-101 40.4 62.0</cell><cell>43.6</cell></row><row><cell>Representation</cell><cell></cell><cell cols="3">Supervision AP AP 50 AP 75 loc. rec.</cell></row><row><cell>bounding box</cell><cell></cell><cell cols="3">36.2 57.3 36.2 57.5</cell><cell>39.8 39.8</cell></row><row><cell></cell><cell></cell><cell cols="3">33.8 54.3</cell><cell>35.8</cell></row><row><cell>RepPoints</cell><cell></cell><cell cols="3">37.6 59.4</cell><cell>40.4</cell></row><row><cell></cell><cell></cell><cell cols="3">38.3 60.0</cell><cell>41.1</cell></row><row><cell>other repr.</cell><cell></cell><cell>init repr.</cell><cell cols="2">AP AP 50 AP 75</cell></row><row><cell>bounding box</cell><cell cols="4">single anchor 36.2 57.3 center point 37.3 58.0</cell><cell>39.8 40.0</cell></row><row><cell>RepPoints</cell><cell cols="4">single anchor 36.9 58.2 center point 38.3 60.0</cell><cell>39.7 41.1</cell></row><row><cell>method</cell><cell></cell><cell>backbone</cell><cell></cell><cell># anchors per scale</cell><cell>AP</cell></row><row><cell cols="2">RetinaNet [25]</cell><cell cols="2">ResNet-50</cell><cell>3 × 3</cell><cell>35.7</cell></row><row><cell cols="4">FPN-RoIAlign [24] ResNet-50</cell><cell>3 × 1</cell><cell>36.7</cell></row><row><cell>YOLO-like</cell><cell></cell><cell cols="2">ResNet-50</cell><cell>-</cell><cell>33.9</cell></row><row><cell>RPDet (ours)</cell><cell></cell><cell cols="2">ResNet-50</cell><cell>-</cell><cell>38.3</cell></row><row><cell cols="2">RetinaNet [25]</cell><cell cols="2">ResNet-101</cell><cell>3 × 3</cell><cell>37.8</cell></row><row><cell cols="4">FPN-RoIAlign [24] ResNet-101</cell><cell>3 × 1</cell><cell>39.4</cell></row><row><cell>YOLO-like</cell><cell></cell><cell cols="2">ResNet-101</cell><cell>-</cell><cell>36.3</cell></row><row><cell>RPDet (ours)</cell><cell></cell><cell cols="2">ResNet-101</cell><cell>-</cell><cell>40.4</cell></row><row><cell cols="5">Table 4. Comparison of the proposed method (RPDet) with</cell></row><row><cell cols="5">anchor-based methods (RetinaNet, FPN-RoIAlign) and an anchor-</cell></row><row><cell cols="5">free method (YOLO-like). The YOLO-like method is adapted</cell></row><row><cell cols="5">from the YOLOv1 method [31] by additionally introducing FPN</cell></row><row><cell cols="5">[24], GN [42] and focal loss [25] for better accuracy.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table /><note>Comparison of different transformation functions from RepPoints to pseudo box, T .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>The effect of applying the deformable RoI pooling layer<ref type="bibr" target="#b3">[4]</ref> on the proposals of the first stages (see Eq. (1) and Eq.(6)). The deformable RoI pooling layer can boost both the methods using bounding boxes and RepPoints, respectively.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Detecting people using mutually consistent poselet activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Kaiming He, and Jian Sun. R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scalable object detection using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2147" to="2154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Piotr Dollár, and Kaiming He</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">High-performance rotation invariant multiview face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihong</forename><surname>Lao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="671" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pedestrian detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Seemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="878" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Light-head r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangdong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07264</idno>
	</analytic>
	<monogr>
		<title level="m">defense of two-stage object detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2277" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Extreme clicking for efficient object annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4930" to="4939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection snip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3578" to="3587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Denet: Scalable real-time object detection with directed sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lachlan</forename><surname>Tychsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Petersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="428" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Region proposal by guided anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to see physics via visual deanimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erika</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="153" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep regionlets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="798" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Metaanchor: Learning to detect objects with customized anchors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="318" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4203" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Anchor box optimization for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00469</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Oriented response networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
