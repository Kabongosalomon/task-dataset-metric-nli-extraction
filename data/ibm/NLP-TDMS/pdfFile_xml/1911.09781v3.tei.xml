<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mason</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilong</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Performing controlled experiments on noisy data is essential in understanding deep learning across noise levels. Due to the lack of suitable datasets, previous research has only examined deep learning on controlled synthetic label noise, and realworld label noise has never been studied in a controlled setting. This paper makes three contributions. First, we establish the first benchmark of controlled real-world label noise from the web. This new benchmark enables us to study the web label noise in a controlled setting for the first time. The second contribution is a simple but effective method to overcome both synthetic and real noisy labels. We show that our method achieves the best result on our dataset as well as on two public benchmarks (CIFAR and WebVision). Third, we conduct the largest study by far into understanding deep neural networks trained on noisy labels across different noise levels, noise types, network architectures, and training settings. The data and code are released at the following link</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Performing experiments on controlled noise is essential in understanding Deep Neural Networks (DNNs) trained on noisy labeled data. Previous work performs controlled experiments by injecting a series of synthetic label noises into a well-annotated dataset such that the dataset's noise level can vary, in a controlled manner, to reflect different magnitudes of label corruption in real applications. Through studying controlled synthetic label noise, researchers have discovered theories and methodologies that have greatly fostered the development of this field. However, due to the lack of suitable datasets, previous work has only examined DNNs on controlled synthetic label noise, and real-world label noise has never been studied in a controlled setting. This leads to two major issues. First, as synthetic noise is generated from an artificial distribution, a tiny change in the distribution may lead to inconsistent or even contradictory findings. For example, contrary to the common understanding that DNNs trained on synthetic noisy labels generalize poorly <ref type="bibr">(Zhang et al., 2017)</ref>, <ref type="bibr" target="#b41">Rolnick et al. (2017)</ref> showed that DNNs can be robust to massive label noise when the noise distribution is made slightly different. Due to the lack of datasets, these findings, unfortunately, have not yet been verified beyond synthetic noise in a controlled setting. Second, the vast majority of previous studies prefer to verify robust learning methods on a spectrum of noise levels because the goal of these methods is to overcome a wide range of noise levels. However, current evaluations are limited because they are conducted only on synthetic label noise. Although there do exist datasets of real label noise e.g. WebVision <ref type="bibr" target="#b28">(Li et al., 2017a)</ref>, Clothing-1M <ref type="bibr" target="#b79">(Xiao et al., 2015)</ref>, etc, they are not suitable for controlled evaluation in which a method must be systematically verified on multiple different noise levels, because the training images in these datasets are not manually labeled and hence their data noise level is fixed and unknown.</p><p>In this paper, we study a realistic type of label noise in a controlled setting called web labels. "Webly-labeled" images are commonly used in the literature <ref type="bibr">(Bootkrajang &amp; Kabán, 2012;</ref><ref type="bibr" target="#b28">Li et al., 2017a;</ref><ref type="bibr" target="#b24">Krause et al., 2016;</ref><ref type="bibr" target="#b5">Chen &amp; Gupta, 2015)</ref>, in which both images and labels are crawled from the web and the noisy labels are automatically determined by matching the images' surrounding text to a class name during web crawling or equivalently by querying the search index afterward. Unlike synthetic labels, web labels follow a realistic label noise distribution but have not been studied in a controlled setting.</p><p>We make three contributions in this paper. First, we establish the first benchmark of controlled web label noise, where each training example is carefully annotated to indicate whether the label is correct or not. Specifically, we automatically collect images by querying Google Image Search using a set of class names, have each image annotated by 3-5 workers, and create training sets of ten controlled noise levels. As the primary goal of our annotation is to identify arXiv:1911.09781v3 <ref type="bibr">[cs.</ref>LG] 27 Aug 2020 images with incorrect labels, to obtain a sufficient number of these images we have to collect a total of about 800,000 annotations over 212,588 images. The new benchmark enables us to go beyond synthetic label noise and study web label noise in a controlled setting. For convenience, we will refer it as web label noise (or red noise) to distinguish it from synthetic label noise (or blue noise) 1 .</p><p>Second, this paper introduces a simple yet highly effective method to overcome both synthetic and real-world noisy labels. It is based on a new idea of minimizing the empirical vicinal risk using curriculum learning. We show that it consistently outperforms baseline methods on our datasets and achieves state-of-the-art performance on two public benchmarks of synthetic and real-world noisy labels. Notably, on the challenging benchmark WebVision 1.0 <ref type="bibr" target="#b28">(Li et al., 2017a)</ref> that consists of 2.2 million images of real-world noisy labels, it yields a significant improvement of 3% in the top-1 accuracy, achieving the best-published result under the standard training setting.</p><p>Finally, we conduct the largest study by far into understanding DNNs trained on noisy labels across a variety of noise types (blue and red), noise levels, training settings, and network architectures. Our study confirms the existing findings of <ref type="bibr">Zhang et al. (2017)</ref> and <ref type="bibr" target="#b1">Arpit et al. (2017)</ref> on synthetic labels, and brings forward new findings that may challenge our preconceptions about DNNs trained on noisy labels. See the findings in Section 5.2. It is worth noting that these findings along with benchmark results are a result of conducting thousands of experiments using tremendous computation power (hundreds of thousands of V100 GPU hours). We hope our (i) benchmark, (ii) new method, and (iii) findings will facilitate future deep learning research on noisy labeled data. We will release our data and code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Datasets of noisy training labels</head><p>While many types of noises exist e.g. image corruption noise <ref type="bibr" target="#b15">(Hendrycks &amp; Dietterich, 2019)</ref>, image registration noise <ref type="bibr" target="#b34">(Mnih &amp; Hinton, 2012)</ref>, or noise from adversarial attacks <ref type="bibr" target="#b4">(Zhang et al., 2019)</ref>, this paper focuses on label noise, and in particular web label noise -a common type of label noise used in the literature. To the best of our knowledge, there have been no datasets of controlled web label noise. The closest to ours is the datasets of two types of noises: controlled synthetic label noise and uncontrolled web label noise.</p><p>In the dataset of controlled synthetic label noise, a series of synthetic label noises are injected into a well-annotated dataset in a controlled manner to reflect different magnitudes of label corruption in real applications. The most common one is the symmetric label noise, in which the label of each example is independently and uniformly changed to a random class with a controlled probability. Many works studied the symmetric label in controlled settings and presented their findings, including famous ones, like <ref type="bibr">(Zhang et al., 2017)</ref> and <ref type="bibr" target="#b1">(Arpit et al., 2017)</ref>. The symmetric label is also commonly used as a benchmark to evaluate robust learning methods in a noise-control setting e.g. in <ref type="bibr" target="#b51">(Vahdat, 2017;</ref><ref type="bibr" target="#b45">Shu et al., 2019;</ref><ref type="bibr" target="#b32">Ma et al., 2018;</ref><ref type="bibr" target="#b13">Han et al., 2018b;</ref><ref type="bibr">Van Rooyen et al., 2015;</ref><ref type="bibr" target="#b26">Li et al., 2019;</ref><ref type="bibr" target="#b0">Arazo et al., 2019;</ref><ref type="bibr">Charoenphakdee et al., 2019)</ref>. Other types of synthetic label noise were also proposed, including class-conditional noises <ref type="bibr" target="#b37">(Patrini et al., 2017;</ref><ref type="bibr" target="#b41">Rolnick et al., 2017;</ref><ref type="bibr" target="#b39">Reeve &amp; Kabán, 2019;</ref><ref type="bibr" target="#b12">Han et al., 2018a)</ref>, noises from other datasets <ref type="bibr" target="#b32">(Wang et al., 2018;</ref><ref type="bibr" target="#b44">Seo et al., 2019)</ref>, etc. However, these noises are still synthetically generated from artificial distributions. Moreover, different works might use different parameters to generate such synthetic noises, which may make their results incomparable. See the example of <ref type="bibr" target="#b41">(Rolnick et al., 2017)</ref> in the introduction.</p><p>In the dataset of uncontrolled web label noise, both images and labels are crawled from the web and the noisy labels are automatically determined by matching the images' surrounding text to a class name. This can be achieved by querying a search index <ref type="bibr" target="#b24">(Krause et al., 2016;</ref><ref type="bibr" target="#b28">Li et al., 2017a;</ref><ref type="bibr" target="#b33">Mahajan et al., 2018)</ref>. For example, In WebVision, <ref type="bibr" target="#b28">Li et al. (2017a)</ref> collected web images with noisy labels by querying Google and Flickr image search using the 1,000 class names from ImageNet. <ref type="bibr" target="#b33">Mahajan et al. (2018)</ref> gathered a large scale set of images with noisy labels by searching hashtags on Instagram. However, these datasets do not provide groundtruth labels for training examples. Their noise level is hence fixed and unknown. As a result, they are not suitable for controlled studies in which different noise levels must be systematically examined. Besides, to get controlled web noise, it may not be a feasible option to annotate images in these datasets due to their imbalanced class distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Robust deep learning methods</head><p>Robust learning is experiencing a renaissance in the deep learning era. Nowadays training datasets usually contain noisy examples. The ability of DNNs to memorize all noisy training labels often leads to poor generalization on the clean test data. Recent contributions based on deep learning handle noisy data in multiple directions including dropout <ref type="bibr" target="#b1">(Arpit et al., 2017)</ref> and other regularization techniques <ref type="bibr" target="#b2">(Azadi et al., 2016;</ref><ref type="bibr" target="#b35">Noh et al., 2017)</ref>, label cleaning/correction <ref type="bibr" target="#b38">(Reed et al., 2014;</ref><ref type="bibr" target="#b10">Goldberger &amp; Ben-Reuven, 2017;</ref><ref type="bibr" target="#b29">Li et al., 2017b;</ref><ref type="bibr" target="#b41">Veit et al., 2017;</ref><ref type="bibr" target="#b46">Song et al., 2019)</ref>, example weighting <ref type="bibr" target="#b40">Ren et al., 2018;</ref><ref type="bibr" target="#b45">Shu et al., 2019;</ref><ref type="bibr" target="#b20">Jiang et al., 2015;</ref><ref type="bibr" target="#b30">Liang et al., 2016)</ref>, cross-validation <ref type="bibr" target="#b36">(Northcutt et al., 2019)</ref>, semi-supervised learning <ref type="bibr" target="#b16">(Hendrycks et al., 2018;</ref><ref type="bibr" target="#b51">Vahdat, 2017;</ref><ref type="bibr">Li et al., 2020;</ref><ref type="bibr">Zhang et al., 2020)</ref>, data augmentation <ref type="bibr" target="#b11">(Zhang et al., 2018;</ref><ref type="bibr" target="#b6">Cheng et al., 2019;</ref><ref type="bibr">2020;</ref><ref type="bibr">Liang et al., 2020)</ref>, among others. Different from prior work, we introduce a simple yet effective method to overcome both synthetic and real-world noisy labels. Compared with semi-supervised learning methods, our method learns DNNs without using any clean label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset</head><p>Our goal is to create a benchmark of controlled noise that resembles a realistic label noise distribution. Unlike existing datasets such as WebVision or Clothing1M, our benchmark provides controlled label noise where every single training example is carefully annotated by several human annotators.</p><p>Our benchmark is built on top of two public datasets: <ref type="bibr">Mini-ImageNet (Vinyals et al., 2016)</ref> for coarse-grained image classification and Stanford Cars <ref type="bibr" target="#b23">(Krause et al., 2013)</ref> for finegrained image classification. Mini-ImageNet has images of size 84x84 with 100 classes from ImageNet <ref type="bibr" target="#b8">(Deng et al., 2009)</ref>. We use all 60K images for training and the 5K images in the ILSVRC12 validation set for testing. Stanford Cars contain 16,185 high-resolution images of 196 classes of cars (Make, Model, Year) split 50-50 into training and validation set. The standard train/validation split is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset Construction</head><p>We build our datasets to replace synthetic noisy labels with web noisy labels in a controlled manner. To recap, let us revisit the construction of existing datasets of noisy labels. Synthetic noisy datasets are generated beginning with a well-labeled dataset. The most common type of synthetic label noise is called symmetric label noise in which the label of each training example is independently changed to a random incorrect class with a probability p called noise level. 2 . The noise level indicates the percentage of training examples with incorrect labels. As the true labels for all images are known, one can enumerate p to obtain training sets of different noise levels and use them in noise-control studies. On class-balanced datasets, this process is equivalent to sampling p% training images from each class and then replacing their labels with the labels uniformly drawn from other classes. The drawback is that the synthetic labels are artificially created and do not follow the distribution of real-world label noise.</p><p>On the other hand, there exist a few datasets of uncontrolled web label noise <ref type="bibr" target="#b79">(Xiao et al., 2015;</ref><ref type="bibr" target="#b28">Li et al., 2017a;</ref><ref type="bibr" target="#b24">Krause et al., 2016)</ref>. In these datasets, the images are crawled from the web and their labels are automatically assigned by matching the images' surrounding text to a class name. This can be achieved by querying a search index. These datasets contain noisy web labels. However, as their training images are not manually labeled, their label noise level is fixed and unknown, rendering existing datasets unsuitable for controlled studies.</p><p>For our benchmark, we follow the construction of synthetic datasets with one important difference -instead of changing the labels of the sampled clean images, we replace the clean images with incorrectly labeled web images while leaving the label unchanged. The advantage of this approach is that we closely match the construction of synthetic datasets while still being able to introduce controlled web label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Noisy Web Label Acquisition</head><p>We collect images with incorrect web labels in three steps:</p><p>(1) images collection, (2) deduplication, and (3) manual annotation. In the first step, we combine images independently retrieved by Google image search from two sources: textto-image and image-to-image search. For the text-to-image search, we formulate a text query for each class using its class name and broader category to retrieve the top images. For image-to-image search, we query the search engine using every training image in Mini-ImageNet and Stanford Cars. Finally, we union the two search results, where the text-to-image search results account for 82% of our final benchmark. Note that the rationale for including a small amount of image-to-image results is to enrich the types of web label noises in our benchmark. We show that an alternative way to construct the dataset by removing all imageto-image results leads to consistent results in the Appendix C.</p><p>In the second step of deduplication, following , we run a CNN-based duplicate detector over all images to remove near-duplicates to any of the images in the validation set. All images are retrieved under the usage rights "free to use or share" 3 . But we still recommend checking their actual usage right for the image.</p><p>Finally, the images are annotated using the Google Cloud labeling service. The annotators are asked to provide a binary question: "is the label correct for this image?". Every image is independently annotated by 3-5 workers and the final label is reached by majority voting. Statistics show annotators disagree only on a small proportion (11%) of the total images. The remainder have unanimous labels agreed by at least 3 annotators.  <ref type="bibr">5,</ref><ref type="bibr">10,</ref><ref type="bibr">15,</ref><ref type="bibr">20,</ref><ref type="bibr">30,</ref><ref type="bibr">40,</ref><ref type="bibr">50,</ref><ref type="bibr">60,</ref><ref type="bibr">80 Blue Stanford Cars symmetric label flipping 8,</ref><ref type="bibr">144 0,</ref><ref type="bibr">5,</ref><ref type="bibr">10,</ref><ref type="bibr">15,</ref><ref type="bibr">20,</ref><ref type="bibr">30,</ref><ref type="bibr">40,</ref><ref type="bibr">50,</ref><ref type="bibr">60,</ref><ref type="bibr">80</ref> 3.3. Dataset Overview</p><p>In total, we collect about 800,000 annotations over 212,588 images, out of which there are 54,400 images with incorrect labels on Mini-ImageNet and 12,629 images on Stanford Cars. The remainder of the images have correct labels. Using web images with incorrect labels, we replace p% of the original training images in the two datasets, and enumerate p in 10 different levels to create the controlled web label noise: {0%, 5%, 10%, 15%, 20%, 30%, 40%, 50%, 60%, 80%}. Similar to synthetic noise, p is made uniform across classes, e.g. p = 20% means that every class has roughly 20% incorrect labels. In Appendix C, we show constructing the dataset only using web images leading to consistent results. <ref type="table" target="#tab_1">Table 1</ref> summarizes our benchmark. For comparison, we also include synthetic (symmetric) labels of the same 10 noise levels. We use blue noise to denote the synthetic label noise and red noise for the web label noise. The sizes of the red and blue training sets are made similar to better compare their difference 4 . Only a subset of web images with incorrect labels is used in our dataset. But we release all 212,588 images along with their annotations, which can be downloaded at http://www.lujiang.info/cnlw.html under the license of Creative Commons.</p><p>For lack of space, we use <ref type="figure" target="#fig_1">Fig. 1</ref> in the Appendix to illustrate noisy images in each dataset. In summary, there are three clear distinctions between images with the synthetic and web label noise. Images with label noise from the web (or red noise) (1) a higher degree of similarity to the true positive images, (2) exist at the instance-level, and <ref type="formula">(3)</ref> come from an open vocabulary outside the class vocabulary of Mini-ImageNet or Stanford Cars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>In this section, we introduce a simple method called Men-torMix to overcome both synthetic and web label noise. As its name suggests, our idea is inspired by MentorNet  and <ref type="bibr">Mixup (Zhang et al., 2018)</ref>. The main idea is to design a new robust loss to overcome noisy labels using curriculum learning and vicinal risk minimization. <ref type="bibr">4</ref> As existing findings on synthetic noise hold on both the full (60K) and subset (50K) of Blue Mini-ImageNet, we choose to report the results on the 60K full set. This results in a slightly larger Blue Mini-ImageNet but may not affect our main contributions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Background on MentorNet and Mixup</head><p>Consider a classification problem with training set D = {x 1 , y 1 ), · · · , (x n , y n )}, where x i denotes the i th training image and y i ∈ [1, m] is an integer-valued noisy label over m possible classes and y i is the corresponding one-hot label. Note that no clean labels are allowed to be used in training. Let g s (x i ; w) denote the prediction of a DNN, parameterized by w ∈ R d . MentorNet  minimizes the following objective:</p><formula xml:id="formula_0">w * = argmin w∈R d ,v∈[0,1] n F(v, w) = 1 n n i=1 v i (g s (x i ;w), y i ) + θ w 2 2 + G(v; γ)<label>(1)</label></formula><p>where (g s (x i ;w), y i ), or (x i , y i ) for short, is the crossentropy loss with the softmax function. θ is the weight decay parameter on the l 2 norm of the model parameters.</p><p>For convenience, we make the weight decay regularization, data augmentation and dropout all subsumed inside g s .</p><p>Eq.</p><p>(1) introduces the latent weight variable v ∈ [0, 1] n for every training example. The regularization term G determines a curriculum <ref type="bibr" target="#b20">(Jiang et al., 2015;</ref><ref type="bibr" target="#b9">Fan et al., 2017)</ref> or equivalently a weighting scheme to compute the latent weight v i to each example. In , the weighting scheme is computed by a neural network called MentorNet. In training, w and v are alternatively minimized inside a mini-batch, one at a time while the other is held fixed. Only w is used at test time.</p><p>Mixup (Zhang et al., 2018) minimizes the empirical vicinal risk calculated from:</p><formula xml:id="formula_1">w * = argmin w 1 n n i=1 1 n n j=1 E λ [ (g s (x ij ;w),ỹ ij )] (2)</formula><p>x andỹ are computed by the mixup function:</p><formula xml:id="formula_2">x ij = λx i + (1 − λ)x j (3) y ij = λy i + (1 − λ)y j<label>(4)</label></formula><p>where λ is drawn from the Beta distribution Beta(α, α) controlled by hyperparameter α. In practice, only the examples from the same mini-batch are mixed up during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">MentorMix</head><p>In the proposed MentorMix, we minimize the empirical vicinal risk using curriculum learning. For simplicity, the self-paced regularizer G(v) = −γ v 1 <ref type="bibr" target="#b25">(Kumar et al., 2010;</ref><ref type="bibr" target="#b20">Jiang et al., 2015)</ref> is used and we have:</p><formula xml:id="formula_3">F(ṽ, w) = 1 n 2 n i=1 n j=1 E λ [ṽ ij (x ij ,ỹ ij ) − γṽ ij ],<label>(5)</label></formula><p>where γ is a hyperparameter. It is easy to derive the optimal weighting scheme when the network parameter w is fixed:</p><formula xml:id="formula_4">v * ij = argmiñ v∈[0,1] n×n F w (ṽ) = 1( (x ij ,ỹ ij ) ≤ γ), (6)</formula><p>where 1 is the indicator function and F w denotes the objective when w is fixed.</p><p>Although Eq. <ref type="formula">(6)</ref> gives a closed-form solution for computing the optimal weight, it is intractable to compute as this requires enumerating all pairs of training examples. We therefore resort to importance sampling to find the "important" examples. To do so, we define a stratum for each x i and draw an example from the following distribution:</p><formula xml:id="formula_5">P v (v i = 1|x i , y i ) = exp(v * i /t) n j=1 exp(v * j /t) ,<label>(7)</label></formula><p>where t is the temperature in the softmax function and is fixed to 1 in our experiments. P v specifies a density function over individual training examples. In theory, the distribution is defined over all training examples but, in practice to enable mini-batch training, we compute the distribution within each mini-batch (See Algorithm 1). v * i is the optimal weight for</p><formula xml:id="formula_6">x i . v * i is calculated from v * i = argmin vi F w (v) in Eq.</formula><p>(1) and can be conveniently obtained by MentorNet. As the optimal v * ij can only have binary values according to Eq. (6), under the importance sampling we rewrite part of the objective in Eq. (5) as:</p><formula xml:id="formula_7">(xi,yi)∼D (xj ,yj )∼D E λ [v ij (x ij ,ỹ ij ) − γv ij ] = (xi,yi)∼D (xj ,yj )∼Pv E λ [ (x ij ,ỹ ij )] − γ<label>(8)</label></formula><p>where the constant γ will be dropped during training. According to Eq. (6), our goal is to find the mixed-up examples of smaller loss. For a given example (x i , y i ), the loss of the mixed-up example (x ij ,ỹ ij ) tends to be smaller when (x j , y j ) is small. Inspired by this idea, we sample x j from P v with respect to the weight v * j that is monotonically decreasing with its loss (x j , y j ) . In this way examples of lower loss are more likely to be selected in the mixup.</p><p>Algorithm 1 shows the four key steps to compute the loss for a mini-batch: weight (Step 2-4), sample (Step 5 and 8),</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 The proposed MentorMix method.</head><p>Input :mini-batch Dm; two hyperparameters γp and α Output :the loss of the mini-batch 1 For every <ref type="bibr">(xi, yi)</ref> in Dm compute (xi, yi) 2 Set p(Dm) to be the γp-th percentile of the loss { (xi, yi)}. 3 γ ← EMA( p(Dm)) // update the moving average</p><formula xml:id="formula_8">4 v * i ← MentorNet( (xi, yi), γ) // MentorNet weight 5 Compute Pv = softmax(v * ), where v * = [v * 1 , · · · , v * |Dm| ] 6 Stop gradient 7 foreach (xi, yi) do 8 Draw a sample (xj, yj) with replacement from Pv 9 λ ← Beta(α, α) 10 λ ← v * i max(λ, 1 − λ) + (1 − v * i ) min(λ, 1 − λ) 11xij ← λxi + (1 − λ)xj 12ỹij ← λyi + (1 − λ)yj 13 Compute i = (xij,ỹij) 14 Weight i using a separate MentorNet // optional 15 end 16 return (1/|Dm|) |Dm| i=1 i mixup (</formula><p>Step 9-12), and weight again <ref type="formula" target="#formula_0">(Step 14)</ref>, where the weighting is achieved by the MentorNet. Following our previous work , we avoid directly setting γ and adopt a moving average (in Step 3) to track the exponential moving average of the γ p -th percentile of the mini-batch loss, in which γ p becomes the new hyperparameter.</p><p>Step 4 computes the weight for individual example using a fixed MentorNet. The simplified MentorNet is used in our paper which is equivalent to computing the weight by a thresholding function v * i = 1( (x i , y i ) ≤ γ). In Step 10, we assign a bigger (binary) weight between λ and 1 − λ to x i unless its v * i is small. This trick is to stabilize importance sampling by encouraging each stratum to receive a bigger weight in the mixup. It leads to marginal performance gains but makes the training more robust to the choice of hyperparameters. In Step 14, the second weighting can be applied optionally using the weights produced by a separate MentorNet. This step is optional for low noise levels but is useful for high noise levels. Our algorithm has the same time and space complexity as that of the Mixup algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>This section verifies the proposed method on four datasets and presents new findings on web label noise. Specifically, in Section 5.1 we first verify the proposed method on our dataset and then compare it with the state-of-the-art on two public benchmarks of synthetic and real-world noisy labels. In Section 5.2, we empirically examine DNNs trained on controlled noisy labels under various settings and present our findings that challenge our previous understandings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Method Comparison</head><p>Evaluation metrics: Following prior works, the peak accuracy is used as the primary evaluation metric that denotes the maximum accuracy on the clean validation set throughout the training. In addition, the final accuracy is also reported in the Appendix D i.e. the validation accuracy after training has converged at the final training step.</p><p>Training setting: On our benchmark, all methods are trained on the noisy training sets of two noise types (blue and red) under 10 noise levels (from 0% to 80%), and tested on the same clean validation set. Two training settings are considered: (i) training from scratch and (ii) fine-tuning from an ImageNet checkpoint where the checkpoint is pretrained on the ImageNet training data. See details in the Appendix. On our datasets, Inception-ResNet-v2  is used as the default backbone for all methods, where we upsample the images in the Mini-ImageNet dataset from 84x84 to 299x299 so that we can keep using the same pretrained ImageNet checkpoint. On the public benchmarks in Section 5.1.2, we train networks from scratch using ResNet-32 for CIFAR and Inception-ResNet-v2 for WebVision.</p><p>Baselines and our method: On our dataset, MentorMix is compared against the following baselines. We extensively search the hyperparameter for each method on every noise level. Vanilla is the standard training using l 2 weight decay, dropout, and data augmentation. Weight Decay and Dropout <ref type="bibr" target="#b47">(Srivastava et al., 2014)</ref> are classical regularization methods. We search the hyperparameter for the weight decay in {e −5 , e −4 , e −3 , e −2 } and the dropout ratio in {0.9, 0.8, 0.7, 0.6, 0.5} as suggested in <ref type="bibr" target="#b1">(Arpit et al., 2017)</ref>. Bootstrap <ref type="bibr" target="#b38">(Reed et al., 2014)</ref> corrects the loss with the learned label. The soft version is used and the hyperparameter for the learned label is tuned in {0.05, 0.25, 0.5, 0.7}. S-model <ref type="bibr" target="#b10">(Goldberger &amp; Ben-Reuven, 2017)</ref> is another way to "correct" the predictions by appending a new layer to a DNN to learn noise transformation. MentorNet  is an example-weighting method. We employ the predefined MentorNet and search the hyperparameter ppercentile in {85%, 75%, 55%, 35%}. Mixup (Zhang et al., 2018) is a robust learning method that minimizes the empirical vicinal risk. Following the advice in <ref type="bibr" target="#b11">(Zhang et al., 2018)</ref> its hyperparameter α is searched in {1, 2, 4, 8}.</p><p>We implement MentorMix in Algorithm 1 in TensorFlow. We search two hyperparameters α in the γ p in the range α = {0.4, 1, 2} and γ p = {90%, 80%, 70%}. The code will be released to reproduce our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">BASELINE COMPARISON</head><p>We first show the comparison to the baseline methods on our dataset. For the lack of space, <ref type="table" target="#tab_2">Table 2</ref> shows a high-level summary of the comparison result, in which each cell shows a method's average best peak accuracy (and 95% confidence interval) across all 10 noise levels. <ref type="table" target="#tab_4">Table 4 -Table 7</ref> in the Appendix D list detailed results on each noise level.</p><p>As shown, the proposed method consistently outperforms the baseline methods across noise types (red and blue) and training settings (finetuning and training from scratch). This is desirable as baseline methods that work well on blue noise may not show consistent improvement over red noise, or vice versa. The red noise appears to be less harmful. Yet it is more difficult to overcome, which suggests the need for a new benchmark for a more comprehensive evaluation. Nevertheless, our method yields consistent improvements on both synthetic and web noisy labels.  The primary reason for our superior performance is that MentorMix can leverage MentorNet and Mixup in a complementary way. Technically, it uses MentorNet weight to identify examples with "cleaner" labels and encourages them to be used in the mixup operation.  showed that MentorNet optimizes an underlying robust loss in empirical risk minimization. From this perspective, our MentorMix introduces a new robust loss to minimize the vicinal risk which turns out to be more resilient to noisy training labels. For example, <ref type="figure" target="#fig_1">Fig. 1</ref> compares Vanilla and MentorMix on the blue and red Mini-ImageNet at 50% noise level. It shows that MentorMix's loss is more robust to noisy labels, and MentorMix improves the peak accuracy of Vanilla by 16.3% on blue noise and 2.4% on red noise.</p><p>As the noise levels span across a wide range, we find the hyperparameters of robust learning methods are important. For the same method, a careful hyperparameter search could well be the difference between good and bad performance. As shown in the Appendix, we find that our method is relatively less sensitive to the choice of hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">COMPARISON TO THE STATE-OF-THE-ART</head><p>In this subsection, we compare MentorMix on two public benchmarks of synthetic and real-world noisy labels. <ref type="table" target="#tab_3">Table 3</ref> compares with the state-of-the-art on the CIFAR dataset with symmetric label noise, where the top shows the classification accuracy on the clean validation set of CIFAR-100 and the bottom is for CIFAR-10. As all methods are trained using networks of similar capacity (ours is ResNet-32), we cite the numbers reported in their papers except for MentorNet and Mixup. Our method achieves the best accuracy across all noise levels. The results validate that our method is effective for synthetic noisy labels. We then compare with the state-of-the-art on the challenging benchmark WebVision 1.0 <ref type="bibr" target="#b28">(Li et al., 2017a</ref>) that contains 2.4 million training images of noisy labels from the web. It uses the same 1,000 classes in the ImageNet ILSVRC12 challenge and thus is evaluated on the same validation set.</p><p>Following prior studies, we train our method on both the full training set (2.4M images on 1K classes) and the mini subset (61K images on 50 classes), and test it on two clean validation sets from ImageNet ILSVRC12 and WebVision. <ref type="table" target="#tab_4">Table 4</ref> shows the comparison results, where the method marked by † uses extra clean labels during training. As shown, the proposed MentorMix improves the prior state-ofthe-art by about 3% in the top-1 accuracy on the ILSVRC12 validation set without using any extra labels. It is worth noting that 3% is a significant improvement on the ILSVRC12 validation <ref type="bibr" target="#b8">(Deng et al., 2009)</ref>. To the best of our knowledge, it achieves the best-published result on the WebVision 1.0 benchmark under the same training setting. The results show that our method is effective for real-world noisy labels. We also apply our method on the Clothing-1M dataset where we train only on the 1M noisy training examples. Our model gets 74.3% accuracy which is competitive to recent published works. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Understanding DNNs trained on noisy labels</head><p>In this subsection, we conduct a large study into understanding DNNs trained on noisy labels across noise levels, noise types, training settings, and network architectures. We focus on three important findings <ref type="bibr">(Zhang et al., 2017;</ref><ref type="bibr" target="#b1">Arpit et al., 2017;</ref>. These works examine vanilla DNN training either on controlled synthetic labels (the former two) or on clean training labels (the last one). Our goal is to revisit them on our benchmark in a controlled setting where the noise in training sets varies from completely clean (0%) to the level where 80% of training labels are incorrect.</p><p>As in these works, we learn DNNs using vanilla training which allows us to compare their findings. Training DNNs using robust learning methods would probably lead to different findings but that would make the findings undesirably depend on the specific method being used.  <ref type="figure" target="#fig_4">Fig 2,</ref> where the training and validation accuracies are shown along with the training steps. The dashed and solid curves represent the validation accuracies for 0% (or clean) and 40% noise levels, respectively, and the color belt plots the 95% confidence interval of the 10 noise levels.</p><p>After the training converges in <ref type="figure" target="#fig_4">Fig 2,</ref> the difference between the dashed and solid curves (in blue) indicates a palpable performance degradation between the clean (0%) and noisy (40%) labels. This can also be seen from the greater width of the blue belt which denotes the accuracy's confidence interval over all 10 noise levels. This confirms Zhang et al.</p><p>(2017)'s finding on synthetic noisy labels. However, the difference is much smaller on the red curves, suggesting DNNs generalize much better on the red noise. This pattern is consistent across our two datasets using both fine-tuning and training from scratch. We hypothesize that DNNs are more robust to web labels because they are more relevant (visually or semantically) to the clean training images. See <ref type="figure" target="#fig_1">Fig. 1</ref> in the Appendix. DNNs may not learn patterns first on red label noise. <ref type="bibr" target="#b1">Arpit et al. (2017)</ref> found that DNNs learn patterns first, revealing an interesting property that DNNs are able to automatically learn generalizable "patterns" in the early training stage before memorizing all noisy training labels. This can be manifested by the gap between the peak and final accuracy, as shown in <ref type="figure" target="#fig_4">Fig. 2(a)</ref>. A larger drop suggests a better pattern is found during the early training stage. For better visualization, <ref type="figure" target="#fig_2">Fig. 3</ref> computes the relative difference, namely the drop, between the peak and final accuracy across noise levels. As shown, the blue curves show a significant drop as the noise level grows. This is consistent with Arpit et al. <ref type="formula" target="#formula_0">(2017)</ref>'s finding on synthetic label noise.</p><p>Interestingly, the drop on the red noise is considerably smaller and even approaches zero on the Stanford Cars dataset. This suggests DNNs may not learn patterns first on red label noise at least for the fine-grained classification task. Our hypothesis is that images of real-world label noise are more complicated than those of the synthetic noise because they are sampled non-uniformly from an infinite number of classes. Therefore, it is much more difficult for DNNs to capture meaningful patterns automatically.</p><p>ImageNet architectures generalize on noisy labels when the networks are fine-tuned.  found that fine-tuning better architectures trained on ImageNet tend to perform better on downstream tasks of clean training labels. It is important to verify whether this holds on noisy training labels because if so, one can conveniently transfer better architectures to better overcome the noisy labels.</p><p>Following , in <ref type="figure">Fig. 4</ref>, we compare the fine-tuning performance using ImageNet architectures, where the x-axis is the accuracy of the pretrained architectures on ImageNet, and the y-axis denotes the peak accuracy on our datasets. The bar plots the 95% confidence interval across 10 noise levels, where the center dot marks the mean. As it shows, there is a reasonable correlation between the Im-ageNet accuracy and the validation accuracy on both red and blue noisy labels. The Pearson correlation for the red noise MobieNet-V2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inception-ResNet-v2</head><p>EfficientNet-b5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inception-v2</head><p>ResNet-50</p><p>ResNet-101</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inception-v3</head><p>(a) Mini-ImageNet (r = 0.91)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MobieNet-V2</head><p>Inception-ResNet-v2</p><p>EfficientNet-b5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inception-v2</head><p>ResNet-50</p><p>ResNet-101</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inception-v3</head><p>(b) Stanford Cars (r = 0.88) <ref type="figure">Figure 4</ref>. Fine-tuning seven ImageNet architectures on the red and blue datasets. The number in parentheses is the Pearson correlation between the architecture's ImageNet accuracy and the fine-tuning accuracy on our dataset of red noise. is 0.91 on Mini-ImageNet and 0.88 on Stanford Cars. This indicates a better-pretrained architecture is likely to perform better even when it is fine-tuned on noisy training labels. We do not find such correlation when these architectures are trained from scratch. These results extend 's finding to noisy training data, and suggest when possible one may use more advanced pretrained architectures to overcome noisy training labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we study web label noise in a controlled setting. We make three contributions. First, we establish the first benchmark of controlled web noise obtained from image search. Second, a simple but effective method is proposed to overcome both synthetic and real-world noisy labels. Our method achieves state-of-the-art results on multiple datasets.</p><p>Finally, we conduct the largest study by far into understanding deep learning on noisy data across a variety of settings. Our studies reveal several new findings: (1) DNNs generalize much better on web label noise; (2) DNNs may not learn patterns first on web label noise in which early stopping may not be very effective; (3) when networks are fine-tuned, Im-ageNet architectures generalize well on both symmetric and web label noise; (4) methods that perform well on synthetic noise may not work as well on the real-world noisy labels from the web; (5) the proposed method yields consistent improvements on both synthetic and real-world noisy labels from the web.</p><p>Based on our observations, we arrive at the following recommendations for training deep neural networks on noisy training data.</p><p>• A simple way to deal with noisy labels is to fine-tune a model that is pre-trained on clean datasets, like Ima-geNet. The better the pre-trained model is, the better it may generalize on downstream noisy training tasks.</p><p>• Early stopping may not be as effective on the label noise from the web, especially on the fine-grained classification task.</p><p>• Methods that perform well on synthetic noise may not work as well on the real-world noisy labels from the web. The proposed MentorMix can better overcome both synthetic and real-world web noisy labels.</p><p>• The real-world label noise from the web appears to be less harmful, yet it is more difficult for our current robust learning methods to tackle. This encourages more future research to be carried out on controlled label noise from the web. A. Dataset Overview: <ref type="figure" target="#fig_1">Fig. 1</ref> shows some example images with correct labels and incorrect labels of symmetric label noise (blue noise) and web label noise (red noise). There are three clear distinctions between images with the synthetic and web label noise as  <ref type="table" target="#tab_1">Table 1</ref>. First, images with noise from the web (or red noise) are more relevant (visually or semantically) to true positive images. Second, synthetic noise (symmetric or asymmetric) is at class-level which means all examples in the same class are treated equally. Web label noise is at instance-level in which certain images are more likely to be mislabelled than others. For example, "Honda Civic" images taken from the side view are more likely to be confused with "Honda Accord" as the two models are lookalike from the side view. Such confusion is rare for car images taken from the front view. Third, images with noise from the web come from an open vocabulary outside the class vocabulary of Mini-ImageNet or Stanford Cars. For example, the noisy images of "ladybug" include "fly" and other bugs that do not belong to any of the classes in Mini-ImageNet. <ref type="figure" target="#fig_4">Fig. 2</ref> illustrates the distribution of correctly-labeled and incorrectly-labeled images across classes, where symmetric label noise is in blue and web label noise is in red. It is worth noting that it is not a feasible option for us to annotate existing datasets of web labels, e.g. WebVision <ref type="bibr" target="#b74">(Li et al., 2017)</ref> or Clothing-1M <ref type="bibr" target="#b79">(Xiao et al., 2015)</ref>. Due to their imbalanced class distribution, for many classes, we simply cannot find sufficient images with incorrect labels to label in these datasets.</p><p>As incorrect images are rare in a few common classes (e.g. hotdog), we need to limit the size of Red Mini-ImageNet to 50K such that every class can get sufficient incorrect images at every noise level. Recall the role of the blue noisy datasets is to confirm existing findings on symmetric noise. Initially, we made the Blue and Red Mini-ImageNet to be the same size of 50K examples. However, we found that existing findings on symmetric noise hold on both the full (60K) and subset (50K) of Blue Mini-ImageNet. In the end, we decided to report the results on the 60K full set which results in a larger size of Blue Mini-ImageNet. This design would not affect our main contributions for the following reasons. First, on our second dataset Stanford Cars, the blue and red set have the same size. Second, our method has been verified by extensive experiments on many other datasets. Third, our main findings are on red noise which may not be affected by the size of the blue set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Detailed Experimental Setups</head><p>This section presents detailed setups for training and testing used in our experiments.</p><p>B.1. Setup on the proposed dataset Network architectures. <ref type="table" target="#tab_2">Table 2</ref> lists the parameter count and input image size for each network architecture used in our experiments. We obtained their model checkpoints trained on the ImageNet 2012 dataset from TensorFlow Slim 1 , EfficienNet TPU 2 , and from . The last two columns list the top-1 accuracy of our obtained models along with the accuracy reported in the original paper. As shown, the top-1 accuracy of these architectures on the ImageNet ILSVRC 2012 validation ranges from 71.6% to 83.6%. We select the above architectures to be representative of diverse capacities.</p><p>Training from scratch (random initialization). For vanilla training, we trained each architecture on the clean dataset (0% noise level) to find the optimal training setting by grid search. Our grid search consisted of 6 start learning rates of {1.6, 0.16, 1.0, 0.5, 0.1, 0.01} and 3 learning rate decay epochs of {1, 2, 3}. The exponential learning rate decay factor was fixed to 0.975. We trained the network to full convergence, and the maximum epoch to train was 200 on Mini-ImageNet (Red and Blue) and 300 epochs on Stanford Cars (Blue and Red), where the learning rate warmup <ref type="bibr" target="#b68">(Goyal et al., 2017)</ref> was used in the first 5 epochs. The training was using Nesterov momentum with a momentum parameter of 0.9 with a batch size of 64, taking an exponential moving average of the weights with a decay factor of 0.9999. We had to reduce the batch size to 8 for EfficientNet for its larger image input. Following , our vanilla training was with batch normalization layers but without label smoothing, dropout, or auxiliary heads. We employed the standard prepossessing in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network</head><p>Parameters Image Size ImageNet Top-1 Acc. Paper Our checkpoint epochs to train. We extensively searched the hyperparameter for each method on every noise level using the hyperparameter range discussed in the main manuscript. The performance variance under different hyperparameters can be found in <ref type="figure" target="#fig_6">Fig. 5,  Fig. 6, Fig. 7, and Fig. 8</ref>, where the black line shows the 95% confidence interval of the accuracy under all searched hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Setup on public benchmarks: CIFAR and WebVsion</head><p>CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b73">(Krizhevsky &amp; Hinton, 2009</ref>) consist of 32 × 32 color images arranged in 10 and 100 classes. Both datasets contain 50,000 training and 10,000 validation images. The ResNet-32  with standard data augmentation function was used as our network backbone. In training, the batch size was set to 128 and we trained 400K iterations for the ResNet model by a distributed asynchronized momentum SGD optimizer (momentum = 0.9) on 8 V100 GPUs. We used the common learning rate scheduling strategy: setting the starting learning rate as 0.1 and using the step-wise exponential learning rate decay which multiplies it by 0.9 every 20K iterations. In this way, training ResNet-32 on the clean training dataset, the validation accuracy reaches 95.2% on CIFAR-10, and 78.0% on CIFAR-100. We searched the hyperparameters of MentorMix in the following range: α = {2, 4, 8, 32} and γ p = {0.8, 0.7, 0.6, 0.5, 0.4, 0.2, 0.1}. We used a simple MentorNet called "Predefined MentorNet" from   <ref type="bibr">4</ref> which, as discussed in the main paper, computes the weight by a thresholding function v * i = 1( (x i , y i ) ≤ γ). WebVision 1.0 <ref type="bibr" target="#b74">(Li et al., 2017)</ref> contains 2.4 million images of real-world noisy labels, crawled from the web using the 1,000 concepts in ImageNet ILSVRC12. We downloaded the resized images from the official website 5 . The inception-resnet v2  was used as our network backbone. In training, the batch size was set to 64 and we trained 4M iterations using a distributed asynchronized momentum optimizer on 32 V100 GPUs. The start learning rate was 0.01 and was discounted by a factor of 0.95 every 562K steps. The weight decay was 4e−5. The batch norm was used and its decay was set to 0.9997 and the epsilon was 0.001. The default data augmentation for the ResNet model is used. We also tested our method on the WebVision mini-training set that contains about 61K Google images on the first 50 classes. All the models were evaluated on the clean ILSVRC12 and WebVision validation set. The best hyperparameter is γ p = 0.7 and α = 0.4 for both the WebVision full training set and the WebVision mini-training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Alternative Approaches for Dataset Construction</head><p>In this section, we study two alternative approaches to construct our red datasets. Our goal is to verify whether our findings in Section 5.3 of the main paper are consistent when the datasets are constructed differently. Please note that it may not be necessary to verify the proposed method MentorMix on these new datasets as it has already been verified in Section 5.2 on the two public benchmarks (CIFAR and WebVision) that contain both synthetic and real-world noisy labels. We consider the following settings to construct our red datasets:</p><p>• Setting 0 (default) uses the approach discussed in the main paper where we replace the clean images in Mini-ImageNet and Stanford Cars with incorrectly labeled images from the web while leaving the label unchanged. The advantage of our approach is that we closely match the construction of synthetic datasets while still being able to introduce controlled levels of noise that better resembles realistic label noise distributions.</p><p>• Setting 1 (web images only): in this setting, the red datasets only contain images from the web (with both correct and incorrect labels). No clean images in the original Mini-ImageNet or Stanford Cars datasets are used. This setting is used to understand the impact of the domain difference in Setting 0.</p><p>• Setting 2 (no image-to-image results): this setting is the same as setting 0 except only web images obtained from the text-to-image search are used. This setting examines the impact after removing the image-to-image search label noise.</p><p>First, we show that DNNs generalize much better on the red label noise under all three settings. We compare the standard deviation of the final accuracies at 10 noise levels (0%-80%) in <ref type="table" target="#tab_3">Table 3</ref>. A higher standard deviation suggests a poorer generalization performance when DNNs are trained on noisy labels. Ideally, we expect to observe a significantly smaller standard deviation in web noise. <ref type="table" target="#tab_3">Table 3</ref> shows the standard deviation of the red noise is at least two times less than that of Second, we show that DNNs may not learn patterns first on red noise i.e. DNNs are able to automatically learn generalizable "patterns" in the early training stage before memorizing all noisy training labels. This is manifested by the gap between the peak and final validation accuracy. <ref type="figure" target="#fig_2">Fig. 3</ref> illustrates the relative difference, namely the drop, between the peak and final accuracy on the clean validation set. Recall a larger drop between the peak and final validation accuracy means a better pattern is found in the early training stage. As it shows, the drops of red noise under all three settings are significantly and consistently smaller than that of the blue noise. These results show the domain differences and the types of label noise (image-to-image search) do not distort this finding. Finally, we show that ImageNet architectures generalize on noisy labels when the networks are fine-tuned. To do so, we compute Pearson correlation r for different red noise settings. The results are shown in <ref type="figure">Fig. 4</ref>, where the x-axis is the accuracy of the pretrained architectures on ImageNet and the y-axis shows the peak validation accuracy on noisy datasets. The bar plots the 95% confidence interval across 10 noise levels, where the center dot marks the mean. As it shows, the correlation is consistent across all types of label noise where the Pearson correlations are shown in the parentheses. These results show a better pretrained architecture is likely to perform better when it is fine-tuned on noisy training labels. This finding seems to be consistent across all types of label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Detailed Method Comparison</head><p>This subsection presents detailed comparison results on our datasets. To be specific, we show the peak/final accuracy on the clean validation set in four tables: <ref type="table" target="#tab_4">Table 4</ref>, <ref type="table">Table 5</ref>, <ref type="table" target="#tab_11">Table 6, and Table 7</ref>, where the best trial out of all searched hyperparameters is shown. The performance variance of all searched hyperparameters is shown in four figures: <ref type="figure" target="#fig_6">Fig. 6, Fig. 5,  Fig. 8, and Fig. 7</ref>, where the black line shows the 95% confidence interval. The results in all tables and figures show that the proposed method MentorMix consistently outperforms baseline methods and has comparable performance variance in the searched hyperparameter range.  <ref type="figure">Figure 4</ref>. Fine-tuning seven ImageNet architectures on the red and blue datasets. The number in parentheses is the Pearson correlation between the architecture's ImageNet accuracy and the performance on our red dataset. All three settings of red noise are illustrated. Better view in color.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Comparison of training and validation accuracy during training. The dataset is Mimi-ImageNet at the 50% noise level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Performance drop from the peak accuracy at different noise levels. Colors are used to differentiate noise types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>The distribution of images with correct and incorrect labels in Mini-ImageNet and Stanford Cars. The grey, blue, and red bar represent images of correct labels, symmetric noisy labels, and web noisy labels, respectively. Classes are ranked by the number of training examples. Better viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Performance drop from the peak accuracy across 10 noise levels. Different colors are used to differentiate noise types. A larger drop (y-axis) means a better pattern is found during the early training stage</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Peak accuracy of robust DNNs (trained from scratch) on Red and Blue Stanford Cars.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Google Research, Mountain View, United States 2 Google Cloud AI, Sunnyvale, United States 3 Cornell University, Ithaca, United States. Correspondence to: Lu Jiang &lt;lu-jiang@google.com&gt;. Proceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the author(s).</figDesc><table /><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Overview of our datasets of controlled red (web) label noise. Blue (synthetic) label noise is also included for comparison.</figDesc><table><row><cell>Dataset</cell><cell>#Class</cell><cell>Noise Source</cell><cell cols="2">Train Size Val Size</cell><cell>Controlled Noise Levels (%)</cell></row><row><cell>Red Mini-ImageNet Blue Mini-ImageNet</cell><cell>100</cell><cell>image search label symmetric label flipping</cell><cell>50,000 60,000</cell><cell>5,000</cell><cell>0, 5, 10, 15, 20, 30, 40, 50, 60, 80 0, 5, 10, 15, 20, 30, 40, 50, 60, 80</cell></row><row><cell>Red Stanford Cars</cell><cell>196</cell><cell>image search label</cell><cell>8,144</cell><cell>8,041</cell><cell>0,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Peak accuracy (%) of the best trial of each method averaged across 10 noise levels. -denotes the method failed to train. 3±1.9 81.6±1.9 58.3±10.3 64.9±5.2 70.0±16.8 82.4±6.9 53.8±24.4 77.7±10.4 3±1.8 82.0±1.9 58.7±10.2 64.6±5.1 69.7±16.8 82.4±7.1 53.9±23.5 77.6±10.2 Bootstrap 83.1±1.6 82.7±1.8 60.1±9.7 65.5±4.9 71.7±16.9 82.8±6.7 55.6±23.9 78.9±9.6 Mixup 81.7±1.8 82.4±1.7 60.7±9.8 66.0±4.9 73.1±16.6 85.0±6.2 64.2±21.6 82.5±8.0 MentorNet 82.9±1.7 82.4±1.7 61.8±10.3 65.1±5.0 75.9±16.8 82.6±6.6 56.8±23.1 78.9±8.9 Our MentorMix 84.2±0.7 83.3±1.9 70.9±3.4 67.0±5.0 78.2±16.2 86.9±5.5 67.7±23.0 83.6±7.5</figDesc><table><row><cell>Method</cell><cell>Mini-ImageNet Fine-tuned Trained from scratch Blue Red Blue Red</cell><cell>Stanford Cars Trained from scratch Red Fine-tuned Blue Blue Red</cell></row><row><cell cols="3">Vanilla 82.WeightDecay 81.9±1.8 81.5±1.8 Dropout 82.8±1.3 81.8±1.8 59.3±9.5 65.7±5.0 71.7±16.9 83.8±6.6 62.8±23.5 84.1±6.7 --72.2±17.5 84.3±6.6 --S-Model 82.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison with the state-of-the-art in terms of the validation accuracy on CIFAR-100 (top) and CIFAR-10 (bottom).</figDesc><table><row><cell>Data</cell><cell>Method</cell><cell>20</cell><cell>Noise level (%) 40 60</cell><cell>80</cell></row><row><cell>CIFAR100 CIFAR10</cell><cell cols="4">Arazo et al. (2019) Zhang &amp; Sabuncu (2018) 67.6 62.6 54.0 29.6 73.7 70.1 59.5 39.5 MentorNet (2018) 73.5 68.5 61.2 35.5 Mixup (2018) 73.9 66.8 58.8 40.1 Huang et al. (2019) 74.1 69.2 39.4 -Ours (MentorMix) 78.6 71.3 64.6 41.2 Arazo et al. (2019) 94.0 92.8 90.3 74.1 Zhang &amp; Sabuncu (2018) 89.7 87.6 82.7 67.9 Lee et al. (2019) 87.1 81.8 75.4 -Chen et al. (2019) 89.7 --52.3 Huang et al. (2019) 92.6 90.3 43.4 -MentorNet (2018) 92.0 91.2 74.2 60.0 Mixup (2018) 94.0 91.5 86.8 76.9 Ours (MentorMix) † 95.6 94.2 91.3 81.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparison</figDesc><table><row><cell cols="3">with the state-of-the-art on the clean vali-</cell></row><row><cell cols="3">dation set of ILSVRC12 and WebVision. The number outside</cell></row><row><cell cols="3">(inside) the parentheses denotes the top-1 (top-5) classification</cell></row><row><cell cols="3">accuracy(%).  † marks the method trained using extra clean labels. Data Method ILSVRC12 WebVision</cell></row><row><cell>Full Lee et al. (2018) † Full Vanilla Full MentorNet (2018) † Full Guo et al. (2018) † Full Saxena et al. (2019) Full Ours (MentorMix) Mini MentorNet (2018) Mini Chen et al. (2019) Mini Ours (MentorMix)</cell><cell>61.0(82.0) 61.7(82.4) 64.2(84.8) 64.8(84.9) -67.5(87.2) 63.8(85.8) 61.6(85.0) 72.9(91.1)</cell><cell>69.1(86.7) 70.9(88.0) 72.6(88.9) 72.1(89.2) 67.5(--) 74.3(90.5) -65.2(85.3) 76.0(90.2)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Rooyen, B., Menon, A., and Williamson, R. C. Learning with symmetric label noise: The importance of being unhinged. In Conference on Neural Information Processing Systems (NeurIPS), 2015.Veit, A., Alldrin, N., Chechik, G., Krasin, I., Gupta, A., and Belongie, S. Learning from noisy large-scale datasets with minimal supervision. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. Comparison of symmetric label noise (Blue noise) and web label noise (Red noise). From left to right, columns are true positive images, images with incorrect symmetric labels, and images with incorrect web labels from text-to-image search and image-to-image search, respectively. The image-to-image search results (the last column) only account for 18% in our dataset and fewer images are shown as a result.</figDesc><table><row><cell cols="4">Supplementary Materials for Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels Lu Jiang Di Huang Mason Liu Weilong Yang Van Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. Matching networks for one shot learning. In Conference on Neural Information Processing Systems (NeurIPS), true positive blue noise red noise</cell></row><row><cell>2016.</cell><cell>synthetic symmetric labels</cell><cell>text search</cell><cell>image search</cell></row><row><cell cols="2">Wang, Y., Liu, W., Ma, X., Bailey, J., Zha, H., Song, L., and Xia, S.-T. Iterative learning with open-set noisy labels. In Conference on Computer Vision and Pattern Recognition</cell><cell></cell><cell></cell></row><row><cell cols="2">(CVPR), 2018. Xiao, T., Xia, T., Yang, Y., Huang, C., and Wang, X. Learn-ing from massive noisy labeled data for image classifi-cation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2015. Ladybug Mini-ImageNet Ladybug</cell><cell>Ladybug</cell><cell>Ladybug</cell></row><row><cell cols="2">Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning requires rethinking gen-eralization. In International Conference on Learning Representations (ICLR), 2017.</cell><cell></cell><cell></cell></row><row><cell cols="2">tional Conference on Learning Representations (ICLR), mixup: Beyond empirical risk minimization. In Interna-Zhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D. Orange Orange</cell><cell>Orange</cell><cell>Orange</cell></row><row><cell>2018.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Zhang, H., Yu, Y., Jiao, J., Xing, E. P., Ghaoui, L. E., and Jordan, M. I. Theoretically principled trade-off between robustness and accuracy. International Conference on Machine Learning (ICML), 2019. Conference on Neural Information Processing Systems for training deep neural networks with noisy labels. In Zhang, Z. and Sabuncu, M. Generalized cross entropy loss Honda Accord Sedan Stanford Cars Honda Accord Sedan</cell><cell>Honda Accord Sedan</cell><cell>Honda Accord Sedan</cell></row><row><cell>(NeurIPS), 2018.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Zhang, Z., Zhang, H., Arik, S. O., Lee, H., and Pfister, T. Conference on Computer Vision and Pattern Recognition Distilling effective supervision from severe label noise. In Hummer SUV Hummer SUV</cell><cell>Hummer SUV</cell><cell>Hummer SUV</cell></row><row><cell>(CVPR), 2020. Figure 1.</cell><cell></cell><cell cols="2">(ICML), 2019. labels. International Conference on Machine Learning inference via generative classifiers for handling noisy Lee, K., Yun, S., Lee, K., Lee, H., Li, B., and Shin, J. Robust</cell></row></table><note>Lee, K.-H., He, X., Zhang, L., and Yang, L. Cleannet: Transfer learning for scalable image classifier training with label noise. Conference on Computer Vision and Pattern Recognition (CVPR), 2018.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 1 .</head><label>1</label><figDesc>Summary of the difference of images with blue and red noisy labels.</figDesc><table><row><cell>arXiv:1911.09781v3 [cs.LG] 27 Aug 2020</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 .</head><label>2</label><figDesc>Overview of the ImageNet architectures used in our study.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 .</head><label>4</label><figDesc>Best peak accuracy (%) for baseline methods fine-tuned on Mini-ImageNet. The peak and final validation accuracies are shown in the format of XXX/YYY.Table 5. Best peak accuracy (%) for baseline methods trained from scratch on Mini-ImageNet. The peak and final validation accuracies are shown in the format of XXX/YYY. '-' denotes the method that has failed to converge.</figDesc><table><row><cell cols="2">Type Noise Level</cell><cell>Vanilla</cell><cell>WeDecay Dropout</cell><cell cols="2">S-Model Reed Soft</cell><cell>Mixup</cell><cell>MentorNet MentorMix</cell></row><row><cell>Blue Noise</cell><cell>0 5 10 15 20 30 40 50 60 80</cell><cell cols="3">85.1/83.3 84.5/81.9 85.0/83.6 85.2/83.5 84.5/82.0 84.2/79.7 84.5/80.8 84.1/81.5 83.9/77.8 83.8/75.9 84.3/78.0 84.3/78.2 83.6/75.8 83.1/73.8 83.6/74.7 83.7/76.1 83.5/74.0 82.8/71.9 83.5/73.9 83.9/73.7 82.9/66.6 82.8/64.1 82.4/67.5 82.4/66.0 81.6/61.0 81.1/58.5 82.4/59.0 82.2/60.2 80.9/50.5 80.5/47.9 82.0/68.1 80.0/50.8 80.5/40.9 79.7/40.3 81.4/38.9 80.7/42.3 76.1/19.7 76.5/17.6 78.9/24.7 76.9/20.2</cell><cell>85.6/84.3 84.7/81.9 84.4/81.5 84.7/80.4 84.5/79.2 83.0/76.7 83.0/78.3 81.6/53.1 81.6/45.2 77.8/22.2</cell><cell>83.4/82.0 83.5/82.0 83.3/80.1 84.0/79.5 82.9/77.9 83.3/74.6 81.8/70.8 79.9/64.6 79.2/63.6 76.1/60.0</cell><cell>84.8/83.4 84.9/84.5 84.5/84.3 84.5/81.2 84.3/76.5 83.2/74.0 82.5/80.4 81.8/71.0 81.0/79.8 77.5/32.3</cell><cell>85.0/83.9 84.8/84.0 85.2/83.6 84.6/83.2 84.5/83.6 84.5/82.8 84.3/81.3 83.7/79.0 83.4/77.9 82.0/73.3</cell></row><row><cell>Red Noise</cell><cell>0 5 10 15 20 30 40 50 60 80</cell><cell cols="3">84.7/82.6 83.6/81.9 84.3/83.0 85.0/83.3 84.6/80.0 84.0/78.9 84.6/80.2 84.7/80.4 83.9/78.1 83.9/78.1 83.9/78.1 84.5/78.4 82.3/77.4 83.0/76.5 83.1/76.5 83.0/77.5 82.9/76.1 82.9/75.6 82.7/75.5 82.5/76.9 82.0/74.6 81.2/74.6 81.6/73.7 82.5/73.8 80.7/74.0 81.2/71.7 81.2/72.6 81.4/73.9 80.3/72.9 80.3/71.4 80.7/71.9 80.5/72.2 78.6/70.3 79.2/68.8 80.1/69.4 78.8/70.1 76.3/64.6 76.1/63.1 76.1/63.4 76.7/65.0</cell><cell>84.8/82.5 85.2/80.9 84.3/78.6 84.3/78.0 84.3/77.4 83.0/74.7 82.3/73.1 81.7/72.4 80.7/69.5 76.7/65.3</cell><cell>83.0/81.8 84.8/82.3 84.1/80.8 83.9/80.8 83.2/79.4 83.3/78.6 82.4/77.6 82.0/77.3 80.4/75.5 76.6/71.3</cell><cell>84.6/83.5 84.6/84.0 84.3/84.1 83.6/82.9 83.5/83.4 82.7/82.6 81.9/80.2 81.1/79.3 80.5/73.4 76.8/72.8</cell><cell>84.4/83.4 85.3/85.0 85.1/85.0 85.5/84.7 85.0/84.6 83.9/81.4 83.1/81.5 82.2/80.4 81.0/77.1 77.2/74.0</cell></row><row><cell cols="2">Type Noise Level</cell><cell>Vanilla</cell><cell>WeDecay Dropout</cell><cell cols="2">S-Model Reed Soft</cell><cell>Mixup</cell><cell>MentorNet MentorMix</cell></row><row><cell>Blue Noise</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 .</head><label>6</label><figDesc>Best peak accuracy (%) for baseline methods trained from scratch on Stanford Cars. The peak and final validation accuracies are shown in the format of XXX/YYY.Table 7. Best peak accuracy (%) for baseline methods trained from scratch on Stanford Cars. The peak and final validation accuracies are shown in the format of XXX/YYY. '-' denotes the method that has failed to converge.</figDesc><table><row><cell cols="2">Type Noise Level</cell><cell>Vanilla</cell><cell>WeDecay Dropout</cell><cell cols="2">S-Model Reed Soft</cell><cell>Mixup</cell><cell>MentorNet MentorMix</cell></row><row><cell>Blue Noise</cell><cell>0 5 10 15 20 30 40 50 60 80</cell><cell cols="3">91.2/90.6 92.4/92.2 91.9/91.3 91.0/90.7 88.8/87.7 90.8/90.5 89.5/88.4 88.8/88.3 86.4/84.6 89.1/87.9 87.7/85.1 85.4/84.2 83.6/81.7 87.5/86.4 85.6/81.7 83.9/81.4 81.3/78.2 84.9/82.9 83.7/77.4 81.2/78.1 76.7/68.2 79.1/75.3 78.6/67.0 75.7/68.5 69.3/56.8 72.9/63.8 71.9/56.0 69.7/58.2 58.8/44.1 61.2/48.7 62.0/44.2 59.2/45.1 47.5/32.8 49.4/36.9 50.9/31.8 46.0/32.4 16.1/10.8 15.2/10.0 15.5/10.1 16.0/10.6</cell><cell>91.3/91.0 88.8/88.8 85.4/87.5 83.9/86.4 81.2/83.9 75.7/79.8 69.7/71.8 59.2/60.0 46.0/47.8 16.0/15.9</cell><cell>91.7/91.6 90.3/90.0 89.1/88.9 87.7/87.2 85.6/85.6 79.8/76.4 73.6/68.1 63.0/56.2 52.0/43.6 18.3/17.2</cell><cell>90.1/90.0 90.3/89.8 89.5/89.5 89.1/89.1 88.1/87.8 85.3/85.2 80.9/79.3 71.1/66.7 58.5/57.0 15.8/13.8</cell><cell>92.9/92.9 92.4/92.2 91.8/91.8 91.3/91.2 90.5/90.4 87.3/86.3 81.9/77.7 71.7/65.1 60.9/52.5 21.2/19.2</cell></row><row><cell>Red Noise</cell><cell>0 5 10 15 20 30 40 50 60 80</cell><cell cols="3">91.0/90.7 92.3/92.1 91.8/91.2 90.9/90.7 90.3/90.1 91.7/91.7 90.6/89.6 89.8/89.2 89.2/88.5 90.7/90.5 90.0/89.1 89.7/89.1 88.1/87.5 90.1/89.5 Mix/88.1 88.2/87.8 86.9/86.2 89.5/89.0 88.4/87.0 87.3/86.8 85.0/84.3 87.0/86.0 86.3/84.0 85.0/84.2 82.2/81.4 82.4/81.3 83.4/81.7 82.4/80.9 78.4/76.7 80.5/80.0 80.3/77.2 78.1/76.6 73.2/71.4 76.8/75.0 75.6/72.1 73.3/71.6 60.0/57.7 62.5/61.2 62.1/57.3 59.0/56.8</cell><cell>91.2/90.8 90.3/89.6 89.4/88.9 88.7/88.4 87.4/86.1 84.5/83.8 82.6/81.6 78.7/76.7 74.7/72.1 60.9/58.1</cell><cell>92.3/92.3 91.9/91.8 90.7/90.5 89.8/89.7 89.2/89.0 87.1/86.9 84.8/84.3 81.7/81.6 77.7/77.4 64.3/63.0</cell><cell>91.2/91.1 89.7/89.3 89.1/88.7 88.2/87.8 87.7/86.7 84.6/84.3 81.9/81.0 78.3/76.8 74.1/72.9 61.2/57.2</cell><cell>93.2/93.2 92.2/92.2 91.9/91.9 91.4/91.4 90.6/90.5 89.3/89.3 87.4/87.4 84.3/83.9 80.2/80.1 68.1/67.9</cell></row><row><cell cols="2">Type Noise Level</cell><cell>Vanilla</cell><cell>WeDecay Dropout</cell><cell cols="2">S-Model Reed Soft</cell><cell>Mixup</cell><cell>MentorNet MentorMix</cell></row><row><cell>Blue Noise</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">From the red and blue pill in the movie "The Matrix (1999), where the red pill is used to refer to the truth about reality.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This is slightly different from(Zhang et al., 2017)  and is the same as. We do not allow examples to be label flipped to their true labels. It makes p denote the exact noise level and independent of the total number of classes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://support.google.com/websearch/answer/29508</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/tensorflow/models/tree/master/research/slim 2 https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Fine-tuning from ImageNet checkpoints. For fine-tuning experiments, we initialized networks with ImageNet-pretrained weights. We used a similar training protocol for fine-tuning as training from scratch. The start learning rate was stable in fine-tuning so we fixed it to 0.01 and only searched the learning rate decay epochs in {1, 3, 5, 10}. Learning rate warmup was not used in fine-tuning. As fine-tuning converges faster, we scaled down the maximum number of epochs to train by a factor of 2 and trained the network to full convergence. Training in this way, we obtained reasonable performance on the clean Stanford Cars test set. For example, our Inception-ResNet-V2 got 92.4 versus 92.0 reported in and our EfficientNet-B5 got 93.8% versus 93.6% reported in<ref type="bibr" target="#b52">(Tan &amp; Le, 2019)</ref>.Baseline comparison. For method comparison, we used Inception-ResNet as the default network. All methods employed the identical setting discussed above, including the same start learning rate, learning rate decay factor, batch size, and the maximum number of epochs to train. For Dropout, as it converges slower, we added another 100 epochs to its #maximum 3 https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/preprocessing.py</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/google/mentornet 5 https://www.vision.ee.ethz.ch/webvision/download.html Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labelsthe blue noise. The standard deviations of red noise are comparable among all three settings. These results show that DNNs generalize much better on red label noise despite specific approaches used to construct the dataset.Table 3. Comparison of the standard deviation of the final accuracies across 10 noise levels. A higher standard deviation suggests a poorer generalization performance of DNNs trained on noisy labels.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank anonymous reviewers for helpful comments and Boqing Gong and Fei Sha for meaningful discussions and kind support. The data labeling is supported by the Google Cloud labeling service.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrzkebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Auxiliary image regularization for deep cnns with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Label-noise robust logistic regression and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bootkrajang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">; N</forename><surname>Kabán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD), 2012. Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels Charoenphakdee</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>On symmetric losses for learning from corrupted labels. International Conference on Machine Learning (ICML)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding and utilizing deep neural networks trained with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Webly supervised learning of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust neural machine translation with doubly adversarial inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Advaug: Robust adversarial augmentation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-paced learning: an implicit regularization perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Training deep neuralnetworks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Curriculumnet: Weakly supervised learning from large-scale web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Masking: A new perspective of noisy supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using trusted data to train deep networks on labels corrupted by severe noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">O2u-net: A simple noisy label detection approach for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-paced learning with diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-paced curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mentornet</surname></persName>
		</author>
		<title level="m">Learning data-driven curriculum for very deep neural networks on corrupted labels. International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Do better imagenet models transfer better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Collecting a large-scale dataset of fine-grained cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Workshop on Fine-Grained Visual Categorization</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of noisy data for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dividemix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02862</idno>
		<title level="m">Webvision database: Visual learning and understanding from web data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to detect concepts from webly-labeled video data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Simaug: Learning robust representations from simulation for trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dimensionalitydriven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to label aerial images from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Regularizing deep neural networks by noise: its interpretation and optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Northcutt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">L</forename><surname>Chuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00068</idno>
		<title level="m">Confident learning: Estimating uncertainty in dataset labels</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishna Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fast rates for a knn classifier robust to unknown asymmetric label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Reeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kabán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rolnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shavit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10694</idno>
		<title level="m">Deep learning is robust to massive label noise</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Data parameters: A new family of parameters for learning a differentiable curriculum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Combinatorial inference against label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Meta-weight-net: Learning an explicit mapping for sample weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Refurbishing unclean samples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Selfie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alemi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Toward robustness against label noise in training deep discriminative neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B5</forename><surname>Efficientnet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<title level="m">Inception V2 (Ioffe &amp; Szegedy</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
		<title level="m">Inception V3</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
		<idno>Inception-ResNet V2</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V2 (</forename><surname>Mobilenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sandler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<idno>23.5M 224 75.2 75.9</idno>
	</analytic>
	<monogr>
		<title level="j">ResNet</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ResNet</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Training in this way, we obtained reasonable performance on the clean Stanford Cars validation set. For example, our Inception-ResNet-V2 got 90.8 (without dropout) and 92.4 (with dropout) versus 89</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kornblith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note>EfficientNet 3 for data augmentation and evaluated on the central cropped images on the validation set</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mobienet-V2</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet-B5</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Inception-v2</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Inception-v3</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Inception-v3</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Inception-v3 (b4) red noise setting 2 (0.920) --Stanford Cars</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<title level="m">MobieNet-V2 Inception-ResNet-v2</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet-B5</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">References</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Accurate</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mentornet</surname></persName>
		</author>
		<title level="m">Learning data-driven curriculum for very deep neural networks on corrupted labels. International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Do better imagenet models transfer better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02862</idno>
		<title level="m">Webvision database: Visual learning and understanding from web data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks. In Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alemi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
