<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MONAURAL SPEECH ENHANCEMENT WITH COMPLEX CONVOLUTIONAL BLOCK ATTENTION MODULE AND JOINT TIME FREQUENCY LOSSES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Trung</roleName><forename type="first">Shengkui</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Speech Lab, Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Speech Lab, Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Speech Lab, Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MONAURAL SPEECH ENHANCEMENT WITH COMPLEX CONVOLUTIONAL BLOCK ATTENTION MODULE AND JOINT TIME FREQUENCY LOSSES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-speech enhancement</term>
					<term>attention mecha- nism</term>
					<term>complex network</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep complex U-Net structure and convolutional recurrent network (CRN) structure achieve state-of-the-art performance for monaural speech enhancement. Both deep complex U-Net and CRN are encoder and decoder structures with skip connections, which heavily rely on the representation power of the complex-valued convolutional layers. In this paper, we propose a complex convolutional block attention module (CCBAM) to boost the representation power of the complexvalued convolutional layers by constructing more informative features. The CCBAM is a lightweight and general module which can be easily integrated into any complex-valued convolutional layers. We integrate CCBAM with the deep complex U-Net and CRN to enhance their performance for speech enhancement. We further propose a mixed loss function to jointly optimize the complex models in both time-frequency (TF) domain and time domain. By integrating CCBAM and the mixed loss, we form a new end-to-end (E2E) complex speech enhancement framework. Ablation experiments and objective evaluations show the superior performance of the proposed approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Speech enhancement is a highly desired task in speech communication applications where the perceptual quality and intelligibility can be severely decreased by the additive background noise. The goal of speech enhancement is to extract the signal of interest from the corrupted noisy speech for better perceptual quality and intelligibility.</p><p>For decades, monaural speech enhancement has been a challenging problem. Recently, deep learning methods have made significant performance improvement over conventional signal processing based methods. Given clean speech and background noise, the speech enhancement task can be formulated as a supervised learning problem with the simulated noisy speech as input and the clean speech as target. Although researchers have tried to build deep learning models directly on time-domain speech signals <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, it is more common to work on time-frequency (TF) representations via short-time-Fourier-transform (STFT). Comparing to the spectral mapping approach <ref type="bibr" target="#b3">[4]</ref>, the TF mask-based approach has been more popular <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. Early studies usually focus on enhancing the magnitude spectrum while reusing the noisy phase spectrum. Recent studies show the phase and magnitude components are relative important in terms of speech perceptual quality and intelligibility, leading researchers to consider both magnitude and phase estimation. The phasesensitive mask (PSM) <ref type="bibr" target="#b7">[8]</ref> was one of the first attempts to incorporate phase information into mask estimation. However, it still estimates a real component. Subsequently, the ideal complex ratio mask (CRM) <ref type="bibr" target="#b8">[9]</ref> was proposed to estimate both the real and the complex components and it theoretically achieves the best oracle speech enhancement performance. The pioneer work <ref type="bibr" target="#b8">[9]</ref> attempts to estimate CRM via realvalued network operations, which are not coincided well with the complex-valued masks. More recently, deep complex networks <ref type="bibr" target="#b9">[10]</ref> were developed and the complex-valued models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> achieve state-of-the-art (SOTA) performance for speech enhancement. We found that the SOTA models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> are built on either the U-Net structure <ref type="bibr" target="#b3">[4]</ref> or the convolutional recurrent network (CRN) structure <ref type="bibr" target="#b13">[14]</ref>, which heavily rely on the representation power of the convolutional layers. The attention mechanisms <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> proposed for image processing that fuse both spatial and channel-wise information to boost the representation power of the convolutional layers have not been exploited in the SOTA complex models for speech enhancement.</p><p>In this paper, we present a lightweight and general complex-valued channel-spatial attention mechanism, built upon previous studies <ref type="bibr" target="#b15">[16]</ref> to boost the representation power of the complex-valued convolutional layers, therefore enhance capabilities of the complex-valued SOTA models, e.g., U-Net and CRN, for speech enhancement. Our attention mechanism is formed as an individual module called complex convolutional block attention module (CCBAM) which can be easily integrated into any complex-valued convolutional layers with negligible overheads and is end-to-end trainable along with base network. The CCBAM can be interpreted as a means of biasing the allocation of available computational resources towards the most informative features for optimizing representation power. Our experiments suggest adding CCBAM to both the decoder layers and the  skip connections provide better results and achieve superior performance. Besides the proposed attention mechanism, we also add mean-squared error (MSE) loss of the real and the complex components of CRM to the scale-invariant sourceto-noise ratio (SI-SNR) loss <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref> to jointly optimize the CRM estimation in the TF domain and the time domain. Our experiments will demonstrate the effectiveness of the proposed joint loss functions for speech enhancement.</p><p>The related works are described as follows. The attention blocks named self-attention were applied for speech enhancement <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref>. The self-attention focused on the response of positions in a sequence while CCBAM focuses on cross-channel and spatial information of feature maps. Another related work is the spatial-channel gating scheme proposed for the U-Net structure <ref type="bibr" target="#b17">[18]</ref>, which addressed medical image segmentation problem. <ref type="bibr" target="#b18">[19]</ref> also applied a concurrent space-channel-wise attention to the redundant convolutional encoder-decoder (RCED) for speech enhancement. However, their work was based on magnitude spectrum mapping. The above mentioned works were based on real-valued networks and we focus on complex-valued networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">THE COMPLEX CONVOLUTIONAL BLOCK ATTENTION MODULE</head><p>Our proposed CCBAM is a refined complex-valued attention mechanism applied in STFT-domain based on the work described in <ref type="bibr" target="#b15">[16]</ref>. It is composed of a complex channel-attention module and a complex spatial-attention module as shown in <ref type="figure" target="#fig_0">Fig. 1</ref> and <ref type="figure" target="#fig_1">Fig. 2</ref>. Both modules consist of a squeeze operation and an excitation operation. The squeeze operation involves pooling process to aggregate information. The excitation process applies the aggregated information to generate recalibration weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Complex channel-attention module</head><p>Let the output complex feature map be U = U r + jU i where U ∈ C H×W ×C from the complex-valued convolutional filter</p><formula xml:id="formula_0">W = W r + jW i with complex input matrix V = V r + jV i .</formula><p>The complex convolution operation can be formulated as:</p><formula xml:id="formula_1">U r = V r * W r − V i * W i U i = V r * W i + V i * W r<label>(1)</label></formula><p>where * stands for real-valued convolutional filtering. Applying global average pooling to U r and U i to aggregate spatial information of the feature map, respectively, we obtain U avg ∈ C C× C r , with r standing for a reduction ratio. The complex-valued channel-attention gate G c ∈ R 1×1×C is then computed as:</p><formula xml:id="formula_2">r = AvgPool(U r ), U avg i = AvgPool(U i ). Similarly, applying max pooling we obtain U max r = MaxPool(U r ), U max i = MaxPool(U i</formula><formula xml:id="formula_3">G c = σ(W FC 2 δ(W FC 1 U avg )) + σ(W FC 2 δ(W FC 1 U max )) (2)</formula><p>where δ refers to complex-valued ReLU function and σ refers to complex-valued sigmoid function <ref type="bibr" target="#b9">[10]</ref>, and both ReLU and sigmoid apply on real and imaginary values, respectively. In Eq. (2), the weights W FC 1 and W FC 2 are shared between U avg and U max . The complex FC operations can be performed in a similar way as the complex convolution in Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Complex spatial-attention module</head><p>Applying average-pooling and max-pooling operations to U r and U i to aggregate channel information of the feature map U , respectively, and then applying a complex-valued twodimensional (2D) convolutional filter F = F r + jF i to the concatenated pooling ouput, we obtain the complex spatialattention gate G s ∈ R H×W ×1 as:</p><formula xml:id="formula_4">G s = σ(F [AvgPool(U ); MaxPool(U )])<label>(3)</label></formula><p>where σ denotes the complex-valued sigmoid function and [AvgPool(U ); MaxPool(U )] represents the concatenation operation along the channel axis. The pooling operations are performed on U r and U i separately. The complex convolution operation works similarly as defined in Eq. (1). The convolution filter size is a hyperparameter and we set to 7 × 7 for better performance in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">INTEGRATION OF CCBAM WITH DEEP COMPLEX U-NET AND CRN ARCHITECTURES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Integration of CCBAM</head><p>The deep complex U-Net (DCUnet) <ref type="bibr" target="#b10">[11]</ref> is a complex-valued network design of the real-valued U-Net structure proposed  in <ref type="bibr" target="#b3">[4]</ref> for image segmentation. The DCUnet is composed of a complex-valued convolutional encoder and a decoder with skip-connections as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. The encoder gradually extracts high-lever features with a down-sampling process and the decoder reconstructs the target map with an upsampling process. The encoder and the decoder use complex 2D convolutional and deconvolutional layers, respectively. Each complex convolutional/deconvolutional operation is followed by complex batch normalization and leaky ReLU activation. The skip-connections connect the convolutional layers from the encoder to the corresponding deconvolutional layers of the decoder. The deep complex CRN (DCCRN) <ref type="bibr" target="#b12">[13]</ref> is a complex-valued network design of the real-valued CRN structure proposed in <ref type="bibr" target="#b13">[14]</ref>. Comparing to the DCUnet, the DCCRN is also composed of a complex-valued convolutional encoder and a decoder with skip-connections. The essential difference is that the DCCRN added LSTM layers between the encoder and the decoder aiming to model the temporal dependencies. Since the DCUnet and DCCRN share equivalent encoder and decoder structure, we integrate CCBAM into DCUnet and DCCRN similarly. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the overall integrated architecture of the convolutional encoder-decoder network with our proposed CCBAM. We insert the two complex channel and spatial attention modules of CCBAM into the decoder paths and the skip connection paths in a sequential manner. The CCBAM performs as self-gating mechanism to recalibrate the TF feature maps adaptively in consideration of the global information. It can help the skip connections to by-pass meaningful information that are beneficial for CRM estimation. Meanwhile, the CCBAM is also expected to help the decoder to concentrate on 'what' and 'where' of feature maps when performing target reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The end-to-end speech enhancement method</head><p>The proposed end-to-end speech enhancement method is illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>. Assuming the noisy speech signal is denoted as x(n) = y(n) + z(n) ∈ R where y(n) is the clean speech signal and z(n) the background noise, the speech enhancement task is to estimate y(n) from x(n). Let X = X r +jX i ∈ C, Y = Y r +jY i ∈ C denote the STFT representations of x(n) and y(n), respectively. The CRM method <ref type="bibr" target="#b8">[9]</ref> is to estimate the complex-valued mask M = M r +jM i ∈ C such that Y = M X, where indicates complex multiplication. The ground-truth CRM can be computed as:</p><formula xml:id="formula_5">M = X r · Y r + X i · Y i X 2 r + X 2 i + j X r · Y i − X i · Y r X 2 r + X 2 i .<label>(4)</label></formula><p>Estimating the unbounded components M r and M i is very challenging. In our speech enhancement model, we propose to add tanh function to bound the estimated real and imaginary components,M r andM i , ofM ∈ C. The estimated clean speech spectrogramŶ is then obtained aŝ</p><formula xml:id="formula_6">Y = (X r ·M r − X i ·M i ) + j(X r ·M i + X i ·M r ). (5)</formula><p>The estimated time domain clean speechŷ(n) is obtained by applying ISTFT onŶ .</p><p>In the SOTA frameworks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref>, the neural models are optimized based only on the time domain loss function SI-SNR or weighted SI-SNR. The SI-SNR loss may not be fully correlated with the CRM estimation. We propose an improved mixed loss function by integrating SI-SNR loss with mean squared error (MSE) losses of the real and the imaginary CRM estimates. Specifically, we optimize the neural model by the following loss function</p><formula xml:id="formula_7">L(y,ŷ) = λ SI−SNR L SI−SNR (y,ŷ) + λ Mask L Mask (M,M ),<label>(6)</label></formula><p>where the time domain SI-SNR loss L SI−SNR (y,ŷ) is defined as <ref type="bibr" target="#b12">[13]</ref>:</p><formula xml:id="formula_8">L SI−SNR (y,ŷ) = 10log10 ||y target || 2 2 ||e noise || 2 2<label>(7)</label></formula><p>where y target is given as: y target = (&lt;ŷ, y &gt; ·y)/||y|| 2 2 and the residual noise e noise is e noise =ŷ − y target . Here, &lt; ·, · &gt; denotes dot product and || · || 2 is L2 norm. The mask loss function is defined as</p><formula xml:id="formula_9">L Mask (M,M ) = t,f [(M r − M r ) 2 + (M i − M i ) 2 ] (8)</formula><p>where t, f are omitted for simplicity. The STFT and ISTFT operations are implemented as 1-D convolutional and deconvolutional layers to facilitate the end-to-end training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluated and compared the proposed methods on two datasets. The dataset-1 is a relative small set where we selected about 50 hours clean speech from WSJ0 <ref type="bibr" target="#b19">[20]</ref> and about 50 hours noise from DEMAND <ref type="bibr" target="#b20">[21]</ref> and RNNoise 1 . The clean speech includes 131 speakers and the noise includes various types of computer fans, office, crowd, airplane, car, train, construction, etc. We used 40 hours of clean speech and 40 hours of noise for training and validation, and the rest for test. The noisy speech mixture was obtained by randomly mixing speech utterances with noise segments. The noise segments were split or concatenated to meet the length of speech utterances. A wide range of SNRs between 0 dB and 20 dB were randomly generated for both training and evaluation. The total training data was about 100 hours and the total evaluation data was about 30 minutes.</p><p>The dataset-2 is a relative big set provided by the DNS challenge <ref type="bibr" target="#b21">[22]</ref>. It contains over 500 hours clean speech and 180 hours noise selected from various public resources. For the noisy speech mixture, we used the SNR range between -5 dB to 20 dB. The clean speech and the noise were randomly mixed and we generated a total of 2000 hours training data. Note that we targeted only clean speech and background noise in this work and leave the reverberant case for future work. We evaluated on the official synthetic test set of the DNS challenge. All waveforms were resampled at 16k Hz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training setup</head><p>For the DCUnet model, we used the same model configuration as the DCUnet-20 shown in <ref type="figure">Fig. 7</ref> in <ref type="bibr" target="#b10">[11]</ref>. The DCUnet-20 has 20 convolutional layers and the total number parameters are 3.5M. We replaced the original SI-SNR loss of the DCUnet-20 with our proposed mixed loss and adopted the model into our end-to-end method as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. The resulted model was denoted as DCUnet-M. We further applied our proposed CCBAM into DCUnet-M and denoted the model as DCUnet-MC. We also followed the same preprocessing in <ref type="bibr" target="#b10">[11]</ref> to apply STFT with a 64ms sized Hann window and 16ms hop length. For the DCCRN model, we used the real-time CRN model configuration as described in <ref type="bibr" target="#b13">[14]</ref>. The original CRN model was proposed for magnitude spectrum mapping. We extended it to complex-valued network as described in <ref type="bibr" target="#b12">[13]</ref> with 20M trainable parameters. Similar to DCUNet, we integrated DCCRN with our proposed mixed loss and CCBAM. The resulted models were denoted as DCCRN-M and DCCRN-MC, respectively. For DCCRN, the window length and hop size were 20 ms and 10 ms. For all model training, we used Pytorch platform and Adam optimizer. We set learning rate to 0.001 and decay it by 0.5 when the validation score increases. For the loss parameters, we set λ SI−SNR = 0.5 and λ Mask = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>For the objective evaluation, we used 4 evaluation metrics: PESQ, STOI, SI-SNR, and FwSegSNR <ref type="bibr" target="#b22">[23]</ref>. <ref type="table" target="#tab_1">Table 1</ref> and 2 show objective metric evaluation results on dataset-1 and dataset-2, respectively. From the results, we can see that the DCUnet-M and DCCRN-M outperform the original baseline DCUnet and DCCRN. This shows the proposed mixed loss is beneficial to the CRM estimation. Moreover, we can see that DCUnet-MC and DCCRN-MC outperforms their corresponding models without attention mechanism. This shows the benefits of our proposed CCBAM for model optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>In this paper, we presented a complex convolutional block attention module (CCBAM) and a mixed loss function to improve the deep complex U-Net and CRN models for monaural speech enhancement. The integrated end-to-end complex speech enhancement framework with the deep complex U-Net and CRN models achieve better performance compared to their original models. We conducted experimental studies and demonstrated the superior performance of the proposed methods on two different datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Diagram of complex channel attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Diagram of complex spatial attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of CCBAM integrated with convolutional encoder-decoder structure with skip connections. The symbol ⊗ stands for element multiplication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Illustration of the proposed end-to-end speech enhancement method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). Then, we apply two complex-valued FC layers to U avg = U avg r</figDesc><table><row><cell>+ jU avg i , respectively. Let the complex weights of the two FC and U max = U max + r layers be W FC jU max i 1 = W FC 1r +jW FC 1i and W FC 2 = W FC 2r +jW FC 2i , where W FC 1 ∈ C C r ×C and W FC 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Average performance evaluation results for different models on dataset-1.</figDesc><table><row><cell>Model</cell><cell cols="4">Evaluation Metrics PESQ STOI SI-SNR FwSegSNR</cell></row><row><cell>Noisy</cell><cell>1.97</cell><cell>87.83</cell><cell>6.28</cell><cell>10.64</cell></row><row><cell>DCCRN</cell><cell>3.17</cell><cell>95.80</cell><cell>17.71</cell><cell>21.35</cell></row><row><cell>DCCRN-M</cell><cell>3.28</cell><cell>96.30</cell><cell>18.03</cell><cell>23.06</cell></row><row><cell>DCCRN-MC</cell><cell>3.34</cell><cell>96.76</cell><cell>18.42</cell><cell>24.18</cell></row><row><cell>DCUnet</cell><cell>3.25</cell><cell>96.26</cell><cell>18.31</cell><cell>20.16</cell></row><row><cell>DCUnet-M</cell><cell>3.38</cell><cell>96.85</cell><cell>18.59</cell><cell>22.10</cell></row><row><cell>DCUnet-MC</cell><cell>3.44</cell><cell>97.10</cell><cell>19.33</cell><cell>23.81</cell></row><row><cell cols="5">Table 2. Average performance evaluation results for different</cell></row><row><cell cols="2">models on dataset-2.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="4">Evaluation Metrics PESQ STOI SI-SNR FwSegSNR</cell></row><row><cell>Noisy</cell><cell>2.45</cell><cell>91.52</cell><cell>9.17</cell><cell>13.55</cell></row><row><cell>DNS Baseline</cell><cell>2.68</cell><cell cols="2">77.62 -26.63</cell><cell>7.39</cell></row><row><cell>DCCRN</cell><cell>3.04</cell><cell>93.95</cell><cell>15.70</cell><cell>19.77</cell></row><row><cell>DCCRN-M</cell><cell>3.15</cell><cell>94.87</cell><cell>16.03</cell><cell>19.92</cell></row><row><cell>DCCRN-MC</cell><cell>3.21</cell><cell>95.10</cell><cell>16.50</cell><cell>20.65</cell></row><row><cell>DCUnet</cell><cell>3.20</cell><cell>94.80</cell><cell>16.48</cell><cell>19.53</cell></row><row><cell>DCUnet-M</cell><cell>3.24</cell><cell>95.88</cell><cell>16.92</cell><cell>19.63</cell></row><row><cell>DCUnet-MC</cell><cell>3.30</cell><cell>96.11</cell><cell>17.46</cell><cell>20.31</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://media.xiph.org/rnnoise</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segan: Speech enhancement generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3642" to="3646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A wavenet for speech denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rethage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">speech, and language processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computerassisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Binary and ratio time-frequency masks for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1486" to="1501" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ideal ratio mask estimation using deep neural networks for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="7092" to="7096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On training targets for supervised speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">speech, and language processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1849" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Phase sensitive and recognition-boosted speech separation using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="708" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Complex ratio masking for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">speech, and language processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="483" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bilaniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Phase-aware speech enhancement with deep complex u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03107</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">PoCoNet: Better speech enhancement with frequency-positional embeddings, semi-supervised conversational data, and biased loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Phansalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Valin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Helwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DCCRN: Deep complex convolution recurrent network for phase-aware speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A convolutional recurrent neural network for real-time speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">CBAM: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Speech enhancement using selfadaptation and multi-head self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yatabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Masuyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Takeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Enhancing u-net with spatial-channel attention gate for abnormal tissue segmentation in medical imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L B</forename><surname>Khanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-T</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Combining multi-perspective attention mechanism with convolutional networks for monaural speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2020.2989861</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="78979" to="78991" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Csr-i (wsj0) complete ldc93s6a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pallett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Web Download. Philadelphia: Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">83</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DEMAND: a collection of multi-channel recordings of acoustic noise in diverse environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thiemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Meetings Acoust</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Beyrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matusevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aazami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13981</idno>
		<title level="m">The interspeech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Evaluation of objective quality measures for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM transactions on audio</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>and language processing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
