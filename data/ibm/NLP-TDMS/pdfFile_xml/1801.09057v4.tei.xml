<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Aligned to the Object, not to the Image: A Unified Pose-aligned Representation for Fine-grained Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Guo</surname></persName>
							<email>peiguo@cs.byu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Brigham Young University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Farrell</surname></persName>
							<email>farrell@cs.byu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Brigham Young University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Aligned to the Object, not to the Image: A Unified Pose-aligned Representation for Fine-grained Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dramatic appearance variation due to pose constitutes a great challenge in fine-grained recognition, one which recent methods using attention mechanisms or second-order statistics fail to adequately address. Modern CNNs typically lack an explicit understanding of object pose and are instead confused by entangled pose and appearance. In this paper, we propose a unified object representation built from a hierarchy of pose-aligned regions. Rather than representing an object by regions aligned to image axes, the proposed representation characterizes appearance relative to the object's pose using pose-aligned patches whose features are robust to variations in pose, scale and rotation. We propose an algorithm that performs pose estimation and forms the unified object representation as the concatenation of hierarchical pose-aligned regions features, which is then fed into a classification network. The proposed algorithm surpasses the performance of other approaches, increasing the state-of-the-art by nearly 2% on the widely-used  dataset and by more than 8% on the much larger NABirds [41] dataset. The effectiveness of this paradigm relative to competing methods suggests the critical importance of disentangling pose and appearance for continued progress in fine-grained recognition.</p><p>Top 25 activated images with feature maps as masks, indicating the interested patterns of one final conv-layer filter.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>What makes fine-grained visual categorization (FGVC), commonly referred to as fine-grained recognition, different from general visual categorization? One important distinction lies in the difficulty of the datasets. General-purpose visual categorization often involves the classification of everyday objects, such as chairs, bicycles and dogs, which are easy for humans to identify. Fine-grained recognition, on the other hand, consists of more detailed classifications such as identifying the species of a bird. This is extremely difficult for non-expert humans as it requires familiarity with  <ref type="figure">Figure 1</ref>: Motivation for Pose-Aligned Regions. Two terns of the same species, but in different poses, have dramatically different appearances while two different species of woodpecker appear nearly identical except for the barring pattern on the outer tail (and the shape of the beak). The large intra-class variance and small inter-class variance make the feature space distance inaccurately reflect the true class relationships. Such observations motivate the use of pose-aligned regions that disentangle intrinsic part appearance from variations in object pose, leading to a feature space that facilitates correctly classifying the species or fine-grained category.</p><p>domain knowledge and hundreds of hours of training. Computer algorithms for fine-grained recognition have the potential to be far more accurate than most humans and can thus benefit millions of people by providing services like species recognition through mobile applications <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>An intrinsic and readily observed quality of fine-grained recognition is small inter-category variance coupled with large intra-category variance. Discriminative features of two visually similar categories often lie in a few key locations, while the appearances of the objects from the same category be dramatically different due simply to pose variation. The entanging of appearance and pose presents a great challenge and motivates the need for stable appearance fea- <ref type="figure">Figure 2</ref>: Filter Visualization as Proof that CNNs Lack Part-Awareness. We visualize the 25 top-activated images for a filter from the ResNet-50 model's final convolutional layer. Modern CNNs are purely appearance-based and lack part-awareness -as a result, different semantic parts (beak, feet, crown, throat, nape or eyes) that are red in color can all activate this filter, causing confusion for the classifier when such semantic parts would be discriminative. Pose-aligned regions can eliminate this problem by disentangling parts and appearance.</p><p>tures, ones that are nearly invariant to variations in pose, scale and rotation.</p><p>It's almost instinct for humans to identify and visually compare key locations across objects in different poses, establishing correspondences. Convolutional neural networks, however, struggle on this task because the convolutional mechanisms are purely appearance-based and lacks an understanding of the pose or geometry. The built in pooling mechanisms can tolerate a certain amount of scale and rotation variation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b5">6]</ref>, but exactly how much is still largely an open question <ref type="bibr" target="#b34">[35]</ref>. We show this in Figure 2 via the visualization of some final convolutional layer responses. We show the top-activated images together with the feature map as a masks. It's evident that this convolutional filter is attuned to red beaks. However, due to its lack of part-awareness, this filter also fires strongly at visually similar parts such as red crowns, red throats, red eyes, etc. This causes confusion for the classifier because of the noisy entangled part-appearance representation.</p><p>In the feature embedding space, dramatic pose variation would make images of the same category farther separated and images of visually-similar categories appear closer together as shown in <ref type="figure">Figure 1</ref>. It is therefore vital that posealigned regions, which explicitly factor out pose variation, should be the building block of the disentangled image representation.</p><p>Recent efforts in fine-grained recognition have largely focused on two directions. One is second-order statistics based algorithms <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b7">8]</ref>. Representative works include Bilinear Pooling <ref type="bibr" target="#b28">[29]</ref> and its reduced-memory variants <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21]</ref> or thosethat extends to higher-order statistics <ref type="bibr" target="#b7">[8]</ref>. The idea is to project the features onto a higherorder space where they can be linearly separated. Secondorder statistics methods have both sound theoretical support and work well in practice. However, they look at the image globally, and thus having little hope of finding subtle highly-localized differences. Also, they lack interpretability and insights for further improvement.</p><p>The other direction is attention-based methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b52">53]</ref> that use subnetworks to propose possible discriminative regions to attend to. However, the regions proposed by these networks are often weakly-supervised by some heuristic loss function, lacking proof that they really attend to the right position. Both of these directions suffer from a lack of pose awareness and moreover the entanglement of pose and appearance features limits their performance. Moreover, training data is often scarce in the longtailed distributions seen in many fine-grained domains; in such cases, both techniques suffer as the limited training imagery does not adequately span the space of pose and viewing angle for each category, hindering their ability to recognize any species in any pose.</p><p>Based on the above observations, we propose to disentagle pose and appearance via a unified object representation built upon pose-aligned regions, defined as rectangular patches defined relative to two keypoints anchors. The final object representation is an aggregation of the features across all of the pose-aligned regions. This representation comprises a pose-invariant and over-complete basis of features from multiple scales. We contrast the pose-aligned regions with weakly-supervised regions that are generated in a purely data-driven fashion and with "axis-aligned" rectangular bounding boxes centered around a keypoint or landmark. The features from these types of regions are subject to the variation of pose, scale and rotation. We experimentally demonstrate that axis-aligned regions are inferior posealigned regions with respect to classification accuracy (see <ref type="figure" target="#fig_4">Figure 6</ref>).</p><p>To automate the process of applying the unified object representation to fine-grained recognition, we propose an algorithm that first does pose-estimation for keypoint detection, enabling the generation of pose-aligned region features. The local features from these aligned regions, regions of varying size/scale relative to the object, are concatenated to comprise the unified representation for the input image and are then fed into a classification network to produce a final classification prediction. We call the proposed algorithm PAIRS: Pose and Appearance Integration for Recognizing Subcategories. It achieves state-of-the-art results on two key fine-grained datasets: CUB-200-2011 <ref type="bibr" target="#b41">[42]</ref> and NABirds <ref type="bibr" target="#b40">[41]</ref>. Keypoint annotations are used only during  <ref type="figure">Figure 3</ref>: Overview of the Proposed Framework for Fine-grained Recognition. We first apply a pose-estimation network to the image for keypoint detection. Pose-aligned regions are then extracted from the image using the predicted keypoint locations. We then extract features from the individual regions using region-specific networks. The concatenated features collectively form a unified multi-scale representation that is invariant to pose, scale and rotation change. This representation is then fed into a classification network for the final fine-grained classification.</p><p>training time. Considering the annotation cost, keypoint annotations are actually less expensive and time-consuming than collecting additional data samples because keypoints can be annotated by human non-experts wheras fine-grained image category annotation requires the consensus of multiple domain experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Related Work</head><p>Fine-grained visual categorization (FGVC) lies between generic category-level object recognition like the VOC <ref type="bibr" target="#b9">[10]</ref>, ImageNet <ref type="bibr" target="#b35">[36]</ref>, COCO <ref type="bibr" target="#b27">[28]</ref>, etc. and instance-level classification like facial recognition and other visual biometrics. The challenges of FGVC are many-fold: differences between similar species are often subtle and highly-localized and thus difficult even for (non-expert) humans to identify. Dramatic pose changes introduce great intra-class variance. Generalization also becomes an issue as the network struggles to find truly useful and discriminative features.</p><p>FGVC has drawn wide attention in the computer vision community. Some early works include <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>. Birdlet, a volumetric poselet representation is proposed to account for the pose and appearance variation in <ref type="bibr" target="#b10">[11]</ref>. <ref type="bibr" target="#b49">[50]</ref> further proposes two pose-normalized descriptors based on computationally-efficient deformable part models. Although these early works seek to integrate pose and appearance like our method does, they rely heavily on hand-engineered descriptors thus have limited success on classification accuracy.</p><p>Our work is related to part-based CNN models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b26">27]</ref>, which seek to decompose the object into semantic parts. <ref type="bibr" target="#b47">[48]</ref> first employs an object detection framework -R-CNN <ref type="bibr" target="#b16">[17]</ref> for object and part detection. Part-Stacked CNN <ref type="bibr" target="#b18">[19]</ref> proposes a fully convolutional network for keypoint detection and a two-stream convolutional network for object and part level feature extraction. Deep LAC <ref type="bibr" target="#b26">[27]</ref> proposes a valve linkage function for back-propagation chaining and form a deep localization, alignment and classification system. <ref type="bibr" target="#b50">[51]</ref> introduces an end-to-end learning framework for joint learning of pose estimation, normalization and recognition. These models are all based on a limited number of single keypoint patches, which could be poorlyaligned in the presence of pose and viewpoint variance.</p><p>Perhaps our work is mostly related to POOF [2], which also uses keypoint pair patch. Our algorithm is different from theirs as we automatically detect keypoints instead use ground truth ones. Also the POOF approach computed 5000 patches with corresponding features in order to produce the final classification, we're computing <ref type="bibr">35-70.</ref> There are also works targeting the object alignment problem. Unlike previous methods which rely on detectors for part localization, <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> proposes to localize distinctive details by roughly aligning the objects using just the overall shape. Spatial transformer network <ref type="bibr" target="#b19">[20]</ref> introduces a differentiable affine transformation learning layer to transform and align the object or part of interest.</p><p>Another direction in fine grained recognition is feature correlation and kernel mapping. <ref type="bibr" target="#b28">[29]</ref> proposes a bilinear pooling layer to compute a second order polynomial kernel mapping on CNN features. Many works has followed this simple paradigm <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b7">8]</ref>. Compact bilinear pooling <ref type="bibr" target="#b13">[14]</ref> proposes a compact representation to approximate the polynomial kernel, reducing memory usage. Low-rank bilinear pooling <ref type="bibr" target="#b20">[21]</ref> represents the covariance features as a matrix and applies a low-rank bilinear classifier. Kernel pooling <ref type="bibr" target="#b7">[8]</ref> proposes a general pooling framework that captures higher order interactions of features in the form of kernels. This line of works achieves relatively good results with weakly supervision, however, they attend to the whole image glob-ally, lacking part-level information discovery mechanism. This limites their success in further accuracy improvement.</p><p>Inspired by human attention mechanism, many attempts have been made to guide the attention of the CNN model to informative object parts. Works along this direction include <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b52">53]</ref>. <ref type="bibr" target="#b53">[54]</ref> proposes a multiattention convolutional neural network (MA-CNN), where part generation and feature learning can reinforce each other. <ref type="bibr" target="#b25">[26]</ref> leverages long short term memory networks (LSTM) to unify new patch candidates generation and informative part evaluation. This work establishes the current state-of-the-art performance on CUB-200-2011 <ref type="bibr" target="#b41">[42]</ref> dataset, achieving an accuracy of 87.5% with part annotations. The key difference is our PAIRS representation integrates pose and appearance information and achieves multilevel attention over semantic object parts explicitly at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PAIRS -Pose and Appearance Integration</head><p>We illustrate our algorithm pipeline in <ref type="figure">Figure 3</ref>. Firstly, we propose a simple yet effective fully convolutional neural network for pose estimation. We follow the prevailing modular design paradigm by stacking convolutional blocks that have similar topology. we show our pose estimation network achieves supreme results on the CUB-200 dataset both qualitatively and quantitatively. Secondly, given detected keypoint locations, a rectangle bounding box enclosing each keypoint pair is cropped from the original image and similarity-transformed to a uniform-sized patch <ref type="figure">(Figure 4)</ref>, such that both keypoints are at fixed position across different images. As the representation is normalized to the keypoint locations, the patches are well-aligned, independent of the pose or the viewer's angle. Thirdly, we train separate CNN models as feature extractors for the pose-aligned patch representation. Lastly, we explore different classification architectures for the unified representation based on the assumption that part contribution should vary for different images and classes. We find surprisingly that the Multi-Layer Perception (MLP), while being the most simple method, achieves the best final classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pose Estimation Network</head><p>Pose estimation networks usually follow one of two paradigms for prediction. The first is to directly regress discrete keypoint coordinates, e.g. (x i , y i ). Representative approaches include <ref type="bibr" target="#b39">[40]</ref>. The alternate approach <ref type="bibr" target="#b38">[39]</ref> instead uses a two-dimensional probability distribution heat map to represent the keypoint location. We call this resulting multichannel probability distribution matrix pose tensor.</p><p>In this paper, we adopt the second strategy, proposing a fully convolutional network to produce the structured output distribution. Specifically, we take a pretrained classification network and remove the final classifier layer(s), retaining what can be seen as an encoder network that encodes strong visual semantics. We follow the prevailing modular design to stack repeated building blocks to the end of the network. The resulting block consists of one upsampling layer, one convolutional layer, one batch normalization layer and one ReLU non-linearity layer. The parameter-free bilinear interpolation layer is used for upsampling. The convolutional layer has 1x1 kernel and reduces the input channel size to half. Additionally, a final convolutional layer and upsampling layer are added to produce the pose tensor. There are many modifications one can make to enhance this basic model, including using larger 3x3 kernels, adding more convolutional layers to the building block, adding residue connection to each block, stacking more building blocks, and using a learnable transposed convolutional layer for upsampling. We find these structures provide only limited improvement but introduce more parameters, so we prefer this simpler architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Beak | Left-eye</head><p>Breast | Belly Left-leg | Tail <ref type="figure">Figure 4</ref>: Pose-aligned Patch Generation. We show the pose-aligned patch generation in this graph. For each pair of keypoints, we fit a rectangle bounding box whose corners are calculated as in (b). Objects of different poses and from different viewpoint can be compared directly by the proposed keypoint pair patch as shown in (a). Details are described in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Patch Generation</head><p>Historically, part-based representations would model parts either as rectangular regions <ref type="bibr" target="#b47">[48,</ref><ref type="bibr">12]</ref> or keypoints. Keypoints are convenient for pose-estimation. However, the square or rectangular patches, each centered on a given keypoint and extracted to characterize the part's appearance, are far from optimal in the presence of rotation or general pose variation. We instead, propose to use keypoint pairs as anchor points in extracting pose-aligned patches.</p><p>Given two keypoints p i = (x i , y i ) and p j = (x j , y j ), we define the vectors r ij = p j − p i , andˆ r ij = r ij /|| r ij || 2 . We also define the vectorˆ t ij =ˆ z ×ˆ r ij , a unit vector perpendicular to r ij , and the distances d = || r ij || 2 and h = d/2 for convenience. We seek to extract a region around p i and p j that is aligned with r ij and has dimensions 2d × d. The four corners of this rectangular region are then given by:</p><formula xml:id="formula_0">( p i − hˆ r ij ) + hˆ t ij ( p j + hˆ r ij ) − hˆ t ij ( p i − hˆ r ij ) − hˆ t ij ( p j + hˆ r ij ) + hˆ t ij<label>(1)</label></formula><p>A similarity-transform is computed to extract the posenormalized patch. Patches generated in this way contain stable pose-aligned features -features near these keypoints appear at the same location in the given patch different images independent of the object's pose or the camera viewing angle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Patch Feature Extraction</head><p>A separate patch classification network is trained for each posed-aligned A | B patch as feature extractor. The softmax output from each network are concatenated as the representation for the input image. Alternatively, the final convolutional layer output after pooling can also be used and the result is comparable. We find that symmetric parts can help reduce the overall classifier number by nearly 50%, which is described in Section 4.2. The proposed patch representation can be seen as spatial pyramid that explicitly captures information of different parts at multiple spatial scales on the object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Classification Network</head><p>To fully utilize the abundant patch representations, we explore different ways to form a strong classification network. Based on the assumption that only a small fraction of the patches contains discriminative information and patches contribution should be weighted, we explores the following strategies.</p><p>1). Fixed patch selection: take the average score for a fixed number of top ranking patches. This strategy can also predicts the potential of our PAIRS representation.</p><p>2). Dynamic patch selection: employ the sparsely gated network <ref type="bibr" target="#b37">[38]</ref> to dynamically learn a selection function to select a fixed number of patches for each input.</p><p>3). Sequential patch weighting: apply a Long Short Term Memory Networks (LSTM) to reweigh different patch features in a sequential way. 4). Static patch weighting: learn a Multi-Layer Perceptron network, which essentially applies a non-linear weighting function to aggregate information from different patches.</p><p>We find surprisingly that the MLP network, while being the simplest network architecture, achieves the best accuracy out of all the attempts we made. Details are included in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head><p>We test our algorithm on two datasets, the CUB-200-2011 dataset and the NABirds dataset. The CUB-200-2011 contains 200 species of birds with 5994 training images and 5794 testing images. The NABirds dataset has 555 common species of birds in North America with a total number of 48,562 images. Class labels and keypoint locations are provided in both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Keypoint Prediction Performance</head><p>We use PCK (Percentage of Correct Keypoints) score to measure the accuracy of keypoint prediction results. A predicted keypoint (p) is "correct" if its within a small neighborhood of the ground truth location (g), or equally speaking,</p><formula xml:id="formula_1">||p − g|| 2 ≤ c * max(h, w)</formula><p>c is a constant factor and max(h, w) is the longer side of the bounding box. We evaluate our pose estimation network on CUB-200 and compare our PCK score with the others in <ref type="table" target="#tab_0">Table 1</ref>. We achieve highest score on all 15 keypoints with considerable leading margin. We do especially well on legs and wings where other models struggle to make precise prediction. Some visualization results are shown in ??</p><p>Although we localize wings and legs better than baselines, they still have the lowest PCK in our model. This is caused by dramatic pose change as well as the appearance similarity between symmetric parts. We note that using keypoints to denote the wings isn't always appropriate. Because wings are two dimensional planar parts that spread over a relatively large area. Designating a keypoint to the wing can be obscure, because it is not easy to decide which point represents the wing location better. In fact, the ground truth keypoint location of the CUB dataset is the average of five annotators' results and it's even hard for them to reach a consensus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Patch Classification Network</head><p>We adopt the ResNet-50 architecture as the patch classification network due to its high performance and compact GPU footprint. Alternate architectures like VGG and Inception can easily be adapted. We now discuss two considerations which facilitate training the patch classification network. Symmetry. For a given object with n keypoints, the total number of patches to be classified is:</p><formula xml:id="formula_2">n 2 = n(n − 1) 2 = O(n 2 )</formula><p>which increases quadratically with n. Most real world objects show some kind of symmetry. Due to the visual similarity inherent in symmetric pairs keypoints (for example, right and left eyes, wings and feet), we treat each pair as a hybrid keypoint in the patch generation process. Many real-world objects, however, like birds, cats, cars,  etc. are symmetric in appearance. Based on this observation, we propose to merge the patches for a symmetric pair of keypoints into a hybrid patch, e.g. left-eye | tail and right-eye | tail can be merged into the hybrid eye | tail pair. As a result, the total number of patch classification networks is reduced from 105 to 69 for the CUB dataset; on the NABirds dataset, the number is reduced from 55 to 37. Visibility. Due to self-occlusion or foreground-occlusion, not all keypoints are visible in the image. Previous works <ref type="bibr" target="#b17">[18]</ref> would eliminate patches with invisible keypoints to purify the input data. Contrarily, we find that this would hurt the performance of the patch classifiers. Details for comparison can be found in <ref type="figure" target="#fig_6">Figure 7</ref>. We believe this degradation is caused by the shrinkage of effective training set size. This is a similar finding with [24] that noisy but abundant data consistently outperforms clean but limitedsized data. Additionally, the pose estimation network would make a reasonable guess even if the keypoint is invisible. So during patch classifier training, all keypoints are considered visible by taking the maximally activated location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Classification Network</head><p>Based on the assumption that image patches should contribute differently to classification. Four different strategies are explored and we describe details of them in this section. Fixed patch selection. We assume only few patches contains useful information and others may merely act as noise. We propose a fixed patch selection strategy to keep the best k patches.A greedy search algorithm would evaluate each n choose k combinations for k from 1 to n. The number of evaluations needed for this algorithm is The complexity grows in the order of n! and quickly becomes intractable. We thus employ the beam search algorithm. Instead of greedily searching the whole parameter space, we only keep a fixed k combinations each iteration and build our search path based only on previously learned k patch combinations. Out of curiosity, we also do beam search on the testing set alone. This operation, although invalid, provides some insights in the potential of our pose aligned patch representation. The results are shown in <ref type="figure" target="#fig_6">Figure 7</ref>. Our observations is that without overfitting, the potential of fixed patch selection should be well above 89%, compared to the current state-of-the-art <ref type="bibr" target="#b25">[26]</ref> 87.5%. Notably, a simple average over all strategy can achieve 87.6%. Dynamic patch selection. As an alternative attempt we experiment with is the sparsely gated network <ref type="bibr" target="#b37">[38]</ref> for dynamic patch selection. Different from the beam search algorithm which selects fixed patches for each input, the gated network would select different combinations depending on the input. A tiny network is trained to predict weights for each patch and an explicit sparsity constraint is exposed on the weight to only allow k non-zero elements. A Sigmoid layer is added to normalize the weight. The network archi-tecture can be described as,</p><formula xml:id="formula_3">G(x) = softmax(top k(H(x)))</formula><p>H(x) represents the mapping function from the input to patch weight. G(x) is the patch selection function. Different architectures for the tiny network are tried and we find a simple linear layer would work decently most of the time. Best accuracy is achieved when k = 105. Interestingly when k=1, our dynamic patch selection performs worse than the fixed patch selection, implying the gated network's inability to learn useful information for decision making. Sequential patch weighting. Recurrent neural networks (RNN) is specialized at processing sequential data like text and speech. RNN has been widely adopted as an attention mechanism to focus on different parts sequentially. We instead employ RNN for sequential patch weighting, aiming to discover different patches for decision making. We employ a one-layer Long Short Term Memory (LSTM) network with 512 nodes. Each node has a hidden layer of size 1024. The last output of sequence is selected as the final output. We get 82.7% in this experiment and this confirms the effectiveness of the LSTM network. Static patch weighting The final and most effective method we tried is the MLP network. The MLP network contains one hidden layer with 1024 parameters, followed by the batch normalization layer, ReLU layer, and then the output layer. On CUB our final accuracy is 88.7% , 1.2% higher than the current state-of-the-art result. We combine pairs patch with single keypoint patch and achieves a new stateof-the-art 89.2% accuracy. We compare our result with several other strong baselines in <ref type="table" target="#tab_1">Table-2.</ref> We test our algorithm also on the NAbirds dataset and the result is shown in 3. Our algorithm attains an accuracy of 87.9%, more than 8% better than a strong baseline.     <ref type="bibr" target="#b29">[30]</ref> 79.4% PAIRS 87.9%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>Axis-Aligned v.s. Pose-Aligned We compare and show patch classification accuracy using pose-aligned patches v.s. single keypoint based axis-aligned patches in <ref type="figure" target="#fig_4">Figure 6</ref>. Single keypoint based patches performs consistently poorly compared to the pairs patches, confirming the effectiveness of disentangled feature representation.</p><p>Patch Size Study One hyper-parameter in our algorithm is the pose-aligned patch size. We tries several size options on the best performing patch. We see that the larger-size patches generally yield better accuracy. We adopt 256×512 empirically because our base model is pretrained for such size.</p><p>Choice of Pose Estimation Network To test the influence of pose estimation network on the proposed algorithm, we train a separate Stacked Hourglass Network <ref type="bibr" target="#b33">[34]</ref> for comparison. Stacked Hourglass Network is about 2% better than the FCN on PCK score, but the final classification accuracy numbers are comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results Visualization</head><p>We show the patch classification accuracy for each patch in the CUB dataset in <ref type="figure" target="#fig_6">Figure 7</ref>. The best performing patch corresponds to belly | crown, achieving 79.6% accuracy. The worst performing patch is the   left-leg | right-leg pair which achieves only 15.7% accuracy. Empirically, global patches perform better than local patches, however local patches are also important for localizing discriminative object parts. Patches found by beam search, as shown in <ref type="figure" target="#fig_6">Figure 7</ref>, can provide insighta combination of global and local patches are selected to achieve an optimal result. As hard cases often can only be classified by a few highly localized discriminative parts, the number of patches with correct predictions reflects the difficulty of the image. We propose to use the correctly predicted patch number as the indicator of the image difficulty. This is a histogram reflecting the count of many images have a given number of patches correctly predicted the class (top right plot in <ref type="figure" target="#fig_6">Figure 7)</ref>. Example images, ranging from hard on the left, to easy on the right, are shown below; the hard cases can be due either to very similar/easily confused classes or to pose-estimation failure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Fine-grained recognition is an area where computer algorithms can assist humans on difficult tasks like recognizing bird species. Pose variation constitutes a major challenge in fine-grained recognition that recent works fail to address. In this paper, we introduce a unified object representation built from pose-aligned patches which disentangle the appearance features from the influence of pose, scale and rotation. Our proposed algorithm attains state-of-theart performance on two fine-grained datasets, suggesting the critical importance of disentangling pose and appearance in fine-grained recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Keypoint Localization ResultsThe top two rows show the keypoint detection results of our pose estimation network. Red dots represent the predicted location and black dots are the ground truth locations. Three failure patterns are shown in the third row. The first one confuses left and right because of the visual similarity between symmetric parts. Dramatic and rare pose causes degradation for keypoint localization as seen in the middle one. The last one shows an interesting case where the nose of a seal is mistakenly predicted as the gull's beak.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Patch Accuracy: Axis-Aligned Patches v.s. PAIRS Patches We compare the patch accuracy of single keypoint patches of keypoint k and PAIRS patches containing k. The x axis is the keypoint id and the y axis is the patch accuracy. Most single keypoint patch are inferior to PAIRS patches in terms of patch accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Result visualization. Top left: we show patch classification network accuracies using two strategies, visible keypoint patch and full keypoint patch separately. This verifies that treating all keypoints as visible will help improve the patch classifier's accuracy. Top right: hard case mining by correct prediction patch number. In the bottom we show sample images from hard to easy. Bottom left: distributions of patch classifier performance. Some examples are shown in the text box. Bottom right: Beam search results using two strategies, patch finding on training set (blue) and testing set (orange). The later one is purely for the estimation of the potential of the pair representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>PCK comparison</figDesc><table><row><cell></cell><cell>back</cell><cell>beak</cell><cell>belly</cell><cell>breast</cell><cell>crown</cell><cell cols="3">forehead left-eye left-leg</cell></row><row><cell>[18]</cell><cell>80.7</cell><cell>89.4</cell><cell>79.4</cell><cell>79.9</cell><cell>89.4</cell><cell>88.5</cell><cell>85.0</cell><cell>75.0</cell></row><row><cell>[51]</cell><cell>85.6</cell><cell>94.9</cell><cell>81.9</cell><cell>84.5</cell><cell>94.8</cell><cell>96.0</cell><cell>95.7</cell><cell>64.6</cell></row><row><cell>Ours</cell><cell>91.3</cell><cell>96.8</cell><cell>89.0</cell><cell>91.5</cell><cell>96.9</cell><cell>97.6</cell><cell>96.9</cell><cell>80.2</cell></row><row><cell></cell><cell cols="5">left-wing nape right-eye right-leg right-wing</cell><cell>tail</cell><cell>throat</cell><cell>Overall</cell></row><row><cell>[18]</cell><cell>67.0</cell><cell>85.7</cell><cell>86.1</cell><cell>77.5</cell><cell>67.8</cell><cell>76.0</cell><cell>90.8</cell><cell>86.6</cell></row><row><cell>[51]</cell><cell>67.8</cell><cell>90.7</cell><cell>93.8</cell><cell>64.9</cell><cell>69.3</cell><cell>74.7</cell><cell>94.5</cell><cell>N/A</cell></row><row><cell>Ours</cell><cell>76.8</cell><cell>94.6</cell><cell>97.4</cell><cell>80.3</cell><cell>75.3</cell><cell>83.6</cell><cell>97.4</cell><cell>90.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Classification score on CUB. Annotation key as follows: GT = class labels; BB = bounding box annotation; KP = keypoint annotations; WEB = images downloaded from the Internet.</figDesc><table><row><cell></cell><cell>Annotations</cell><cell>Accuracy</cell></row><row><cell>Huang et al. [18]</cell><cell>GT+BB+KP</cell><cell>76.2</cell></row><row><cell>Zhang et al. [48]</cell><cell>GT + BB</cell><cell>76.4</cell></row><row><cell>Krause et al. [23]</cell><cell>GT+BB</cell><cell>82.8</cell></row><row><cell>Jaderberg et al. [20]</cell><cell>GT</cell><cell>84.1</cell></row><row><cell>Shu et al. [21]</cell><cell>GT</cell><cell>84.2</cell></row><row><cell>Zhang et al. [52]</cell><cell>GT</cell><cell>84.5</cell></row><row><cell>Xu et al. [45]</cell><cell>GT+BB+KP+WEB</cell><cell>84.6</cell></row><row><cell>Lin et al. [29]</cell><cell>GT+BB</cell><cell>85.1</cell></row><row><cell>Cui et al. [8]</cell><cell>GT</cell><cell>86.2</cell></row><row><cell>Lam et al. [26]</cell><cell>GT+KP</cell><cell>87.5</cell></row><row><cell>PAIRS Only</cell><cell>GT + KP</cell><cell>88.7</cell></row><row><cell>PAIRS+Single</cell><cell>GT + KP</cell><cell>89.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance Comparison on the NABirds dataset.</figDesc><table><row><cell>Algorithm</cell><cell>Accuracy</cell></row><row><cell>ResNet-50 Baseline</cell><cell>79.2%</cell></row><row><cell>Bilinear CNN (PAMI 2017)</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Merlin</forename><surname>Bird</surname></persName>
		</author>
		<ptr target="http://merlin.allaboutbirds.org/.Accessed" />
		<imprint>
			<biblScope unit="page" from="2018" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Birdsnap: Large-scale fine-grained visual categorization of birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2011" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bird Species Recognition Using Pose Normalized Deep Convolutional Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10130</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Spherical cnns. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08498</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Steerable cnns. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Kernel Pooling for Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discovering localized attributes for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Birdlets: Subordinate categorization using volumetric primitives and pose-normalized appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Look Closer to See Better: Recurrent Attention Convolutional Neural Network for Fine-Grained Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Compact Bilinear Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fine-Grained Categorization by Alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Local Alignments for Fine-Grained Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="212" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Part-Stacked CNN for Fine-Grained Visual Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Part-stacked cnn for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial Transformer Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Low-rank Bilinear Pooling for Fine-Grained Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning Features and Parts for Fine-Grained Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fine-Grained Recognition without Part Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Leafsnap: A computer vision system for automatic plant species identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Kress</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V B</forename><surname>Soares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 12th European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fine-Grained Recognition as HSnet Search for Informative Image Parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep lac: Deep localization, alignment and classification for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1666" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<editor>D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars</editor>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="740" to="755" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bilinear CNN Models for Fine-Grained Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<title level="m">Bilinear Convolutional Neural Networks for Fine-grained Visual Recognition. PAMI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dog Breed Classification Using Part Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Localizing by Describing: Attribute-Guided Attention Localization for Fine-Grained Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Rotation equivariant vector field networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ruderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04438</idno>
		<title level="m">Learned deformation stability in convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention for Fine-Grained Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1701.0</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems -Volume 1, NIPS&apos;14</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems -Volume 1, NIPS&apos;14<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Building a Bird Recognition App and Large Scale Dataset With Citizen Scientists: The Fine Print in Fine-Grained Dataset Collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning steerable filters for rotation equivariant cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Storath</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The Application of Two-Level Attention Models in Deep Convolutional Neural Network for Fine-Grained Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Augmenting Strong Supervision Using Web Data for Fine-Grained Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A codebook-free and annotation-free approach for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Combining randomization and discrimination for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Part-Based R-CNNs for Fine-Grained Category Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pose pooling kernels for sub-category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Fine-grained pose prediction, normalization, and recognition. ICLR Workshops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Picking Deep Filter Responses for Fine-Grained Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Diversified Visual Attention Networks for Fine-Grained Object Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="5219" to="5227" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
