<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IMRAM: Iterative Matching with Recurrent Attention Memory for Cross-Modal Image-Text Retrieval *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>BNRist</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
							<email>dinggg@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>BNRist</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Liu</surname></persName>
							<email>liuxudong@kuaishou.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Kwai</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijia</forename><surname>Lin</surname></persName>
							<email>zijlin@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
							<email>jungonghan77@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">WMG Data Science</orgName>
								<orgName type="institution">University of Warwick</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">IMRAM: Iterative Matching with Recurrent Attention Memory for Cross-Modal Image-Text Retrieval *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Enabling bi-directional retrieval of images and texts is important for understanding the correspondence between vision and language. Existing methods leverage the attention mechanism to explore such correspondence in a finegrained manner. However, most of them consider all semantics equally and thus align them uniformly, regardless of their diverse complexities. In fact, semantics are diverse (i.e. involving different kinds of semantic concepts), and humans usually follow a latent structure to combine them into understandable languages. It may be difficult to optimally capture such sophisticated correspondences in existing methods. In this paper, to address such a deficiency, we propose an Iterative Matching with Recurrent Attention Memory (IMRAM) method, in which correspondences between images and texts are captured with multiple steps of alignments. Specifically, we introduce an iterative matching scheme to explore such fine-grained correspondence progressively. A memory distillation unit is used to refine alignment knowledge from early steps to later ones. Experiment results on three benchmark datasets, i.e. Flickr8K, Flickr30K, and MS COCO, show that our IMRAM achieves state-of-the-art performance, well demonstrating its effectiveness. Experiments on a practical business advertisement dataset, named KWAI-AD, further validates the applicability of our method in practical scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Due to the explosive increase of multimedia data from social media and web applications, enabling bi-directional * This work was supported by the National Natural Science Foundation of China (Nos. U1936202, 61925107). Corresponding author: Guiguang Ding cross-modal image-text retrieval is in great demand and has become prevalent in both academia and industry. Meanwhile, this task is challenging because it requires to understand not only the content of images and texts but also their inter-modal correspondence <ref type="bibr" target="#b9">[10]</ref>.</p><p>In recent years, a large number of researches have been proposed and achieved great progress. Early works attempted to directly map the information of images and texts into a common latent embedding space. For example, Wang et al. <ref type="bibr" target="#b25">[26]</ref> adopted a deep network with two branches to, respectively, map images and texts into an embedding space. However, these works coarsely capture the correspondence between modalities and thus are unable to depict the finegrained interactions between vision and language.</p><p>To gain a deeper understanding of such fine-grained correspondences, recent researches further explored the attention mechanism for cross-modal image-text retrieval. Karpathy et al. <ref type="bibr" target="#b8">[9]</ref> extracted features of fragments for each image and text (i.e. image regions and text words), and proposed a dense alignment between each fragment pair. Lee et al. <ref type="bibr" target="#b11">[12]</ref> proposed a stacked cross attention model, in which attention was used to align each fragment with all fragments from another modality. It can neatly discover the fine-grained correspondence and thus achieves state-of-theart performance on several benchmark datasets.</p><p>However, due to the large heterogeneity gap between images and texts, existing attention-based models, e.g. <ref type="bibr" target="#b11">[12]</ref>, may not well seize the optimal pairwise relationships among a number of region-word fragments pairs. Actually, semantics are complicated, because they are diverse (i.e. composed by different kinds of semantic concepts with different meanings, such as objects (e.g. nouns), attributes (e.g. adjectives) and relations (e.g. verbs)). And there generally exist strong correlations among different concepts, e.g. relational terms (e.g. verbs) usually indicate relation-ships between objects (e.g. nouns). Moreover, humans usually follow a latent structure (e.g. a tree-like structure <ref type="bibr" target="#b24">[25]</ref>) to combine different semantic concepts into understandable languages, which indicates that semantics shared between images and texts exhibit a complicated distribution. However, existing state-of-the-art models treat different kinds of semantics equally and align them together uniformly, taking little consideration of the complexity of semantics.</p><p>In reality, when humans perform comparisons between images and texts, we usually associate low-level semantic concepts, e.g. objects, at the first glimpse. Then, higherlevel semantics, e.g. attributes and relationships, are mined by revisiting images and texts to obtain a better understanding <ref type="bibr" target="#b19">[20]</ref>. This intuition is favorably consistent with the aforementioned complicated semantics, and meanwhile, it indicates that the complicated correspondence between images and texts should be exploited progressively.</p><p>Motivated by this, in this paper, we propose an iterative matching framework with recurrent attention memory for cross-modal image-text retrieval, termed IMRAM. Our way of exploring the correspondence between images and texts is characterized by two main features: (1) an iterative matching scheme with a cross-modal attention unit to align fragments across different modalities; (2) a memory distillation unit to dynamically aggregate information from early matching steps to later ones. The iterative matching scheme can progressively update the cross-modal attention core to accumulate cues for locating the matched semantics, while the memory distillation unit can refine the latent correspondence by enhancing the interaction of cross-modality information. Leveraging these two features, different kinds of semantics are treated distributively and well captured at different matching steps.</p><p>We conduct extensive experiments on several benchmark datasets for cross-modal image-text retrieval, i.e. Flickr8K, Flickr30K, and MS COCO. Experiment results show that our proposed IMRAM can outperform the state-of-the-art models. Subtle analyses are also carried out to provide more insights about IMRAM. We observe that: (1) the finegrained latent correspondence between images and texts can be well refined during the iterative matching process; (2) different kinds of semantics, respectively, play dominant roles at different matching steps in terms of contributions to the performance improvement.</p><p>These observations can account for the effectiveness and reasonableness of our proposed method, which encourages us to validate its potential in practical scenarios. Hence, we collect a new dataset, named KWAI-AD, by crawling about 81K image-text pairs on an advertisement platform, in which each image is associated with at least one advertisement textual title. We then evaluate our proposed method on the KWAI-AD dataset and make comparisons with the state-of-the-art models. Results show that our method per-forms considerably better than compared models, further demonstrating the effectiveness of our method in the practical business advertisement scenario. The source code is available at: https://github.com/HuiChen24/ IMRAM.</p><p>The contributions of our work are three folds: 1) First, we propose an iterative matching method for cross-modal image-text retrieval to handle the complexity of semantics.</p><p>2) Second, we formulate the proposed iterative matching method with a recurrent attention memory which incorporates a cross-modal attention unit and a memory distillation unit to refine the correspondence between images and texts.</p><p>3) Third, we verify our method on benchmark datasets (i.e. Flickr8K, Flickr30K, and MS COCO) and a real-world business advertisement dataset (i.e. our proposed KWAI-AD dataset). Experimental results show that our method outperforms compared methods in all datasets. Thorough analyses on our model also well demonstrate the superiority and reasonableness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Our work is concerned about the task of cross-modal image-text retrieval which essentially aims to explore the latent correspondence between vision and language. Existing matching methods can be roughly categorized into two lines: (1) coarse-grained matching methods aiming to mine the correspondence globally by mapping the whole images and the full texts into a common embedding space, (2) fine-grained matching ones aiming to explore the correspondence between image fragments and text fragments at a fine-grained level.</p><p>Coarse-grained matching methods. Wang et al. <ref type="bibr" target="#b25">[26]</ref> used a deep network with two branches of multilayer perceptrons to deal with images and texts, and optimized it with intra-and inter-structure preserving objectives. Kiros et al. <ref type="bibr" target="#b10">[11]</ref> adopted a CNN and a Gate Recurrent Unit (GRU) with a hinge-based triplet ranking loss to optimize the model by averaging the individual violations across the negatives. Alternatively, Faghri et al. <ref type="bibr" target="#b3">[4]</ref> reformed the ranking objective with a hard triplet loss function parameterized by only hard negatives.</p><p>Fine-grained matching methods. Recently, several works have been devoted to exploring the latent fine-grained vision-language correspondence for cross-modal imagetext <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b11">12]</ref>. Karpathy et al. <ref type="bibr" target="#b8">[9]</ref> extracted features for fragments of each image and text, i.e. image regions and text words, and aligned them in the embedding space. Niu et al. <ref type="bibr" target="#b18">[19]</ref> organized texts as a semantic tree with each node corresponding to a phrase, and then used a hierarchical long short term memory (LSTM, a variant of RNN) to extract phrase-level features for text. Huang et al. <ref type="bibr" target="#b5">[6]</ref> presented a context-modulated attention scheme to selectively attend to salient pairwise image-sentence instances. Then a</p><formula xml:id="formula_0">! * # * $ * ! % &amp; CNN …</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bi-GRU</head><p>A horse walks on the road. multi-modal LSTM was used to sequentially aggregate local similarities into a global one. Nam et al. <ref type="bibr" target="#b16">[17]</ref> proposed a dual attention mechanism in which salient semantics in images and texts were obtained by two attentions, and the similarity was computed by aggregating a sequence of local similarities. Lee et al. <ref type="bibr" target="#b11">[12]</ref> proposed a stacked cross attention model which aligns each fragment with all other fragments from the other modality. They achieved stateof-the-art performance on several benchmark datasets for cross-modal retrieval. While our method targets the same as <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>, differently, we apply an iterative matching scheme to refine the fragment alignment. Besides, we adopt a memory unit to distill the knowledge of matched semantics in images and texts after each matching step. Our method can also be regarded as a sequential matching method, as <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b5">6]</ref>. However, within the sequential computations, we transfer the knowledge about the fragment alignment to the successive steps with the proposed recurrent attention memory, instead of using modality-specific context information. Experiments also show that our method outperforms those mentioned works.</p><formula xml:id="formula_1">I … … ! # $ = 1 = 3 ! * % * &amp; * … … … … ! ' # ' $ ' … … … … ! ( % ( &amp; ( … … CAUs MDUs # # ) gate() tanh() 1- # # [ # , # ) ] # # * objective = 2</formula><p>We also noticed that some latest works make use of large-scale external resources to improve performance. For example, Mithun et al. <ref type="bibr" target="#b15">[16]</ref> collected amounts of image-text pairs from the Internet and optimized the retrieval model with them. Moreover, inspired by the recent great success of contextual representation learning for languages in the field of natural language processing (ELMO <ref type="bibr" target="#b20">[21]</ref>, BERT <ref type="bibr" target="#b2">[3]</ref> and XLNet <ref type="bibr" target="#b26">[27]</ref>), researchers also explored to apply BERT into cross-modal understanding field <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref>. However, such pretrained cross-modal BERT models 1 require large amounts of annotated image-text pairs, which are not easy to obtain in the practical scenarios. On the contrary, our method is general and unlimited to the amount of data. We leave the exploration of large-scale external data to future works. <ref type="bibr" target="#b0">1</ref> Corresponding codes and models are not made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we will elaborate on the details of our proposed IMRAM for cross-modal image-text retrieval. <ref type="figure" target="#fig_0">Figure 1</ref> shows the framework of our model. We will first describe the way of learning the cross-modal feature representations in our work in section 3.1. Then, we will introduce the proposed recurrent attention memory as a module in our matching framework in section 3.2. We will also present how to incorporate the proposed recurrent attention memory into the iterative matching scheme for cross-modal image-text retrieval in section 3.3. Finally, the objective function is discussed in section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Cross-modal Feature Representation</head><p>Image representation. Benefiting from the development of deep learning in computer vision, different convolution neural networks have been widely used in many tasks to extract visual information for images. To obtain more descriptive information about the visual content for image fragments, we employ a pretrained deep CNN, e.g. Faster R-CNN. Specifically, given an image I, a CNN detects image regions and extracts a feature vector f i for each image region r i . We further transform f i to a d-dimensional vector v i via a linear projection as follows:</p><formula xml:id="formula_2">v i = W v f i + b v<label>(1)</label></formula><p>where W v and b v are to-be-learned parameters. For simplicity, we denote the image representation as</p><formula xml:id="formula_3">V = {v i |i = 1, ..., m, v i ∈ R d },</formula><p>where m is the number of detected regions in I. We further normalize each region feature vector in V as <ref type="bibr" target="#b11">[12]</ref>.</p><p>Text representation. Basically, texts can be represented at either sentence-level or word-level. To enable the finegrained connection of vision and language, we extract the word-level features for texts, which can be done through a bi-directional GRU as the encoder.</p><p>Specifically, for a text S with n words, we first represent each word w j with a contiguous embedding vector e j = W e w j , ∀j ∈ <ref type="bibr">[1, n]</ref>, where W e is a to-be-learned embedding matrix. Then, to enhance the word-level representation with context information, we employ a bi-directional GRU to summarize information from both forward and backward directions in the text S:</p><formula xml:id="formula_4">− → h j = − −−− → GRU (e j , − → h j−1 ); ← − h j = ← −−− − GRU (e j , ← − h j+1 )<label>(2)</label></formula><p>where − → h j and ← − h j denote hidden states from the forward GRU and the backward GRU, respectively. Then, the repre-</p><formula xml:id="formula_5">sentation of the word w j is defined as t j = − → h j + ← − h j 2 .</formula><p>Eventually, we obtain a word-level feature set for the text S, denoted as T = {t j |j = 1, ..., n, t j ∈ R d }, where each t j encodes the information of the word w j . Note that each t j shares the same dimensionality as v i in Eq. 1. We also normalize each word feature vector in T as <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">RAM: Recurrent Attention Memory</head><p>The recurrent attention memory aims to align fragments in the embedding space by refining the knowledge about previous fragment alignments in a recurrent manner. It can be regarded as a block that takes in two sets of feature points, i.e. V and T , and estimates the similarity between these two sets via a cross-modal attention unit. A memory distillation unit is used to refine the attention result in order to provide more knowledge for the next alignments. For generalization, we denote the two input sets of features as a query set</p><formula xml:id="formula_6">X = {x i |i ∈ [1, m ], x i ∈ R d } and a response set Y = {y j |j ∈ [1, n ], y j ∈ R d },</formula><p>where m and n are the numbers of feature points in X and Y , respectively. Note that X can be either of V and T , while Y will be the other.</p><p>Cross-modal Attention Unit (CAU). The cross-modal attention unit aims to summarize context information in Y for each feature x i in X. To achieve this goal, we first compute the similarity between each pair (x i , y j ) using the cosine function:</p><formula xml:id="formula_7">z ij = x T i y j ||x i || · ||y j || , ∀i ∈ [1, m ], ∀j ∈ [1, n ]<label>(3)</label></formula><p>As <ref type="bibr" target="#b11">[12]</ref>, we further normalize the similarity score z as:</p><formula xml:id="formula_8">z ij = relu(z ij ) m i=1 relu(z ij ) 2<label>(4)</label></formula><p>where relu(x) = max(0, x). Attention is performed over the response set Y given a feature x i in X:</p><formula xml:id="formula_9">c x i = n j=1 α ij y j , s.t. α ij = exp(λz ij ) n j=1 exp(λz ij )<label>(5)</label></formula><p>where λ is the inverse temperature parameter of the softmax function <ref type="bibr" target="#b1">[2]</ref> to adjust the smoothness of the attention distribution.</p><p>We define C x = {c x i |i ∈ <ref type="bibr">[1, m ]</ref>, c x i ∈ R d } as Xgrounded alignment features, in which each element captures related semantics shared by each x i and the whole Y .</p><p>Memory Distillation Unit (MDU). To refine the alignment knowledge for the next alignment, we adopt a memory distillation unit which updates the query features X by aggregating them with the corresponding X-grounded alignment feature C x dynamically:</p><formula xml:id="formula_10">x * i = f (x i , c x i )<label>(6)</label></formula><p>where f () is a aggregating function. We can define f () with different formulations, such as addition, multilayer perceptron (MLP), attention and so on. Here, we adopt a modified gating mechanism for f ():</p><formula xml:id="formula_11">g i = gate(W g [x i , c x i ] + b g ) o i = tanh(W o [x i , c x i ] + b o ) x * i = g i * x i + (1 − g i ) * o i<label>(7)</label></formula><p>where W g , W o , b g , b o are to-be-learned parameters. o i is a fused feature which enhances the interaction between x i and c x i . g i performs as a gate to select the most salient information.</p><p>With the gating mechanism, information of the input query can be refined by itself (i.e. x i ) and the semantic information shared with the response (i.e. o i ). The gate g i can help to filter trivial information in the query, and enable the representation learning of each query fragment (i.e. x i in X) to focus more on its individual shared semantics with Y . Besides, the X-grounded alignment features C x summarize the context information of Y with regard to each fragment in X. And in the next matching step, such context information will assist to determine the shared semantics with respect to Y , forming a recurrent computation process as described in the subsequent section 3.3. Therefore, with the help of C x , the intra-modality relationships in Y are implicitly involved and re-calibrated during the recurrent process, which would enhance the interaction among crossmodal features and thus benefit the representation learning.</p><p>RAM block. We integrate the cross-modal attention unit and the memory distillation unit into a RAM block, formulated as:</p><formula xml:id="formula_12">C x , X * = RAM(X, Y )<label>(8)</label></formula><p>where C x and X * are derived by Eq. 5 and 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Iterative Matching with Recurrent Attention Memory</head><p>In this section, we describe how to employ the recurrent attention memory introduced above to enable the iterative matching for cross-modal image-text retrieval. Specifically, given an image I and a text S, we derive two strategies for iterative matching grounded on I and S, respectively, using two independent RAM blocks:</p><formula xml:id="formula_13">C v k , V k = RAM v (V k−1 , T ) C t k , T k = RAM t (T k−1 , V )<label>(9)</label></formula><p>where V k , T k indicate the step-wise features of the image I and the text S, respectively. And k is the matching step, and V 0 = V , T 0 = T . We iteratively perform RAM() for a total of K steps. And at each step k, we can derive a matching score between I and S:</p><formula xml:id="formula_14">F k (I, S) = 1 m m i=1 F k (r i , S) + 1 n n j=1 F k (I, w j ) (10)</formula><p>where F (r i , S) and F (I, w j ) are defined as the regionbased matching score and the word-based matching score, respectively. They are derived as follows:</p><formula xml:id="formula_15">F k (r i , S) = sim(v i , c v ki ); F k (I, w j ) = sim(c t kj , t j )<label>(11)</label></formula><p>where sim() is the cosine function that measures the similarity between two input features as Eq. 3. And v i ∈ V corresponds to the region r i . t j ∈ T corresponds to the word w j . c v ki ∈ C v k and c t kj ∈ C t k are, respectively, the context feature corresponding to the region r i and the word w j . m and n are the numbers of image regions and text words, respectively.</p><p>After K matching steps, we derive the similarity between I and S by summing all matching scores:</p><formula xml:id="formula_16">F (I, S) = K k=1 F k (I, S)<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Function</head><p>In order to enforce matched image-text pairs to be clustered and unmatched ones to be separated in the embedding spaces, triplet-wise ranking objectives are widely used in previous works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b3">4]</ref> to train the model in an end-to-end manner. Following <ref type="bibr" target="#b3">[4]</ref>, instead of comparing with all negatives, we only consider the hard negatives within a minibatch, i.e. the negative that is closest to a training query:</p><formula xml:id="formula_17">L = B b=1 [∆ − F (I b , S b ) + F (I b , S b * )] + + B b=1 [∆ − F (I b , S b ) + F (I b * , S b )] +<label>(13)</label></formula><p>where [x] + = max(x, 0), and F (I, S) is the semantic similarity between I and S defined by Eq. 12. Images and texts with the same subscript b are matched examples. Hard negatives are indicated by the subscript b * . ∆ is a margin value.</p><p>Note that in the loss function, F (I, S) consists of F k (I, S) at each matching step (i.e. Eq. 12), and thus optimizing the loss function would directly supervise the learning of image-text correspondences at each matching step, which is expected to help the model to yield higherquality alignment at each step. With the employed tripletwise ranking objective, the whole model parameters can be optimized in an end-to-end manner, using widely-used optimizers like SGD, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metric</head><p>Three benchmark datasets are used in our experiments, including: (1) Flickr8K: contains 8,000 images and provides 5 texts for each image. We adopt its standard splits as <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b14">15]</ref>, using 6,000 images for training, 1,000 images for validation and another 1,000 images for testing. <ref type="bibr" target="#b1">(2)</ref> Flickr30K: consists of 31,000 images and 158,915 English texts. Each image is annotated with 5 texts. We follow the dataset splits as <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b3">4]</ref> and use 29,000 images for training, 1,000 images for validation, and the remaining 1,000 images for testing. (3) MS COCO: is a large-scale image description dataset containing about 123,287 images with at least 5 texts for each. As previous works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b3">4]</ref>, we use 113,287 images to train all models, 5,000 images for validation and another 5,000 images for testing. Results on MS  COCO are reported by averaging over 5 folds of 1K test images and testing on the full 5K test images as <ref type="bibr" target="#b11">[12]</ref>.</p><p>To further validate the effectiveness of our method in practical scenarios, we build a new dataset, named KWAI-AD. We collect 81,653 image-text pairs from a real-world business advertisement platform, and we randomly sample 79,653 image-text pairs for training, 1,000 for validation and the remaining 1,000 for testing. The uniqueness of our dataset is that the provided texts are not detailed textual descriptions of the content in the corresponding images, but maintain weakly associations with them, conveying strong affective semantics instead of factual semantics (seeing <ref type="figure" target="#fig_1">Figure 2)</ref>. And thus our dataset is more challenging than conventional datasets. However, it is of great importance in the practical business scenario. Learning subtle links of adver-tisement images with related well-designed titles could not only enrich the understanding of vision and language but also benefit the development of recommender systems and social networks.</p><p>Evaluation Metric. To compare our proposed method with the state-of-the-art methods, we adopt the same evaluation metrics in all datasets as <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b3">4]</ref>. Namely, we adopt Recall at K (R@K) to measure the performance of bi-directional retrieval tasks, i.e. retrieving texts given an image query (Text Retrieval) and retrieving images given a text query (Image Retrieval). We report R@1, R@5, and R@10 for all datasets as in <ref type="bibr" target="#b11">[12]</ref>. And to well reveal the effectiveness of the proposed method, we also report an extra metric "R@sum", which is the summation of all evaluation metrics as <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>To systematically validate the effectiveness of the proposed IMRAM, we experiment with three of its variants: (1) Image-IMRAM only adopts the RAM block grounded on images (i.e. only using the first term in Eq. 10); (2) Text-IMRAM only adopts the RAM block grounded on texts (i.e. only using the first term in Eq. 10); (3) Full-IMRAM. All models are implemented by Pytorch v1.0. In all datasets, for each word in texts, the word embedding is initialized by random weights with a dimensionality of 300. We use a bidirectional GRU with one layer and set its hidden state (i.e. − → h j and ← − h j in Eq. 2) dimensionality as 1,024. The dimensionality of each region feature (i.e. v i in V ) and and each word feature (i.e. t j in T ) is set as 1,024. On three benchmark datasets, we use Faster R-CNN pre-trained on Visual Genome to extract 36 region features for each image. For our KWAI-AD dataset, we simply use Inception v3 <ref type="bibr" target="#b23">[24]</ref> to extract 64 features for each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on Three Benchmark Datasets</head><p>We compare our proposed IMRAM with published stateof-the-art models in the three benchmark datasets 2 . We directly cite the best-reported results from respective papers when available. And for our proposed models, we perform 3 steps of iterative matching by default.</p><p>Results. Comparison results are shown in <ref type="table" target="#tab_0">Table 1</ref>, <ref type="table">Table 2 and Table 3</ref> for Flickr8K, Flickr30K and MS COCO, respectively. '*' indicates the performance of an ensemble model. '-' means unreported results. We can see that our proposed IMRAM can consistently achieve performance improvements in terms of all metrics, compared to the stateof-the-art models.</p><p>Specifically, our Full-IMRAM can significantly outperform the previous best model, i.e. SCAN* <ref type="bibr" target="#b11">[12]</ref>, by a large margin of 12.6%, 19.2%, 8.7% and 5.6% in terms of the overall performance R@sum in Flickr8K, Flickr30K, MS COCO (1K) and MS COCO (5K), respectively. And among recall metrics for the text retrieval task, our Full-IMRAM can obtain a maximal performance improvement of 3.2% (R@5 in Flickr8K), 6.7% (R@1 in Flickr30K), 4.0% (R@1 in MS COCO (1K)) and 3.3% (R@1 in MS COCO (5K)), respectively. As for the image retrieval task, the maximal improvements are 2.7% (R@1 in Flickr8K), 5.3% (R@1 in Flickr30K), 2.9% (R@1 in MS COCO (1K)) and 1.1% (R@1 in MS COCO (5K)), respectively. These results well demonstrate that the proposed method exhibits great effectiveness for cross-modal image-text retrieval. Besides, our models can consistently achieve state-of-theart performance not only in small datasets, i.e. Flickr8K and Flickr30K, but also in the large-scale dataset, i.e. MS COCO, which well demonstrates its robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Model Analysis</head><p>Effect of the total steps of matching, K. For all three variants of IMRAM, we gradually increase K from 1 to 3 to train and evaluate them on the benchmark datasets. Due to the limited space, we only report results on MS COCO (5K test) in <ref type="table" target="#tab_2">Table 4</ref>. We can observe that for all variants, K = 2 and K = 3 can consistently achieve better performance   Effect of the memory distillation unit. The aggregation function f (x, y) in Eq. 6 is essential for the proposed iterative matching process. We enumerate some basic aggregation functions and compare them with ours: (1) add:</p><formula xml:id="formula_18">x + y; (2) mlp: x + tanh(W y + b); (3) att: αx + (1 − α)y</formula><p>where α is a real-valued number parameterized by x and y; (4) gate: βx+(1−β)y where β is a real-valued vector parameterized by x and y. We conduct the analysis with Text-IMRAM (K = 3) in Flickr30K in <ref type="table" target="#tab_3">Table 5</ref>. We can observe that the aggregation function we use (i.e. Eq. 7) achieves substantially better performance than baseline functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Analysis</head><p>We intend to explore more insights for the effectiveness of our models here. For the convenience of the explanation, we mainly analyze semantic concepts from the view of language, instead of from the view of vision, i.  word in the text as one semantic concept. Therefore, we conduct the qualitative analysis on Text-IMRAM. We first visualize the attention map at each matching step in Text-IMRAM (K = 3) corresponding to different semantic concepts in <ref type="figure" target="#fig_2">Figure 3</ref>. We can see that the attention is refined and gradually focuses on the matched regions.</p><formula xml:id="formula_19">= 1 = 2 = 3 = 1 = 2 = 3</formula><p>To quantitatively analyze the alignment of semantic concepts, we first define a semantic concept in Text-IMRAM as a salient one at the matching step k as follows: 1) Given an image-text pair, at the matching step k, we derive the word-based matching score by Eq. 11 for each word with respect to the image, and derive the image-text matching score by averaging all the word-based scores (see Eq. 10).</p><p>2) A semantic concept is salient if its corresponding wordbased score is greater than the image-text score. For a set of image-text pairs randomly sampled from the testing set, we can compute the percentage of such salient semantic concepts for each model at different matching steps.</p><p>Then we analyze the change of the salient semantic concepts captured at different matching steps in Text-IMRAM (K = 3). Statistical results are shown in <ref type="table" target="#tab_4">Table 6</ref>. We can see that at the 1st matching step, nouns are easy to be recognized and dominant to help to match. While during the subsequent matching steps, contributions of verbs and adjectives increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Results on the Newly-Collected Ads Dataset</head><p>We evaluate our proposed IMRAM on our KWAI-AD dataset. We compare our models with the state-of-the-art SCAN models in <ref type="bibr" target="#b11">[12]</ref>. Comparison results are shown in <ref type="table">Table 7</ref>. We can see that the overall performance on this dataset is greatly lower than those on benchmark datasets, <ref type="table">Table 7</ref>. Results on the Ads dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Text Retrieval Image Retrieval R@1 R@10 R@1 R@10 i-t AVG <ref type="bibr" target="#b11">[12]</ref> 7.4 21. which indicates the challenges of cross-modal retrieval in real-world business advertisement scenarios. Results also show that our models can obtain substantial improvements over compared models, which demonstrates the effectiveness of the proposed method in this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose an Iterative Matching method with a Recurrent Attention Memory network (IMRAM) for cross-modal image-text retrieval to handle the complexity of semantics. Our IMRAM can explore the correspondence between images and texts in a progressive manner with two features: (1) an iterative matching scheme with a cross-modal attention unit to align fragments from different modalities; (2) a memory distillation unit to refine alignments knowledge from early steps to later ones. We validate our models on three benchmarks (i.e. Flickr8K, Flickr30K and MS COCO) as well as a new dataset (i.e. KWAI-AD) for practical business advertisement scenarios. Experiment results on all datasets show that our IMRAM outperforms compared methods consistently and achieves state-of-theart performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Framework of the proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Difference between our KWAI-AD dataset and standard datasets, e.g. MS COCO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of attention at each matching step in Text-IMRAM. Corresponding matched words are in blue, followed by the matching similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with the state-of-the-art models on Flickr8K. As results of SCAN<ref type="bibr" target="#b11">[12]</ref> are not reported on Flickr8K, here we show our experiment results by running codes provided by authors.</figDesc><table><row><cell>Method</cell><cell>R@1</cell><cell>Text Retrieval R@5</cell><cell>R@10</cell><cell>R@1</cell><cell>Image Retrieval R@5</cell><cell>R@10</cell><cell>R@sum</cell></row><row><cell>DeViSE [5]</cell><cell>4.8</cell><cell>16.5</cell><cell>27.3</cell><cell>5.9</cell><cell>20.1</cell><cell>29.6</cell><cell>104.2</cell></row><row><cell>DVSA [9]</cell><cell>16.5</cell><cell>40.6</cell><cell>54.2</cell><cell>11.8</cell><cell>32.1</cell><cell>44.7</cell><cell>199.9</cell></row><row><cell>m-CNN [15]</cell><cell>24.8</cell><cell>53.7</cell><cell>67.1</cell><cell>20.3</cell><cell>47.6</cell><cell>61.7</cell><cell>275.2</cell></row><row><cell>SCAN*</cell><cell>52.2</cell><cell>81.0</cell><cell>89.2</cell><cell>38.3</cell><cell>67.8</cell><cell>78.9</cell><cell>407.4</cell></row><row><cell>Image-IMRAM</cell><cell>48.5</cell><cell>78.1</cell><cell>85.3</cell><cell>32.0</cell><cell>61.4</cell><cell>73.9</cell><cell>379.2</cell></row><row><cell>Text-IMRAM</cell><cell>52.1</cell><cell>81.5</cell><cell>90.1</cell><cell>40.2</cell><cell>69.0</cell><cell>79.2</cell><cell>412.1</cell></row><row><cell>Full-IMRAM</cell><cell>54.7</cell><cell>84.2</cell><cell>91.0</cell><cell>41.0</cell><cell>69.2</cell><cell>79.9</cell><cell>420.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Comparison with state-of-the-art models on Flickr30K. Comparison with state-of-the-art models on MS COCO.</figDesc><table><row><cell>Method</cell><cell>R@1</cell><cell>Text Retrieval R@5</cell><cell>R@10</cell><cell>R@1</cell><cell>Image Retrieval R@5</cell><cell>R@10</cell><cell>R@sum</cell></row><row><cell>DPC [28]</cell><cell>55.6</cell><cell>81.9</cell><cell>89.5</cell><cell>39.1</cell><cell>69.2</cell><cell>80.9</cell><cell>416.2</cell></row><row><cell>SCO [7]</cell><cell>55.5</cell><cell>82.0</cell><cell>89.3</cell><cell>41.1</cell><cell>70.5</cell><cell>80.1</cell><cell>418.5</cell></row><row><cell>SCAN* [12]</cell><cell>67.4</cell><cell>90.3</cell><cell>95.8</cell><cell>48.6</cell><cell>77.7</cell><cell>85.2</cell><cell>465.0</cell></row><row><cell>VSRN* [14]</cell><cell>71.3</cell><cell>90.6</cell><cell>96.0</cell><cell>54.7</cell><cell>81.8</cell><cell>88.2</cell><cell>482.6</cell></row><row><cell>Image-IMRAM</cell><cell>67.0</cell><cell>90.5</cell><cell>95.6</cell><cell>51.2</cell><cell>78.2</cell><cell>85.5</cell><cell>468.0</cell></row><row><cell>Text-IMRAM</cell><cell>68.8</cell><cell>91.6</cell><cell>96.0</cell><cell>53.0</cell><cell>79.0</cell><cell>87.1</cell><cell>475.5</cell></row><row><cell>Full-IMRAM</cell><cell>74.1</cell><cell>93.0</cell><cell>96.6</cell><cell>53.9</cell><cell>79.4</cell><cell>87.2</cell><cell>484.2</cell></row><row><cell>Method</cell><cell>R@1</cell><cell>Text Retrieval R@5</cell><cell>R@10</cell><cell>R@1</cell><cell>Image Retrieval R@5</cell><cell>R@10</cell><cell>R@sum</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1K</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DPC [28]</cell><cell>65.6</cell><cell>89.8</cell><cell>95.5</cell><cell>47.1</cell><cell>79.9</cell><cell>90.0</cell><cell>467.9</cell></row><row><cell>SCO [7]</cell><cell>69.9</cell><cell>92.9</cell><cell>97.5</cell><cell>56.7</cell><cell>87.5</cell><cell>94.8</cell><cell>499.3</cell></row><row><cell>SCAN* [12]</cell><cell>72.7</cell><cell>94.8</cell><cell>98.4</cell><cell>58.8</cell><cell>88.4</cell><cell>94.8</cell><cell>507.9</cell></row><row><cell>PVSE [23]</cell><cell>69.2</cell><cell>91.6</cell><cell>96.6</cell><cell>55.2</cell><cell>86.5</cell><cell>93.7</cell><cell>492.8</cell></row><row><cell>VSRN* [14]</cell><cell>76.2</cell><cell>94.8</cell><cell>98.2</cell><cell>62.8</cell><cell>89.7</cell><cell>95.1</cell><cell>516.8</cell></row><row><cell>Image-IMRAM</cell><cell>76.1</cell><cell>95.3</cell><cell>98.2</cell><cell>61.0</cell><cell>88.6</cell><cell>94.5</cell><cell>513.7</cell></row><row><cell>Text-IMRAM</cell><cell>74.0</cell><cell>95.6</cell><cell>98.4</cell><cell>60.6</cell><cell>88.9</cell><cell>94.6</cell><cell>512.1</cell></row><row><cell>Full-IMRAM</cell><cell>76.7</cell><cell>95.6</cell><cell>98.5</cell><cell>61.7</cell><cell>89.1</cell><cell>95.0</cell><cell>516.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>5K</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DPC [28]</cell><cell>41.2</cell><cell>70.5</cell><cell>81.1</cell><cell>25.3</cell><cell>53.4</cell><cell>66.4</cell><cell>337.9</cell></row><row><cell>SCO [7]</cell><cell>42.8</cell><cell>72.3</cell><cell>83.0</cell><cell>33.1</cell><cell>62.9</cell><cell>75.5</cell><cell>369.6</cell></row><row><cell>SCAN* [12]</cell><cell>50.4</cell><cell>82.2</cell><cell>90.0</cell><cell>38.6</cell><cell>69.3</cell><cell>80.4</cell><cell>410.9</cell></row><row><cell>PVSE [23]</cell><cell>45.2</cell><cell>74.3</cell><cell>84.5</cell><cell>32.4</cell><cell>63.0</cell><cell>75.0</cell><cell>374.4</cell></row><row><cell>VSRN* [14]</cell><cell>53.0</cell><cell>81.1</cell><cell>89.4</cell><cell>40.5</cell><cell>70.6</cell><cell>81.1</cell><cell>415.7</cell></row><row><cell>Image-IMRAM</cell><cell>53.2</cell><cell>82.5</cell><cell>90.4</cell><cell>38.9</cell><cell>68.5</cell><cell>79.2</cell><cell>412.7</cell></row><row><cell>Text-IMRAM</cell><cell>52.0</cell><cell>81.8</cell><cell>90.1</cell><cell>38.6</cell><cell>68.1</cell><cell>79.1</cell><cell>409.7</cell></row><row><cell>Full-IMRAM</cell><cell>53.7</cell><cell>83.2</cell><cell>91.0</cell><cell>39.7</cell><cell>69.1</cell><cell>79.8</cell><cell>416.5</cell></row><row><cell></cell><cell cols="2">Affective: Do not</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">make us alone!</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>V.S.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Factual: A yellow</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">dog lies on the grass.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>The effect of the total steps of matching, K, on variants of IMRAM in MS COCO (5K).</figDesc><table><row><cell>Model</cell><cell>K</cell><cell cols="2">Text Retrieval Image Retrieval R@1 R@10 R@1 R@10</cell></row><row><cell></cell><cell cols="2">1 40.8 85.7 34.6</cell><cell>76.2</cell></row><row><cell>Image</cell><cell cols="2">2 51.5 89.5 37.7</cell><cell>78.3</cell></row><row><cell cols="3">-IMRAM 3 53.2 90.4 38.9</cell><cell>79.2</cell></row><row><cell></cell><cell cols="2">1 46.2 87.0 34.4</cell><cell>75.9</cell></row><row><cell>Text</cell><cell cols="2">2 50.4 89.2 37.4</cell><cell>78.3</cell></row><row><cell cols="3">-IMRAM 3 51.4 89.9 39.2</cell><cell>79.2</cell></row><row><cell></cell><cell cols="2">1 49.7 88.9 35.4</cell><cell>76.7</cell></row><row><cell>Full</cell><cell cols="2">2 53.1 90.2 39.1</cell><cell>79.5</cell></row><row><cell cols="3">-IMRAM 3 53.7 91.0 39.7</cell><cell>79.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>The effect of the aggregating function in the proposed memory distillation unit of Text-IMRAM (K = 3) in Flickr30K.</figDesc><table><row><cell>Memory</cell><cell cols="4">Text Retrieval Image Retrieval R@1 R@10 R@1 R@10</cell></row><row><cell>add</cell><cell>64.5</cell><cell>95.1</cell><cell>49.2</cell><cell>84.9</cell></row><row><cell>mlp</cell><cell>66.6</cell><cell>96.4</cell><cell>52.8</cell><cell>86.2</cell></row><row><cell>att</cell><cell>66.1</cell><cell>95.5</cell><cell>52.1</cell><cell>86.2</cell></row><row><cell>gate</cell><cell>66.2</cell><cell>96.4</cell><cell>52.5</cell><cell>86.1</cell></row><row><cell>ours</cell><cell>68.8</cell><cell>96.0</cell><cell>53.0</cell><cell>87.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Statistical results of salient semantics at each matching step, k, in Text-IMRAM (K = 3) in MS COCO.</figDesc><table><row><cell cols="4">k nouns (%) verbs (%) adjectives (%)</cell></row><row><cell>1</cell><cell>99.0</cell><cell>32.0</cell><cell>35.3</cell></row><row><cell>2</cell><cell>99.0</cell><cell>38.8</cell><cell>37.9</cell></row><row><cell>3</cell><cell>99.0</cell><cell>40.2</cell><cell>39.1</cell></row><row><cell cols="4">than K = 1. And K = 3 performs better or comparatively,</cell></row><row><cell cols="4">compared with K = 2. This observation well demon-</cell></row><row><cell cols="4">strates that the iterative matching scheme effectively im-</cell></row><row><cell cols="4">proves model performance. Besides, our Full-IMRAM con-</cell></row><row><cell cols="4">sistently outperforms Image-IMRAM and Text-IMRAM for</cell></row><row><cell cols="2">different values of K.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>e. we treat each A woman in an orange coat and jeans is squatting on a rock wall.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">Two people standing outside of a beautiful oriental building.</cell></row><row><cell>jeans(0.374)</cell><cell>jeans(0.546)</cell><cell>jeans(0.507)</cell><cell></cell><cell>v building(0.376)</cell><cell>v building(0.424)</cell><cell>v building(0.424)</cell></row><row><cell cols="3">A woman and girl dressed up in beautiful dresses.</cell><cell cols="4">A person in a green and white jacket and green pants is practicing on</cell></row><row><cell>beautiful(0.336)</cell><cell>beautiful(0.404)</cell><cell>beautiful(0.423)</cell><cell>his snowboard.</cell><cell>v green(0.536)</cell><cell>v green(0.671)</cell><cell>green(0.728)</cell></row><row><cell cols="2">An open book laid on top of a bed.</cell><cell></cell><cell cols="4">A child holding a flowered umbrella and petting a yak.</cell></row><row><cell>laid(0.241)</cell><cell>laid(0.412)</cell><cell>laid(0.421)</cell><cell></cell><cell>petting(0.223)</cell><cell>petting(0.360)</cell><cell>petting(0.351)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We omit models that require additional data augmentation<ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b7">8]</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Uniter: Learning universal image-text representations. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Jan K Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Vse++: Improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Devise: A deep visualsemantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Instance-aware image and sentence matching with selective multimodal lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2310" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning semantic concepts and order for image and sentence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6163" to="6171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Saliency-guided attention network for image-sentence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5754" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="201" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual semantic reasoning for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4654" to="4662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multimodal convolutional neural networks for matching image and sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2623" to="2631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Webly supervised joint embedding for cross-modal image-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Niluthpol Chowdhury Mithun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><forename type="middle">E</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><forename type="middle">K</forename><surname>Papalexakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Multimedia</title>
		<meeting>the 26th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1856" to="1864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="299" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-task learning of hierarchical vision-language representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Duy-Kien Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical multimodal lstm for dense visual-semantic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1899" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Language and cognition interaction neural mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Perlovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence and Neuroscience</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Knowledge aware semantic concept expansion for imagetext matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</meeting>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Polysemous visualsemantic embedding for cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Soleymani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved semantic representations from treestructured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5005" to="5013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xlnet</surname></persName>
		</author>
		<title level="m">Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Dong</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05535</idno>
		<title level="m">Dual-path convolutional imagetext embedding with instance loss</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
