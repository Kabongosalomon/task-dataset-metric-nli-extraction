<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AlphaNet: Improved Training of Supernet with Alpha-Divergence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
						</author>
						<title level="a" type="main">AlphaNet: Improved Training of Supernet with Alpha-Divergence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weight-sharing neural architecture search (NAS) is an effective technique for automating efficient neural architecture design. Weight-sharing NAS builds a supernet that assembles all the architectures as its sub-networks and jointly trains the supernet with the sub-networks. The success of weight-sharing NAS heavily relies on distilling the knowledge of the supernet to the subnetworks. However, we find that the widely used distillation divergence, i.e., KL divergence, may lead to student sub-networks that over-estimate or under-estimate the uncertainty of the teacher supernet, leading to inferior performance of the sub-networks. In this work, we propose to improve the supernet training with a more generalized α-divergence. By adaptively selecting the α-divergence, we simultaneously prevent the over-estimation or under-estimation of the uncertainty of the teacher model. We apply the proposed α-divergence based supernet training to both slimmable neural networks and weightsharing NAS, and demonstrate significant improvements. Specifically, our discovered model family, AlphaNet, outperforms prior-art models on a wide range of FLOPs regimes, including BigNAS, Once-for-All networks, FBNetV3, and AttentiveNAS. We achieve ImageNet top-1 accuracy of 80.0% with only 444 MFLOPs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Designing accurate and computationally efficient neural network architectures is an important but challenging task. Neural architecture search (NAS) automates the neural network design by exploring an enormous architecture space and achieves state-of-the-art (SOTA) performance on various applications including image classification <ref type="bibr" target="#b43">(Zoph &amp; Le, 2017;</ref>, object detection <ref type="bibr">(Ghiasi et al.,</ref> * Equal contribution 1 Facebook 2 Department of Computer Science, The University of Texas at Austin. Correspondence to: Dilin Wang &lt;wdilin@fb.com&gt;. Preprint 2019), semantic segmentation  and natural language processing <ref type="bibr" target="#b37">(Wang et al., 2020b)</ref>.</p><p>Conventional NAS approaches can be prohibitively expensive as hundreds of candidate architectures need to be trained from scratch and evaluated (e.g., . Supernet based approach has recently emerged to be a promising approach for efficient NAS. A supernet assembles all candidate architectures into a weight sharing network with each architecture corresponding to one sub-network. By training the sub-networks simultaneously with the supernet, different architectures can directly inherit the weights from the supernet for evaluation and deployment, which eliminates the huge cost of training or fine-tuning each architecture individually.</p><p>Though promising, supernet training is highly challenging in order to ensure that all the sub-networks are well trained (e.g., <ref type="bibr" target="#b4">Cai et al., 2019a;</ref><ref type="bibr" target="#b36">Wang et al., 2020a)</ref>. To stabilize the supernet training and improve the performance of sub-networks, one widely used approach is in-place knowledge distillation (KD) <ref type="bibr" target="#b39">(Yu &amp; Huang, 2019)</ref>. Inplace KD leverages the soft labels predicted by the largest architecture in the supernet to supervise all the other sub-networks. By distilling the knowledge of the teacher model, the performance of the sub-networks can be improved significantly <ref type="bibr" target="#b39">(Yu &amp; Huang, 2019;</ref>.</p><p>Standard knowledge distillation uses KL divergence to measure the discrepancy between the teacher and student networks. However, KL divergence penalizes the student model much more when it fails to cover one or more local modes of the teacher model <ref type="bibr" target="#b24">(Murphy, 2012)</ref>. Hence, the student model tends to over-estimate the uncertainty of the teacher model and suffers from inaccurate approximation of the most important mode, i.e., the correct prediction of the teacher model.</p><p>To further enhance the supernet training, we propose to replace the KL divergence with a more generalized αdivergence <ref type="bibr" target="#b1">(Amari, 1985;</ref><ref type="bibr" target="#b23">Minka et al., 2005)</ref>. Specifically, we show that by adaptively controlling α in the proposed divergence metric, we can penalize both the underestimation and over-estimation of the teacher model uncertainty to encourage a more accurate approximation for the student models. While directly optimizing the proposed arXiv:2102.07954v1 [cs.CV] 16 Feb 2021 adaptive α-divergence may suffer from a high variance of the gradients, we further propose a simple technique to clip the gradients of our adaptive α-divergence to stabilize the training process. We show the clipped gradients still define a valid divergence metric implicitly and hence, yielding a proper optimization objective for KD.</p><p>We empirically verify the proposed adaptive α-divergence in two notable applications of supernet -slimmable networks <ref type="bibr" target="#b39">(Yu &amp; Huang, 2019;</ref><ref type="bibr" target="#b40">Yu et al., 2018)</ref> and weightsharing NAS <ref type="bibr" target="#b36">Wang et al., 2020a)</ref> on ImageNet. For weight-sharing NAS, we train a supernet containing both small (200M FLOPs) and large (2G FLOPs) sub-networks following <ref type="bibr" target="#b36">Wang et al. (2020a)</ref>. With the proposed adaptive α-divergence, we are able to train high-quality sub-networks, called AlphaNet, that surpass all prior state-of-the-art models in the range of 200 to 800 MFLOPs, like EfficientNets , OFANets <ref type="bibr" target="#b4">(Cai et al., 2019a)</ref>, FBNetV3 <ref type="bibr" target="#b8">(Dai et al., 2020)</ref>, and BigNas . Specifically, AlphaNet-A4 achieves 80.0% accuracy with only 444 MFLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Training high-quality supernet is fundamental for weightsharing NAS but non-trivial <ref type="bibr" target="#b2">(Benyahia et al., 2019)</ref>. Recently, in-place KD is shown to be an effective mechanism that significantly improves the supernet performance <ref type="bibr" target="#b39">(Yu &amp; Huang, 2019;</ref>.</p><p>To formalize the supernet training and in-place KD, consider a supernet with trainable parameter θ. Let A denote the collection of all sub-networks contained in the supernet. The goal of training a supernet is to learn θ such that all the sub-networks in A can be optimized simultaneously to achieve good accuracy.</p><p>The supernet training process with the in-place KD is illustrated in <ref type="figure">Figure 1</ref>. At each training step, given a mini-batch of data, the supernet as well as several sub-networks are sampled. While the supernet is trained with the real labels, all the sampled sub-networks are supervised with the soft labels predicted by the supernet. Then, the gradients from all the sampled networks are aggregated before the supernet parameters are updated. More formally, at the training step t, the supernet parameters θ are updated by</p><formula xml:id="formula_0">θ t ← θ t−1 + g(θ t−1 ),</formula><p>where is the step size, and</p><formula xml:id="formula_1">g(θ t−1 ) = ∇ θ L D (θ) + γE s L KD ([θ, s]; θ t−1 ) θ=θt−1 .</formula><p>(1) Here, L D (θ) is the standard cross entropy loss of the supernet on a training dataset D, γ is the weight coefficient,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inactivated components</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft labels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training with true labels</head><p>Training with KD Sampled Sub-network (student)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supernet (teacher)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>True labels</head><p>Knowledge <ref type="figure">Figure 1</ref>. An illustration of training supernet with KD. Subnetworks are part of the supernet with weight-sharing.</p><p>and L KD ([θ, s]; θ t ) is the KD loss for distilling the supernet into a randomly sampled sub-network s, for which KL divergence has been widely used (e.g., .</p><p>Let p(x; θ) and q(x; θ, s) denote the output probability of the supernet and the sub-network s given input x, then, we have</p><formula xml:id="formula_2">L KD ([θ, s], θ t ) = E x∼D [KL(p(x; θ t ) || q(x; θ, s))],<label>(2)</label></formula><p>where KL(p || q) = E p [log(p/q)]. Note that the gradient on p(x; θ t ) in the KD loss is stopped as (2) indicated. For notation simplicity, we denote p as our teacher model and q (or q θ ) as student models in the following.</p><p>Additionally, note that the way KD is used in supernet training is different from the standard settings such as <ref type="bibr">Hinton et al. (e.g., 2015)</ref>, where the teacher network is pretrained and fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Supernet training with α α α-divergence</head><p>In this section, we analyze the limitations of using KL divergence in KD and propose to replace KL divergence with a more generalized α-divergence. We study the impact of different choices of α in the proposed divergence metric and further propose an adaptive algorithm to select α during the supernet training. Meanwhile, we also show that while directly optimizing α-divergence is challenging due to large gradient variances, a simple clipping strategy on α-divergence can be very effective to stabilize the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Classic KL based KD and its limitations</head><p>KL divergence has been widely used to measure the discrepancy in output probabilities between the teacher and student models in KD. One main drawback with KL divergence is that it cannot sufficiently penalize the student model when it over-estimates the uncertainty of the teach model. Let p and q denote the output probability of the teacher and student models, respectively. The KL divergence between the teacher and student models is calculated by KL(p||q) = E p [log p/q]. When p &gt; 0, to ensure KL(p||q) remains finite, we must have q &gt; 0. This is the zero avoiding property of KL. In contrast, when p = 0, q &gt; 0 does not get penalized. For example, as shown in <ref type="figure" target="#fig_0">Figure 2</ref> (b) and (c), even though the student model overestimates the uncertainty of the teacher model and predicts the wrong class ("class 4"), the KL divergence is still small.</p><p>The aforementioned over-estimation in Example 2 would be penalized at a larger magnitude when using other types of divergences, e.g., reverse KL divergence KL(q||p). For reverse KL divergence, KL(q || p) = E q [log q/p] is infinite if p = 0 and q &gt; 0. Hence if p = 0 we must ensure q = 0, this is known as the zero forcing property (Murphy, 2012). Therefore, minimizing reverse KL divergence encourages the student model q to avoid low probability modes of p while focusing on the modes with high probabilities, and thus, may under-estimate the uncertainty of the teacher model, as shown in Example 1 in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>Hence, a natural question is whether it is possible to generalize the KL divergence to simultaneously suppress both the under-estimation and over-estimation of the teacher model uncertainty during the supernet training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">KD with adaptive α α α-divergence</head><p>Our observations shown in <ref type="figure" target="#fig_0">Figure 2</ref> motivate us to design a new KD objective that simultaneously penalize both overestimation and under-estimation of the teacher model uncertainty. We first generalize the typical KL divergence with a more flexible α-divergence <ref type="bibr" target="#b23">(Minka et al., 2005)</ref>.</p><formula xml:id="formula_3">Consider α ∈ R \ {0, 1}, the α-divergence is defined as D α (p || q) = 1 α(α − 1) m i=1 q i p i q i α − 1 , (3) where q = [q i ] m i=1 and p = [p i ] m i=1</formula><p>are two discrete distributions on m categories. The α-divergence includes a large spectrum of classic divergence measures. In particular, the KL divergence KL(p || q) is the limit of D α (p || q) with α → 1 while the reverse KL divergence KL(q || p) is the limit of D α (p || q) with α → 0.</p><p>A key feature of α-divergence is that we can decide to focus on penalizing different types of discrepancies (underestimation or over-estimation) by choosing different α values. For example, as shown in <ref type="figure" target="#fig_0">Figure 2</ref> (c), when α is negative, D α (p || q) is large when q is more widely spread than p (when q over-estimates the uncertainty in p), and is small when q is more concentrated than p (when q underestimates the uncertainty in p). The trend is opposite when α is positive: under-estimation would be more heavily penalized than over-estimation.</p><p>To simultaneously alleviate the over-estimation and underestimation problem when training the supernet, we consider a positive α + and a negative α − , and propose to use the maximum of D α+ (p || q) and D α− (p || q) in the KD loss function:</p><formula xml:id="formula_4">D α+,α− (p q) = max D α− (p q) penalizing over-estimation , D α+ (p q) penalizing under-estimation .</formula><p>Our KL loss now changes from Eq <ref type="formula" target="#formula_2">(2)</ref> to</p><formula xml:id="formula_5">L KD ([θ, s], θ t ) = E x∼D [D α+,α− (p(x; θ t ) || q(x; θ, s))].</formula><p>(4) We denote this KD strategy that always chooses the maximum of D α− and D α+ to optimize as Adaptive-KD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Stabilizing α α α-divergence KD</head><p>One would prefer to set both |α + | and |α − | to be large to ensure the student model is sufficiently penalized when it either under-estimates or over-estimates the uncertainty the teacher model. However, directly optimizing the αdivergence with large |α| is often challenging in practice. Consider the gradient of α-divergence:</p><formula xml:id="formula_6">∇ θ D α (p || q θ ) = − 1 α E q θ p q θ α ∇ θ log q θ .</formula><p>If |α| is large, then the powered term (p/q θ ) α can be quite significant and cause the training process to be unstable. To enhance the training stability, we clamp the maximum value of (p/q θ ) α to be β, and obtaiñ</p><formula xml:id="formula_7">∇ θ D α (p || q θ ) def = − 1 α E q θ Clip β p q θ α ∇ θ log q θ ,<label>(5)</label></formula><p>where Clip β (t) = min(t, β).</p><p>Eqn. <ref type="formula" target="#formula_7">(5)</ref> is a simple yet effective heuristic approximation of</p><formula xml:id="formula_8">∇ θ D α (p || q θ ).</formula><p>It is important to note that Eqn. (5) equals the exact gradient of a special f divergence between p and q θ . Hence, our updates still amount to minimizing a valid divergence. Note that the clipping function Clip β (·) is only partially differentiable. So naively clipping on (p/q θ ) α in Eqn.</p><p>(3) may stop gradients back-propagating from the density ratio terms, hence yielding gradients that are not from a valid divergence.</p><p>To show that we are still optimizing a valid divergence with Eqn. (5), note that, for a convex function f : [0, +∞) → R, the f -divergence between p and q θ is defined as</p><formula xml:id="formula_9">D f (p || q θ ) = E q θ f p q θ − f (1) .<label>(6)</label></formula><p>Its gradient w.r.t. θ is <ref type="bibr" target="#b35">(Wang et al. (2018)</ref>). Note that α-divergence is a special case of f -divergence when f (t) = t α /(α(α − 1)).</p><formula xml:id="formula_10">∇ θ D f (p || q θ ) = −E q θ ρ f p q θ ∇ θ log q θ , where ρ f (t) = f (t)t − f (t)</formula><p>Proposition 3.1. There exists a convex function</p><formula xml:id="formula_11">f : (0, +∞) → R, such that∇ θ D α (p || q θ ) in (5) is the exact gradient of D f (p || q θ ), that is, ∇ θ D α (p || q θ ) = ∇ θ D f (p || q θ ). Proof. Let ρ * (t) = 1 α Clip β (t) α . We just need to find an f such that ρ f (t) = f (t)t − f (t) = ρ * (t).</formula><p>Taking derivation on both sides, we get f (t)t = ρ * (t). This gives f (t) = ρ * (t)/t and hence f (t) = ρ * (t)/tdt, where denotes second-order antiderivative (or indefinite integral).</p><p>Because ρ * (t) is nondecreasing, we have ρ * (t)/t ≥ 0 for t &gt; 0, and hence f is convex on (0, +∞).</p><p>Algorithm 1 Training supernet with α-divergence 1: Input: Adaptive α-divergence range given by α − and α + , a clipping factor β, a supernet with parameter θ, search space A. 2: while not converging do 3:</p><p>Sample a mini-batch of data B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Train the supernet with true labels from B</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Draw k subnetworks {s 1 , · · · , s k } from A; train sub-networks to mimic the supernet on the minibatch data B with the KD loss defined in Eqn. (4) using clipped gradients in Eqn. (5). 6: end while</p><p>In practice, we apply equation <ref type="formula" target="#formula_7">(5)</ref> to the α-divergence used in equation <ref type="formula">(4)</ref>. By clipping the value of importance weights, what we optimize is still a divergence metric but is more friendly to gradient-based optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We apply our Adaptive-KD to improve notable supernetbased applications, including slimmable neural networks <ref type="bibr" target="#b39">(Yu &amp; Huang, 2019)</ref> and weight-sharing NAS (e.g., <ref type="bibr" target="#b4">Cai et al., 2019a;</ref><ref type="bibr" target="#b36">Wang et al., 2020a)</ref>. We provide an overview of our algorithm for training supernet in Algorithm 1.</p><p>Adaptive-KD settings In our algorithm, α − and α + control the magnitude of penalizing on over-estimation and under-estimation, respectively. And, β controls the range of density ratios between the teacher model and the student model. We find our method performs robustly w.r.t. a wide of range of choices of α − , α + and β, yielding consistent improvements over the KL based KD baseline. Throughout the experimental section, we set α − = −1, α + = 1 and β = 5.0 as default for our method. We provide detailed ablation studies on these hyper-parameters in section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Slimmable Neural Networks</head><p>Slimmable neural networks <ref type="bibr" target="#b40">(Yu et al., 2018;</ref><ref type="bibr" target="#b39">Yu &amp; Huang, 2019)</ref> are examples of supernet that support a wide range of channel width configurations. The search space A of slimmable networks contains networks with different width and all the other architecture configurations (e.g. depth, convolution type, kernel size) are the same. This way, slimmable networks allow different devices or applications to adaptively adjust the model width on the fly according to on-device resource constraints to achieve the optimal accuracy vs. energy efficiency trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Settings</head><p>We closely follow the training recipe provided in <ref type="bibr" target="#b39">Yu &amp; Huang (2019)</ref> We adopt the sandwich rule sampling proposed in <ref type="bibr" target="#b39">Yu &amp; Huang (2019)</ref> for training. At each training iteration, we sample the supernet with the largest channel width, the smallest sub-network with the smallest channel width and two random sub-networks to accumulate the gradients. We train the supernet with ground truth labels and train all subsampled sub-networks with KD following (1). For our baseline KD strategy, we set the KD coefficient γ to be the number of sub-networks sampled, i.e., γ = 3, as default following <ref type="bibr" target="#b39">Yu &amp; Huang (2019)</ref>. To evaluate the effectiveness of our method, we simply replace the baseline KL-based KD loss used in <ref type="bibr" target="#b39">Yu &amp; Huang (2019)</ref> with our adaptive KD loss in (4).</p><p>Additionally, we train all models for 360 epochs using SGD optimizer with momentum as 0.9, weight decay as 10 −5 and dropout as 0.2. We use cosine learning rate decay, with an initial learning rate of 0.8, and batch size of 2048 on 16 GPUs. Following <ref type="bibr" target="#b39">Yu &amp; Huang (2019)</ref>, we evaluate on Ima-geNet <ref type="bibr" target="#b9">(Deng et al., 2009)</ref>. We note that the baseline models trained with our hyper-parameter settings outperform those reported in <ref type="bibr" target="#b39">Yu &amp; Huang (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We summarize our results in <ref type="table" target="#tab_1">Table 1</ref>. We report the top-1 accuracy on the ImageNet. Here, w/o KD denotes the training strategy that excludes the effect of KD. All such sub-networks are trained with ground truth labels via cross entropy.</p><p>As we can see from Comparison to KD with different temperature coefficients As discussed in <ref type="bibr" target="#b11">Hinton et al. (2015)</ref>, for standard KL based KD, one can soften (or sharpen) the probabilities of the teacher and the student model by applying a temperature in their softmax layers. The best distillation performance might be achieved with a different temperature other than the normally used temperature of 1.</p><p>To ensure a fair comparison, we further evaluate the baseline KL based KD under different temperature (T ) settings following the approach in <ref type="bibr" target="#b11">Hinton et al. (2015)</ref>. We refer the reader to Appendix C for detailed discussion on this topic.</p><p>In particular, we test a number of temperatures -0.5, 2, 4. We summarize our results in <ref type="table" target="#tab_0">Table 2</ref>. We find all these settings to systematically perform worse than the simple KD strategy without temperature scaling, i.e., T = 1. Additionally, the models trained via our method yield the best performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Weight-sharing NAS</head><p>We apply our Adaptive-KD to improve supernet for weightsharing NAS <ref type="bibr" target="#b4">(Cai et al., 2019a;</ref><ref type="bibr" target="#b36">Wang et al., 2020a)</ref>. Please see Appendix A for a brief introduction on weight-sharing NAS. Note that one main procedure of weight-sharing NAS is to simultaneously train all sub-networks specified in the search space to converge. Similar to training Slimmable neural networks, this is often achieved by enforcing all sub-networks to learn from the supernet with KL based KD, (e.g., .</p><p>Training. Our training recipe follows <ref type="bibr" target="#b36">Wang et al. (2020a)</ref> except we use uniform sampling for simplicity. We pursue minimum code modifications to ablate the effectiveness of our KD strategy. We evaluate on the ImageNet dataset <ref type="bibr" target="#b9">(Deng et al., 2009</ref>). All training details and the search space we used are discussed in Appendix B.</p><p>We use the update rule defined in (1) to train the supernet. Following <ref type="bibr" target="#b36">Wang et al. (2020a)</ref> and , at each iteration, we train the supernet with ground truth labels and simultaneously we train the smallest sub-network and two random sub-networks with KD. In this way, a total of 4 networks are trained at each iteration.</p><p>Evaluation We compare the accuracy vs. FLOPs Pareto formed by the supernet learned by different KD strategies. To estimate the performance Pareto, we proceed as follows: 1) we first randomly sample 512 sub-networks from the supernet and estimate their accuracy on the ImageNet validation set; 2) we apply crossover and random mutation on the best performing 128 sub-networks following <ref type="bibr" target="#b36">Wang et al. (2020a)</ref>. We fix both the crossover size and mutation size to be 128, yielding 256 new sub-networks. We then evaluate the performance of these sub-networks; 3) We repeat the second step 20 times. The total number of sub-networks thus evaluated is 5, 376.</p><p>Results As we can see from <ref type="figure">Figure 3</ref>(a), Adaptive-KD achieves a significantly better Pareto frontier compared to the KL-based KD baseline (denoted as w/ KL-KD) and the simple training strategy without KD (denoted as w/o KD). <ref type="figure">Figures 3(b)</ref> and (c) plot the convergence curve of the smallest sub-network and the supernet, respectively. Our method adaptively optimizes a more difficult KD loss between the supernet and the sub-networks, yielding slightly MFLOPs <ref type="figure">Figure 5</ref>. Comparison with prior art NAS approaches on Ima-geNet. #75ep denotes the models are further finetuned for 75 epochs with weights inherited from the corresponding supernet.</p><p>slower convergence in the early stage of the training but better performance towards the end of the training.</p><p>In <ref type="figure">Figure 4</ref>, we group sub-networks according to their FLOPs and visualize five statistics for each group of subnetworks, including the minimum, the first quantile, the median, the third quantile and the maximum accuracy. Our method learns significantly better sub-networks in a quantitative way.</p><p>Improvement on SOTA As we use the same search space as in <ref type="bibr" target="#b36">Wang et al. (2020a)</ref>, we further evaluate the discovered AttentiveNAS models (from A0 to A6) with the supernet weights learned by our adaptive KD. We call our models as AlphaNet.</p><p>As we can see from <ref type="table" target="#tab_2">Table 3</ref>, our Adaptive-KD significantly improves on classic KL based KD, yielding an average of 0.7% improvements in the top-1 accuracy from A0 to A6. Also, our AlphaNet outperform all corresponding AttentiveNAS models <ref type="bibr" target="#b36">(Wang et al., 2020a)</ref>, which requires building Pareto-aware sampling distributions with additional computational overhead.</p><p>We further compare our AlphaNet against prior art NAS baselines, including EfficientNet , FB- NetV3 <ref type="bibr" target="#b8">(Dai et al., 2020)</ref>, BigNAS , OFA <ref type="bibr" target="#b4">(Cai et al., 2019a)</ref>, MobileNetV3 , FairNAS  and MNasNet , in <ref type="figure">Figure 5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transfer learning</head><p>We take our AlphaNet-A0 and AlphaNet-A6 models that are pretrained on ImageNet and fine-tune them on a number of transfer learning benchmarks. We closely follow the training settings in EfficientNet  and GPipe . Specifically, we use SGD with momentum of 0.9, label smoothing of 0.1 and dropout of 0.5. All models are fine-tuned for 150 epochs with batch size of 64. Following , we search the best learning rate and weight decay on a hold-out subset (20%) of the training data.</p><p>Transfer learning results We evaluated on five transfer learning datasets, including Oxford Flowers <ref type="bibr" target="#b25">(Nilsback &amp; Zisserman, 2008)</ref> </p><formula xml:id="formula_12">α − =-2, α + =1 α − =-1, α + =1 α − =0, α + =1 α − =-1, α + =0.5 α − =-1, α + =2 α − =α + =-1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MFLOPs ±50</head><p>MFLOPs ±50 (a) Ablation study of β (b) Ablation study of α− and α+  101 <ref type="bibr" target="#b3">(Bossard et al., 2014)</ref>, Stanford Cars <ref type="bibr" target="#b19">(Krause et al., 2013)</ref> and Aircraft <ref type="bibr" target="#b22">(Maji et al., 2013)</ref>. As we can see from <ref type="table">Table 4</ref>, our AlphaNet-A0 and AlphaNet-A6 models lead to significant better transfer learning accuracy compared to those from EfficientNet-B0 and EfficientNet-B1 models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Additional results</head><p>Robustness w.r.t. clipping factor β β β We follow the training and evaluation settings in section 4.2 and study the effect of β. In <ref type="figure" target="#fig_4">Figure 6</ref> (a), we group sub-networks according to their FLOPs, and report the relative top-1 accuracy improvements of the maximum top-1 accuracy of each FLOPs group over the result from the KL based KD baseline. As shown in <ref type="figure" target="#fig_4">Figure 6</ref>(a), our algorithm is robust to the choice of β. Our algorithm works with a large range of β, from 1 to 10, yielding consistent improvements over the classic KL based KD baseline. And our default setting β = 5 achieves best performance on all FLOPs regimes evaluated.</p><p>Robustness w.r.t. α α α We ablate the impact of both α − and α + under the same settings as in section 4.2. In this case, we fix β = 5. We present our findings in <ref type="figure" target="#fig_4">Figure 6(b)</ref>. Firstly, we test with α − = −2, −1, 0, with α + fixed as 1. With a more negative α (e.g., α − = −2), this defines a more difficult objective that brings optimization challenges. With a large α − (e.g., α − = 0), the resulting KD loss is less discriminative regarding uncertainty overestimation. Overall, α − = −1 achieves a good balance between optimization difficulty and over-estimation penalization, yielding the best performance; Secondly, we vary α + from 0.5 to 2, with α − fixed as −1. Similarly, we find that large α + (e.g, α + = 1) yields the best performance; Lastly, we set both α − = α + = −1. In this case, we still achieve better performance compared to the results of our KL based KD baseline, indicating the importance of penalizing over-estimation in training sub-networks. Also, our adaptive KD that regularizes on both over-estimation and over-estimation achieves better performance in general.</p><p>Improvement on single network training Additionally, we apply our Adaptive-KD to train a single neural network with a pretrained teacher model, as in convectional KD setup (See Appendix C). Specifically, we use MobileNetV3 1.0× ) as our teacher model and train MobileNetV3 0.5× and 0.75× as our student models. We summarize the top-1 validation accuracy on ImageNet from the models trained with different KD strategies in <ref type="table">Table 5</ref>. The student models trained via our method yield the best accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we propose a method to improve the training of supernet with α-divergence based knowledge distillation. By adaptively selecting an α-divergence to optimize, our method simultaneously penalizes over-estimation and under-estimation in KD. Applying our method for neural architecture search, the searched AlphaNet models establish new state-of-the-art accuracy vs. FLOPs trade-offs on the ImageNet dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Weight-sharing NAS</head><p>Most RL-based NAS (e.g., <ref type="bibr" target="#b43">Zoph &amp; Le, 2017)</ref> and differentiable NAS <ref type="bibr" target="#b5">Cai et al., 2019b)</ref> consist of the following two stages as shown in <ref type="figure" target="#fig_6">Figure 7</ref>:</p><p>1) Stage 1 (architecture searching) -search potential architectures following a single resource constraint by using blackbox optimization techniques (e.g., <ref type="bibr" target="#b43">Zoph &amp; Le, 2017)</ref> or differentiable weight-sharing based approaches (e.g., <ref type="bibr" target="#b5">Cai et al., 2019b)</ref>;</p><p>2) Stage 2 (retraining) -retrain deep neural networks (DNNs) found in step 1) from scratch for best accuracy and final deployment.  Though promising results have been demonstrated, these NAS methods usually suffer from the following disadvantages: 1) need to re-do the NAS search for different hardware resource constraints; 2) require training the selected candidate from scratch to achieve desirable accuracy; 3) 1) especially for RL-based NAS that uses black-box optimization techniques, it requires training a large number of neural networks from scratch or on proxy tasks; These disadvantages significantly increase the computational cost of NAS and make the NAS search computationally expensive.</p><p>Supernet-based Weight-sharing NAS To alleviate the aforementioned issues, supernet-based weight-sharing NAS transforms the previous NAS training and search procedures as follows; see <ref type="figure" target="#fig_7">Figure 8</ref>.</p><p>1) Stage 1 (supernet pretraining): jointly optimize the supernet and all possible sub-networks specified in the search space, such that all searchable networks simultaneously achieve good performance at the end of the training phase.</p><p>2) Stage 2 (searching &amp; deployment): After stage 1 training, all the sub-networks are optimized simultaneously. One could then use typical searching algorithms, like evolutionary algorithms, to search the best model of interest. The model weights of each sub-network are directly inherited from the pre-trained supernet without any further re-training or fine-tuning.  Compared to RL-based NAS and differentiable NAS algorithms, the key advantages of the supernet-based weight-sharing NAS pipeline are: 1) one needs to only perform the computationally expensive supernet training for once. All sub-networks defined in the search space are ready to use after stage 1 is fully optimized. No retraining or fine-tuning is required; 2) all sub-networks of various model sizes are jointly optimized in stage 1, finding a set of Pareto optimal models that naturally supports various resource considerations.</p><p>Notable examples of supernet-based weights-sharing NAS include BigNAS , OFA <ref type="bibr" target="#b4">(Cai et al., 2019a)</ref>, AttentiveNAS <ref type="bibr" target="#b36">(Wang et al., 2020a)</ref> and HAT <ref type="bibr" target="#b37">(Wang et al., 2020b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Weights-sharing NAS training settings</head><p>We exactly follow the training settings in <ref type="bibr" target="#b36">Wang et al. (2020a)</ref> 1 . Specifically, we train our supernets for 360 epochs with cosine learning rate decay. We adopt SGD training on 64 GPUs. The mini-batch size is 32 per GPU. We use momeutm of 0.9, weight decay of 10 − 5, dropout of 0.2, stochastic layer dropout of 0.2. The base learning rate is set as 0.1 and is linearly scaled up for every 256 training samples. We use AutoAugment <ref type="bibr" target="#b7">(Cubuk et al., 2018)</ref> for data augmentation and set label smoothing coefficient to 0.1.</p><p>We use the same search space provided in <ref type="bibr" target="#b36">Wang et al. (2020a)</ref>, see <ref type="table" target="#tab_8">Table 6</ref>. Here Conv denotes regular convolutional layers and MBConv refers to inverted residual block proposed by <ref type="bibr" target="#b32">Sandler et al. (2018)</ref>. We use swish activation. Channel width represents the number of output channels of the block. MBPool denotes the efficient last stage in . SE represents the squeeze and excite layer <ref type="bibr" target="#b14">(Hu et al., 2018)</ref>. Input resolution denotes the candidate resolutions. To simplify the data loading procedure, we always pre-fetch training patches of a fixed size, e.g., 224x224 on ImageNet, and then rescale them to our target resolution with bicubic interpolation following   KD provides an effective way to train q by distilling knowledge from a teacher model in addition to the one-hot labels. The teacher network is often a relative larger network with better performance. Specifically, let p be the teacher network, KD enforces q to mimic the output of p by minimizing the closeness between q and p, which is often specified by the KL divergence D KL (p q), yielding the following loss function,</p><formula xml:id="formula_13">L(θ) = (1 − β)L ERM (θ) + βL KD (θ), with L ERM (θ) = E (x,y)∼D train L(y, q(x; θ)) , L KD (θ) = E x∼D train D KL (p(x) q(x; θ)) .<label>(7)</label></formula><p>Here L(·) represents the empirical loss, e.g., the typical cross entropy loss L(y, q(x; θ)) = m i=1 −y i log q i with q i be the i-class probability produced by q. And D KL (p q) = E p [log(p/q)]. Furthermore, β ∈ [0, 1] is the distilling weight that balances of the empirical loss and KD loss.</p><p>One could also apply a temperature T to soften (or sharpen) the outputs the teacher model and the student model in KD. More precisely, given an input x, we assume z p i (x) and z q i (x) the logit for the i-th class produced by p and q, respectively. Then the corresponding predictions of p and q after temperature scaling are as follows,</p><formula xml:id="formula_14">p i (x; T ) = softmax(z p i ; T ), q i (x; T ) = softmax(z q i ; T ),</formula><p>with softmax(z i ; T ) = exp(z i /T )/ i exp(z i /T ). In this way, the previous KD objective <ref type="formula" target="#formula_13">(7)</ref> is now adapted to the following,</p><formula xml:id="formula_15">L(θ; T ) = (1 − β)L ERM (θ) + βT 2 L KD (θ; T ), with L KD (θ; T ) = E x D KL (p(x; T ) q(x; T, θ)) .<label>(8)</label></formula><p>Here T 2 is introduced to ensure the gradients from the KD loss is at the same scale w.r.t the gradients from the empirical loss, see (e.g., <ref type="bibr" target="#b11">Hinton et al., 2015)</ref>. We set β = 0.9 as default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Related work</head><p>Neural architecture search (NAS) NAS offer a powerful tool to automate the design of neural architectures for challenging machine learning tasks. Early NAS solutions usually build upon black-box optimization, e.g. reinforcement learning (e.g., <ref type="bibr" target="#b43">Zoph &amp; Le, 2017)</ref>, Bayesian optimisation (e.g., <ref type="bibr" target="#b18">Kandasamy et al., 2018)</ref>, evolutionary algorithms (e.g., <ref type="bibr" target="#b30">Real et al., 2019)</ref>. These methods find good networks but are extremely computationally expensive in practice. More recent NAS approaches have adopted weight-sharing <ref type="bibr" target="#b29">(Pham et al., 2018)</ref> to improve search efficiency. Weight-sharing based approaches often frame NAS as a constrained optimization and solve with continuous relaxation (e.g., <ref type="bibr" target="#b5">Cai et al., 2019b)</ref>. However, these methods require to run NAS for each deployment consideration, e.g. a specific latency constraint for a particular mobile device, the total search cost grows linearly with the number of deployment considerations <ref type="bibr" target="#b4">(Cai et al., 2019a)</ref>. To further alleviate the aforementioned limitations, one-shot supernet-based NAS (e.g., <ref type="bibr" target="#b4">Cai et al., 2019a;</ref><ref type="bibr" target="#b36">Wang et al., 2020a)</ref> proposes to first jointly train all candidate sub-networks specified in the weight-sharing graph such that all sub-networks reach good performance at the end of training; then one can apply typical search algorithms, e.g., genetic search, to find a set of Pareto optimal networks for various deployment scenarios. Overall, one-shot supernet-based methods provide a highly flexible and efficient NAS framework, yielding state-of-the-art empirical NAS performance on various challenging applications (e.g., <ref type="bibr" target="#b4">Cai et al., 2019a;</ref><ref type="bibr" target="#b37">Wang et al., 2020b)</ref>.</p><p>Knowledge Distillation Our knowledge distillation works by forcing the student model to mimic the predictions of the teacher model. As shown in the literature, the feature activations of intermediate layers of the teacher model can also be used as knowledge to supervise the training of the student model, notable examples include <ref type="bibr" target="#b31">(Romero et al., 2014;</ref><ref type="bibr" target="#b16">Huang &amp; Wang, 2017;</ref><ref type="bibr" target="#b0">Ahn et al., 2019;</ref><ref type="bibr" target="#b17">Jang et al., 2019;</ref><ref type="bibr" target="#b28">Passalis &amp; Tefas, 2018;</ref><ref type="bibr">Li et al., 2019, e.g.,)</ref>. Furthermore, correlations between different training examples (e.g. similarity) learned by the teacher model also provide rich information, which could be distilled to the student model <ref type="bibr" target="#b26">(Park et al., 2019;</ref><ref type="bibr" target="#b38">Yim et al., 2017)</ref>. However, in our work, our KD involves training a large amount of sub-networks (student models) with different architecture configurations, e.g., different network depth, channel width, etc. It is less clear on how to define a good matching in the latent feature space between the teacher supernet and student sub-networks in a consistent way. While our method offers a simple distillation mechanism that is easy to use in practice and in the meantime, leads to significant empirical improvements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional results on ablation studies</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>(a) Example 1 -uncertainty under-estimation. The student network under-estimates the uncertainty of the teacher model and misses important local modes of the teacher model. (b) Example 2 -Uncertainty over-estimation. In this case, the student network over-estimates the uncertainty of the teacher model and mis-classify the most dominant mode of the teacher model. (c) plots the corresponding α-divergences between the student model and the teacher model for Examples 1 and 2. Note that α = 1 is a special case of KL divergence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Training curve of the smallest sub-network (c) Training curve of the supernet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>(a) Comparison of Pareto-set performance of the supernet trained via KL based KD and our adaptive KD, respectively. Each dot represents a sub-network evaluated during the evolutionary search step. (b-c) Training curves of the smallest sub-network and the largest sub-network (i.e., the supernet). Top-1 accuracy on ImageNet from weight-sharing NAS with KL-based KD and adaptive-KD. Each box plot shows the performance of sampled sub-networks within each FLOPs regime. From bottom to top, each horizontal bar represents the minimum accuracy, the first quartile, the median, the third quartile and the maximum accuracy, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, Oxford Pets (Parkhi et al., 2012), Food-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Relative accuracy compared to the results of KL based KD. Figure (a): we fix α− = −1, α+ = 1 and study the effect of our clipping factor β.Figure (b): we set β = 5 as default and study the impact of α− and α+.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>An overview of convectional NAS pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>An overview of supernet-based weight-sharing NAS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>FollowingMFLOPsFigure 9 .Figure 10 .</head><label>910</label><figDesc>the settings in section 4.2, we provide further analyses on the performance of sub-networks learned under different α and β settings. -KD (α−=0, α+=1, β=5) Additional results on ablation studies. Each box plot shows the performance of sampled sub-networks within each FLOPs regime. From bottom to top, each horizontal bar represents the minimum accuracy, the first quartile, the median, the third quartile and the maximum accuracy, respectively. Additional results on ablation studies. Each box plot shows the performance of sampled sub-networks within each FLOPs regime. From bottom to top, each horizontal bar represents the minimum accuracy, the first quartile, the median, the third quartile and the maximum accuracy, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>, and use slimmable MobileNetV1 Model Method 0.25× 0.3× 0.35× 0.4× 0.45× 0.5× 0.55× 0.6× 0.65× 0.7× 0.75× MbV1 w/o KD 53.9 55.3 57.1 59.1 61.1 62.9 64.0 65.8 66.9 67.9 68.8 w/ KL-KD 56.4 57.8 59.5 61.0 63.0 64.4 65.5 67.1 68.3 69.1 69.8 w/ Adaptive-KD (ours) 56.4 57.9 59.7 61.7 63.4 65.0 66.2 67.7 68.8 69.5 70.1 Table 1. Top-1 validation accuracy on ImageNet for Slimmable MobileNetV1 networks (denoted by MbV1) and Slimmable Mo-bileNetV2 networks (denoted by MbV2) trained with different KD strategies. Comparison to KL based KD with different temperature (T) settings. We report top-1 validation accuracy on ImageNet for slimmable MobileNetV1 and MobileNetV2 networks, denoted by MbV1 and MbV2, respectively.(Howard et al., 2017) and slimmable MobileNetV2 (Sandler et al., 2018) as our testbed. Specifically, we train slimmable MobileNetV1 to support arbitrary dynamic width in the range of [0.25, 1.0], and train slimmable Mo-bileNetV2 to support dynamic widths of [0.35, 1.0].</figDesc><table><row><cell></cell><cell>w/o KD</cell><cell>-</cell><cell>-</cell><cell>61.9 62.8 63.7 64.5 65.1 67.2 67.7 68.3 69.0</cell></row><row><cell>MbV2</cell><cell>w/ KL-KD</cell><cell>-</cell><cell>-</cell><cell>63.2 64.4 65.1 66.0 66.5 68.4 69.2 69.5 70.1</cell></row><row><cell></cell><cell>w/ Adaptive-KD (ours)</cell><cell>-</cell><cell>-</cell><cell>63.7 64.6 65.6 66.3 66.9 68.7 69.3 69.9 70.5</cell></row><row><cell cols="2">Model Method</cell><cell cols="3">0.25× 0.3× 0.35× 0.4× 0.45× 0.5× 0.55× 0.6× 0.65× 0.7× 0.75×</cell></row><row><cell></cell><cell>w/ KL-KD (T=0.5)</cell><cell cols="3">55.1 56.0 57.6 59.1 61.4 62.5 64.0 65.6 66.9 67.9 68.7</cell></row><row><cell>MbV1</cell><cell>T=2.0</cell><cell cols="3">55.4 57.0 58.8 60.7 62.6 64.1 65.3 66.6 67.9 68.7 69.5</cell></row><row><cell></cell><cell>T=4.0</cell><cell cols="3">48.7 50.7 53.1 55.9 58.8 60.9 62.7 64.6 66.0 67.4 68.3</cell></row><row><cell></cell><cell>w/ KL-KD (T=0.5)</cell><cell>-</cell><cell>-</cell><cell>61.7 62.9 63.8 64.6 65.0 67.4 68.4 68.8 69.8</cell></row><row><cell>MbV2</cell><cell>T=2.0</cell><cell>-</cell><cell>-</cell><cell>62.6 63.9 64.8 65.6 66.4 68.1 68.6 69.1 70.0</cell></row><row><cell></cell><cell>T=4.0</cell><cell>-</cell><cell>-</cell><cell>59.3 60.9 62.2 63.1 64.0 66.3 67.1 67.7 68.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table /><note>, both baseline KL based KD (denoted as w/ KL-KD) and our adaptive KD (denoted as w/ Adaptive-KD) yield significant performance improvements compared to w/o KD. Our results confirm the importance of KD for training Slimmable networks. Meanwhile, our Adaptive-KD further improves on KL based KD for all the channel width configurations evaluated for both Slimmable MobileNetV1 (denoted by MbV1) and Slimmable Mo- bileNetV2 (denoted by MbV2).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Performance on the discovered networks in<ref type="bibr" target="#b36">Wang et al. (2020a)</ref>. Each (#M) denotes the MFLOPs of the corresponding model. † uses additional attentive sampling<ref type="bibr" target="#b36">(Wang et al., 2020a)</ref> for training supernet. We denote our models as AlphaNet.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>A0 (203M) A1(279M) A2(317M) A3(357M) A4(444M) A5 (491M) A6 (709M)</cell></row><row><cell></cell><cell cols="3">w/o KD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>73.8</cell><cell>75.4</cell><cell>75.6</cell><cell>76.0</cell><cell>76.8</cell><cell>77.1</cell><cell>77.9</cell></row><row><cell></cell><cell cols="3">w/ KL-KD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>77.0</cell><cell>78.2</cell><cell>78.5</cell><cell>78.8</cell><cell>79.3</cell><cell>79.6</cell><cell>80.1</cell></row><row><cell></cell><cell cols="7">w/ KL-KD + Attentive Sampling  †</cell><cell>77.3</cell><cell>78.4</cell><cell>78.8</cell><cell>79.1</cell><cell>79.8</cell><cell>80.1</cell><cell>80.7</cell></row><row><cell></cell><cell cols="7">w/ Adaptive-KD (ours -AlphaNet)</cell><cell>77.8</cell><cell>78.9</cell><cell>79.1</cell><cell>79.4</cell><cell>80.0</cell><cell>80.3</cell><cell>80.8</cell></row><row><cell></cell><cell>81</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">AlphaNet-A6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>A5</cell><cell></cell></row><row><cell>Top-1 validation accuracy</cell><cell>76 77 78 79 80</cell><cell>A0</cell><cell>A1</cell><cell>A2</cell><cell>A3</cell><cell>A4</cell><cell cols="2">AlphaNet (ours) Efficientnet Mobilenetv3 MNasNet FairNAS BigNas FBNetv3 OFA (#75ep)</cell></row><row><cell></cell><cell>75</cell><cell>200</cell><cell cols="2">300</cell><cell>400</cell><cell>500</cell><cell>600</cell><cell>700</cell><cell>800</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>An illustration of our search space. Every row denotes a block group.C. Knowledge distillationConsider the image classification task over a set of classes [m] := {1, · · · , m}, where we have a collection of training images and one-hot labels D train = {(x, y)} with (x, y) ∈ X × Y and y ∈ {0, 1} m . We are interested in designing a deep neural network q(x; θ) : X → Y that captures the relationship between x and y. Here θ is the network parameters of interest.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/facebookresearch/AttentiveNAS</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Variational information distillation for knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungsoo</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shell</forename><forename type="middle">Xu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwen</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9163" to="9171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Differential-geometrical methods in statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shun-Ichi Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes on Statistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Overcoming multi-model forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Benyahia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaicheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamil</forename><surname>Bennani Smires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudiu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Musat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="594" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Once-for-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixiang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01845</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Dandelion Mane, Vijay Vasudevan, and</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02049</idno>
		<title level="m">Joint architecture-recipe search using neural acquisition function</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7036" to="7045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Like what you like: Knowledge distill via neuron selectivity transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01219</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning what and where to transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhun</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hankook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3030" to="3039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural architecture search with bayesian optimisation and optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Collecting a large-scale dataset of fine-grained cars. Second Workshop on Fine-Grained Visual Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hint-based training for nonautoregressive machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Darts: Differentiable architecture search. International Conference on Learning Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Divergence measures and message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Minka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Machine learning: a probabilistic perspective, chapter 21</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonpyo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3967" to="3976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning deep representations with probabilistic knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Passalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Tefas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="268" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4095" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the aaai conference on artificial intelligence</title>
		<meeting>the aaai conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11943</idno>
		<title level="m">Variational inference with tail-adaptive f-divergence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Attentivenas: Improving neural architecture search via attentive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09011</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Hat: Hardware-aware transformers for efficient natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14187</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggyu</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Universally slimmable networks and improved training techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1803" to="1811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">Slimmable neural networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">Ruoming Pang, and Quoc Le. Bignas: Scaling up neural architecture search with big singlestage models. European conference on computer vision</title>
		<imprint>
			<publisher>Xiaodan Song</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Customizable architecture search for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11641" to="11650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<title level="m">Neural architecture search with reinforcement learning. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
